{"DOI":{"0":"10.1016\/j.cageo.2022.105218","1":"10.1016\/j.cageo.2022.105214","2":"10.1016\/j.cageo.2022.105217","3":"10.1016\/j.cageo.2022.105187","4":"10.1016\/j.cageo.2022.105200","5":"10.1016\/j.cageo.2022.105212","6":"10.1016\/j.cageo.2022.105208","7":"10.1016\/j.cageo.2022.105213","8":"10.1016\/j.cageo.2022.105210","9":"10.1016\/j.cageo.2022.105139","10":"10.1016\/j.cageo.2022.105198","11":"10.1016\/j.cageo.2022.105192","12":"10.1016\/j.cageo.2022.105199","13":"10.1016\/j.cageo.2022.105196","14":"10.1016\/j.cageo.2022.105185","15":"10.1016\/j.cageo.2022.105188","16":"10.1016\/j.cageo.2022.105193","17":"10.1016\/j.cageo.2022.105190","18":"10.1016\/j.cageo.2022.105181","19":"10.1016\/j.cageo.2022.105186","20":"10.1016\/j.cageo.2022.105182","21":"10.1016\/j.cageo.2022.105176","22":"10.1016\/j.cageo.2022.105180","23":"10.1016\/j.cageo.2022.105189","24":"10.1016\/j.cageo.2022.105191","25":"10.1016\/j.cageo.2022.105183","26":"10.1016\/j.cageo.2022.105184","27":"10.1016\/j.cageo.2022.105179","28":"10.1016\/j.cageo.2022.105161","29":"10.1016\/j.cageo.2022.105177","30":"10.1016\/j.cageo.2022.105166","31":"10.1016\/j.cageo.2022.105162","32":"10.1016\/j.cageo.2022.105174","33":"10.1016\/j.cageo.2022.105164","34":"10.1016\/j.cageo.2022.105178","35":"10.1016\/j.cageo.2022.105118","36":"10.1016\/j.cageo.2022.105143","37":"10.1016\/j.cageo.2022.105163","38":"10.1016\/j.cageo.2022.105150","39":"10.1016\/j.cageo.2022.105152","40":"10.1016\/j.cageo.2022.105142","41":"10.1016\/j.cageo.2022.105148","42":"10.1016\/j.cageo.2022.105153","43":"10.1016\/j.cageo.2022.105146","44":"10.1016\/j.cageo.2022.105109","45":"10.1016\/j.cageo.2022.105144","46":"10.1016\/j.cageo.2022.105147","47":"10.1016\/j.cageo.2022.105140","48":"10.1016\/j.cageo.2022.105151","49":"10.1016\/j.cageo.2022.105149","50":"10.1016\/j.cageo.2022.105125","51":"10.1016\/j.cageo.2022.105136","52":"10.1016\/j.cageo.2022.105126","53":"10.1016\/j.cageo.2022.105124","54":"10.1016\/j.cageo.2022.105123","55":"10.1016\/j.cageo.2022.105127","56":"10.1016\/j.cageo.2022.105120","57":"10.1016\/j.cageo.2022.105122","58":"10.1016\/j.cageo.2022.105121","59":"10.1016\/j.cageo.2022.105108","60":"10.1016\/j.cageo.2022.105117","61":"10.1016\/j.cageo.2022.105119","62":"10.1016\/j.cageo.2022.105104","63":"10.1016\/j.cageo.2022.105097","64":"10.1016\/j.cageo.2022.105106","65":"10.1016\/j.cageo.2022.105102","66":"10.1016\/j.cageo.2022.105083","67":"10.1016\/j.cageo.2022.105078","68":"10.1016\/j.cageo.2022.105076","69":"10.1016\/j.cageo.2022.105085","70":"10.1016\/j.cageo.2022.105099","71":"10.1016\/j.cageo.2022.105100","72":"10.1016\/j.cageo.2022.105087","73":"10.1016\/j.cageo.2022.105098","74":"10.1016\/j.cageo.2022.105081","75":"10.1016\/j.cageo.2022.105086","76":"10.1016\/j.cageo.2022.105057","77":"10.1016\/j.cageo.2022.105080","78":"10.1016\/j.cageo.2022.105082","79":"10.1016\/j.cageo.2022.105079","80":"10.1016\/j.cageo.2022.105074","81":"10.1016\/j.cageo.2022.105072","82":"10.1016\/j.cageo.2022.105075","83":"10.1016\/j.cageo.2022.105061","84":"10.1016\/j.cageo.2022.105073","85":"10.1016\/j.cageo.2022.105059","86":"10.1016\/j.cageo.2022.105044","87":"10.1016\/j.cageo.2022.105056","88":"10.1016\/j.cageo.2022.105041","89":"10.1016\/j.cageo.2022.105040","90":"10.1016\/j.cageo.2022.105058","91":"10.1016\/j.cageo.2022.105032","92":"10.1016\/j.cageo.2022.105042","93":"10.1016\/j.cageo.2022.105037","94":"10.1016\/j.cageo.2022.105045","95":"10.1016\/j.cageo.2022.105043","96":"10.1016\/j.cageo.2021.105026","97":"10.1016\/j.cageo.2022.105038","98":"10.1016\/j.cageo.2021.105027","99":"10.1016\/j.cageo.2021.105030","100":"10.1016\/j.cageo.2021.105028","101":"10.1016\/j.cageo.2021.104993","102":"10.1016\/j.cageo.2021.104986","103":"10.1016\/j.cageo.2022.105033","104":"10.1016\/j.cageo.2022.105035","105":"10.1016\/j.cageo.2022.105034","106":"10.1016\/j.cageo.2021.105025","107":"10.1016\/j.cageo.2021.105023","108":"10.1016\/j.cageo.2021.104976","109":"10.1016\/j.cageo.2021.105007","110":"10.1016\/j.cageo.2021.105024","111":"10.1016\/j.cageo.2021.105013","112":"10.1016\/j.cageo.2021.105021","113":"10.1016\/j.cageo.2021.105012","114":"10.1016\/j.cageo.2021.105014","115":"10.1016\/j.cageo.2021.105015","116":"10.1016\/j.cageo.2021.105020","117":"10.1016\/j.cageo.2021.105008","118":"10.1016\/j.cageo.2021.105005","119":"10.1016\/j.cageo.2021.105019"},"Title":{"0":"Paw-Net: Stacking ensemble deep learning for segmenting scanning electron microscopy images of fine-grained shale samples ","1":"Constrained non-linear AVO inversion based on the adjoint-state optimization ","2":"Multi-mineral segmentation of micro-tomographic images using a convolutional neural network ","3":"Preliminary geological mapping with convolution neural network using statistical data augmentation on a 3D model ","4":"Using a neural network \u2013 Physics-based hybrid model to predict soil reaction fronts ","5":"Continuous conditional generative adversarial networks for data-driven solutions of poroelasticity with heterogeneous material properties ","6":"A nearest neighbor multiple-point statistics method for fast geological modeling ","7":"Fast approximate viewshed analysis based on the regular-grid digital elevation model: X-type partition proximity-direction-elevation spatial reference line algorithm ","8":"genES-MDA: A generic open-source software package to solve inverse problems via the Ensemble Smoother with Multiple Data Assimilation ","9":"Computing dip-angle gathers using Poynting vector for elastic reverse time migration in 2D transversely isotropic media ","10":"QDM\n                  \n                     l\n                     a\n                     b\n                  \n               : A MATLAB toolbox for analyzing quantum diamond microscope (QDM) magnetic field maps ","11":"Pyeo: A Python package for near-real-time forest cover change detection from Earth observation using machine learning ","12":"3D coupled FEM-IBIEM for non-linear seismic response of a sedimentary basin using equivalent-linearized soil model ","13":"Densely multiscale framework for segmentation of high resolution remote sensing imagery ","14":"A robust, fast, and accurate algorithm for point in spherical polygon classification with applications in geoscience and remote sensing ","15":"A variability aware GAN for improving spatial representativeness of discrete geobodies ","16":"Adaptive conditional bias-penalized Kalman filter with minimization of degrees of freedom for noise for superior state estimation and prediction of extremes ","17":"Efficient extraction of seismic reflection with Deep Learning ","18":"Real-time and post-hoc compression for data from Distributed Acoustic Sensing ","19":"A Fast 3-D finite element modeling algorithm for land transient electromagnetic method with OneAPI acceleration ","20":"A Reinforcement Learning approach to the location of the non-circular critical slip surface of slopes ","21":"Polar night jet characterization through artificial intelligence ","22":"An automatic graph-based method for characterizing multichannel networks ","23":"Accelerating the Lagrangian simulation of water ages on distributed, multi-GPU platforms: The importance of dynamic load balancing ","24":"Discontinuity interpretation and identification of potential rockfalls for high-steep slopes based on UAV nap-of-the-object photogrammetry ","25":"SDCnet: An Unet with residual blocks for extracting dispersion curves from seismic data ","26":"GeoGravGOCE: A standalone MATLAB GUI for processing GOCE satellite gradient data ","27":"An improved algorithm for extracting crossovers of satellite ground tracks ","28":"A scalable method for the estimation of spatial disaggregation models ","29":"SWE_of_Bathymetry.m: A geomorphometric tool to automate discrimination between detachment and magmatic seafloor at slow-spreading ridges from shipboard multibeam bathymetry ","30":"Simulating electrochemical migration and anion exclusion in porous and fractured media using PFLOTRAN\n                  \n                     \n                     \n                        NP\n                     \n                  \n               \n             ","31":"Coupled THMC modeling of dissociation induced deformation of gas hydrate bearing media ","32":"NMR transverse relaxation of the clay-rich shale in inhomogeneous magnetic field: A numerical study ","33":"Identifying microseismic events using a dual-channel CNN with wavelet packets decomposition coefficients ","34":"A new geochemical reactive transport model for sandstone acidizing ","35":"\n               \n                  \u03bc\n               CT scans permeability computation with an unfitted boundary method to improve coarsening accuracy ","36":"An ocean current-oriented graph-based model for representing Argo trajectories ","37":"CALM: A software tool for rapid analysis and modeling of converted shear waves in wide-angle seismic data ","38":"An optimization method for paleomagnetic Euler pole analysis ","39":"Location-Refining neural network: A new deep learning-based framework for Heavy Rainfall Forecast ","40":"A computer vision algorithm for interpreting lacustrine carbonate textures at Searles Valley, USA ","41":"Schumann resonance data processing programs and four-year measurements from Sierra Nevada ELF station ","42":"Identifying multivariate geochemical anomalies via tensor dictionary learning over spatial-elemental dimensionalities ","43":"Spatiotemporal flow features in gravity currents using computer vision methods ","44":"Graph-based deep learning segmentation of EDS spectral images for automated mineral phase analysis ","45":"Local scale optimization of geomorphometric land surface parameters using scale-standardized Gaussian scale-space ","46":"Research on CO2 transfer in basalt and sandstone using 3D pore space model generation and mathematical morphology ","47":"Optimization of support vector machine parameters in modeling of Iju deposit mineralization and alteration zones using particle swarm optimization algorithm and grid search method ","48":"A 3D reconstruction method of porous media based on improved WGAN-GP ","49":"A committee machine with intelligent experts (CMIE) for estimation of fast and slow shear wave velocities utilizing petrophysical logs ","50":"Modified block shape characterization method for classification of fractured rock: A python-based GUI tool ","51":"Deformation induced by distributions of single forces in a layered half-space: \n                  \n                     E\n                     F\n                     G\n                     R\n                     N\n                     \/\n                     E\n                     F\n                     C\n                     M\n                     P\n                  \n               \n             ","52":"A comparative machine learning study for time series oil production forecasting: ARIMA, LSTM, and Prophet ","53":"Adaptive ground-roll attenuation using local nonlinear filtering ","54":"Improving remote sensing classification: A deep-learning-assisted model ","55":"A hierarchical stochastic modeling approach for representing point bar geometries and petrophysical property variations ","56":"Partial automation of the seismic to well tie with deep learning and Bayesian optimization ","57":"CUDA-based parallelization of time-weighted dynamic time warping algorithm for time series analysis of remote sensing data ","58":"3D model generated from UAV photogrammetry and semi-automated rock mass characterization ","59":"Diffuser: A user-friendly program for diffusion chronometry with robust uncertainty estimation ","60":"Relative importance of parameters controlling scour at bridge piers using the new toolbox ScourAPP ","61":"A two-step algorithm for acoustic emission event discrimination based on recurrent neural networks ","62":"Calibration, inversion and sensitivity analysis for hydro-morphodynamic models through the application of adjoint methods ","63":"Hydrologic similarity based on width function and hypsometry: An unsupervised learning approach ","64":"A new operator-splitting finite element scheme for reactive transport modeling in saturated porous media ","65":"MTH5: An archive and exchangeable data format for magnetotelluric time series data ","66":"Scalability and composability of flow accumulation algorithms based on asynchronous many-tasks ","67":"A new approach considering temporal correlations for GPS campaign time series ","68":"Increasing the interoperability of snow\/ice hyperspectral observations ","69":"Generating unrepresented proportions of geological facies using Generative Adversarial Networks ","70":"Core box image recognition and its improvement with a new augmentation technique ","71":"A geologically-constrained deep learning algorithm for recognizing geochemical anomalies ","72":"Laudatio for Prof. Philippe Renard, recipient of the IAMG John Cedric Griffiths Teaching Award ","73":"APPMAR 1.0: A Python application for downloading and analyzing of WAVEWATCH III\u00ae wave and wind data ","74":"Automatic identification of semi-tracks on apatite and mica using a deep learning method ","75":"Prediction of diffusional conductance in extracted pore network models using convolutional neural networks ","76":"AnalyZr: A Python application for zircon grain image segmentation and shape analysis ","77":"A Matlab script for the morphometric analysis of subaerial, subaquatic and extra-terrestrial rivers, channels and canyons ","78":"Knowledge graph construction and application in geosciences: A review ","79":"PSO-WELLSVM: An integrated method and its application in urban waterlogging susceptibility assessment in the central Wuhan, China ","80":"Learning 3D mineral prospectivity from 3D geological models using convolutional neural networks: Application to a structure-controlled hydrothermal gold deposit ","81":"Effective training strategies for deep-learning-based precipitation nowcasting and estimation ","82":"Applications of data augmentation in mineral prospectivity prediction based on convolutional neural networks ","83":"Hybrid and automated machine learning approaches for oil fields development: The case study of Volve field, North Sea ","84":"Volunteered geographic information mobile application for participatory landslide inventory mapping ","85":"MeePaSoL: A MATLAB-based GUI software tool for shoreline management ","86":"Using a ray tracing program to calculate sunrise times over a digital terrain model based visible horizon using a simplified atmospheric model, part II ","87":"Learning spatial patterns with variational Gaussian processes: Regression ","88":"Full waveform inversion using Random Mixing ","89":"Unsupervised detection of Saturn magnetic field boundary crossings from plasma spectrometer data ","90":"Corrigendum to \u201cParallel assignment of flow directions over flat surfaces in massive digital elevation models\u201d [Comput. Geosci. 159 (2021) 105015] ","91":"Statistical comparison of variogram-based inversion methods for conditioning to indirect data ","92":"MLFC-net: A multi-level feature combination attention model for remote sensing scene classification ","93":"WlCount: Geological lamination detection and counting using an image analysis approach ","94":"Flood monitoring by integration of Remote Sensing technique and Multi-Criteria Decision Making method ","95":"Automated segmentation of textured dust storms on mars remote sensing images using an encoder-decoder type convolutional neural network ","96":"Direct Sequential Simulation for spherical linear inverse problems ","97":"Automatic extraction of outcrop cavity based on a multiscale regional convolution neural network ","98":"Acceleration strategies for large-scale sequential simulations using parallel neighbour search: Non-LVA and LVA scenarios ","99":"3D magnetotelluric modeling using high-order tetrahedral N\u00e9d\u00e9lec elements on massively parallel computing platforms ","100":"gTOOLS, an open-source MATLAB program for processing high precision, relative gravity data for time-lapse gravity monitoring ","101":"Front and skeleton features based methods for tracking salinity propagation in the ocean ","102":"Hybrid parametric\/smooth inversion of electrical resistivity tomography data ","103":"Efficient estimation of aquifer intrinsic permeability anisotropy using perturbation theory ","104":"A hybrid grid-based finite-element approach for three-dimensional magnetotelluric forward modeling in general anisotropic media ","105":"A review of Earth Artificial Intelligence ","106":"Comparison of machine learning algorithms for emulation of a gridded hydrological model given spatially explicit inputs ","107":"GeoSPARQL query support for scientific raster array data ","108":"Integrating tsunami simulations in web applications using BROWNI, an open source client-side GPU-powered tsunami simulation library ","109":"SphGLLTools: A toolbox for visualization of large seismic model files based on 3D spectral-element meshes ","110":"BASIN-3D: A brokering framework to integrate diverse environmental data ","111":"Testing scenarios on geological models: Local interface insertion in a 2D mesh and its impact on seismic wave simulation ","112":"SDZM: Software for determining shear damage zones of rock joints ","113":"A Python library for exploratory data analysis on twitter data based on tokens and aggregated origin\u2013destination information ","114":"Corrigendum to \u201cAn algorithmic framework for investigating the temporal relationship of magnetic field pulses and earthquakes applied to California\u201d [Comput. Geosci. 133 (2019) 104317] ","115":"Parallel assignment of flow directions over flat surfaces in massive digital elevation models ","116":"Refrapy: A Python program for seismic refraction data analysis ","117":"Predictive digital rock physics without segmentation ","118":"GeoReservoir: An ontology for deep-marine depositional system geometry description ","119":"Random forests-based error-correction of streamflow from a large-scale hydrological model: Using model state variables to estimate error terms "},"Abstract":{"0":"\n                  Segmentation of scanning electron microscopy (SEM) images is critical yet time-consuming for geological analyses, as it needs to differentiate the boundaries for different mineral objects to facilitate subsequent analyses, such as porosity calculation. Recently, various machine learning methods, especially convolutional neural networks (CNNs), have been explored to segment SEM images of fine-grained shale samples. However, we found that general CNNs do not yield optimal performance due to insufficient training data and imbalanced objects in SEM images. This work has revised the U-Net architecture, a popular approach for biomedical image analyses, by incorporating a loss function that addresses the imbalance issue. Furthermore, we used the ensemble learning method to train multiple models and combined the results to improve the overall performance of segmentation. We prepared 2162 sub-images from raw SEM images in our experiments and divided them into training, validation, and testing datasets. The overall results show that our method improves the average Intersection over Union (IOU) of mineral objects from 0.49 to 0.58, compared to the original U-Net model. Our method can clearly distinguish each object from others with boundaries, even in highly imbalanced images. Training our models takes less than 3\u00a0mins using a single GPU, while manual labeling can take up to 3\u00a0hrs for each image. Therefore, the method helps geoscientists gain insights quickly and effectively by building neural network models from a small dataset of SEM images.\n               ","1":"\n                  Pre-stack AVO inversion of seismic data is a modeling tool for estimating subsurface elastic properties. Our focus is on the model-based inversion method where then unknown variables are estimated by minimizing the misfit to the observed data. Standard approaches for non-linear AVO inversion are based on the gradient descent optimization algorithms that require the calculation of the gradient equations of the objective function. To improve the accuracy and efficiency of these methods, we developed a technique that uses an implementation of the adjoint-state-based gradient computation. The inversion algorithm relies on three basic modeling components consisting of a convolution-based forward model using a linearized approximation of the Zoeppritz equation, the definition of the objective function, and the adjoint-computed gradient. To achieve an accurate solution, we choose a second-order optimization algorithm known as the Limited memory-BFGS (L-BFGS) that implicitly approximates the inverse Hessian matrix. This approach is more efficient than traditional optimization methods. The main novelty of the proposed approach is the derivation of the adjoint-state equations for the gradient of the objective function. The application of the proposed method is demonstrated using 1D and 2D synthetic datasets based on data from the Edvard Grieg oil field. The seismic data for these applications is generated by using both convolutional modeling and finite difference methods. The results of the proposed method are accurate and the computational approach is efficient. The results show that the algorithm reliably retrieves the elastic variables, P- and S-wave velocities and density for both convolutional and finite difference models.\n               ","2":"\n                  Micro X-ray-Computed Tomographic (micro-CT) images are often used for calibration of interpretation models that relate physical rock properties to the rock microstructure. Some computed properties, for example, permeability, may be relatively insensitive to the mineral composition of the rock matrix, while electrical conductivity and stiffness show strong dependence on the volume fractions and spatial distribution of minerals. This information could potentially be extracted from X-ray density of these images through a procedure commonly called segmentation. However, the range of X-ray density overlaps for different minerals, hence an effective segmentation workflow need to consider both the density values and shapes of the minerals. Such image processing workflows consist of many stages, and thus become time and computationally expensive and involve a lot of manual labor. This paper proposes an automated workflow for the multi-mineral segmentation of micro X-ray-Computed Tomographic (micro-CT) images using a convolutional neural network (CNN). The CNN model is trained using labels of two sets of images of a Bentheimer sandstone that are segmented into pore, quartz, clay and feldspar using a sophisticated interactive workflow. The trained model is then used to segment a new set of images of the Bentheimer sandstone. The segmented multi-mineral labels can achieve an accuracy of \u223c97% and the process takes only \u223c10\u00a0min as compared with interactive workflow which takes \u223c3\u00a0h. Although, CNN-based segmentation algorithms were published in the literature before, the proposed model is capable of more sophisticated segmentation and achieves superior accuracy on a completely blind test set. Potentially, our approach might generalize well to other lithology for the micro-CT image analysis.\n               ","3":"\n                  Airborne magnetic data are commonly processed and interpreted by geologists to produce preliminary geological maps of unexplored areas. Machine learning can partly fulfill this task rapidly and objectively as convolutional neural networks applied to image segmentation showed promising results in comparable applications. As these algorithms require a large and high-quality dataset to be trained, we developed an innovative geostatistical data augmentation workflow that uses a 3D conceptual lithological and magnetic susceptibility model as input. The workflow uses soft-constrained Multi-Point Statistics to create equiprobable synthetic 3D geological models and Sequential Gaussian Simulation to populate these models with a geologically meaningful magnetic susceptibility spatial distribution. Then, geophysical forward modeling is computed to get the airborne magnetic responses of the synthetic models, which are associated with their counterpart synthetic geological maps. We applied this workflow on a 3D model of the Malartic Mine area to obtain a large synthetic airborne magnetic and counterpart geological map dataset. Then, a Gated Shape Convolutional Network is trained on this dataset to perform a preliminary geological mapping. A semi-supervised approach using clustering on the feature maps from the trained network is also implemented. Testing the trained network on synthetic data and remote areas of similar geological context shows that the methodology is suitable for producing preliminary geological maps using airborne magnetic data. Notably, the clustering module shows a precise and adaptive segmentation of the magnetic anomalies into pertinent preliminary geological maps. The quality of the results empirically validates our data augmentation method. Our method allows producing geological maps by training a convolutional neural network using an area where a detailed geological and petrophysical 3D model exists and then permits applying the pre-trained model in new areas of the same geological context, where only airborne magnetic data is available.\n               ","4":"\n                  Analytical and numerical solutions have been proposed to model reaction fronts to study soil formation. With growing access to large geo-datasets and powerful computational capacity, data-driven models are becoming increasingly useful. We therefore explored the use of a neural network (NN) guided by a physics-based model (PBM) to simulate the depth profile of feldspar dissolution in soils. Specifically, we explored this hybrid neural network (HNN) to see if it could predict reaction fronts as a function of important variables known from domain knowledge: site climate characteristics (temperature T; precipitation P), geomorphic parameters (soil residence time t; erosion rate E), and parent material mineralogy (quartz content Q; albitic feldspar content of the feldspar A). We evaluated the mean square error (MSE) for 63 HNNs, each using a different combination of training data (i.e., soil profiles) and environmental variables. The HNNs trained to four or five soil profiles that used a subset of t, T, Q, E, and A as predictor variables yielded lower MSEs than the PBM, and showed global convergence. At least two variables are needed to achieve an MSE within 1% of the corresponding PBM. The HNNs generally predicted the slope better than the depth of the front because the PBM was not used to predict depth. HNN results identify t and P as the most and least useful variable in predicting the reaction front, respectively. This is the first time a NN was hybridized to a PBM to simulate reactions in soils. As part of this effort, we developed a tool to identify cases which have converged to a global solution, and cases which present local solutions. The approach shows promise for future efforts but should be applied to larger sets of soil profile data and PBMs that predict both the depth and slope of reaction fronts.\n               ","5":"\n                  Machine learning-based data-driven modeling can allow computationally efficient time-dependent solutions of PDEs, such as those that describe subsurface multiphysical problems. In this work, our previous approach (Kadeethum et\u00a0al., 2021d) of conditional generative adversarial networks (cGAN) developed for the solution of steady-state problems involving highly heterogeneous material properties is extended to time-dependent problems by adopting the concept of continuous cGAN (CcGAN). The CcGAN that can condition continuous variables is developed to incorporate the time domain through either element-wise addition or conditional batch normalization. Moreover, this framework can handle training data that contain different timestamps and then predict timestamps that do not exist in the training data. As a numerical example, the transient response of the coupled poroelastic process is studied in two different permeability fields: Zinn & Harvey transformation and a bimodal transformation. The proposed CcGAN uses heterogeneous permeability fields as input parameters while pressure and displacement fields over time are model output. Our results show that the model provides sufficient accuracy with computational speed-up. This robust framework will enable us to perform real-time reservoir management and robust uncertainty quantification in poroelastic problems.\n               ","6":"\n                  Multiple-point statistics (MPS) is a powerful method to generate realistic geological models. Given a training image as a prior model, the program iteratively reproduces spatial patterns in the simulation grid. However, running speed becomes a limitation to practical applications. MPS has to handle complicated and high-dimensional structures at the cost of simulation time. With the objective to accelerate geostatistical modeling with categorical variables, we propose a nearest neighbor simulation (NNSIM) method. Several k-nearest neighbor (kNN) classifiers are incorporated into MPS framework. First, we identify representative patterns with a prototype selection method. Different from existing MPS programs, our method selects training patterns according to their influences on simulation quality. A pattern subset of small size has a positive effect on searching time. Second, a teacher-student architecture is suggested to improve the pattern subset. In order to address missing data, our program augments the subset with key patterns during simulation. A cosine distance metric is applied to compare the original dataset and pattern subset. Third, our program organizes patterns with a ball tree. Pattern groups with low similarity are dynamically removed to fulfill fast search. We examine the proposed NNSIM by a benchmark channel simulation, a 2D flume model, and a 3D sandstone modeling. Many quantitative approaches are employed to evaluate geometrical and physical properties. The experimental results indicate that our NNSIM significantly improves the computational efficiency while exhibits comparable simulation quality to traditional MPS programs.\n               ","7":"\n                  Viewshed analysis using regular-grid digital elevation models (DEM) is the basis of many analysis applications in geographic information systems. However, XDraw and reference plane, which have, until recently, acted a foundation of many viewshed analysis methods, have problems with accuracy and error-point aggregation. The proximity-direction-elevation spatial reference line (PDERL) algorithm, which is twice as slow as XDraw, has no accuracy problem, but not all applications can sacrifice speed for absolute accuracy. This study developed an \u201cX-type partition PDERL\u201d (XPDERL) algorithm based on PDERL by adjusting the partition mode of the PDERL and the combination mode of its partition results to maintain or even exceed the computational speed of traditional approximate fast algorithms while improving accuracy. The computational speed of XPDERL is stable at elevated heights from ground, slightly faster than XDraw and slightly slower than the reference plane algorithm; however, at lower elevations, it is significantly faster than both, especially in mountainous areas near the ground. In addition, the algorithm does not produce false-negative errors (identifying visible points as non-visible points) and can significantly reduce the error rate and degree of error-point aggregation. XPDERL can effectively alleviate the longstanding contradiction between speed and accuracy in viewshed analysis algorithms while providing a possible means for accurate and reliable large-scale viewshed analysis.\n               ","8":"\n                  Ensemble Kalman filter methods have been successfully applied for data assimilation and parameter estimation through inverse modeling in various scientific fields. We have developed a new generic software package for the solution of inverse problems implementing the Ensemble Smoother with Multiple Data Assimilation (genES-MDA). It is an open-source, platform-independent Python-based program. Its aim is to facilitate the management and configuration of the ES-MDA through several programming tools that help in the preparation of the different steps of ES-MDA. genES-MDA has a flexible workflow that can be easily adapted for the implementation of different variants of the ensemble Kalman filter and for the solution of generic inverse problems. This paper presents a description of the package and some application examples. genES-MDA has been tested in three synthetic case studies: the solution of the reverse flow routing for the estimation of the inflow hydrograph to a river reach using observed water levels and a calibrated forward model of the river system, the identification of a hydraulic conductivity field using piezometric observations and a known forward flow model, and the estimation of the release history of a contaminant spill in an aquifer from measured concentration data and a known flow and transport model. The results of all these tests have demonstrated the flexibility of genES-MDA and its capabilities to efficiently solve different types of inverse problems.\n               ","9":"\n                  Angle-domain common-image gathers (ADCIGs) have wide applications in seismic imaging. For isotropic elastic reverse time migration (ERTM), we can produce dip-angle ADCIGs by using conventional Poynting vectors. Seismic anisotropy widely occurrs in Earth materials, such as in the shales and fractured reservoirs. ERTM based on the isotropic elastic wave equation is unable to accurately image anisotropic structures, leading to noise, discontinuous reflection events and mispositioning in migration results. Hence, conducting ERTM and generating dip-angle ADCIGs for anisotropic media require solving the anisotropic elastic wave equation. However, it is challenging to compute the gathers using the conventional Poynting vector for anisotropic elastic media. In this study, we derive the group and phase Poynting vectors, in which we use the optical flow method to stabilize the phase-related Poynting vectors. This approach admits efficient calculation of dip-angle ADCIG in anisotropic media. It also suppresses noise in anisotropic ERTM. The results of numerical experiments demonstrate the effectiveness of the proposed method.\n               ","10":"\n                  Paleomagnetic measurements of rock magnetizations are typically performed using classical net moment rock magnetometers on bulk, millimeter- to centimeter-sized samples. In this case, the limited spatial resolution effectively averages across the signal of multiple populations of magnetic grains, each of which may have a distinct geological history. Magnetic field imaging with the quantum diamond microscope (QDM) allows for the measurement of weakly magnetic (10\u221216 Am2) samples at micrometer spatial resolution, potentially isolating the signal of magnetic grain populations and resolving ambiguities from bulk sample analyses. To achieve such high resolution, the QDM retrieves the energy spectrum of nitrogen\u2013vacancy (NV) color centers within micrometer-scale pixels across a millimeter-scale field of view. Therefore, large amounts of data need to be processed to generate a magnetic field map, which itself often requires further specialized analysis. Until now, no freely-available, comprehensive, open-source software package existed that was able to process this type of data. Here we give an overview of the most important features of QDM\n                        \n                           l\n                           a\n                           b\n                        \n                     , our open-source MATLAB toolbox for generating and analyzing QDM magnetic field maps of geologic samples. QDM\n                        \n                           l\n                           a\n                           b\n                        \n                      utilizes modern computational techniques like graphics processing unit (GPU) and spectral fitting routines as well as automated image alignment algorithms. QDM\n                        \n                           l\n                           a\n                           b\n                        \n                      contains easy-to-use functions for (1) generating magnetic field maps from raw QDM data, (2) map editing, and (3) quantifying the net magnetic moment and rock magnetic properties of rock and mineral samples.\n               ","11":"\n                  Monitoring forest cover change from Earth observation data streams in near-real-time presents a challenge for automated change detection by way of a continuously updated big dataset. Even though deforestation is a significant global problem, forest cover changes in pairs of subsequent images happen relatively infrequently. Detecting a change can require the download and processing of tens, hundreds or even thousands of images. In geoscientific applications of Earth observation, machine learning algorithms are increasingly used. Once trained, a machine learning model can be applied to new images automatically.\n                  This paper introduces the open-access Python 3 package Pyeo - \u201cPython for Earth Observation\u201d. Pyeo provides a set of portable, extensible and modular Python functions for the automation of machine learning applications from Earth observation data streams, including automated search and download functionality, pre-processing and atmospheric correction, re-projection, creation of thematic base layers and machine learning classification or regression. Pyeo enables users to train their own machine learning models and then apply the models to newly downloaded imagery over their area of interest. This paper describes in detail how Pyeo works, its requirements, benefits, and a description of the libraries used. An application to the automated forest cover change detection in a region in Kenya is given. Pyeo can be used on cloud computing architectures such as Amazon Web Services, Microsoft Azure and Google Colab to provide scalable applications and processing solutions for the geosciences.\n               ","12":"\n                  Based on an equivalent-linearized dynamic constitutive model of the soil, the coupled finite element method - indirect boundary integral equation method (FEM-IBIEM) is developed to carry out the non-linear seismic response of a three-dimensional (3-D) sedimentary basin. The indirect boundary integral equation method (IBIEM), which can significantly reduce the solution dimension, is employed to solve the exterior response of the sedimentary basin. Due to the inhomogeneity and nonlinearity of the soft soil, the finite element method (FEM) is used to analyze the response of the sedimentary basin. Based on the verification of numerical accuracy, the scattering issue in a sedimentary basin with linearly varying shear wave velocity under incident P and SV waves is investigated. Numerical results show that: the coherence effect increases with the increase of the incident frequency, and the amplification of surface displacement and acceleration is distinct. Due to the soil-nonlinearity, the peak ground acceleration (PGA) at a specific location on the basin surface under P waves decreases by approximately 51%, more significant than the decrease under SV waves. The effect of the soil-nonlinearity on the acceleration response spectrums is a decrease of the acceleration amplitude and an increase in the predominant period.\n               ","13":"\n                  Semantic segmentation has gained research attention in recent times, especially within the remote sensing community. The deep neural network has proven to be the most effective approach for segmentation applications due to its automatic feature extraction capability. Research results indicate that the multiscale segmentation frameworks are more suitable for high-level feature extraction, especially from complex remote sensing images. However, most existing multiscale frameworks are either complex or highly parameterized, making them inefficient for real-time remote sensing applications. In this work, we propose an accurate and highly efficient densely multiscale segmentation network specifically for real-time segmentation of remotely sensed imagery. We significantly improve the representation capability of the network by embedding its structure with the dense connection, which allows gradient to flow with ease through the network. The proposed network with few trainable parameters performed significantly on two publicly available challenging datasets, making it suitable for deployment on resource-constrained devices for real-time remote sensing applications.\n               ","14":"\n                  The point in spherical polygon problem addresses the task of classifying an arbitrary query point as inside, outside, or on the boundary of a polygon drawn using great circle segments on the spherical surface. Point in spherical polygon has numerous applications in the geosciences and remote-sensing, where the problem of sorting point position data onto geographic regions on the surface of the earth or sensor field-of-view often arises. By resorting to a direct solution on the spherical surface, distortions arising from projection can be avoided. We examine the problem, with and without preprocessing, where preprocessing is a step applied when a large number of classifications are evaluated against the same spherical polygon. Several improvements are introduced to the existing point in spherical polygon algorithm (which does not include preprocessing), resulting in significantly faster inclusion queries and better handling of edge cases. Furthermore, we introduce a preprocessing algorithm by means of recursive, nonuniform spatial subdivision of longitudinal regions (\u201clunes\u201d) on the sphere. With an \n                        \n                           O\n                           \n                              (\n                              n\n                              log\n                              n\n                              )\n                           \n                        \n                      preprocessing step and \n                        \n                           O\n                           \n                              (\n                              n\n                              )\n                           \n                        \n                      space requirement, the algorithm decreases query time from \n                        \n                           O\n                           \n                              (\n                              n\n                              )\n                           \n                        \n                      to \n                        \n                           O\n                           \n                              (\n                              log\n                              n\n                              )\n                           \n                        \n                      for most \u201crealistic\u201d polygons found in geoscience and remote sensing applications. Several empirical tests demonstrate that the algorithm performs to theoretical expectations.\n               ","15":"\n                  Generative Adversarial Networks (GAN) have shown great potential in not only producing acceptable realizations of geologically complex models but also successfully reparametrizing them. Training GANs is quite challenging. One such challenge is mode collapse. When generating realizations of spatial property, mode collapse causes reduction in variability, compared to the input training dataset, and thus, the realizations become spatially biased at specific locations. To address this issue, we developed a new GAN architecture where a regularization term is introduced to maintain the variability and reduce mode collapse. This is achieved by using a probability map to evaluate variability and spatial bias of generated realizations and modifying the GAN loss function to minimize this bias. We applied the new architecture to a binary channelized permeability distribution and compared the results with those generated by Deep Convolutional GAN (DCGAN) and Wasserstein GAN with gradient penalty (WGAN-GP). Our results show that the proposed architecture significantly enhances variability and reduces the spatial bias induced by mode collapse, outperforming both DCGAN and WGAN-GP in the application of generating subsurface property distributions.\n               ","16":"\n                  We describe adaptive conditional bias (CB)-penalized ensemble Kalman filter (AEnKF) to improve estimation and prediction of extremes. Environmental variables are generally observed with significant uncertainties. Geoscientific data assimilation (DA) is hence often subject to CB which adversely impacts estimation of extremes. A generalization of EnKF, AEnKF accounts for CB by dynamically reflecting the flow-dependent information content in the model prediction relative to that in the observation via a scaler weight. The implicit dependence of the weight on the posterior state renders the AEnKF solution nonlinear for superior performance over the tails of the predictand as well as in the (unconditional) mean sense. AEnKF prescribes the weight in real time by minimizing the degrees of freedom for noise. Real-time optimization of the weight also means that AEnKF obviates or reduces the need for calibration of uncertainty parameters which is often subjective and expensive. Comparative evaluation for flood prediction shows that AEnKF outperforms EnKF by a very significant margin but is about two to three times more expensive computationally. Because AEnKF uses the EnKF solution, it can be easily implemented in any EnKF code with the addition of the weight optimization module. Given superior performance and relative ease of implementation, AEnKF should be favored over KF in a wide range of geoscientific DA, particularly when performance over tails is important.\n               ","17":"\n                  We propose a procedure for the interpretation of horizons in seismic reflection data based on a Neural Network (NN) approach, which can be at the same time fast, accurate and able to reduce the intrinsic subjectivity of manual or control-points based methods. The training is based on a Long Short Term Memory architecture and is performed on synthetic data obtained from a convolutional model-based scheme, while the extraction step can be applied to any type of field seismic dataset. Synthetic data are contaminated with different types of noise to improve the performance of the NN in a large variety of field conditions. We tested the proposed procedure on 2-D and 3-D synthetic and field seismic datasets. We have successfully applied the procedure also to Ground Penetrating Radar data, verifying its versatility and potential. The proposed algorithm is based on a fully 1-D approach and does not require the input of any interpreter, because the necessary thresholds are automatically estimated. An added benefit is that the prediction has an associated probability, which automatically quantifies the reliability of the results.\n               ","18":"\n                  Distributed Acoustic Sensing (DAS) is an emerging sensing technology that records the strain-rate along fiber optic cables at high spatial and temporal resolution. This technique is becoming a popular tool in seismology, hydrology, and other subsurface monitoring applications. However, due to the large coverage (10\u2019s of km) and high density of measurements (1m spacing at 100\u2019s of Hz), a DAS installation could produce terabytes of data records per day. Because many DAS instruments are deployed in remote locations, this large data size poses significant challenges to its transfer and storage. In this paper, we explore lossless compression methods to reduce the storage requirement in both real-time and post-hoc scenarios. We propose a two-stage compression method to improve the compression ratio and compression speed. This two-stage compression method could reduce the storage requirement by 40%, which is 20% more than other lossless methods, such as ZSTD. We demonstrate that the compression method could complete its operation well before the DAS instrument needs to output the next file, making it suitable for real-time DAS acquisition. We also implement a parallel compression method for a post-hoc scenario and demonstrate that our method could effectively utilize a parallel computer. With 256 CPU cores, our parallel compression method achieves the speed of 26GB\/second.\n               ","19":"\n                  The land transient electromagnetic (TEM) method is a geophysical prospecting method with a wide range of applications. In this paper, we develop an algorithm that carries out 3-D TEM modeling for loop-source devices. The algorithm is based on the time-domain finite element method with unstructured edge-based meshes, and an unconditional stable adaptive time-stepping method is developed to acquire stable solutions. We verify the algorithm with a 1-D analytical solution. And as case studies, we discuss the TEM responses of both large-loop and small-loop devices in the presence of topography. It is found that the modeling accuracy is more sensitive to space meshing than time discretization. The proposed algorithm is packed as software, TEMF3DT, which is open-source and publicly available together with the above-mentioned sample models. TEMF3DT is designed for PC users, but is also constructed through a higher-level MPI parallelization and a lower-level OpenMP parallelization for potential workstation or cluster uses. Also, TEMF3DT is accelerated by the modern Fortran and Intel OneAPI library to deliver state-of-the-art computational efficiency.\n               ","20":"\n                  We propose a numerical procedure to locate the critical slip surface of slopes with the method of slices. We employ the Deep-Q Learning algorithm with experience replay and target memory to determine a non-circular slip surface. The overall stability analysis is performed with Janbu\u2019s simplified method. Our approach, however, is flexible and can accommodate other Limit Equilibrium Methods. The accuracy of the method is demonstrated by using typical verification examples and comparing the results to the search methods implemented in SLOPE\/W and Slide2. It is demonstrated that the Deep-Q learning algorithm can be efficiently applied to layered slopes.\n               ","21":"\n                  The stratospheric polar vortex is a cyclonic circulation that forms over the winter pole, whose edge is characterized by a strong westerly jet (also called polar night jet, PNJ). The PNJ plays a key role in processes such as the distribution of atmospheric constituents in the polar stratosphere or the wave propagation. Further, variations in PNJ can also affect the troposphere, being behind the occurrence of extreme events near the Earth\u2019s surface. Thus, it is important to correctly characterize the mean state of the PNJ and its variability. Already existing algorithms, although working, may present several issues. The simplest ones, those based on zonal mean wind, can miss important information. In contrast, the 2-dimensional ones usually involve multiple calculations with several fields, some of them not always included in typical datasets.\n                  In this study, we describe a new artificial intelligence technique to characterize the PNJ. The algorithm only requires data of zonal wind that is classified each time step with a decision trees algorithm with 95.5% accuracy, trained with images processed by a climate science researcher. The classifier is applied to JRA-55 reanalysis data and the output of simulations of three climate models and is found to perform reasonably well when validated against traditional zonal-mean methods. Indeed, it provides more information about the PNJ, as it offers in one step the PNJ region, averaged magnitudes and even identify if the PNJ is under perturbed conditions. We have explored two examples of potential applications of the classifier such as the study of the influence of climate change on the PNJ and the variability of the PNJ on monthly and daily scales. In both cases, our algorithm has produced coherent results with those produced with previous studies, but with more detail obtained at a single step.\n               ","22":"\n                  Assessment and quantitative description of river morphology using widely recognized river planview measures (e.g., length, width and sinuosity of channels, bifurcation angles and island shape) for multichannel rivers are regarded as fundamental parts of the toolkit of geomorphologists and river engineers. However, conventional assessment methods including field surveys or exiting algorithms for the extraction of multichannel planviews might be suboptimal. More recently, the potential for the application of complex network analysis to the study of river morphology has led to emphasis on the accurate characterization and definition of multichannel network topology. Therefore, we developed a novel algorithm called RivMACNet (River Morphological Analysis based on Complex Networks) that enables the extraction of multichannel network topology using satellite sensor images as the input. We applied RivMACNet to a meandering reach of the Yangtze River and a strongly anastomosing reach of the Indus River to construct their network topologies, and then calculated a series of common topological measures including weighted degree (WD), clustering coefficient (CC) and weighted characteristic path length (WCPL). The network analysis indicated that both networks exhibit poor transitivity with small clustering coefficients. The topological properties of the Indus at the reach scale are independent of flow conditions, while they vary across space at the subnetwork scale. In addition, comparison between RivMACNet and an alternative common river network analysis engine (RivaMap) demonstrated that RivMACNet is superior in terms of representation accuracy and network connectivity and, thus, is more suitable for multichannel fluvial systems with complex planviews. RivMACNet is, thus, a useful tool to support further investigation of multichannel river networks using graph theory.\n               ","23":"\n                  Water age is a fundamental descriptor of the source, storage, and mixing of water in watersheds. The Lagrangian, particle tracking, approach is a powerful tool for physically-based modeling of water age distributions, but its application has been hampered because it is computationally demanding. Here, we present a parallel approach for particle tracking simulations. This approach uses a multi-GPU with MPI parallelism based on domain decomposition. An inherent challenge of the distributed parallelization of Lagrangian approaches is the disparity in computational work or load imbalance (LIB) among different processing elements (PEs). In this study, load balancing (LB) schemes were proposed to dynamically balance the distribution of particles across PEs during runtime. In the followed hillslope simulations, LIB was observed in all LB-disabled runs, e.g., with a load ratio of 4.3 by using 2-GPU in the test case. LB schemes then accurately balanced the load distribution and improved the parallel scaling. Additionally, the parallel approach showed an excellent overall speedup: a 25-fold improvement using 4-GPU relative to 128 OpenMP threads. A regional-scale application further demonstrated the LB performance. The wall-clock time used by 8-GPU without LB was reduced by 31.33% after the LB was activated. Increasing 8-GPU with LB to 16-GPU with LB showed parallel scalability by reducing the wall-clock time by \u223c50%. This work shows how massively parallel computing can be applied to particle tracking in water age simulations. It also demonstrates the practical importance of load balancing in this context, which enables large-scale simulations with the increased complexity of flow paths.\n               ","24":"\n                  Discontinuity extraction and interpretation of fractured masses is of high importance when analyzing rock slope stability. Regarding high-steep slopes, which are areas that are difficult to reach, traditional methods to obtain discontinuities, such as the sample window method (SWM), are unlikely to be implemented, resulting in challenges for the identification of potential rockfalls. With the development of the unmanned ariel vehicle (UAV) technology, discontinuity extraction can overcome by noncontact photogrammetry. However, there is still a lack of comprehensive and practical solutions to fulfill rockfall identification from field investigation to in-door analysis. For this purpose, a practical case study was carried out in Wanzhou, Chongqing, China, where a 400\u00a0m vertical rock slope prone to rockfall was collected as a typical example. The centimeter-level 3D Textured Digital Outcrop Model (TDOM) and dense Point Cloud (PC) were established using high-resolution photos acquired by nap-of-the-object photogrammetry. The discontinuity of the fractured mass was interpreted by fully taking advantage of both 2D images (texture information-dominated) and 3D PCs (depth information-dominated). Furthermore, a new parameter rock cavity rate (RCR) and the corresponding semiautomatic extraction method based on point clouds are proposed. Subsequently, the possibility of various failure modes and their joint combinations were determined by kinematic analysis. Finally, the rock slope stability was determined using a matrix that considers the slope mass rating (SMR) value and the parameter RCR. The proposed process flow and relevant techniques in this study provide an operable and practical solution for further application regarding discontinuity interpretation and potential rockfall identification on high-steep slopes.\n               ","25":"\n                  The ambient noise cross-correlation technique has been widely used to obtain the S-wave velocity structure. Accurate and efficient extraction of dispersion curve from the spectrum energy diagram is extremely important. In this study, a data-driven deep learning network, SDCnet, is proposed for dispersion curve extraction. SDCnet is an improved Unet with a residual block structure. It automatically and intelligently identifies the peak values at different frequencies as a case segmentation task. A trainable upper and lower sampling strategy was introduced into the residual block to improve the ability of feature extraction and avoid network degeneration. Training set from real and synthetic data widely improves the SDCnet's generalization ability. In our study, SDCnet has demonstrated its high accuracy and efficiency in applications of real data from USArray in the western USA. Compared with the traditional model-driven methods, SDCnet can accurately extract dispersion curves without prior information. The network has the advantage of saving labor and time for extracting massive dispersion curves.\n               ","26":"\n                  The exploitation of data from the Global Ocean Circulation Explorer (GOCE) mission still offers unique insights in Earth system processes despite the fact that the mission has been completed more than eight years ago. The use of GOCE data requires tedious and cumbersome processing in order to extract them from their original format, process them in the various reference frames and filter them to derive final products at the orbital altitude. GeoGravGOCE is an open-source software, developed in MATLAB, which aims to provide a friendly Graphical User Interface (GUI) to process the original GOCE gradiometric observations, i.e., second-order derivatives of the gravitational potential as provided by the European Space Agency. It is composed of four main tabs, which first allows the GOCE gradient pre-processing and the transformation of gradients from a geopotential models to the gradiometric reference frame. Then, the spectral filtering and the mutual transformations of gravity gradients between the four reference systems used in satellite gravimetry can be carried out.. Three main filtering methods are implemented within the software, referring to: Finite Impulse Response (FIR), Infinite Impulse Response (IIR), and Wavelet Multi-Resolution Analysis (WL-MRA) filters. The user is free to select the desired degree of filtering and evaluate the derived results in terms of both the power spectral density of the filtered signals and their spatial distribution. The present work gives an analytical outline of the program, along with its various functionalities, while results derived from the processing of original GOCE data are presented as well. GeoGravGOCE is a free software under the GNU license and can be freely accessed via the project webpage http:\/\/olimpia.topo.auth.gr\/GeoGravGOCE\/.\n               ","27":"\n                  A crossover refers to the intersection of two satellite ground tracks. Crossovers are required for performing crossover differences adjustment to remove the orbit error and for establishing the time series of elevation in surfaces, which is why crossovers are important for satellite altimetry measurements of ice sheet or ice shelf elevation changes. How to extract more crossovers precisely is the objective of this paper. On the basis of the traditional method of solving crossovers and computer graphics, this paper proposes an improved algorithm called improved rapid rejection and straddle test (IRST) for computing the position of the crossover point. This algorithm efficiently and accurately searches for two points in ascending pass and two points in descending pass that can form a crossover, and then computes crossovers\u2019 geolocation. By using CryoSat-2 satellite altimeter data, we conducted our study on the Antarctic Ross ice shelf and Filchner\u2013Ronne ice shelf using the fixed iteration (FI) algorithm, rapid rejection and straddle test (RST) algorithm, and the IRST algorithm. Results show that IRST is superior to the two other algorithms in terms of the number of crossovers, geolocation accuracy and computational efficiency. These advantages are more noticeable in the border areas between the ice shelf and the mountain with large terrain slope, which solves the problem that crossovers are less in these areas with poor-quality data coverage.\n               ","28":"\n                  Gaining information about detailed processes using aggregation information is a frequent challenge in research involving geospatial data, with examples in different fields of knowledge such as agronomy, soil science, meteorology, public health, epidemiology, and others. Analyses using aggregated data lead to distorted conclusions since they disregard local patterns, and such a problem has motivated different approaches for reconstructing the information in a finer resolution from the aggregated data. However, most existing methods focus on the particular case where the volume of data does not exceed the amount of memory available for computations, a situation that has become increasingly less frequent with the fast pace of data generation nowadays. In practice, this problem limits either spatial resolution or coverage of applications, thus precluding their use in a more general context. In this paper, we address the problem of disaggregation of spatial data with huge datasets by proposing a scalable method to estimate the parameters of a well-established model. We propose an iterative scheme for model estimation and prove its convergence to a critical point of the likelihood function derived. To test the method, we provide a controlled simulation and a real example for sugarcane production in Brazil. In the simulation, the results indicate a successful reconstruction of 1 million pixels from 90 block areas. In the real example, the results had a compatible match with the agronomic literature, indicating a reasonable prediction of sugarcane production in a 100\u00a0m spatial resolution (i.e., approx. \n                        \n                           5\n                           \u00d7\n                           1\n                           \n                              \n                                 0\n                              \n                              \n                                 8\n                              \n                           \n                        \n                      pixels) from 5,565 block-areas. Compared to the most similar previous work, scalability allowed us to use a nearly 100 times higher resolution, which corresponds to 10,000 times more pixels. With our methods, we expect to assist researchers from different fields in disaggregating spatial information to larger areas or higher resolutions.\n               ","29":"\n                  The shapes and directionality of the oceanic crust at slow-spreading ridges are key to understanding its magmatic or tectonic emplacement. At slow-spreading ridges, magmatic terrain is marked by linearly fault-bounded abyssal hills, while a more tectonic emplacement termed detachment terrain is marked by long-lived detachment faults forming Oceanic Core Complexes (OCCs). However, the quantitative description of the magmatic and detachment regimes is still limited. We develop a novel geomorphometric technique to automate terrain classification based on the parameterisation of the shape, directionality, and curvature of the seafloor. The algorithm consists of two steps: (1) characterising the pattern observed in the horizontal axes by computing the horizontal eigenvalues of the slope vectors at each multibeam cells and (2) building a weight matrix derived from the computed slopes. The eccentricity of the horizontal eigenvalues defines the dipping pattern in the horizontal axes, hence the term slope-weighted eccentricity (SWE). The technique is applied through a moving window and is tested at 12.5\u00b0\u201315.5\u00b0 N on the Mid-Atlantic Ridge (MAR), where the two distinct modes of spreading occur. The application of this novel geomorphometric technique yields results consistent with published qualitative interpretation and the distribution of seismicity observed from the peak amplitudes of the tertiary waves (T-waves) in the study area. Using the established algorithm, we found that 41% of the seafloor in our study area experienced detachment faulting (up to 28% are identified as OCCs), 25% experienced typical magmatic accretion, and a buffer zone termed extended terrain affects 34% of the seafloor, where the morphology shows a transition from detachment to magmatic spreading or vice versa. These findings provide new insights into seafloor classification based on the observed morphology and the potential to automate such mapping at other slow-spreading ridge regions.\n               ","30":"\n                  Multicomponent transport of dissolved charged species often involves electrostatic interactions among the different ions. These interactions are the result of the different diffusion rates of the chemical species, which create a diffusion potential and thus an electromigration term in the transport equation that is additive to the Fickian diffusion term due to the concentration gradient. The explicit consideration of the electromigration term involves the use of the Nernst\u2013Planck equation, which tightly couples the transport of the charged species through the electro-diffusive term. Here, we have implemented the Nernst\u2013Planck equation in the open source computer code PFLOTRAN. The advantages of the customised code, denoted as PFLOTRAN\n                        \n                           \n                           \n                              NP\n                           \n                        \n                     , in comparison with other recent similar developments, are that (i) the implementation is extended also to the multiple continuum module of PFLOTRAN, which makes the code suitable for the modelling of electromigration processes in fractured rock, (ii) PFLOTRAN\n                        \n                           \n                           \n                              NP\n                           \n                        \n                      leverages the parallelisation capabilities of one of the latest stable PFLOTRAN versions and thus it is suited for large-size simulations in supercomputers and (iii) the developed code is made available through a public repository. PFLOTRAN\n                        \n                           \n                           \n                              NP\n                           \n                        \n                      was successfully validated against a standard benchmark involving 1D transport of a multicomponent electrolyte solution and a verification case, involving a fracture\u2013matrix system, was also presented. Finally, PFLOTRAN\n                        \n                           \n                           \n                              NP\n                           \n                        \n                      was used to interpret field and experimental data at Olkiluoto (Finland), which show that an imbalance exists between chloride concentration in the matrix pore water and in the fracture filling groundwater. The results of the simulations show that this imbalance is indeed caused by anion exclusion processes, which are more significant in gneiss rock types than in pegmatitic granitic rock.\n               ","31":"\n                  Methane gas recovery from hydrate bearing sediment (HBS) by depressurization causes significant pore pressure decrease, loss of solid load bearing hydrate from pores and small temperature drop. The decrease in pore pressure increases the effective compressive stress arising from overburden load on the HBS and thus causes the settlement of the HBS. The reduction of medium strength by the loss of load bearing hydrate and thermal contraction also influences the deformation of HBS. We have developed a THMC solver to simulate the complex geomechanics problem along with hydrate phase change kinetics, non-isothermal multiphase flow in porous media, and the alteration of porosity, permeability and bearing capacity. The deformation behavior of the hydrate-bearing media depends on the host sediment and hydrate morphology, effective porosity and critical porosity. We have included the influence of loss of load bearing hydrate and compaction due to increase of effective compressive stress as the result of pore pressure decrease, fraction of hydrate morphology, effective porosity and critical porosity on the effective medium bulk and shear moduli using a modified effective medium model. The effects of hydrate saturation decrease and effective compressive pressure increase on the deformation of HBS are thoroughly investigated. We have performed numerical simulations to understand the deformation behavior for different porosities and matrix supporting hydrate fractions. Our results show that the deformation is about 20% less when the fraction of matrix supporting hydrate increases from 0.1 to 0.4 because of increased soil bonding.\n               ","32":"\n                  We conducted comprehensive numerical simulations to probe the low field nuclear magnetic resonance (NMR) relaxation of the clay-rich shale in inhomogeneous magnetic field. Under the guidance of the Bloch equation, the relaxation property of a unimodal distributed clay-rich shale is simulated using the Carr-Purcell-Meiboom-Gill (CPMG) pulse sequence. Both the magnetization and the transverse relaxation time (T2) spectrum are obtained to investigate influential factors such as the excitation pulse angle, the refocusing pulse angle, the pulse duration, the echo spacing, as well as the off-resonance frequency. The simulation result showed that the field inhomogeneity contributes great influence on the NMR relaxation, particular in high magnetic field strength and high off-resonance frequency. We observed that in standard CMPG pulse sequence, the amplitude of the T2 distribution is negatively correlated with the off-resonance frequency and can be compensated by the empirical regression. Moreover, the excitation pulse angle poses great impact on the amplitude of the T2 distribution, but has less influence on the T2 distribution and its peak value. On the contrary, both the amplitude and the peak value of the T2 spectrums are also impacted by the refocusing pulse angle, especially in higher magnetic field inhomogeneity. The refocusing pulse angle is preferable higher than 150\u00b0 for the clay-rich shale since it helps the spins to rephrase quickly. In additional, the echo spacing is ideally reduced to as low as its minimal value for the measurement of samples with short relaxation component. The result provides comprehensive knowledge on influential factors of the NMR relaxation of the clay-rich shale, which is useful for the optimization of the pulse sequence, the acquisition conditions, as well as the manipulation of the magnetization data.\n               ","33":"\n                  Microseismic monitoring is a widely used technique in coal mine safe production. Microseismic events classification is one of the most important steps in microseismic monitoring. CNN has been proven to be the potential tool to identify microseismic events automatically. When the noise is serious and the signal is weak, the recognition ability of CNN is limited. Some scholars use denoising methods, such as wavelet transform, to enhance the signal-to-noise ratio (SNR) of the input data. However, the effective signal will be broken if the incorrect threshold parameters are set in this wavelet transform-based denoising method. In this paper, we intend to make the data volume of the time-frequency graph obtained by WPT as the input of CNN. But it may lead to a dimensional disaster if taking the time-frequency domain signals as input directly. To utilize the effective information extraction and reduce the data dimension, we propose a dual-channel CNN model by combining time domain information and wavelet packet decomposition coefficients (T-WPD CNN). The wavelet packet decomposition coefficients highlight the characteristics of the signal and suppress the characteristics of noise. In addition, the coefficients have the same dimension as the original signal. These advantages are useful for enhancing the performance of CNN. Two field datasets are used to test the network. One from Australia and another from Jiayang coal mine, Sichuan, China. The feature map of the convolutional layer with different inputs are shown to illustrate the influence effect of raw time domain signal and WPD coefficient on the classification performance. The final results show that the T-WPD CNN is superior to the traditional CNN method in accuracy and robustness.\n               ","34":"\n                  Understanding the chemistry of sandstone acidizing is important in designing an effective treatment for subsurface rock formations. The complex chemistry of sandstone systems leads to the precipitation of minerals that contribute to formation damage. Thus, monitoring the concentration and location of precipitates is necessary. In this work, a continuum-scale sequential implicit LEA\/PLEA reactive transport model is developed and programmed through coupling OpenFOAM and Reaktoro to improve the model prediction. The proposed LEA\/PLEA models are compared for core acidizing simulations at relatively high and low Damk\u00f6hler numbers. We found that the common assumption of kinetically-controlled flow regimes in sandstone acidizing is valid even at practically high Damk\u00f6hler numbers. The code is verified against a reference reactive transport code. Our results are consistent with two experimental research conducted on sandstones and shales. Finally, we demonstrate the role of precipitation mechanisms in permeability alteration by taking the example of silica and discussing how different arrangements of silica precipitates impact permeability.\n               ","35":"\n                  Flow simulations on porous media, reconstructed from Micro-Computerised Tomography (\n                        \u03bc\n                     CT) scans, is becoming a common tool to compute the permeability of rocks. Still, some conditions need to be met to obtain accurate results. Only if the sample size is equal or larger than the Representative Elementary Volume will the computed effective permeability be representative of the rock at a continuum scale. Moreover, the numerical discretisation of the digital rock needs to be fine enough to reach numerical convergence. In the particular case of using Finite Elements (FE) and cartesian meshes, studies have shown that the meshes should be at least two times finer than the original image resolution in order to reach the simulation\u2019s mesh convergence. These two conditions and the increased resolution of \n                        \u03bc\n                     CT-scans to observe finer details of the microstructure, can lead to extremely computationally expensive numerical simulations. In order to reduce this cost, we couple a FE numerical model for Stokes flow in porous media with an unfitted boundary method for cartesian meshes, which allows to improve results precision for coarse meshes. Indeed, this method enables to obtain a definition of the pore\u2013grain interface as precise as for a conformal mesh, without a computationally expensive and complex mesh generation for\u00a0\n                        \u03bc\n                     CT-scans of rocks. From the benchmark of three different rock samples, we observe a clear improvement of the mesh convergence for the permeability value using the unfitted boundary method on cartesian meshes. An accurate permeability value is obtained for a mesh coarser than the initial image resolution. The method is then applied to a large sample of a high-resolution\u00a0\n                        \u03bc\n                     CT-scan to showcase its advantage.\n               ","36":"\n                  Argo (array for real-time geostrophic oceanography) trajectories, generated by ocean currents at different depths, are a vital data source for studying ocean currents. One traditional method for representing the trajectories \u2012 which uses a time-ordered series of spatial locations \u2012 confuses the Argo trajectories corresponding to the parking depths and the one corresponding to the sea surface. It is, therefore, a great challenge to study ocean currents directly based on the Argo trajectories available in the Argo Global Data Assembly Centers (GDACs). Based on the working principles of Argo floats, in this paper, a hierarchical semantic representation of Argo trajectories is proposed and an ocean current-oriented graph model for representing Argo trajectories, named CoGAT, is designed. CoGAT is based on the following ideas. 1). The complete trajectory of an Argo float is represented by a node named TrajNode, which includes several nodes labeled ParkingTrajNode and labeled SurfaceTrajNode by edges, i.e. IncludingEdges. Each ParkingTrajNode represents the trajectory that an Argo float follows at its parking depth, whereas the SurfaceTrajNode represents the trajectory at the sea surface. 2) By IncludingEdges, each ParkingTrajNode or SurfaceTrajNode includes many sub-nodes, each of which is a ParkingSubTrajNode or SurfaceSubTrajNode, and each of these sub-nodes represents a sub-trajectory within a cycle of an Argo float at a parking depth or at the sea surface. A time-ordered relationship between two ParkingSubTrajNodes or two SurfaceSubTrajNodes is represented by the edge class SequenceEdge. 3) Nodes named LocationNode record the spatial locations of an Argo float, and two LocationNodes make up a sub-trajectory within a cycle; i.e., they constitute a ParkingSubTrajNode or a SurfaceSubTrajNode. A time-ordered relationship between two LocationNodes means a direction of a sub-trajectory at a parking depth or at the sea surface; this relationship is denoted as a ParkingSubTrajEdge or SurfaceSubTrajEdge. 4) The six classes of nodes and four classes of edges described above allow CoGAT to not only separate the Argo trajectories into separate trajectories at different parking depths and at the sea surface but also enable the Argo trajectories to be stored using a temporally granular representation; e.g., monthly, seasonal or annual. In this study, global Argo trajectories for the period January 2000 to April 2020 were stored in a Neo4j-based graph database named GATDB, and comparisons with the spatial Oracle database of Argo trajectories demonstrated the ability of CoGAT to provide a spatial and temporal representation of Argo trajectories at different depths and to store these trajectories.\n               ","37":"\n                  Modeling of the converted shear waves is an issue of major interest in wide-angle seismic prospecting, which is indicative of the lithological properties in sediments, the crust and the uppermost mantle. The active-source seismological community has long called for a rapid, automatic and comprehensive modeling of these data. In this paper, we present a user-friendly software tool named CALM (standing for Converted wAve veLocity Modeling) for the converted shear wave modeling in wide-angle seismic data, which provides the first Graphical User Interface that is capable of determination of conversion modes\/interfaces and automatic modeling of shear wave structures. The capacity of the presented software is demonstrated by a realistic case in the northern South China Sea continental margin, which analyzes the potential conversion modes of subsurface interfaces and obtains the crustal Vp\/Vs structures from wide-angle seismic data.\n               ","38":"\n                  Owing to the axial symmetry of the Earth\u2019s magnetic field, paleomagnetic data only directly record the latitudinal and azimuthal positions of crustal blocks in the past, and paleolongitude cannot be constrained. An ability to overcome this obstacle is thus of fundamental importance to paleogeographic reconstruction. Paleomagnetic Euler pole (PEP) analysis presents a unique means to recover such information, but prior implementations of the PEP method have incorporated subjective decisions into its execution, undercutting its fidelity and rigor. Here we present an optimization approach to PEP analysis that addresses some of these deficiencies\u2014namely the objective identification of change-points and small-circle arcs that together approximate an apparent polar wander path. We elaborate on our novel methodology and conduct some experiments with synthetic data to demonstrate its performance. We furthermore present implementations of our methods both as adaptable, stand-alone scripts in Python and as a streamlined interactive workflow that can be operated through a web browser.\n               ","39":"\n                  Precipitation nowcasting aims to predict the rainfall distribution within a short-term period. However, it pays the same attention to all locations instead of emphasizing those regions with heavy rainfall that has more threats to human activity. Therefore, we develop an important task named Heavy Rainfall Forecast (HRF), which mainly focuses on the movement and change of heavy rainfall areas. It sets aside one hour to give meteorological administration sufficient time to issue warning information. To tackle this task, firstly, we rebuild the meteorological radar dataset based on three criteria to obtain the samples involving heavy rainfall. Secondly, we propose the Location-Refining (LR) neural network to combine the advantages of the optical flow-based and deep learning-based methods in predicting higher intensity and more accurate position, respectively. LR neural network consists of a location network and a refining network. The former is responsible for the accurate predictions of position and trend of rainfall, and the later accounts for more accurately estimating the intensity. To make the model pay more attention to the high echo region, we design new loss functions and introduce auxiliary information of high echo values. A series of experiments show that our model has a significant improvement on this task. Specifically, compared with existing methods, we improve the valid mean square error by 6.4% for the threshold being 20 and 15.1% for the threshold being 30. The critical success indexes are improved by 12.8% for the threshold being 20 and 24.8% for the threshold being 30. We also improve the heidke skill score by 9.9% for the threshold being 20 and 21.4% for the threshold being 30. Furthermore, the proposed framework can be well transferred to other deep learning-based models, and improves their performance.\n               ","40":"\n                  Investigations of the paleohydrologies of pluvial lake systems have often employed lake carbonate deposits called \u201ctufa\u201d that grow subaqueously and can be preserved long after the drying of the lake. For this reason, tufa have been used as a proxy for minimum lake level. However, they exhibit a variety of textures that hold the potential to reveal richer paleoclimatological information. With the goal of determining if tufa texture can be used as a proxy for lake environment, this study investigates the textures of tufa at Mono Lake, California in comparison to the fossil tufa in Searles Valley, California. While observations in the last century suggest that the tufa in the Mono basin grew in waters similar to the modern, the tufa at Searles formed during the last glacial period, when the Great Basin contained a system of pluvial lakes on the scale of the modern Great Lakes. The tufa at both basins have been observed to have a range of classifiable textures, and new methods of inspecting visual data could be informative about what factors control these textures. To this end, a t-Distributed Stochastic Neighbor Embedding (t-SNE) algorithm is used to project images of the tufa at Searles and Mono into a coordinate space, allowing for simple, quantitative comparisons of the visual similarity of textures. The textures of tufa at Searles are compared to each other, as well as to the tufa at Mono. This study performs a robust assessment of the feasibility of Mono Lake as a modern analogue for Searles Valley. It finds that there is a justifiable basis for the comparison of certain fossil facies at Searles to the tufa at Mono, significant progress towards the goal of using texture as a metric for the environment in which tufa formed.\n               ","41":"\n                  In this work, we present to the scientific community the measurements taken during four years, from March 2013 to February 2017 inclusive, by the Extremely Low Frequency Sierra Nevada station, Spain, together with the data processing programs developed in Python (version 3.8) to extract the Schumann resonance (SR) parameters (i.e., amplitudes, resonant frequencies, resonance widths) in 10\u00a0min time periods from these records. The measurements correspond to the voltage induced by the atmospheric electromagnetic field at the north-south and east-west oriented magnetometers of the station. The process comprises four stages. The spectrum, calibrated in the frequency band ranging from 6\u00a0Hz to 25\u00a0Hz, is obtained at the first stage using the Welch method with Hann windows. The second step eliminates the anthropogenic noise generated by different undesired sources. Next, a non-linear fit of the measured spectrum combining Lorentzian functions together with a linear term is carried out in order to identify the presence of SRs and quantitatively characterize them. This third step is carried out using the Python package Lmfit, which implements the Levenberg-Marquad algorithm. Finally, a compact and easy-to-read output is generated at the fourth stage, using the power of the Numpy arrays and the npz format. In addition, four Jupyter notebooks with the description of the code and the possibility of their use in interactive mode are presented as supplementary material with this paper.\n               ","42":"\n                  Geochemical data is a typical geospatial-data with high-dimensional elemental properties. Existing methods for recognizing multivariate geochemical anomalies are limited in fully profiting from the inherent spatial-elemental structures of the geochemical data. This paper presents a novel method for the identification of multivariate geochemical anomalies, which leverages tensor dictionary learning over spatial-elemental dimensionalities to learn the joint representation of the spatial-elemental structures of geochemical data. The learned representations in spatial and elemental dictionaries are capable of capturing both the non-local features over space and the high-correlated distributed features across elements. The sparse representations over the spatial and elemental dimensions are exploited to reconstruct the geochemical background values, which permits us to identify multivariate geochemical anomalies considering the spatial-elemental structure of geochemical data. The proposed method is applied to recognize geochemical anomalies related to Au mineralization in the northwest Jiaodong Peninsula, Eastern China. By comparing with the anomalies recognized by deep autoencoder and local singularity analysis, we conclude that the tensor dictionary learning method is more effective to identify the mineralization-associated anomalies attribute to its ability to fuse spatial-elemental information of the geochemical data.\n               ","43":"\n                  Relationships between the features visually identified at the front of the flow\u2019s current and parameters regarding its velocity and turbulence were observed in early experimental works on the characterization of gravity currents. Researches have associated front features, like lobes and clefts, with the flow\u2019s turbulence, and have used these associations ever since. In more recent works using numerical simulations, these connections were still being validated for various flow parameters at higher front velocities. The majority of works regarding measurements at the front of a gravity current rely on the front\u2019s images for making its analysis and establish relationships. Besides that, there is an interdisciplinary field related to computer science called computer vision, devoted to study how digital images can be analyzed and how these results can be automated. This paper describes the use of computer vision algorithms, particularly corner detection and optical flow, to automatically track features at the front of gravity currents, either from physical or numerical experiments. To determine the proposed approach\u2019s accuracy, we establish a ground-truth method and apply it to numerical simulation results data sets. The technique used to trace the front features along the flow showed promising results, especially with higher Reynolds numbers flows.\n               ","44":"\n                  We introduce a novel method for graph-based segmentation of spectral images obtained using a Scanning Electron Microscope (SEM) equipped with an Energy Dispersive X-ray spectroscopy (EDS) detector. The method exploits deep learning along with fusion of rasterized electron microscopy images with sparse EDS samples to obtain accurate mineralogy segmentation with high efficiency. Improvements over previous methods are with respect to the goal of an improved quantitative and qualitative assessment of segmentation, so that volumetric composition is indirectly addressed. We describe the principles of the novel method, show experimental results on real samples and demonstrate its advantages in comparison to the state of the art. The new method performs unsupervised clustering on sparsely measured EDS spectra, allowing for classification of unseen mineralogical compounds. Then, the processed spectra are combined with single channel SEM measurements through an optimized lattice, where a Markov Field is used to perform spatial segmentation in image. The benefit of this material-agnostic method is that clusters can then be (separately) classified, analyzed, and small grains with distinct EDS measurements are more accurately separated than in previous methods. These improved results are evaluated quantitatively on ground-truth electron microscope measurements with dense high-count EDS data, as well as visually through analysis by a mineralogist.\n               ","45":"\n                  It is long recognized that the processes that define the shape of the landscape operate at specific scales, and that mismatch between the scale of analysis and the scale of the process leads to erroneous results. A priori knowledge is often used in specific applications when appropriate scales are known; otherwise selecting an optimal scale of analysis has proven much more challenging. Existing attempts to address scale optimization treat scale as a spatially stationary parameter, and often rely on the generation and analysis of large volumes of data. This research introduces a method to represent topographically complex information at locally optimized scales. A constant time Gaussian scale-space implementation is used to manipulate topographic structure prior to the computation of local derivatives, followed by scale-standardization to facilitate cross-scale comparison. The method demonstrates how Gaussian smoothing can be used to efficiently explore the scaling characteristics of many topographic metrics, and how scale-standardization facilitates the identification of characteristic scales. The spatial distributions of the optimal scale and both the measured value and scale-standardized value measured at the optimal scale provide new information with which to characterize topography. These data improve topographic characterization while eliminating the redundancy of homogenous scale representation.\n               ","46":"\n                  The performance of basalt and sandstone for CO2 storage mainly depends on the CO2 migration and solidification in the pore network of rock. This study provides points of view for the simulation of the three-dimensional pore space and the numerical prediction of gas transfer in the microstructure of the studied rocks. The biphasic change on Gaussian random field forms a randomly shaped pore network and the combination of biphasic fields creates a porous model that satisfies the real rock porosity and pore size distribution. Based on the generated model, several calculation scenarios using the mathematical morphology are proposed and numerically implemented to investigate the pore space characteristics, to extract the potential CO2 transfer paths, to analyze the geometric features and thus to evaluate the CO2 storage performance.\n               ","47":"\n                  The support vector classifier (SVC) is one of the most powerful machine learning algorithms. This algorithm has been accepted as an effective method in three-dimensional geological modeling. Although the model selection has a great impact on the performance of SVC algorithm, most of mining studies have neglected it and used the grid search method. Therefore, in this study, a new approach is proposed for improving the selection of SVC models. This approach uses particle swarm optimization (PSO) to determine the important parameters of SCV such as penalty and kernel parameters. The proposed approach was applied in the modeling process of the Iju porphyry copper deposit to delineate alteration and mineralization zones. The optimal penalty and kernel parameters were found to be 27.2 and 2\u22124.75 for alteration zone, and 22.72 and 2\u22126.23 for mineralization zone, respectively. With 97.4% and 97.01% rates of accuracy for mineralization and alteration zones, the PSO results showed reasonable performance in classification. The proposed approach had better accuracy than grid search method. Therefore, because of its better performance, the geological models were developed using the PSO method to be used as a basis for future resource evaluation.\n               ","48":"\n                  The reconstruction of porous media is important to the development of petroleum industry, but the accurate characterization of the internal structures of porous media is difficult since these structures cannot be directly described using some formulae or languages. As one of the mainstream technologies for reconstructing porous media, numerical reconstruction technology can reconstruct pore structures similar to the real pore spaces through numerical generation and has the advantages of low cost and good reusability compared to imaging methods. One of the recent variants of generative adversarial network (GAN), Wasserstein GAN with gradient penalty (WGAN-GP), has shown favorable capability of extracting features for generating or reconstructing similar images with training images. Therefore, a 3D reconstruction method of porous media based on an improved WGAN-GP is presented in this paper, in which the original multi-layer perceptron (MLP) in WGAN-GP is replaced by convolutional neural network (CNN) since CNN is composed of deep convolution structures with strong feature learning abilities. The proposed method uses real 3D images as training images and finally generates 3D reconstruction of porous media with the features of training images. Compared with some traditional numerical generation methods and WGAN-GP, this method has certain advantages in terms of reconstruction quality and efficiency.\n               ","49":"\n                  Petrophysical properties determination of a reservoir such as shear wave velocity plays a crucial role in exploration and production management. The purpose of this study is to develop a robust committee machine model for estimating fast and slow shear wave velocities from petrophysical logs. To this end, appropriate inputs were determined and different algorithms including artificial neural network, fuzzy logic and neuro-fuzzy were applied. Afterward, the obtained outputs were merged by utilizing optimization methods consisting of Ant Colony Optimization for Continuous Domain, Genetic and Simulated Annealing approaches. The construction of the committee machine was done using a case study including 2000 data samples from the Sarvak Formation in one of the southwestern Iran oilfields. The utilized dataset was divided into training and testing data which contain 1600 and 400 samples, respectively. Mean Squared Error was presented as a determinative factor for evaluating system performance. To obtain the best system performance with the lowest Mean Squared Error, the parameters setting of each algorithm were coded in different ranges. A total 52416, 76896 and 29400 cases were run to optimize the parameter settings of genetic algorithms, simulated annealing and continues ant colony optimization models, respectively. A weight factor is assigned to each method indicating their contribution in the final estimation. The desirable weights' combination is acquired using optimization methods. Among the individual systems, fuzzy logic and artificial neural network obtained the best performance for estimating fast and slow shear wave velocities, respectively. Also, the results of committee machine algorithms indicate a well optimization on individual systems and have superior performance over them. The most efficient optimization algorithms in the structure of committee machines for fast shear wave velocity are Genetic and Simulated Annealing and for slow shear wave velocity is Genetic Algorithm, in terms of Mean Squared Error criteria.\n               ","50":"\n                  The presence of discontinuities in fractured rocks contributes to the formation of blocks. Characterizing the size and shape of the congregation of blocks in the rock mass provides a comprehensive understanding for studying its engineering properties. The existing block shape characterization method (BSCM) considers two factors: \u03b1 describes the shortening of the minor principal axis of the block, and \u03b2 describes the elongation of the major axis. The parameter \u03b2 used the average angular relation between the chords greater than the median, considering the average angle could produce skewed results towards elongated blocks. This study proposed a modified block shape classification method (MBSCM), where parameter \u03b2 is provided with a new definition and procedure for calculation. To reckon the elongation index (\u03b2) of the block, the maximum angular extension between the chords was considered and the parameter \u03b1 remained unchanged in the modified approach. The developed method was validated with synthetic rock masses of known shapes (i.e., cubic, elongated, elongated platy) constructed in the 3DEC (Three-dimensional distinct element code). Two case studies were also conducted on the Himalayan slopes to demonstrate the new method's applicability. Discrete Fracture Network (DFN) were generated for both the slopes to find the block related data formed by the intersection of the fracture network. The results show that the first slope was dominated with elongated (73.52%), elongated platy (16.53%) and platy (6.1%) blocks while the second slope was composed of elongated (59.17%), cubic elongated\/platy (22.3\/11.98%) and cubic (4.21%) blocks. The slopes were also classified using the existing method to compare the outcomes. The result shows that the existing method categorized about 5% more elongated blocks as compared to the proposed modified approach. A python-based GUI tool was developed for the modified approach and was successfully used to directly plot the classification diagrams by importing the raw data file.\n               ","51":"\n                  In the present paper we introduce a numerical model for the representation of displacement, strain and stress due to single forces embedded in a layered elastic half-space. The code \n                        \n                           E\n                           F\n                           G\n                           R\n                           N\n                           \/\n                           E\n                           F\n                           C\n                           M\n                           P\n                        \n                      (Elastic Forces GReeN functions\/Elastic Forces CoMPutation) is able to represent the mechanical effects due to pre-assigned distributions of single forces. Even if internal deformation sources can be described by distributions of equivalent body forces with vanishing resultant and moment, single forces are employed in geophysics to represent hydraulic and\/or lithostatic loads, effects of internal density anomalies, and even some kind of seismic events. A distribution of single forces is also used to describe the effects of an inelastic inclusion located inside an elastic medium. In fact, the recent literature shows that poro-elastic and thermo-elastic inclusions can be represented using single forces distributed on their boundaries. \n                        \n                           E\n                           F\n                           G\n                           R\n                           N\n                           \/\n                           E\n                           F\n                           C\n                           M\n                           P\n                        \n                      shares the benefits of rapid and semi-analytical calculation offered by the parent code, \n                        \n                           E\n                           D\n                           G\n                           R\n                           N\n                           \/\n                           E\n                           D\n                           C\n                           M\n                           P\n                        \n                     , which is instead suitable for the representation of extended dislocation sources, as seismic faults. The present code also provides an option for computing the effects of a distribution of single forces embedded in a homogeneous half-space, by using the analytical solutions of Mindlin. Accordingly, \n                        \n                           E\n                           F\n                           G\n                           R\n                           N\n                           \/\n                           E\n                           F\n                           C\n                           M\n                           P\n                        \n                      can be a valid support both for the representation of forward models of deformation sources and for the procedures of inversion of geodetic data in a layered medium. We show some applications of the code and we provide several scripts in MATLAB language which help the user to quickly start using \n                        \n                           E\n                           F\n                           G\n                           R\n                           N\n                           \/\n                           E\n                           F\n                           C\n                           M\n                           P\n                        \n                     .\n               ","52":"\n                  It is challenging to predict the production performance of unconventional reservoirs because of the sediment heterogeneity, intricate flow channels, and complex fluid phase behavior. The traditional oil production prediction methods (e.g., decline curve analysis and reservoir simulation modeling forecasting) are subjective. This paper presents a machine learning-based time series forecasting method, which considers the existing data as time series and extracts the salient characteristics of historical data to predict values of a future time sequence. We used time series forecasting because of the historical fluctuations in production well and reservoir operations. Three algorithms were studied and compared to address the limitations of traditional production forecasting: Auto-Regressive Integrated Moving Averages (ARIMA), Long-Short-Term Memory (LSTM) network, and Prophet. This study starts with the representative oil production data from a well located in an unconventional reservoir in the Denver-Julesburg (DJ) Basin. 70% of the data was used for model training, whereas the remaining 30% of data was used to evaluate the performance of the above-mentioned methods. Then, the decline curve analysis and reservoir simulation modeling forecasting were applied for comparison. The advantages of the machine-learning models include a simple workflow, no prior assumption about the reservoir type, fast prediction, and reliable performance prediction for a typical fluctuating declining curve. More importantly, the \u2018Prophet\u2019 model captures production fluctuation caused by winter impact, which can attract the operator's attention and prevent potential failures. This has rarely been explored and discussed by previous studies. The application of ARIMA, LSTM, and Prophet methods to 65 wells in the DJ Basin show that ARIMA and LSTM perform better than Prophet\u2014probably because not all oil production data include seasonal influences. Furthermore, the wells in the nearby pads can be studied using the same parameter values in ARIMA and LSTM for predicting oil prediction in a transferred learning framework. Specifically, we observed that ARIMA is robust in predicting the oil production rate of wells across the DJ Basin.\n               ","53":"\n                  Source-generated coherent noise suppression is a long-standing issue in land seismic data processing. Traditional methods\u2014such as f\u2013k filtering, \n                        \n                           \u03c4\n                           -\n                           p\n                        \n                      transform, and median filtering\u2014often fail to remove coherent noise in irregular data, especially in the case that the noise highly overlaps with the signal. To address this issue, we developed a method based on local nonlinear filtering (LNF) to attenuate coherent noise in seismic data. In the proposed method, a novel similarity coefficient algorithm is presented to determine the optimal velocity of coherent noise. An initial model of coherent noise is constructed using a mixed median\u2013mean filter and is then refined by the proposed weighting correction algorithm. After modeling the noise, coherent noise in seismic data can be then suppressed by adaptively subtracting the estimated noise from the raw seismic data. To verify the effectiveness of the proposed method, we applied it to regular and irregular synthetic data and a 3D field dataset containing strong ground roll. The results demonstrate that the new method successfully attenuates the ground roll and obtains better results in ground-roll attenuation and the preservation of useful signals compared with the three commonly used methods. Synthetic and real data examples indicate that the method is applicable to the attenuation of coherent noise in irregular data.\n               ","54":"\n                  In many industries and applications, obtaining and classifying remote sensing imagery plays a crucial role. The accuracy of classification, in particular the machine learning methods, mainly depends on a multitude of factors, among which one of the most important ones is the amount of training data. Obtaining sufficient amounts of training data, however, can be very difficult or costly, and one must find alternative ways to improve the accuracy of predictions. To this end, a possible solution that we provide in this study is to use a stochastic method for producing variations of the training images that will retain the important class-wide features and thereby enrich the machine learning's \u201cunderstanding\u201d of the variabilities. As such, we applied a stochastic algorithm to produce additional realizations of the limited input imagery and thereby significantly increase the final overall accuracy in a deep learning method. We found that by enlarging the initial training set by additional realizations, we are able to consistently improve classification accuracy, compared with generic image augmentation approaches. The results of this study show that there is a great opportunity to increase the accuracy of predictions when enough data are not available.\n               ","55":"\n                  The flow of fluids in point bars is affected by the existence of heterogeneities like shale drapes that are found on the surfaces of inclined heterolithic stratifications. In fact, these shale drapes can act as fluid flow baffles; therefore, developing a framework for modeling point bars and their associated heterogeneities is vital. In this study, a stochastic process-based modeling approach is presented for capturing the main point bar heterogeneities: accretion surfaces (i.e., the aerial heterogeneity) and inclined heterolithic stratifications (i.e., the vertical heterogeneity). The former was modeled using a sine-generation function and the latter, with a sigmoidal function after which they were combined into a 3D point bar model. To ensure proper modeling of petrophysical properties, we developed a more representative gridding scheme which generates curvilinear grids representative of the point bar geometry. This grid was then transformed into a rectilinear grid to allow for geostatistical simulation after which all petrophysical properties were mapped back into the original curvilinear grid. An essential element of this modeling approach is the stochastic representation of shale drapes at the interface between successive accretion surfaces. The workflow was tested using a real field dataset for the Cranfield field, Mississippi. The constructed model was then subjected to a flow simulation study mimicking a CO2 storage scenario. Various sensitivities were simulated to evaluate the effect of heterogeneities on CO2 flow within the point-bar. Results demonstrate the importance of representing point-bar related heterogeneity and the spatial distribution of shale drapes on CO2 plume migration and storage.\n               ","56":"\n                  Tying the seismic to the well logs is an important operation in the processing and interpretation workflow. It is performed multiple times during the exploration phase to understand the wavelet and the time-depth relation at the well position. It is an iterative process where the geoscientist needs to adjust a number of key parameters in order to converge towards an accepted solution. Potential noise in the data as well as various sources of uncertainties can complicate the task and lead to a time consuming and sometimes frustrating experience for the interpreter. In this work, we automate the overall workflow using deep learning and Bayesian search. First, we build a variational convolutional neural network and train it to solve the wavelet extraction problem. The network learns the deconvolution process from synthetic pairs of reflectivity and seismic traces. Once trained, it can robustly estimate wavelets given input series of arbitrary lengths. The variational nature of the network allows to quantify the uncertainties of the process, in particular for the phase of the wavelet. Secondly, we resort to a global Bayesian optimizer to automatically tune the parameters. The search space is composed of a number of key parameters, such as the time-depth table bulk shift, and the optimizer iteratively tries new combinations in order to maximize the quality of the tie. The user controls the bounds of the space, making the overall algorithm more interpretable. We also extend the method to perform prestack well-ties and explore the possibilities for joint ties. We validate our approach on two challenging real datasets and show that it yields accurate results using reasonable computing resources.\n               ","57":"\n                  The time weighted dynamic time warping (TWDTW) algorithm is an important algorithm for the time series analysis of remote sensing images. However, due to the limitation of computational complexity, it is difficult to apply this algorithm to large datasets. Therefore, combined with the characteristics of the TWDTW algorithm. This paper proposes a time weighted dynamic warping time parallel algorithm based on CUDA. First, a GPU many-core stream processor is used to process independent pixel data in parallel, and the cumulative cost matrix is established through a multithreading architecture to reduce the dependence between data. Then, the memory model is optimized to reduce memory access transactions in order to improve the memory access efficiency of the algorithm. Taking the Sentinel-2 remote sensing image as experimental data, the algorithm based on different time series lengths, data scales and thread organizations is experimentally verified, and the execution performance of the algorithm under different conditions is explored and analyzed. The experimental results show that this method can significantly improve the computational efficiency of the algorithm.\n               ","58":"\n                  This work develops a rock mass characterization of a limestone quarry in northern Spain based on a 3D model obtained by using photographs taken from an unmanned aerial vehicle (UAV) flight and structure-from-motion algorithms. This methodology permits to obtain photogrammetric information in a rapid and low-cost way.\n                  Geological planar facets (stratification, faults) are related to the tectonic history of a geological formation and permit assessing slope stability. The spatial orientation of planar features is usually measured with a compass and clinometer, which requires experience and knowledge, it is space limited, and sometimes hazardous. Geological 3D models can mitigate these limitations. A 3D point cloud generated from a series of images obtained using an UAV is included in the open-access applications DSE and FACET, which serve to determine the main discontinuity planes within a limestone quarry. Comparison of the results from software analyses with data hand-measured directly in the field reveals the effectiveness of the use of UAV to develop a virtual outcrop model that permits to obtain accurate measurements. The resultant quarry 3D model has been included in Sketchfab, an open access platform that permits easy and quick access and availability for a wide audience.\n                  This study shows that the use of UAV combined with structure-from-motion algorithms is of great interest for geosciences as well as other related disciplines such as mining or civil engineering, and can facilitate decision-making for policy makers and authorities. In addition, it is a technique of great use to develop rock mass characterization in a low-cost, rapid, and easy way, and permits to reach areas with difficult accessibility. This way, this methodology can also be very useful for geosciences teaching purposes, as a complement to traditional field lectures, or to develop virtual field trips and laboratories.\n               ","59":"\n                  Chemical diffusion in minerals has shown great potential to quantify timescales of geological processes. The presence of chemical gradients, along with favorable temperature and time conditions, lead to the formation of measurable diffusion profiles. Temporal information can be extracted from measured diffusion profiles using either analytical or numerical solutions of Fick's second law. Currently, there is a lack of widely adopted programs for diffusion studies. In addition, the uncertainties associated with timescales derived from diffusion chronometry are critical for geological studies, but are not always robustly evaluated. In many cases, only uncertainties in curve fitting parameters and temperature are considered, whereas other uncertainties, such as those associated with the experimentally determined diffusion coefficients themselves, are rarely propagated into the calculated timescales. Ignoring these uncertainties reduces the reproducibility and intercomparability of results. In response to these challenges, we present Diffuser, a user-friendly program to standardize diffusion chronometry with transparent and robust propagation of uncertainties. Using analytical and numerical methods, our program provides an automatic, visual, and efficient curve fit to extract chronological information from diffusion profiles. The method is complemented by an algorithm to propagate all uncertainties (i.e., measurement, temperature, curve fitting, and diffusion coefficient) to derived timescales. Three examples are provided to highlight how the program can recover timescales with internal consistency, efficient computing, and easy-to-use features. Our freely available and user-friendly program will hopefully increase the accessibility and consistency of diffusion modeling and thereby to facilitate more high-quality diffusion studies.\n               ","60":"\n                  The new toolbox ScourApp is presented and applied for analysis of the complex interactions between local scour and sediment deposition at bridges during floods and to determine the relative importance of the controlling parameters on scour depth. ScourApp represents the time-dependent sediment transport processes through modern empirical formulations to compute the evolution of scour depth at a bridge pier. Results show that the mathematical model reproduces observed scour depths with high accuracy and confirm that ScourApp is a suitable tool for scour analysis. The model parameters controlling the time dependent scour depth were the equilibrium scour depth, and two constants representing the effects of pier geometry, flow and sediment. Sensitivity analysis showed that all three scour model parameters exhibited a nearly linear relation with the final scour depth. Moreover, results considering sediment deposition obtained for the Rapel Bridge showed that scour depths are highly sensitive to the sediment supply rate. ScourApp is recommended in scour assessment for bridge planning, design, management, and forensic analysis, as well as for engineering education.\n               ","61":"\n                  We present an algorithm for seismic event discrimination and event approximate location based on multi-station seismograms. A deep learning approach was applied using a two-step algorithm: (i) signal onsets were identified in individual tracks based on the use of long-short-term memory neural network layers; (ii) if a sufficient number of onsets were reliably identified, a preliminary location was determined. We adopted a \u201creverse location approach\u201d where the time sense of a seismogram is reverted and the origin time is predicted using a neural network approach based on previously determined onsets. Successful location or origin time prediction also served as a feedback for confirming previous onset identification.\n                  The method was tested using a data set of Acoustic Emission generated from the uniaxial loading of a Westerly Granite specimen. Accuracy of the method was better than 97%. Discriminated events were automatically located and their seismic moment tensor was determined. Both types of results were in good agreement with the baseline data set.\n                  With respect to the particular nature of processed data, we provide a demo code which shows examples presented in the article. In addition, a detailed description of the algorithm, including the control parameter values, is provided in the text. Based on this information the method can be applied on any data.\n               ","62":"\n                  The development of reliable, sophisticated hydro-morphodynamic models is essential for protecting the coastal environment against hazards such as flooding and erosion. There exists a high degree of uncertainty associated with the application of these models, in part due to incomplete knowledge of various physical, empirical and numerical closure related parameters in both the hydrodynamic and morphodynamic solvers. This uncertainty can be addressed through the application of adjoint methods. These have the notable advantage that the number and\/or dimension of the uncertain parameters has almost no effect on the computational cost associated with calculating the model sensitivities. Here, we develop the first freely available and fully flexible adjoint hydro-morphodynamic model framework. This flexibility is achieved through using the pyadjoint library, which allows us to assess the uncertainty of any parameter with respect to any model functional, without further code implementation. The model is developed within the coastal ocean model Thetis constructed using the finite element code-generation library Firedrake. We present examples of how this framework can perform sensitivity analysis, inversion and calibration for a range of uncertain parameters based on the final bedlevel. These results are verified using so-called dual-twin experiments, where the \u2018correct\u2019 parameter value is used in the generation of synthetic model test data, but is unknown to the model in subsequent testing. Moreover, we show that inversion and calibration with experimental data using our framework produces physically sensible optimum parameters and that these parameters always lead to more accurate results. In particular, we demonstrate how our adjoint framework can be applied to a tsunami-like event to invert for the tsunami wave from sediment deposits.\n               ","63":"\n                  The prediction of hydrologic conditions in watersheds has manifold applications, ranging from flood disaster preparedness to water supply and environmental flow management. In watersheds with scarce or no flow data, it is difficult to make accurate hydrologic predictions. Past work has used similarity in single-valued properties of the terrain (for example, drainage area, mean slope) as the basis to relate flow conditions in gauged watersheds to the ungauged ones. The resulting predictions show modest accuracy and have a weak physical basis. In this study, we develop a physics-informed machine learning approach to extract features that represent the hydrologic dynamics \u2014 width function and hypsometric curve. These two geomorphometric measures are computed using functional forms fitted to estimates derived from digital elevation data. Furthermore, dynamically-similar groups are identified based on results from unsupervised clustering and divergence measures. Our approach paves the way towards a flexible and scalable machine learning approach that can be used to assess hydrologic similarity and improve prediction, one informed by physics of surface flow generation and transport in watersheds. A case study involving 72 sub-watersheds in the Narmada River Basin (India) is used to illustrate the new methodology.\n               ","64":"\n                  The operator splitting approach has been widely accepted since it was introduced as a means to solve reactive transport problems. The conventional operator-splitting finite element scheme (Nodal-OS) handles speciation calculations on nodes, resulting in a mixing of heterogeneous reactions on opposing sides in multi-layer systems. Such mixing, however, is not physically accurate. In this context, we propose a new operator-splitting finite element scheme (IP-OS) for reactive transport modeling in saturated porous media. In contrast to the conventional scheme, speciation calculations are performed on integration points rather than on nodes in the new scheme. The implementation of the IP-OS scheme is verified through comparison with an analytical solution of a coupled diffusion\u2013dissolution problem. On this basis, two representative benchmarks are used to examine the advantages and disadvantages of IP-OS. IP-OS is found to have the following advantages and disadvantages compared to Nodal-OS: (1) IP-OS is more accurate; (2) IP-OS is more straightforward to implement; (3) IP-OS is less sensitive to grid resolution and is numerically more stable with coarser grid spacing; and (4) IP-OS is computationally more expensive. In light of the above pros and cons, we recommend using Nodal-OS in cases where chemical reactions do not affect transport properties, and IP-OS in multi-layer heterogeneous cases where chemical reactions alter transport properties of porous media.\n               ","65":"\n                  Magnetotellurics (MT) is a passive electromagnetic geophysical method that measures variations in subsurface electrical resistivity. MT data are collected in the time domain and processed in the frequency domain to produce estimates of a transfer function representing the Earth\u2019s electrical structure. Unfortunately, the MT community lacks metadata and data standards for time series data. As the community grows and findability, accessibility, interoperability, and reuse of digital assets (FAIR) data principles are enforced by government and funding agencies, a standard is needed for time series data. Presented here is a hierarchical data format (MTH5) that is logically formatted to how MT data are collected. Open-source Python packages are also described to read, write, and manipulate MTH5 files. These include a package to deal with metadata (mt_metadata) based on standards developed by the Working Group for Magnetotelluric Data Handling and Software assembled by the Incorporated Research Institutions for Seismology (IRIS), and mth5: a package to interact with MTH5 files that uses mt_metadata. Example code and workflows are presented.\n               ","66":"\n                  Models simulating the state of the biological and physical environment can be built using frameworks that contain pre-developed data structures and operations. To achieve good model performance it is important that individual modelling operations perform and scale well. Flow accumulation operations that support the use of criteria for selecting how much material flows downstream are an important part in several Earth surface simulation models. For these operations, no algorithms exist that perform, scale, and compose well. The objective of this study is to develop these algorithms, and evaluate their performance, scalability, and composability. We base our algorithms on the asynchronous many-task approach for parallel and concurrent computations, which avoids the use of synchronization points and supports composability of modelling operations. The relative strong and weak scaling efficiencies when scaling a flow accumulation operation over six CPU cores in a NUMA node are 83% and 84% respectively. The relative strong and weak scaling efficiencies when scaling a case-study model over four cluster nodes are 73% and 84%. Our algorithms are composable: the latency of executing two flow accumulation operations combined is lower than the sum of their individual latencies.\n               ","67":"\n                  GPS campaign-mode surveys are periodically collected measurements and their time series has a considerable percentage of data gaps unlike continuous time series. Studying error characteristics of a time series is imperative to compute reliable parameter uncertainties. The power-law process can best describe the background noise for a GPS continuous time series and may be introduced into the analysis through well-known methods. However, the power-law process cannot be applied successfully over GPS campaign time series due to the constraints mentioned above. Here we demonstrate a new approach enabling one to project the stochastic properties of campaign time series into the stochastic domain of the permanent stations of International GNSS Service (IGS) network. The stochastic domain, the combination of white noise (WN) and flicker noise (FN), was obtained from the power spectrum of a continuous time series consisting of truncated daily RINEX observations. We implemented it in the Python3 environment as an open-source package called GPS\/GNSS campaign time series analysis (GCTS v1.0). The analysis showed that the velocity uncertainties may be, on average, underestimated by a factor of 2 when assuming the WN-only noise model using campaign data from the UNAVCO data archive collected in San Bernardino and its vicinity. Moreover, to reveal the progress of the new approach, we modelled buried screw dislocations of the San Jacinto, the San Andreas, and parallel faults in their vicinity in elastic half-space. When using the new approach with the combination of WN and FN instead of the WN-only model, we achieved a significant improvement of 15% on average in the chi-square value. Our outcomes showed that the noise process should be considered even for campaign time series. The proposed approach facilitates the analysis of GPS campaign time series even if they include considerable data gaps.\n               ","68":"\n                  This study aims to set up a metadata profile useful for preparing an interoperable dataset containing snow and ice hyperspectral measurements. The proposed Snow and Ice Spectral Library (SISpec) scheme was prepared for sharing a data collection focused on Antarctica, including 70 observations. Following the perspective to grant \u201copen access\u201d to such a dataset, we found a compromise between the ERC (European Research Council) guidelines, the FAIR (Findability, Accessibility, Interoperability, and Reuse) Data principles defined by the RDA (Research Data Alliance), and the GEO (Group on Earth Observation) Data Sharing Principles. The ISO (International Organization for Standardization) standard 19115 was chosen as the standard framework for describing SISpec. When the available metadata scheme was not sufficient or suitable, metadata extensions or new detailed metadata components were created to be compliant with the ISO 19115 standard. We also considered the INSPIRE (Infrastructure for Spatial Information in Europe) requirements and the result is a metadata model that can be useful to share SISpec metadata both in the European and international contexts. Particularly detailed metadata sections and elements were created for describing spectral signatures and microphysical snow parameters.\n               ","69":"\n                  In this work, we investigate the capacity of Generative Adversarial Networks (GANs) in interpolating and extrapolating facies proportions in a geological dataset. The new generated realizations with unrepresented (aka. missing) proportions are assumed to belong to the same original data distribution. Specifically, we design a conditional GANs model that can drive the generated facies toward new proportions not found in the training set. The presented study includes an investigation of various training settings and model architectures. In addition, we devised new conditioning routines for an improved generation of the missing samples. The presented numerical experiments on images of binary and multiple facies showed good geological consistency as well as strong correlation with the target conditions.\n               ","70":"\n                  Most methods for automated full-bore rock core image analysis (description, colour, properties distribution, etc.) are based on separate core column analyses. The core is usually imaged in a box because of the significant amount of time taken to get an image for each core column. The work presents an innovative method and algorithm for core columns extraction from core boxes. The conditions for core boxes' imaging may differ tremendously. Such differences are disastrous for machine learning algorithms which need a large dataset describing all possible data variations. Still, such images have some standard features \u2013 a box and core. Thus, we can emulate different environments with a unique augmentation described in this work. It is called template-like augmentation (TLA). The method is described and tested on various environments, and results are compared on an algorithm trained on both \"traditional\" data and a mix of traditional and TLA data. The algorithm trained with TLA data provides better metrics and can detect core on most new images, unlike the algorithm trained on data without TLA. The algorithm for core column extraction implemented in an automated core description system speeds up the core box processing by a factor of 20.\n               ","71":"\n                  The effective identification of geochemical anomalies is essential in mineral exploration. Recently, data-driven deep learning algorithms have gained popularity for recognizing the geochemical patterns linked to mineralization. While purely data-driven deep learning algorithms can exploit geochemical patterns well, but the predicted and extracted results may be inconsistent with the geologic knowledge. In this study, a geologically-constrained deep learning algorithm was proposed to extract multivariate geochemical anomalies associated with W polymetallic mineralization in the south Jiangxi Province, China. The construction of the proposed algorithm involved two steps: (1) quantifying the spatial distribution of the known mineral deposits via fractal analysis, and (2) using prior knowledge obtained by the fractal analysis as a geological constraint to restrain an adversarial autoencoder network for delineating geochemical anomalies associated with mineralization. We conducted a comparative study of geologically-constrained and purely data-driven deep learning algorithms. We found that the former obtained more reasonable and interpretable geochemical anomalies linked to W mineralization. The results obtained by a geologically-constrained deep learning algorithm were more consistent with the regional metallogenic law. Therefore, this geological constraint can improve the generalization ability of the deep learning algorithm and enhance the interpretation of the obtained results in geosciences.\n               ","72":"","73":"\n                  This work presents APPMAR 1.0, an application written in the Python programming language that downloads, processes, and analyzes wind and wave data. This application is composed of a graphical user interface (GUI) that contains two main modules: the first module downloads data from WAVEWATCH III\u00ae (WW3) production hindcasts by the National Oceanic and Atmospheric Administration (NOAA); the second module applies statistical mathematics for processing and analyzing wave and wind data. This application provides useful graphical results that describe mean and extreme wave and wind climate. APPMAR generates plots of exceedance probability, joint probability distribution, wave direction, Weibull distribution, and storm frequency analysis. Currently, APPMAR only downloads and analyzes wave and wind data from WW3 hindcasts, but it is under development to other datasets and marine climate parameters. This application has been tested in the Magdalena River mouth, Colombia, and Canc\u00fan, M\u00e9xico, where observational wave and wind data are scarce.\n               ","74":"\n                  Fission track dating is a widely used thermochronological approach to constrain the thermal history of rocks. Conventionally this approach requires manual identification of fission tracks under the microscope which can be time-consuming and labor-intensive. In this study, we proceed with the newly developed approach to identify fission tracks on transmitted light images based on a deep learning method. In this new approach, we use a convolution neural network (CNN) to extract semi-tracks through image semantic segmentation. Considering the boundary ambiguity inherent in the CNN, we also extract the multi-scale boundary of the images in order to refine the semantic segmentation. We then calculate an area threshold of semi-tracks to determine whether semi-tracks are overlapping or not. These non-overlapping tracks are counted directly from the refined semantic segmentation images. For these overlapping tracks, we develop a boundary-superimposed method by using the refined semantic segmentation and the multi-scale boundary images with the help of the reflected-light images to split them before counting. We used 101 images of spontaneous fission tracks and 7 images of induced fission tracks for training with this new approach and tested the resulting convolutional neural networks on 114 spontaneous fission track images and 60 induced fission track images. Most of the test samples show high precision, recall, F1-score, and overall accuracy, highlighting the potential usage of this approach to identify fission tracks automatically.\n               ","75":"\n                  Pore network modeling (PNM) based on networks extracted from tomograms is a well-established tool for simulating pore-scale transport behavior in porous media. A key element of this approach is the accurate determination of pore-to-pore conductance values, which is a complex task that greatly affects the accuracy of flow and diffusive mass transport studies. Classic methods of conductance estimation based on analytical solutions and shape factors only apply to simple pore geometries, whereas real porous media contain irregular-shaped pores. Although direct numerical simulations (DNS) can accurately estimate conductance considering pores' real morphology, it has a high computational cost that becomes infeasible for large tomograms. The present work remedies this problem using a deep learning (DL) approach, with a specific focus on diffusional transport which has received less attention than hydraulic conductance. A convolutional neural network (CNN) model was trained to estimate diffusive conductance of PNM elements from volumetric images of porous media. The developed framework estimates the diffusive conductance by analyzing individual pore-to-pore 3D images isolated from the tomogram to fully capture the topology and shapes. A key outcome of the present work is that only images of the pore regions are used as input data, avoiding excessive preprocessing time for data preparation. The results of the diffusive conductance prediction show good agreement with the test data obtained by DNS method, with 0.94 \n                        \n                           \n                              R\n                              2\n                           \n                        \n                      prediction accuracy and a speedup of 500x in prediction runtime.\n               ","76":"\n                  Zircon grain shape is traditionally interpreted as a product of the physico-chemical conditions during crystal growth and may be modified during grain transport processes. The analysis of magmatic zircon grain shape has been proposed to inform on crystallization conditions, whereas detrital zircon grain shape has been proposed to complement traditional sediment provenance analysis. Shape parameters can be automatically measured from digital images of zircon mounts; however, this requires extraction of individual grain boundaries for measurement. Existing image segmentation software may require the use of proprietary languages, or knowledge of scripting to develop automated image segmentation routines, and is typically not tailored towards the geosciences. Furthermore, the separation of touching zircon grains in images remains a challenge for existing algorithms. To facilitate zircon grain shape analysis, we present AnalyZr, an open-source graphical Python application designed to segment reflected and transmitted light images of zircons mounted in resin. A new segmentation algorithm is implemented to improve the separation of touching zircon grains. Shape parameters are automatically measured from the segmented images and may be output to a .csv or .mdb file. Two case studies demonstrate the use of the application in resolving geologically relevant information in zircon grains sourced from: i) compositionally and age-distinct granite, diorite, and gabbro samples from across Western Australia, and ii) age-distinct detrital zircons from the Canning Basin, Western Australia.\n               ","77":"\n                  Morphometric characteristics play an important role in the classification and modelling of fluvial systems. With increasing numbers of detailed surveys of subaquatic canyon and channel systems in lakes, reservoirs, oceans and extra-terrestrial systems, previous research has repeatedly tried to draw parallels between poorly understood subaquatic and extra-terrestrial systems and much studied subaerial fluvial systems. Often, these studies only considered a few morphometric characteristics (e.g., bed slope, bankfull width, centreline radius in bend apices, bankfull depth), because an efficient tool for the determination of these characteristics was missing. To address this, we present a simple-to-use Matlab script for determining the most important morphometric characteristics of subaerial, subaquatic and extra-terrestrial rivers, channels and canyons. The only inputs required are the levee or bank crests that enable the definition of a centreline as the basis of a channel-centred curvilinear reference system and the calculation of planform characteristics (e.g., bankfull width, gradually varying curvature, sinuosity). If available, digital elevation data of bathymetry or topography are converted into the same channel-centred curvilinear reference system, thus enabling the determination of the longitudinal bed slope and further morphometric characteristics in cross-sectional planes (e.g., bankfull depth, cross-sectional area, and slopes of banks or levees). The script is applied to a subaquatic canyon in Lake Constance as an example. This script provides an efficient tool for analysing the ever-growing amount of digital elevation models (DEMs) in subaerial, subaquatic and extra-terrestrial rivers, channels and canyons. The reported script is particularly appropriate for the poorly understood subaquatic systems and will contribute to the understanding of the largest canyon and channel systems on our planet.\n               ","78":"\n                  Knowledge graph (KG) is a topic of great interests to geoscientists as it can be deployed throughout the data life cycle in data-intensive geoscience studies. Nevertheless, comparing with the large amounts of publications on machine learning applications in geosciences, summaries and reviews of geoscience KGs are still limited. The aim of this paper is to present a comprehensive review of KG construction and implementation in geosciences. It consists of four major parts: 1) concepts relevant to KG and approaches for KG construction, 2) KG application in data collection, curation, and service, 3) KG application in data analysis, and 4) challenges and trends of geoscience KG creation and application in the near future. For each of the first three parts, a list of concepts, exemplar studies, and best practices are summarized. Those summaries are synthesized together in the challenge and trend analyses. As artificial intelligence and data science are thriving in geosciences, we hope this review of geoscience KGs can be of value to practitioners in data-intensive geoscience studies.\n               ","79":"\n                  Urban waterlogging susceptibility assessment can be used in preventing urban waterlogging disaster, which can cause serious damage. An integrated method based on particle swarm optimization (PSO) and weakly labeled support vector machine (WELLSVM), is presented to assess urban waterlogging susceptibility for a certain rainstorm. This method incorporated twelve explanatory factors, including daily precipitation, 3-day cumulated rainfall prior to flood occurrences, elevation, slope, curvature, aspect, topographic wetness index, stream power index, distance to river, leaf area index, impervious surface percentage, distance to road, to perform waterlogging susceptibility analysis. The rainstorm on July 6, 2016 in the main districts of Wuhan, China was used as the scenario to test its feasibility. Cohen's kappa index, accuracy, precision, and recall were used to evaluate the performance of the proposed model. The accuracy of the proposed model (93.6% for training data and 90.1% for testing data) is higher than WELLSVM, support vector machine, and logistic regression, demonstrating the advantages of utilizing unlabeled data and optimized parameter selection. The proposed model can also well identify the waterlogging susceptibility zones. The highest waterlogging susceptibility areas are located in the area with high impervious surface percent, intersections, culverts and overpasses, lakeshore, and riverbank. Elevation and precipitation factors are the most influential factors to waterlogging susceptibility. The proposed model was also tested by the other two storms on July 7, 2013 and June 21, 2019, proving the validity of it. The proposed model is helpful for instant waterlogging susceptibility analysis and can help decision-making of urban waterlogging control.\n               ","80":"\n                  Three-dimensional (3D) geological models are typical data sources in 3D mineral prospectivity modeling. However, identifying prospectivity-informative predictor variables from 3D geological models is a challenging and work-intensive task. Motivated by the ability of convolutional neural networks (CNNs) to learn intrinsic features, in this paper, we present a novel method that leverages CNNs to learn 3D mineral prospectivity from 3D geological models. By exploiting this learning ability, the proposed method simplifies the complex correlations of mineralization and circumvent the need for designing the predictor variables. Specifically, to analyze unstructured 3D geological models using CNNs\u2014whose inputs should be structured\u2014we develop a 2D CNN framework where the geometry of geological boundary is compiled and reorganized into multi-channel images and fed into the CNN. This ensures the effective and efficient training of the CNN while facilitating the representation of mineralization control. The presented method is applied to a typical structure-controlled hydrothermal deposit, the Dayingezhuang gold deposit in eastern China; the presented method is compared with prospectivity modeling methods using designed predictor variables. The results show that the presented method has a performance boost in terms of the 3D prospectivity modeling and decreases the workload and prospecting risk in the prediction of deep-seated orebodies.\n               ","81":"\n                  Deep learning has been successfully applied to precipitation nowcasting. In this work, we propose a pre-training scheme and a new loss function for improving deep-learning-based nowcasting. First, we adapt U-Net, a widely-used deep-learning model, for the two problems of interest here: precipitation nowcasting and precipitation estimation from radar images. We formulate the former as a classification problem with three precipitation intervals and the latter as a regression problem. For these tasks, we propose to pre-train the model to predict radar images in the near future without requiring ground-truth precipitation, and we also propose the use of a new loss function for fine-tuning to mitigate the class imbalance problem. We demonstrate the effectiveness of our approach using radar images and precipitation datasets collected from South Korea over seven years. It is highlighted that our pre-training scheme and new loss function improve the critical success index (CSI) of nowcasting of heavy rainfall (at least 10 mm\/hr) by up to 95.7% and 43.6%, respectively, at a 5-hr lead time. We also demonstrate that our approach reduces the precipitation estimation error by up to 10.7%, compared to the conventional approach, for light rainfall (between 1 and 10 mm\/hr). Lastly, we report the sensitivity of our approach to different resolutions and a detailed analysis of four cases of heavy rainfall.\n               ","82":"\n                  The supervised deep learning methods applied in mineral prospectivity mapping usually need sufficient samples for training models. However, mineralization is a rare event. Insufficient known mineral deposits cannot meet the sample requirement of supervised learning methods, resulting in lower predictive accuracies and poor generalization abilities. For the purpose of solving this issue, this paper adopted a data augmentation method to make mineral prospectivity prediction of gold deposit in the Fengxian Region, China. This data augmentation method adopted cropping operations to generate sufficient training samples without changing spatial directions of geological data. Meanwhile, this paper utilized the continuous buffer distance method to quantify faults and anticline axes, overcoming the loss of geological information caused by using the discrete buffer distance mode. To prove the effectiveness of the data augmentation method, this paper utilized three different convolutional neural networks (LeNet, AlexNet, and VggNet) to extract relationships between multisource ore-indicating factors and mineral deposits. In addition, this paper discussed effects of different parameters on predictive performances. According to series of comparisons, the LeNet model outperformed other models, achieving superior values of accuracy (91.38%), Kappa coefficient (0.8119), and AUC (0.958). Moreover, the LeNet model successfully caught 81.8% of known gold deposits within 18.6% of the study area. The delineated high potential areas offer intuitive guides for exploring more gold deposits in the Fengxian region. The proposed data augmentation method is available for mineral prospectivity modeling by supervised deep learning methods for the areas of lower exploration degrees. For mineral prospectivity modeling based on convolutional neural networks, utilizing the continuous buffer distance to transform faults and anticline axes into predictor variables of the image form is conducive to improve the predictive performance than utilizing the discrete buffer distance.\n               ","83":"\n                  The field development workflow contains numerous tasks involving decision-making processes. The modern machine learning methods, including automatic machine learning (AutoML), reduce the geophysics or machine learning experts\u2019 time required to solve routine tasks. In the paper, we focus on the automated solution of the location of the wells optimization problem, namely, improving the quality of oil production estimation and estimating reservoir characteristics for appropriate wells placement and parametrization, using the same AutoML approach. Ideas of making several parallel or consequent tasks automatically within one framework are arising as Composite AI. We implemented and investigated the quality of forecasting models for oil production estimation: physics equation-based, pure data-driven, and hybrid. CRMIP (Capacitance\u2013Resistance Model Injector\u2013Producer) model is chosen as a physics-related approach. We automated the seismic analysis using evolutionary identification of convolutional neural network structure for reservoir detection to help investigate reservoir characteristics for wells location choice. The Volve oil field dataset was used as a case study to conduct the experiments. The implemented approaches can analyze different oil fields and even be adapted to similar physics-related problems.\n               ","84":"\n                  Participatory landslide inventory mapping using the Volunteered Geographic Information (VGI) mobile app is a promising method to produce a landslide inventory map. The aim of this research is to describe the development and implementation of the VGI mobile app for participatory landslide inventory mapping. The architecture VGI mobile app is developed on the basis of Free Open-source Software for Geospatial Application server-client software to ensure reproducibility and flexibility, and to reduce cost. Anyone can reproduce, modify, and share the code, which suggests improvement in the collective ability to use, prepare, and landslide inventory update. Landslide inventory using VGI mobile app shows that the tool and method successfully map landslides in the landslide prone area (Magelang Regency, Central Java Province, Indonesia) with fairly high levels of effectiveness and convenience. Magelang Regency, one of the landslide prone areas in Java, is located in the intermountain basin surrounded by Menoreh Mountain, Merapi, Merbabu, Suropati-Telomoyo Complex, and Sumbing Volcano. In this study, landslide inventory mapping using VGI mobile app was applied in Magelang Regency by 17 volunteers from BPBD (Regional Agency for Disaster Management) Magelang Regency for three days. Landslides area occurred from 2017 to 2019 were properly identified and mapped by the volunteers. The sizes of landslides varied from 5.2\u00a0m2 to 4,632.5\u00a0m2, and the average was 208.2\u00a0m2. A team of volunteer was able to map 7-10 landslides per day. Participatory mapping using VGI mobile app reduces the time in transferring field data to a GIS database, in contrast to conventional participatory landslide inventory mapping. VGI mobile app allows users to provide new geographical landslide data, share landslide data rapidly, ensure consistency of landslide data, and improve accessibility of landslide data. The use of the VGI mobile app for participatory landslide inventory mapping provides new opportunities to improve risk assessment, preparedness, and early action and warning to landslide hazard.\n               ","85":"\n                  Headland-bay beaches (HBBs) are ubiquitous on Earth and many coastal scientists and engineers have attempted to define their planform since the 1940s, by employing laboratory tests, analytical tools, numerical models, and empirical equations. One of the latest attempts in numerical modeling is the development of a MATLAB-based GUI software called \u201cMeePaSoL\u201d to facilitate the application of the empirical parabolic model for HBBs. This visualization package can be applied to determine the predominant wave direction to the shoreline planform of a HBB in static equilibrium, and to minimize the uncertainty in locating downdrift control point while using other software. The background concepts (e.g., parabolic model, equilibrium beach and wave orthogonality hypothesis) are introduced, and the operation procedure with stepwise guides for applying MeePaSoL are given to assist the users. The reliability of MeePaSoL is validated by comparing the wave direction given by this package with that by the wave energy flux approach derived from field data. Examples of engineering applications are then demonstrated, covering local to regional wave direction, the effect of jetty\/breakwater length and direction, harbor siltation in a macro-tidal environment, and devising engineering works for shoreline management in Korea. Sensitivity and uncertainty are also discussed.\n               ","86":"\n                  In a previous paper we compared observations to calculated values of the sunrise times for one place in Jerusalem. It was shown that the sunrise over the actual physical horizon could be modeled to an accuracy of about \u00b115\u00a0s for most of the year. Success of those calculations suggested that terrain modeling was the most important factor in obtaining this result. To investigate this point, we repeated our calculations using the ray tracing method and simplified atmosphere developed by Siebren Van der Werf, whose atmospheric model is easily applied to other places in the world. Our results show that the new calculation method maintained our requirement of \u00b115\u00a0s accuracy (required for publication of sunrise time tables, critical for daily Jewish observances worldwide). Standard formulas for atmospheric refraction produce approximately the same results even for near horizon zenith angles. For portions of the year where low altitude inversions are especially important, e.g., during those days associated with maximum radiative cooling, these additional atmospheric effects can be effectively modeled in a very simple way.\n               ","87":"\n                  A variational Gaussian process (VGP) model specialized in spatial data is introduced, capitalizing on recent advances in the machine learning field. The model is modular and customizable, being capable of handling different assumptions about the data. This work focusses on multivariate robust regression, using an adaptation of the \n                        \u03b5\n                     -insensitive loss function. The VGP possibilitates end-to-end modeling: normal score transform, detection of the spatial pattern, and prediction. A methodology to deal with large datasets is presented. An open-source implementation is available.\n               ","88":"\n                  Most deterministic full waveform inversion (FWI) approaches require an initial model, which serves as a starting point for the inversion. Determining such an initial model, however, is not straight forward. In this paper, a geostatistical inversion approach called Random Mixing (RM) for FWI is presented. Instead of an initial model, RM requires the assumption of a spatial dependence structure and a univariate marginal distribution. RM uses linear combinations of spatial random fields to constrain the velocity field during inversion. As it is realization-based, RM allows quantification of estimation uncertainty in terms of estimation variance. The presented algorithm uses the finite element method to discretize the seismic forward model and Message Passing Interface is used to parallelize the algorithm. Two synthetic examples are presented to demonstrate the applicability of RM for FWI.\n               ","89":"\n                  We investigate unsupervised time-series analysis techniques for real-time identification of magnetic field boundaries in data collected by the Cassini Plasma Spectrometer (CAPS) on the Cassini mission to Saturn. Previous research has sought to identify these boundaries either in a supervised setting, where a subset of labeled boundary crossing events are available to the algorithms, or using magnetometer data in which these boundaries are significantly clearer. We find that Saturn bow shock transitions can be reliably detected by the methods we consider, while magnetopause transitions are harder to identify across different years, indicating that generalization of algorithm parameters remains a challenge. We identify the Plasma Instrument for Magnetic Sounding (PIMS) on the upcoming Europa Clipper Mission as a promising beneficiary of the use of similar processing methods. By performing onboard detection of events such as Jovian plasma sheet crossings, Europa ionosphere boundaries, and water vapor plumes, Europa Clipper could benefit from in-situ adaptation to increase science return and data quality, such as by changing the instrument\u2019s observing mode depending on the spacecraft\u2019s current environment.\n               ","90":"","91":"\n                  This study compared the performance of five variogram-based calibration approaches for five different geological problems. The geostatistical calibration methods studied were the sequential spectral turning band method (S-STBM), gradual deformation (GD), iterative spatial resampling (ISR), phase annealing (PA), and fast Fourier transform moving average simulated annealing (FFTMA-SA). The first two problems aimed to produce continuous and categorical simulations with known theoretical distributions. The other two problems were hydrogeological and aimed to calibrate the conductivity field to the pressure heads and travel time between wells. The final problem sought to generate non-Gaussian fields that exhibit spatial directional asymmetry. Two methods, S-STBM and FFTMA-SA, obtained good calibration results for all five problems, with S-STBM being the best overall, particularly for the categorical scenarios. The other methods, GD, ISR, and PA, exhibited a more variable performance. ISR did not properly calibrate simulations for the majority of the problems owing to slow convergence. PA adequately calibrated the five problems, but for the first two, the statistical distributions of the calibrated realizations departed significantly from the theoretical distributions. Similarly, GD exhibited slow convergence for the second problem, which resulted in significant differences compared with the known theoretical distributions. Because the S-STBM constructs the calibrated field directly, it avoids the difficulty encountered by the other calibration methods of having to begin from an unfavorable initial state. The examples clearly illustrate that the choice of the calibration algorithm is significant for some inversion problems.\n               ","92":"\n                  The image labeling task of remote sensing image scene classification (RSSC) is based on the semantic content of remote sensing images. The semantic information within remote sensing photographs has become more complicated and difficult to detect as remote sensing technology has progressed. As a result, extracting more important semantic elements could aid in the completion of the RSSC assignment. Thus, in this research, we offer MLFC-Net, a multi-level semantic feature clustering attention model based on deep convolution neural networks (DCNNs) that extracts more accurate feature information. The concept of MLFC-Net stems from the utilization of rich spatial information found in remote sensing photos, but few approaches in the RSSC application considered merging general semantic feature information with clustered semantic feature information. By rearranging the weight of corresponding information, such as feature maps and tensor blocks of the feature map, we implemented the attention mechanism. To build a model with minimal computational cost and good portability, we use a channel-wise attention mechanism and an ensemble structure. We were able to improve the representation of several critical semantic aspects using the MLFC model. In the EuroSAT, UCM, and NWPU-RESISC45 RSSC datasets, the MLFC model's performance is demonstrated. And, on average, the MLFC model enhanced accuracy by 2.56 percent, 1.25 percent and 2.00 percent, respectively, producing results that were equivalent to the state-of-the-art.\n               ","93":"\n                  The manual identification and count of laminae in layered textures is a common practice in the study of geological records, which can be time consuming and carry large uncertainty for dense or disturbed lamina textures. We present here a novel image analysis approach to detect and count laminae in geoscientific imagery, called WlCount. Based on Dynamic Time Warping and Wavelet analysis, WlCount firstly aligns persistent vertical elements to increase the continuity of the lamina structure. Then, using a graphical interface, the user extracts the most significant signal frequencies and allows the automatic count of the laminae. The software, tested on a series of stalagmite cut images showing different types of laminations and a tree-ring image, provides an estimation of the laminae detection and count comparable to the manual one. WlCount presents as a useful open-source tool to help geoscientists, sensibly speeding up the lamination count process.\n               ","94":"\n                  Traditional methodologies of flood monitoring are generally time-consuming and demanding tasks. In most cases, there is no possibility of flood monitoring in large areas. Due to the major drawbacks of conventional methods in flood detection of large districts, Remote Sensing (RS) has been efficiently employed as the best solution owing to its being synoptic view and cost-effective methodologies. One of the most challenging issues in RS technologies is choosing the optimal spectral bands to detect changes in the natural environment. In this research, Elimination and Choice Expressing Reality (ELECTRE), as one of the most widely used Multi-Criteria Decision Making (MCDM) techniques, was applied to select the optimal bands of Sentinel-2 satellite images for detection of flood-affected areas. For this purpose, the decision-making method was implemented during ten options and six criteria. The properties of the Sentinel-2 satellite images consisted of ten bands (with 10 and 20m spatial resolutions) and the criteria are the signal to noise ratio (SNR) related to sensor, standard deviation, variance, the SNR related to the bands, spatial resolution, and wavelength. Afterward, the ELECTRE technique was used to select six optimal bands among ten bands. The ELECTRE algorithm was programmed in MATLAB programming language that could make decisions with multiple options and multiple criteria. Furthermore, the Support Vector Machine (SVM) classification method, as one of the most powerful Machine Learning (ML) models, has been applied to classify the water bodies related to before and after the flood. According to the results of optimal bands classification, Overall Accuracy (OA) and Kappa Coefficient (KC) for the pre-flood classification were 93.65 percent and 0.923, respectively, and for the post-flood classification, the OA and KC values were 94.52 percent and 0.935 respectively. In the case of before and after flooding, the results of classification model for optimal bands had more accuracy levels in comparison with those obtained by original bands. Generally, it was found that the ELECTRE technique for selecting the best bands of Sentinel-2 satellite images and detection of flood-affected areas, in a short period of time with high accuracy, offers remarkable and consistent results.\n               ","95":"\n                  We propose a method for detecting Martian dust storms and recognizing their size and shape on remote sensing images. The method is based on a convolutional neural network, one of algorithms that use deep learning for image categorization and recognition. We trained models with three different structures using images of two regions of Mars in visible wavelengths observed over several seasons, together with ground truth images manually prepared by the authors that give the true shapes of the dust storms. The two regions were the western Arcadia Planitia in the northern hemisphere and the Hellas Basin in the southern hemisphere, both of which are areas where high dust storm activity has been observed. The case study showed that models trained on images of the Arcadia Planitia tended to perform better than comparable models trained by images of the Hellas Basin. While third models trained by images of both regions showed little degradation relative to the dedicated models when tested on image of the Arcadia Planitia, their performances clearly decreased in the case of the Hellas Basin. Furthermore, the performance degradation was more pronounced for a model with moderate depth than for a deepest model. This is partially because the Hellas Basin is brighter than the adjacent areas throughout the year and high optical thickness of dust in its interior makes the textures of dust storms relatively unclear. In contrast, any models showed comparable performances in dust storm segmentation in the Arcadia Planitia and mixing data from the two regions with completely different surface patterns produced only a slight degradation of performance. It suggests that training the model with images from various regions may yield a region-independent model that can be effectively applied to the segmentation of dust storms over a wide area.\n               ","96":"\n                  We present a method for obtaining efficient probabilistic solutions to geostatistical and linear inverse problems in spherical geometry. Our Spherical Direct Sequential Simulation (SDSSIM) framework combines information from possibly noisy observations, that provide either point information on the model or are related to the model by a linear averaging kernel, and statistics derived from a-priori information. It generates realizations from marginal posterior probability distributions of model parameters that are not limited to be Gaussian. We avoid the restriction to Cartesian geometry built into many existing geostatistical simulation codes, and work instead with grids in spherical geometry relevant to problems in Earth and Space sciences.\n                  We demonstrate our scheme using a synthetic example, showing that it produces realistic posterior realizations consistent with the known solution while fitting observations within their uncertainty and reproducing the distribution of model parameters and covariance statistics of a-priori models. Secondly, we present an application to real satellite observations, estimating the posterior probability distribution for the geomagnetic field at the core\u2013mantle boundary. Our results reproduce well-known features of the core\u2013mantle boundary magnetic field, and also allow probabilistic investigations of the magnetic field morphology. Small-length scale features in the posterior realizations are not determined by the observations but match the covariance statistics extracted from geodynamo simulations. The framework presented here represents a step towards more general approaches to probabilistic inversion in spherical geometry.\n               ","97":"\n                  Determination of pore space characteristics is important for carbonate reservoir interpretation and evaluation. Field outcrops can reflect the geology of the subsurface reservoir. The most common method of traditional outcrop research is field investigation, which is likely to cause human errors. Digital image recognition of geological outcrop areas can reduce the problem of inaccurate descriptions of geological structures caused by human factors. Automatic extraction based on convolutional neural networks can greatly reduce the amount of such work. By enhancing the deep-learning model of a convolution neural network, the mask region-convolutional neural network (Mask R-CNN) is proposed. This method adapts to the multiscale features of the cavity by manipulating the scales of the input image. To verify the applicability of the model, the method is applied to automatic cavity identification in the digital outcrop profile of the Dengying Formation (2nd Member) in Xianfeng, Ebian. The parameters are calculated layer by layer, and their distribution characteristics are quantitatively analysed, indicating good application results. To verify the advanced nature of the model, the cavity extraction results of this method are compared with those of traditional image segmentation, machine learning and other depth learning methods. Precision analysis shows that the performance of the method proposed in this paper is superior to the traditional methods. In addition, the cavity feature parameters extracted by this method are compared with the manual extraction. The accuracy for the cavity number, surface porosity, and average cavity area is over 80%, 88%, and 72%, respectively for the proposed method. The results show that the proposed method is reliable and accurate, hence it will help to provide a basis for reservoir prediction in the process of regional geological exploration.\n               ","98":"\n                  This paper describes the application of acceleration techniques into existing implementations of Sequential Gaussian Simulation and Sequential Indicator Simulation. These implementations might incorporate Locally Varying Anisotropy (LVA) to capture non-linear features of the underlying physical phenomena. The implementation focuses on a novel parallel neighbour search algorithm, which can be used on both non-LVA and LVA codes. Additionally, parallel shortest path executions and optimized linear algebra libraries are applied with focus on LVA codes. Execution time, speedup and accuracy results are presented. Non-LVA codes are benchmarked using two scenarios with approximately 50 million domain points each. Speedup results of \n                        \n                           2\n                           \u00d7\n                        \n                      and \n                        \n                           4\n                           \u00d7\n                        \n                      were obtained on SGS and SISIM respectively, where each scenario is compared against a baseline code published in Peredo et\u00a0al. (2018). The aggregated contribution to speedup of both works results in \n                        \n                           12\n                           \u00d7\n                        \n                      and \n                        \n                           50\n                           \u00d7\n                        \n                      respectively. LVA codes are benchmarked using two scenarios with approximately 1.7 million domain points each. Speedup results of \n                        \n                           56\n                           \u00d7\n                        \n                      and \n                        \n                           1822\n                           \u00d7\n                        \n                      were obtained on SGS and SISIM respectively, where each scenario is compared against the original baseline sequential codes.\n               ","99":"\n                  We present a routine for 3D magnetotelluric (MT) modeling based upon high-order edge finite element method (HEFEM), tailored and unstructured tetrahedral meshes, and high-performance computing (HPC). This implementation extends the PETGEM modeller capabilities, initially developed for active-source electromagnetic methods in frequency-domain. We assess the accuracy, robustness, and performance of the code using a set of reference models developed by the MT community in well-known reported workshops. The scale and geological properties of these 3D MT setups are challenging, making them ideal for addressing a rigorous validation. Our numerical assessment proves that this new algorithm can produce the expected solutions for arbitrarily 3D MT models. Also, our extensive experimental results reveal four main insights: (1) high-order discretizations in conjunction with tailored meshes can offer excellent accuracy; (2) a rigorous mesh design based on the skin-depth principle can be beneficial for the solution of the 3D MT problem in terms of numerical accuracy and run-time; (3) high-order polynomial basis functions achieve better speed-up and parallel efficiency ratios than low-order polynomial basis functions on cutting-edge HPC platforms; (4) a triple helix approach based on HEFEM, tailored meshes, and HPC can be extremely competitive for the solution of realistic and complex 3D MT models and geophysical electromagnetics in general.\n               ","100":"\n                  gTOOLS is an open-source software for the processing of relative gravity data. gTOOLS is available in MATLAB and as a compiled executable to be run under the free MATLAB Runtime Compiler. The software has been designed for time-lapse (temporal) gravity monitoring. Although programmed to read the Scintrex CG-5 and CG-6 gravimeters output data files, it can be easily modified to read data files from other gravimeters. The software binds together single-task processing modules within a very simple user interface that is based on one text file. Gravity processing involves three modules: (a) gravimeter calibration; (b) automatic processing of gravity data to find adjusted gravity differences; and (c) post processing of results. Each module is optional and runs independently from the others. Data processing includes (a) averaging out the measurements noise, and correction for solid Earth tides, and ocean loading, and residual instrumental drift, and (b) calculate the residual instrumental drift and gravity differences between the base station and monitoring sites, and their uncertainties, by a weighted least square analysis of the gravity data. The software allows the automatic processing of a gravity campaign spanning multiple days in a single run. The software is tested on gravity data from 2015 eruption at Cotopaxi volcano, Ecuador.\n               ","101":"\n                  The Bay of Bengal (BoB) fosters several monsoon depressions and cyclones, playing a crucial role in the Asian summer and winter monsoons. The capacity of the bay to remain warm and energize such weather systems is attributed to its strong vertical stratification sustained by the large freshwater input into the bay. River runoff and rainfall into the northern bay in contrast to the high salinity water intrusion in the south creates a strong north\u2013south salinity gradient. Here, we present a visual analysis tool to trace the path of the high salinity core (HSC) entering into the BoB from the Arabian Sea. We introduce two feature definitions that represent the movement and shape of the HSC, and algorithms to track their evolution over time. The two feature representations, namely fronts and skeletons, are based on geometric and topological analysis of the HSC. The method is validated via comparison with well established observations on the flow of the HSC in the BoB, including its entry from the Arabian Sea and its movement near Sri Lanka. Further, the visual analysis and tracking framework enable new detailed observations on forking behavior near the center of the BoB and subsequent northward movement of the HSC. The tools that we have developed offer new perspectives on the propagation of high salinity water and its mixing with the ambient low salinity waters.\n               ","102":"\n                  The standard smooth electrical resistivity tomography inversion produces an estimate of subsurface conductivity that has blurred boundaries, damped magnitudes, and often contains inversion artifacts. In many problems the expected conductivity structure is well constrained in some parts of the subsurface, but incorporating prior information in the inversion is not a trivial task. In this study we developed an electrical resistivity tomography inversion algorithm that combines parametric and smooth inversion strategies. In regions where the subsurface is well constrained, the model was parameterized with only a few variables, while the rest of the subsurface was parameterized with voxels. We tested this hybrid inversion strategy on two synthetic models that contained a well constrained highly resistive or conductive near-surface horizontal layer and a target beneath. In each testing scenario, the hybrid inversion improved resolution of feature boundaries and magnitudes and had fewer inversion artifacts than the standard smooth inversion. A sensitivity analysis showed that the hybrid inversion successfully recovered subsurface features when a range of regularization parameters, initial models, and data noise levels were tested. The hybrid inversion strategy can potentially be expanded to a range of applications including marine surveys, permafrost\/frozen ground studies, urban geophysics, or anywhere that prior information allows part of the model to be constrained with simple geometric shapes.\n               ","103":"\n                  Inverse modeling of aquifer measurements aims at estimating the spatial distribution of rock properties that best fits the available measurements which, in turn, allows predicting water yields and pressures to an acceptable level of accuracy. Nonlinear automatic inverse methods require multiple solutions of the diffusion equation, also known as the groundwater flow equation. Although numerical solutions to the forward problems improve the understanding of complex groundwater flow systems, they are often computationally expensive, thereby rendering the inversion process inefficient. We introduce a gradient-based inversion algorithm that leverages perturbation theory to efficiently estimate the tensorial intrinsic permeability distribution of porous media from single-phase transient pressure measurements. With a maximum of two numerical simulations, the Joint Perturbation-Superposition (JPS) method allows one to compute flow-history-dependent Intrinsic permeability Sensitivity Functions (PSF) on the spatial-temporal domain. Regardless of the gradient-based iterative inversion technique, our method efficiently adapts the sensitivity functions yielded by perturbation theory to calculate the entries of the associated Jacobian matrix at every iteration. A two-dimensional synthetic model comprising multi-well conditions is examined for an anisotropic and spatially heterogeneous groundwater flow system. The perturbation method yields accurate and efficient parameter estimation by leveraging sensitivity functions for (1) the appropriate selection of input transient pressure measurements and (2) reducing the sequential calculation of Jacobian matrices. As a result, Monte Carlo uncertainties as low as <5% are obtained for the estimated tensorial intrinsic permeability distribution, with inversion times halved when compared to conventional techniques.\n               ","104":"\n                  Magnetotelluric (MT) forward modeling often requires the consideration of the deviation generated by anisotropic structures to avoid misleading the detection results, which is implemented by the flexible unstructured finite-element (FE) method based on tetrahedrons. However, the unstructured FE method needs a large number of small elements near the sharp boundary of the electrical structure to produce sufficiently stable grids, and this leads to a sharp increase in the degrees of freedom (DoFs) of the FE system. To this end, we develop an FE approach based on a hybrid grid. It uses the stretched prismatic elements to divide the regions around the abrupt interfaces, which can significantly reduce the number of elements needed to capture the corresponding changes of the physical fields compared to using tetrahedrons. The remaining regions are still divided with tetrahedrons in this hybrid grid in order to keep the total elements needed for the entire computational region at a minimum. By showing numerical examples of the sea- and land-based electrical anisotropy models, as well as a real anisotropic inversion model, the advantages of our method are tested by evaluating the MT response curves, the number of elements, the DoFs, the computational time, and the consumed memory. The results show that the prismatic elements are suitable for discretizing the near-surface region, seawater layer, and electrical anisotropic blocks. This approach can maintain the high accuracy of the numerical solution and reduce the number of elements required for discretizing these regions and the DoFs, thus requiring less computer memory. With this hybrid grid, the computational efficiency of the FE method can be improved for MT forward modeling for a complicated model, which combined with the lower memory consumption is very suitable to implement inversion.\n               ","105":"\n                  In recent years, Earth system sciences are urgently calling for innovation on improving accuracy, enhancing model intelligence level, scaling up operation, and reducing costs in many subdomains amid the exponentially accumulated datasets and the promising artificial intelligence (AI) revolution in computer science. This paper presents work led by the NASA Earth Science Data Systems Working Groups and ESIP machine learning cluster to give a comprehensive overview of AI in Earth sciences. It holistically introduces the current status, technology, use cases, challenges, and opportunities, and provides all the levels of AI practitioners in geosciences with an overall big picture and to \u201cblow away the fog to get a clearer vision\u201d about the future development of Earth AI. The paper covers all the majorspheres in the Earth system and investigates representative AI research in each domain. Widely used AI algorithms and computing cyberinfrastructure are briefly introduced. The mandatory steps in a typical workflow of specializing AI to solve Earth scientific problems are decomposed and analyzed. Eventually, it concludes with the grand challenges and reveals the opportunities to give some guidance and pre-warnings on allocating resources wisely to achieve the ambitious Earth AI goals in the future.\n               ","106":"\n                  This study compares the performance of several machine learning algorithms in reproducing the spatial and temporal outputs of the process-based, hydrological model, ParFlow.CLM. Emulators or surrogate models are often used to reduce complexity and simulation times of complex models, and have typically been applied to evaluate parameter sensitivity or for model parameter tuning, without explicit treatment of variation resulting from spatially explicit inputs to the model. Here we present a case study in which we evaluate candidate machine learning algorithms for suitability emulating model outputs given spatially explicit inputs. We find that among random forest, gaussian process, k-nearest neighbors, and deep neural networks, the random forest algorithm performs the best on small training sets, is not as sensitive to hyperparameters chosen for the machine learning model, and can be trained quickly. Although deep neural networks were hypothesized to be able to better capture the potential nonlinear interactions in ParFlow.CLM, they also required more training data and much more refined tuning of hyperparameters to achieve the potential benefits of the algorithm.\n               ","107":"\n                  This paper presents the design of a GeoSPARQL query processing solution for scientific raster array data, called GeoLD. The solution enables the implementation of SPARQL endpoints on top of OGC standard Web Coverage Processing Services (WCPS). Thus, the semantic querying of scientific raster data is supported without the need of specific raster array functions in the language. To achieve this, first Coverage to RDF mapping solutions were defined, based on the well-known W3C standard mappings for relational data. Next, the SPARQL algebra is extended with a new operator that delegates part of the GeoSPARQL query in WCPS services. Query optimization replaces those parts of the SPARQL query plan that may be delegated to a WCPS service by instances of such new WCPS operator. A first prototype has been implemented by extending the ARQ SPARQL query engine of Apache Jena. Petascope was used as the WCPS implementation on top of the Rasdaman raster array database. An initial evaluation with real meteorological data shows, as it was initially expected, that the approach outperforms an existing reference relational database based GeoSPARQL implementation.\n               ","108":"\n                  Tsunami simulation software is a key component of state-of-the-art early warning systems but the inherent complexities in phases of installation, execution, pre and post-processing prevent their use in other areas of risk management such as communication and education. Recent advances in software and computational capacities such as the efficiency of GPU computing and the ubiquity of web browsers bring new opportunities to bridge the gap between expert and non-expert users. Here we present a Javascript library to enable a web browser to facilitate gathering and analyzing data from tsunami simulations, by means of interactive and efficient visualizations. At its core, the library uses WebGL, the browser\u2019s standard 3D graphics API, to run GPU accelerated computations of a tsunami model. A far-field tsunami model is implemented (linear shallow water equations discretized on spherical coordinates), and its implementation is validated against real tsunami observations, and benchmarked with two other tsunami software-packages. Two software platforms that use this library are presented to illustrate the powerful applications that can be developed for risk communication and education. These applications are characterized by their interactivity and fast computations, which enable users to focus on the understanding of the phenomena of tsunami propagation and iterate quickly to assess different scenarios and potential implications to tsunami risk management. Some limitations on this approach are discussed, in aspects such as scalability, performance, multi-threading and batch-processing, that can be relevant for other users. In our experience, the before mentioned benefits very well compensate the discussed limitations for this kind of applications. The library has an open source license, and is meant to be imported without modifying its source code to facilitate the creation of new applications as the ones herein presented.\n               ","109":"\n                  Adjoint tomography, a full-waveform inversion technique based on 3D wave simulations, has become a commonly used tool in passive-source seismology, drawing on advances in computational power and numerical methods. From global to reservoir scales, seismic models can iteratively be updated in adjoint inversions by extracting information from full seismic waveforms. Seismic models are typically constructed on the numerical mesh used for wave simulations. Thus the size of model files depends on the minimum resolvable period achieved in simulations controlled by the numerical mesh (the higher the mesh resolution, the larger the model and mesh files). This is specifically a concern for recent global-scale adjoint tomographic models where the size and format of numerical meshes pose challenges for model visualization, analysis, interpretation, and sharing model files. Here, we present SphGLLTools, an open-source toolbox that intends to diminish these challenges by expanding global adjoint models onto spherical harmonic functions, which are widely used in global seismology. Our tools are initially designed for spectral-element meshes used in recent global adjoint tomography studies. SphGLLTools facilitate many commonplace tasks for model visualization and analysis, including spherical harmonic expansion of models sampled on spectral-element meshes together with associated tools for easy sharing, visualization, and interpretation of large-scale seismic model files. All the developed routines are accompanied by user instructions and are available through GitHub. For transparency, reproducibility, and educational purposes, we also include Colab notebooks, which provide an intuitive and comprehensive review of the principles and methods for spectral-element meshes, spherical harmonic expansion, and other model analysis tools.\n               ","110":"\n                  Diverse observational and simulation datasets are needed to understand and predict complex ecosystem behavior over seasonal to decadal and century time-scales. Integration of these datasets poses a major barrier towards advancing environmental science, particularly due to differences in the structure and formats of data provided by various sources. Here, we describe BASIN-3D (Broker for Assimilation, Synthesis and Integration of eNvironmental Diverse, Distributed Datasets), a data integration framework designed to dynamically retrieve and transform heterogeneous data from different sources into a common format to provide an integrated view. BASIN-3D enables users to adopt a standardized approach for data retrieval and avoid customizations for the data type or source. We demonstrate the value of BASIN-3D with two use cases that require integration of data from regional to watershed spatial scales. The first application uses the BASIN-3D Python library to integrate time-series hydrological and meteorological data to provide standardized inputs to analytical and machine learning codes in order to predict the impacts of hydrological disturbances on large river corridors of the United States. The second application uses the BASIN-3D Django framework to integrate diverse time-series data in a mountainous watershed in East River, Colorado, United States to enable scientific researchers to explore and download data through an interactive web portal. Thus, BASIN-3D can be used to support data integration for both web-based tools, as well as data analytics using Python scripting and extensions like Jupyter notebooks. The framework is expected to be transferable to and useful for many other field and modeling studies.\n               ","111":"\n                  In this work, we propose a local updating method to test different contact depth scenarios and assess their impact on wave propagation in the subsurface. We propose to locally modify a 2D geological model and run time-dependent elastic simulations. The input model triangulation is conforming to geological structures. The 2D meshed model is locally updated, which means that only the reservoir compartment is modified. Several model geometries are generated by inserting a new interface, in this paper a gas\u2013water contact that is defined by a scalar field. We quantitatively evaluate the impact of the gas\u2013water contact depth on elastic wave propagation. We run the numerical simulations with Hou10ni2D code, which is based on a Discontinuous Galerkin method. The simulation results are compared to a reference depth by computing the L2-norm at a set of seismic receivers. Results show a consistent behavior: we observe a positive correlation between the depth difference and global L2-norm for all receivers. This approach could therefore be integrated into an inversion loop to determine the position of the fluid contact and reduce uncertainties in the reservoir model from a few seismic sensors. The algorithms are available on Github and distributed under a GPL license, allowing reproducibility.\n               ","112":"\n                  Determination of the shear damage zones of rock joints leads to a better understanding of shear mechanical behavior. Here, we propose the SDZM toolbox (Shear Damage Zones Measurement Toolbox for Rock Joint), easy-to-use Python-based software for measuring shear damage zones of post-shearing rock joints using photos collected by digital cameras. Images of post-shearing specimens can be loaded, cropped, and scaled before analysis, and regions of interest (ROIs) can be drawn interactively to mask the shear damage based on its distribution. Then, shear damage analysis is performed based on an image segmentation algorithm to analyze the image according to the ROIs, and a variety of postprocessing tools allow users to view, filter, save and export analysis results visually. Eight specimens of three different lithologies were analyzed using the SDZM toolbox, and two traditional methods to verify the performance. The results show that the SDZM toolbox is more fast, precise and fine in analyzing the shear damage zones, saves 88.80% of the time compared to the manual method, and has higher accuracy and adaptability relative to the Riss method. In addition, suggestions for parameter selection of the postprocessing have been studied and the robustness of the SDZM toolbox has been demonstrated.\n               ","113":"\n                  Twitter is perhaps the social media more amenable for research. It requires only a few steps to obtain information, and there are plenty of libraries that can help in this regard. Nonetheless, knowing whether a particular event is expressed on Twitter is a challenging task that requires a considerable collection of tweets. This proposal aims to facilitate, to a researcher interested, the process of mining events on Twitter by opening a collection of processed information taken from Twitter since December 2015. The events could be related to natural disasters, health issues, and people\u2019s mobility, among other studies that can be pursued with the library proposed. Different applications are presented in this contribution to illustrate the library\u2019s capabilities: an exploratory analysis of the topics discovered in tweets, a study on similarity among dialects of the Spanish language, and a mobility report on different countries. In summary, the Python library presented is applied to different domains and retrieves a plethora of information in terms of frequencies by day of words and bi-grams of words for Arabic, English, Spanish, and Russian languages. As well as mobility information related to the number of travels among locations for more than 200 countries or territories.\n               ","114":"","115":"\n                  The assignment of flow directions in flat regions needs to be treated with special algorithms to obtain hydrologically correct flow patterns. Based on the sequential algorithm and the three-step parallel framework, we propose a parallel algorithm for assigning the hydrologically correct flow directions in flat regions. The proposed parallel algorithm assigns pre-divided tiles to multiple consumer processes, which construct local graphs that encode the geodesic distance information among tile border flat cells and higher terrain or lower terrain. Based on the local graphs in all tiles, the producer process constructs the global graph and computes the global geodesic distances of tile border flat cells from higher terrain and lower terrain. The consumer processes then compute the final geodesic distances in each tile and assign the hydrologically correct flow directions. Four experiments are conducted to evaluate the performance of our proposed algorithm. The speed-up ratios are greater than 3 when the number of consumer processes is greater than 9. When the number of consumer processes reaches 20, the average strong scaling efficiency is around 40%. The weak scaling efficiency is lower than 20% when the number of consumer processes is greater than 5. Compared with the sequential algorithm that is simpler to implement, the proposed parallel algorithm requires less memory on consumer processes than the sequential algorithm and can process massive digital elevation models that cannot be successfully processed using the sequential algorithm. It fills the void in the processing pipeline of automatic drainage network extraction based on the three-step parallel framework and enables parallel implementation of the entire processing pipeline possible, which should substantially improve the attractiveness of the three-step parallel framework.\n               ","116":"\n                  The development and use of open-source software in geophysics over the last decade have grown considerably. However, there are few open-software alternatives for seismic methods and even fewer regarding seismic refraction data processing and inversion. We present Refrapy, an open-source package for seismic refraction data analysis written in Python, using some of the main libraries for scientific computing and geosciences. Divided into two main programs (Refrapick and Refrainv), it is possible to perform basic waveform processing, picking of first breaks, and inversion through time-terms analysis or traveltimes tomography, all with GUIs interaction. To evaluate and validate the products obtained using Refrapy, we analyzed sets of synthetic data and real field data. The calculated models recovered the geometry and velocities values of the actual synthetic models satisfactorily. The joint interpretation of the results obtained with both available inversion techniques permitted a more detailed interpretation. As for the analysis of real data, the results presented a compatible response compared to the model obtained by a well-established commercial software, satisfactorily representing the geological context interpreted initially. The software presented in this work is expected to contribute to the activities of researchers, practitioners, and students regarding the analysis of seismic refraction data as a free, open, and user-friendly alternative.\n               ","117":"\n                  Measuring physical properties from rock samples is necessary in geosciences to calibrate models from geophysical surveys. Digital rock physics is one way to estimate these properties. X-ray computed tomography (CT) images can be used to create 3D numerical models of rocks. Numerical simulations on such models are proxies for tests performed in the lab. Commonly, segmentation is considered an essential digital rock physics processing step, where each voxel in a 3D model is assigned a property of a mineral phase or pore fluid. Any errors in this process are carried forward and affect all estimations that follow (including density, porosity, permeability, and elastic properties). The density and porosity analyses from segmentation are not predictive, as they are typically calibrated to lab tests. We explore a method that does not use segmentation and instead preserves the scaling relationship between the voxel values that are originally recorded in units of CT attenuation. The method is \u201csegmentation-less\u201d. Phantoms of known density were scanned alongside our sandstone samples and used as calibration points to convert from CT attenuation to density. A porosity model can be created as this property is negatively related to density. We use effective medium theory to assign a bulk and shear modulus to each voxel. We then experiment with several wave velocity models to estimate P and S-wave speed in order to be most similar to laboratory measurements. The ratio of our P-wave estimations to the laboratory measurements is an average of 0.98 with one effective medium theory. The density and porosity estimates do not require any external laboratory calibration. Overall, the method yields densities with an uncertainty of 44\u00a0kg\/m3 and porosities with an uncertainty of 1.67%. This method allows rock properties to be estimated quickly and accurately, on large samples, rare samples, and uniquely shaped samples that may not be the right shape for standard laboratory equipment. The technique is also less arbitrary than segmentation based digital rock physics, and is not as invasive or cumbersome as some laboratory techniques.\n               ","118":"\n                  An ontology is a logical theory that accounts for a domain vocabulary\u2019s intended meaning, allowing us to develop a computational artifact that explicitly and formally represents the community\u2019s conceptualization related to a vocabulary. This paper presents the GeoReservoir ontology \u2014 the result of the ontological analysis of the terminology adopted by geologists in sedimentological studies about deep-marine depositional systems, which is one of the most relevant types of oil and gas reservoirs around the world. Despite the variety of studies describing the patterns of productive reservoirs, this domain demanded new approaches in conceptual modeling methodologies because the available terminology presents issues such as ambiguity and contamination of different interpretations in the terms\u2019 definitions. Previous work has dealt with these issues in geology, resulting in the GeoCore, an ontology that explicitly defines the most generic entities in geology. However, the domain in focus still demanded a specialized ontology containing the particular terms found in reports about deep-marine deposits. GeoReservoir is an extension of GeoCore, which, in its turn, extends the Basic Formal Ontology (BFO), a foundational ontology for scientific domains. GeoReservoir takes advantage of both BFO and GeoCore\u2019s foundations, allowing us to focus our effort on defining the domain-specific entities. A team of professional reservoir geologists supported the knowledge acquisition process. We developed the ontology in iterative steps from an initial prototype to a complete artifact containing the taxonomy of entities and the relations between the entities, being increasingly refined by the experts. We further present a case study demonstrating how one could describe a depositional system in GeoReservoir terms. Moreover, we validated the ontology against defined competency questions (CQs). This work\u2019s final result is offering a sound, consistent and unambiguous terminology to support the integration of data and knowledge about deep-marine depositional system geometry and lithology.\n               ","119":"\n                  To improve streamflow predictions, researchers have implemented updating procedures that correct predictions from a simulation model using machine learning methods, in which simulated streamflow and meteorological data are used as predictors. Few studies however have included an extensive set of meteorological and hydrological state variables simulated by the simulation model. We developed and evaluated a Random Forests (RF)-based approach to correct predictions from a global hydrological model PCR-GLOBWB. From PCR-GLOBWB, meteorological input as well as its simulated hydrological state variables were used as predictors in the RF to estimate errors of PCR-GLOBWB streamflow predictions, which were then applied to correct simulated hydrograph. The RF was trained and applied separately at three streamflow gauging stations in the Rhine basin with different physiographic characteristics. Daily streamflow simulations from an uncalibrated PCR-GLOBWB run were improved by applying the RF-based error-correction model (KGE improved from 0.37 to 0.62 to 0.76\u20130.89, NSE from 0.19 to 0.39 to 0.64\u20130.80). A similar improvement was found in the simulations from a calibrated PCR-GLOBWB run (KGE 0.72\u20130.87 and NSE 0.60\u20130.78). The PCR-GLOBWB state variables that are informative to the improvement differed between catchments. Variables related to groundwater are informative in catchments dominated by the sedimentary basins characterizing large aquifers, while snow cover and surface water state variables are informative in a nival regime with large lakes. Here we quantified the improvement from combining a process-based and machine learning approach.\n               "},"Text":{"0":"","1":"\n\n1\nIntroduction\nIn seismic inversion, the objective is to reconstruct unknown model variables in the subsurface, for example, elastic velocities, from a set of seismic measurements, including seismic amplitudes and travel time measured at the surface\u00a0(Aki and Richards, 1980). The inversion results are retrieved from seismic reflection data by solving the non-unique and ill-posed inverse problem and they provide a quantitative model of predicted physical properties varying laterally and vertically\u00a0(Buland and Omre, 2003). Seismic data inversion schemes can be split into two main classes, post-stack (or acoustic) impedance, and pre-stack (or elastic) inversions. Post-stack inversion aims to predict acoustic impedance from stacked seismic data and it is often used in stratigraphic interpretation\u00a0(Ghosh, 2000) but it does not give any information about the shear wave velocity\u00a0(Morozov and Ma, 2009; Maurya et al., 2018). On the other hand, pre-stack inversion is based on the concept of amplitude variations with offset\/angle (AVO\/A) and aims to predict a set of elastic attributes such as seismic velocities, impedances and density\u00a0(Downton, 2005). AVO inversion results are typically correlated with petrophysical attributes like porosity, saturation of fluids, and reservoir litho-facies. These properties play a significant role in lithology prediction, geofluids identification, and quantitative reservoir characterization\u00a0(Chiappa and Mazzotti, 2009; Zhao et al., 2014; Luo et al., 2019; Grana, 2020). In addition, AVO inversion can also be used in time-lapse seismic monitoring studies to predict the changes in pressure, saturation, and porosity, as an example, for CO\n\n\n\n2\n\n\n sequestration in depleted reservoirs\u00a0(Lang and Grana, 2019; Dupuy et al., 2021).\nA seismic reflection event at the recording point is described generally by the convolution of the seismic source and the reflectivity series based on the wave equations\u00a0(Mallick, 2007), Zoeppritz equations\u00a0(Kurt, 2007; Skopintseva et al., 2011; Liu et al., 2016) or linearized approximations\u00a0(Aki and Richards, 1980; Buland and Omre, 2003; Buland and El\u00a0Ouair, 2006; Downton and Ursenbach, 2006; Rabben and Ursin, 2011; Xiao et al., 2020). The inversion can be performed according to deterministic or probabilistic inversion methods. For example, based on Aki and Richards linearized approximation,\u00a0Hu et al. (2011) presents a joint AVO inversion technique in the Bayesian framework to extract seismic velocities and density parameters,\u00a0Sengupta et al. (2021) perform Bayesian inversion directly in the depth domain by using linearized Aki and Richards equation, and\u00a0Liu et al. (2021) present a joint PP and PS inversion method based on the calculation of a Jacobian matrix of the Zoeppritz approximation. In addition to gradient-based optimization, the Monte Carlo inversion method\u00a0(Jin and Madariaga, 1994) and Bayesian linearized AVO inversion technique based on Gaussian distributions\u00a0(Tarantola, 1987; Buland and Omre, 2003) have also been utilized to solve the inverse problems and quantify the uncertainty of the predicted model.\u00a0Feng-Qi et al. (2013) present a Bayesian linearized pre-stack inversion based on a trivariate Cauchy distribution. Ensemble-based methods such as ensemble Kalman filter and ensemble smoother\u00a0(Evensen et al., 2009) have been successfully applied to inverse and data assimilation problems, especially for history matching of borehole and geophysical data. For example,\u00a0Luo et al. (2015) developed an iterative ensemble smoother based on a regularized Levenberg\u2013Marquardt (RLM) algorithm for reservoir data assimilation, and\u00a0Luo et al. (2017) applied the iterative ensemble smoother to 4D-seismic history-matching.\u00a0Kolbj\u00f8rnsen et al. (2020) develop a Bayesian inversion for litho-geofluids prediction and\u00a0Grana (2020) extends the Bayesian litho-geofluids approach to multiple prior models. Bayesian methods can also be integrated with stochastic sampling. For example,\u00a0Azevedo et al. (2020) uses the stochastic perturbation optimization approach for the inversion of seismic data for rock properties and facies. In the Bayesian method, prior information about the subsurface model is included in the inversion in the form of probability distributions \u00a0(Gouveia and Scales, 1997). However, the prior information is often difficult to define and the prior uncertainty generally impacts the model predictions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn deterministic methods, the goal is to predict a best-fit model that is consistent with the observed data, according to the objective or cost function that defines the dissimilarity between the true data and the predicted model. The inversion is performed by searching for the optimal model that minimizes the objective function. Numerous iterative algorithms such as the Levenberg\u2013Marquardt (LM) algorithm\u00a0(Levenberg, 1944; Marquardt, 1963), Occam\u2019s inversion\u00a0(Constable et al., 1987), genetic algorithm\u00a0(Mallick, 1995), conjugate gradient method\u00a0(Golub and Van\u00a0Loan, 2013), simulated annealing method\u00a0(Ma, 2001) and particle swarm optimization\u00a0(Shaw and Srivastava, 2007) have been introduced for solving the least-squares optimization problems. These methods have been used in seismic AVO inversion problems. For example,\u00a0Luo et al. (2020a, b) adopt the Fr\u00e9chet derivatives to compute the derivatives of the propagator matrix with respect to variables and used the L-BFGS approach for the optimization of the objective function.\nIn this work, we present a constrained non-linear AVO inversion scheme by using the Aki and Richards linearized approximation and the inversion algorithm is based on the minimization of an objective function. We adopt a gradient descent optimization algorithm depending on the calculation of the gradients of the L2-norm objective function with respect to the elastic properties, P- and S-wave velocities, and density. The adjoint-state numerical technique\u00a0(Plessix, 2006) is used for computing the gradients of the objective function efficiently by employing zero-lag cross-correlation between forward and reverse propagated data residual. The adjoint-state solves a linear system and computes the gradient of the objective function. The advantage of this method is that the computational cost of computing the gradient is in practice independent of the number of model variables \n\n(\nN\n)\n\n. Hence, the number of forwarding models required to compute the gradients through the adjoint-state method is independent of the number of unknown model variables. This makes adjoint-state faster and more efficient than other methods, such as finite difference and Fr\u00e9chet derivatives\u00a0(Plessix, 2006). Adjoint methods have been recently used in several geophysical inversion problems including seismic full waveform inversion \u00a0(Zheglova and Malcolm, 2019; Pan et al., 2020; Le et al., 2020; Biondi et al., 2021; Assis and Schleicher, 2021; Hu et al., 2021; Zhu et al., 2021; Ravasi and Vasconcelos, 2021). Furthermore, the gradient equations obtained via the adjoint-state technique are exact within the numerical precision\u00a0(Epelle and Gerogiorgis, 2020). In this work, to minimize the L2-norm objective function, we use the L-BFGS method\u00a0(Broyden, 1970; Fletcher, 1970; Goldfarb, 1970; Shanno, 1970; Nocedal, 1980; Liu and Nocedal, 1989), a widely used version of the quasi-Newton iteration method that does not explicitly calculate the Hessian operator, which reduces the computing time and memory storage unlike other classical iterative methods such as Newton\u2013Raphson or Gauss\u2013Newton methods\u00a0(Tarantola, 2005). Indeed, the L-BFGS method exclusively stores model information from a limited number of previous \nl\n iterations (usually \n\nl\n\u2264\n10\n\n) and provides much faster computational time and improved convergence rates for geophysical inverse problems\u00a0(Brossier et al., 2010). The main novelty of the proposed approach is the calculation of the gradient of the objective function with respect to the elastic properties using the adjoint-state method. The analytical derivatives computed by the adjoint-state lead to advantages in computational and numerical performance. To stabilize our AVO inversion results, especially in the case of noisy data, we apply a Tikhonov regularization method \u00a0(Aster et al., 2018). The Tikhonov regularization weights improve the stability of the solution and the accuracy of the optimal model.\nWe tested the proposed approach using synthetic well logs and seismic data generated for the Edvard Grieg oil field, in the North Sea, with different noise levels. We also adopt the staggered-grid finite difference (FD) method\u00a0(Virieux, 1986) to simulate the seismic response. The FD approach contains wave propagation effects such as seismic refraction, reflection, multiple reflections, and offset-dependent geometrical spreading. To reconcile the results of the convolutional approximation, we define an amplitude scaling factor that compensates for the effects of offset-dependent geometrical spreading and provides a good match between the two seismic modeling approaches.\nIn the following, we first discuss the seismic forward modeling approaches including, the convolutional model and finite difference method. Then, we describe the mathematical formulation for the proposed non-linear inversion algorithm based on the adjoint-state technique. Next, we illustrate the implementation of the inversion algorithm to a synthetic multilayered dataset, with and without seismic noise, obtained from well logs assuming no multiples nor other wave propagation effects. Then, a 2D synthetic data example with complex structural features such as inclined strata and fault, is presented. Finally, we apply the approach to a synthetic dataset based on the velocity\u2013stress finite difference (FD) model, including wave propagation effects.\n\n\n2\nMethod\n\n2.1\nSeismic modeling\nIn AVO studies, seismic amplitudes are approximated by a convolutional model. In continuous form, the convolutional model is written as: \n\n(1)\n\n\nd\n\n(\nt\n,\n\u03b8\n)\n\n=\n\u222b\nW\n\n(\n\u03c4\n,\n\u03b8\n)\n\n\n\nR\n\n\nP\nP\n\n\n\n(\nt\n\u2212\n\u03c4\n,\n\u03b8\n)\n\nd\n\n(\n\u03c4\n)\n\n\n\n\nwhere \nd\n, \n\n\nR\n\n\nP\nP\n\n\n and \nW\n are the seismic data, reflectivity, and wavelet respectively. In\u00a0Eq.\u00a0(1), \nt\n is the two-way travel time (TWT), and \n\u03b8\n is the incident angle. This approximation does not consider multiples and wave propagation effects like offset-dependent geometrical spreading, attenuation, or absorption effects. The Ricker wavelet\u00a0(Ricker, 1953) is commonly used for the convolutional model: \n\n(2)\n\n\nW\n\n(\nt\n)\n\n=\n\n\n1\n\u2212\n\n\n1\n\n\n2\n\n\n\n\n\u03c9\n\n\n\u2218\n\n\n2\n\n\n\n\nt\n\n\n2\n\n\n\n\nexp\n\n\n\u2212\n\n\n1\n\n\n4\n\n\n\n\n\u03c9\n\n\n\u2218\n\n\n2\n\n\n\n\nt\n\n\n2\n\n\n\n\n\n\n\nwhereas \n\n\n\u03c9\n\n\n\u2218\n\n\n is the dominant frequency. The reflectivity function \n\n\nR\n\n\nP\nP\n\n\n represents the reflection coefficients of P-to-P waves as a function of \nt\n and \n\u03b8\n and is often modeled using Aki and Richards equation\u00a0(Aki and Richards, 1980), which is a linear approximation of the non-linear Zoeppritz equation\u00a0(Zoeppritz, 1919) for weak elastic contrasts across the geological layers. For incident angles less than the acquisition critical angle, the Aki and Richards equation provides an accurate approximation of the reflection coefficients for small elastic contrasts. In theory, the proposed methodology could be extended to the Zoeppritz equation, however, the analytical evaluation of the mathematical formulation of the gradient is more challenging to derive. The discrete version of Aki and Richards approximation for \n\n\nR\n\n\nP\nP\n\n\n is: \n\n(3)\n\n\n\n\n\nR\n\n\nP\n\n\nP\n\n\n\n[\ni\n]\n\n\n\n\n\n\n(\n\u03b8\n)\n\n=\n\n\n1\n\n\n2\n\n\n\n\n1\n+\n\n\ntan\n\n\n2\n\n\n\u03b8\n\n\n\n\n\u0394\n\u03b1\n\n[\ni\n]\n\n\n\n\u03b1\n\n[\ni\n]\n\n\n\n\u2212\n4\n\n\n\n\n\n\n\u03b2\n\n[\ni\n]\n\n\n\n\u03b1\n\n[\ni\n]\n\n\n\n\n\n\n\n2\n\n\n\n\n\u0394\n\u03b2\n\n[\ni\n]\n\n\n\n\u03b2\n\n[\ni\n]\n\n\n\n\n\nsin\n\n\n2\n\n\n\u03b8\n+\n\n\n1\n\n\n2\n\n\n\n\n1\n\u2212\n4\n\n\n\n\n\n\n\u03b2\n\n[\ni\n]\n\n\n\n\u03b1\n\n[\ni\n]\n\n\n\n\n\n\n\n2\n\n\n\n\nsin\n\n\n2\n\n\n\u03b8\n\n\n\n\n\u0394\n\u03c1\n\n[\ni\n]\n\n\n\n\u03c1\n\n[\ni\n]\n\n\n\n\n\n\n\nwhere \n\n\u0394\n\u03b1\n\n, \n\n\u0394\n\u03b2\n\n, and \n\n\u0394\n\u03c1\n\n are the variations P- and S-wave velocities and density across the reflecting interface \ni\n whereas \n\u03b1\n, \n\u03b2\n and \n\u03c1\n are the corresponding average P- and S-wave velocities and density.\nAlternatively, the seismic response can be modeled by solving the wave equation. The finite difference (FD) method is often adopted to approximate the partial derivatives of the wave equation and compute the propagation of seismic waves\u00a0(Carcione et al., 1988). Several numerical schemes have been proposed. In this work, we adopt a high order (8th) staggered-grid FD scheme\u00a0(Virieux, 1986) to numerically model in a discretized grid the P-SV elastic seismic energy propagating through a heterogeneous medium using the velocity\u2013stress field. At the time \n0\n, the wave propagating medium is considered to be in equilibrium; then time-integrated particle velocity and stress are propagated. Absorbing boundary conditions (ABCs) are generally assumed for the mathematical simulation of seismic wave propagation, to avoid artificial boundary reflection (ABR). In our approach, we consider a perfectly matched layer (PML) to attenuate modeling boundary reflections. The numerical formulation for the velocity\u2013stress FD method is given by\u00a0Virieux (1986). In this work, we adopt this formulation to compute a synthetic dataset with a different operator than the convolutional model used for the inversion, to validate the proposed formulation.\n\n\n2.2\nAVO inversion\nThe forward modeling equation of seismic AVO inversion is written as \n\nf\n\n(\nm\n)\n\n=\nd\n\n, where \nm\n represents the unknown model variables. The objective of inverse modeling is to estimate the model variables \nm\n from the seismic AVO data \nd\n. In the proposed formulation, \nm\n represents the elastic model variables as \n\nm\n=\n\n[\n\n\nV\n\n\nP\n\n\n\n(\nt\n)\n\n,\n\n\nV\n\n\nS\n\n\n\n(\nt\n)\n\n,\n\u03c1\n\n(\nt\n)\n\n]\n\n\n, including P- and S-wave velocities and density.\nTo solve the inverse problem, we first define an objective function to model the misfit \n\nJ\n=\nd\n\u2212\nf\n\n(\nm\n)\n\n\n between the real and predicted data, and then implement an optimization algorithm to minimize the objective function. Numerous options for the definition of the objective functions are available in the literature\u00a0(Alessandrini et al., 2019; Faucher et al., 2019). We use the Euclidean norm (\n\nL\n2\n\n-norm), as it is widely used in inversion, especially for problems with a natural scattering of the error components. Mathematically, the \n\nL\n2\n\n-norm objective function is written as: \n\n(4)\n\n\nJ\n\n(\nm\n)\n\n=\n\n\n1\n\n\n2\n\n\n\n\n\u2016\nd\n\n(\nt\n,\n\u03b8\n)\n\n\u2212\nf\n\n(\nm\n)\n\n\u2016\n\n\n2\n\n\n\n\n\nIn our approach, the forward modeling operator is given by the convolutional model: \n\n(5)\n\n\nf\n\n(\nm\n)\n\n=\nW\n\n(\nt\n)\n\n\u2217\n\n\nR\n\n\nP\nP\n\n\n\n(\nt\n,\n\u03b8\n|\nm\n)\n\n+\nn\n\n(\nt\n,\n\u03b8\n)\n\n\n\n\nhere, the term \nn\n represents the random ambient noise and * indicates convolution.\nThe optimization requires the calculation of the partial derivatives of the gradient \n\n\n\n\n\u2202\nJ\n\n\n\u2202\n\n\nV\n\n\nP\n\n\n\n\n,\n\n\n\u2202\nJ\n\n\n\u2202\n\n\nV\n\n\nS\n\n\n\n\n,\n\n\n\u2202\nJ\n\n\n\u2202\n\u03c1\n\n\n\n\n of the objective function \nJ\n with respect to elastic properties (\n\n\n\nV\n\n\nP\n\n\n,\n\n\nV\n\n\nS\n\n\n,\n\u03c1\n\n). We adopt the adjoint-state technique to calculate the gradient of the objective function. The adjoint-state solution of Aki and Richards equation (1980) and derivation of the gradient is given in Appendix\u00a0A. The so-obtained partial derivatives with respect to (\n\n\n\nV\n\n\nP\n\n\n,\n\n\nV\n\n\nS\n\n\n,\n\u03c1\n\n) at a given interface \ni\n are: \n\n(6)\n\n\n\n\n\u2202\nJ\n\n\n\u2202\n\n\nV\n\n\nP\n\n\n\n[\ni\n]\n\n\n\n=\n\n\n1\n\n\n2\n\n\n\n\n1\n+\n\n\ntan\n\n\n2\n\n\n\u03b8\n\n\n\n\n\u2212\n\n\n1\n\n\n\u03b1\n\n[\ni\n]\n\n\n\n\u2212\n\n\n\u0394\n\u03b1\n\n[\ni\n]\n\n\n\n2\n\u03b1\n\n\n\n[\ni\n]\n\n\n\n2\n\n\n\n\n\n\n\u22c5\n\u03bb\n\n[\ni\n]\n\n\n+\n\n\n\n1\n\n\n2\n\n\n\n\n1\n+\n\n\ntan\n\n\n2\n\n\n\u03b8\n\n\n\n\n\n\n1\n\n\n\u03b1\n\n[\ni\n\u2212\n1\n]\n\n\n\n\u2212\n\n\n\u0394\n\u03b1\n\n[\ni\n\u2212\n1\n]\n\n\n\n2\n\u03b1\n\n\n\n[\ni\n\u2212\n1\n]\n\n\n\n2\n\n\n\n\n\n\n\u22c5\n\u03bb\n\n[\ni\n\u2212\n1\n]\n\n\n+\n\n\n\n\n\n2\n\u03b2\n\n\n\n[\ni\n\u2212\n1\n]\n\n\n\n2\n\n\n\n\n\u03b1\n\n\n\n[\ni\n\u2212\n1\n]\n\n\n\n3\n\n\n\n\n\n\n\n\n2\n\n\n\u0394\n\u03b2\n\n[\ni\n\u2212\n1\n]\n\n\n\n\u03b2\n\n[\ni\n\u2212\n1\n]\n\n\n\n+\n\n\n\u0394\n\u03c1\n\n[\ni\n\u2212\n1\n]\n\n\n\n\u03c1\n\n[\ni\n\u2212\n1\n]\n\n\n\n\n\n\u22c5\n\n\nsin\n\n\n2\n\n\n\u03b8\n\u22c5\n\u03bb\n\n[\ni\n\u2212\n1\n]\n\n\n+\n\n\n\n\n\n2\n\u03b2\n\n\n\n[\ni\n]\n\n\n\n2\n\n\n\n\n\u03b1\n\n\n\n[\ni\n]\n\n\n\n3\n\n\n\n\n\n\n\n\n2\n\n\n\u0394\n\u03b2\n\n[\ni\n]\n\n\n\n\u03b2\n\n[\ni\n]\n\n\n\n+\n\n\n\u0394\n\u03c1\n\n[\ni\n]\n\n\n\n\u03c1\n\n[\ni\n]\n\n\n\n\n\n\u22c5\n\n\nsin\n\n\n2\n\n\n\u03b8\n\u22c5\n\u03bb\n\n[\ni\n]\n\n,\n\n\n\n\n\n\n(7)\n\n\n\n\n\u2202\nJ\n\n\n\u2202\n\n\nV\n\n\nS\n\n\n\n[\ni\n]\n\n\n\n=\n\n\n\u2212\n\n\n2\n\u0394\n\u03b2\n\n[\ni\n\u2212\n1\n]\n\n\n\n\u03b1\n\n\n\n[\ni\n\u2212\n1\n]\n\n\n\n2\n\n\n\n\n\u2212\n\n\n4\n\u03b2\n\n[\ni\n\u2212\n1\n]\n\n\n\n\u03b1\n\n\n\n[\ni\n\u2212\n1\n]\n\n\n\n2\n\n\n\n\n\n\n\u22c5\n\n\nsin\n\n\n2\n\n\n\u03b8\n\u22c5\n\u03bb\n\n[\ni\n\u2212\n1\n]\n\n\n\u2212\n\n\n\n\n\n2\n\u0394\n\u03b2\n\n[\ni\n]\n\n\n\n\u03b1\n\n\n\n[\ni\n]\n\n\n\n2\n\n\n\n\n\u2212\n\n\n4\n\u03b2\n\n[\ni\n]\n\n\n\n\u03b1\n\n\n\n[\ni\n]\n\n\n\n2\n\n\n\n\n\n\n\u22c5\n\n\nsin\n\n\n2\n\n\n\u03b8\n\u22c5\n\u03bb\n\n[\ni\n]\n\n\n\u2212\n\n\n\n\n\n2\n\u03b2\n\n[\ni\n\u2212\n1\n]\n\n\n\n\u03b1\n\n\n\n[\ni\n\u2212\n1\n]\n\n\n\n2\n\n\n\n\n\u22c5\n\n\n\u0394\n\u03c1\n\n[\ni\n\u2212\n1\n]\n\n\n\n\u03c1\n\n[\ni\n\u2212\n1\n]\n\n\n\n\n\n\u22c5\n\n\nsin\n\n\n2\n\n\n\u03b8\n\u22c5\n\u03bb\n\n[\ni\n\u2212\n1\n]\n\n\n\u2212\n\n\n\n\n\n2\n\u03b2\n\n[\ni\n]\n\n\n\n\u03b1\n\n\n\n[\ni\n]\n\n\n\n2\n\n\n\n\n\u22c5\n\n\n\u0394\n\u03c1\n\n[\ni\n]\n\n\n\n\u03c1\n\n[\ni\n]\n\n\n\n\n\n\u22c5\n\n\nsin\n\n\n2\n\n\n\u03b8\n\u22c5\n\u03bb\n\n[\ni\n]\n\n\n\n\n and \n\n(8)\n\n\n\n\n\u2202\nJ\n\n\n\u2202\n\u03c1\n\n[\ni\n]\n\n\n\n=\n+\n\n\n1\n\n\n2\n\n\n\n\n1\n\u2212\n4\n\n\n\n\n\n\n\u03b2\n\n[\ni\n\u2212\n1\n]\n\n\n\n\u03b1\n\n[\ni\n\u2212\n1\n]\n\n\n\n\n\n\n\n2\n\n\n\u22c5\n\n\nsin\n\n\n2\n\n\n\u03b8\n\n\n\n\n\n\n1\n\n\n\u03c1\n\n[\ni\n\u2212\n1\n]\n\n\n\n\u2212\n\n\n\u0394\n\u03c1\n\n[\ni\n\u2212\n1\n]\n\n\n\n2\n\u03c1\n\n\n\n[\ni\n\u2212\n1\n]\n\n\n\n2\n\n\n\n\n\n\n\u22c5\n\u03bb\n\n[\ni\n\u2212\n1\n]\n\n\n\u2212\n\n\n\n1\n\n\n2\n\n\n\n\n1\n\u2212\n4\n\n\n\n\n\n\n\u03b2\n\n[\ni\n]\n\n\n\n\u03b1\n\n[\ni\n]\n\n\n\n\n\n\n\n2\n\n\n\u22c5\n\n\nsin\n\n\n2\n\n\n\u03b8\n\n\n\n\n\n\n1\n\n\n\u03c1\n\n[\ni\n]\n\n\n\n+\n\n\n\u0394\n\u03c1\n\n[\ni\n]\n\n\n\n2\n\u03c1\n\n\n\n[\ni\n]\n\n\n\n2\n\n\n\n\n\n\n\u22c5\n\u03bb\n\n[\ni\n]\n\n\n\n\n\n\nWe then apply a non-linear optimization algorithm, namely L-BFGS, to update the model variables by minimizing the objective function \nJ\n according to the L-BFGS iteration Eq.\u00a0(9). L-BFGS is a limited-memory quasi-Newton optimization method often used for solving large-scale non-linear optimization problems where the Hessian cannot be efficiently computed. The L-BFGS optimization method iteratively approximates the inverse Hessian using the curvature information from the previous iterations. The L-BFGS optimization method can be represented as: \n\n(9)\n\n\n\n\nm\n\n\nk\n+\n1\n\n\n=\n\n\nm\n\n\nk\n\n\n\u2212\n\n\n\u03b1\n\n\nk\n\n\n\n\nH\n\n\nk\n\n\n\u2207\nJ\n,\n\nk\n=\n0\n,\n1\n,\n2\n,\n3\n,\n\u2026\n,\n\n\n\nwhere \nk\n represents the iteration, \n\n\n\u03b1\n\n\nk\n\n\n is the scalar step length at iteration \nk\n, \n\n\u2207\nJ\n\n is the gradient of the objective function respectively, and \n\n\nH\n\n\nk\n\n\n describes the inverse Hessian approximation \n\n(\n\n\nH\n\n\nk\n\n\n\u2248\n\n\n\u2207\n\n\n2\n\n\n\n\nJ\n\n\n\u2212\n1\n\n\n)\n\n at iteration \nk\n. The inverse Hessian \n\n\nH\n\n\nk\n\n\n is approximated as: \n\n(10)\n\n\n\n\nH\n\n\nk\n+\n1\n\n\n=\n\n\nV\n\n\nk\n\n\nT\n\n\n\n\nH\n\n\nk\n\n\n\n\nV\n\n\nk\n\n\n+\n\n\n\u03c1\n\n\nk\n\n\n\n\ns\n\n\nk\n\n\n\n\ns\n\n\nk\n\n\nT\n\n\n\n\n\nwhere \n\n\n\nV\n\n\nk\n\n\n=\nI\n\u2212\n\n\n\u03c1\n\n\nk\n\n\n\n\ny\n\n\nk\n\n\n\n\ns\n\n\nk\n\n\nT\n\n\n\n, \n\n\n\ns\n\n\nk\n\n\n=\n\n\nm\n\n\nk\n+\n1\n\n\n\u2212\n\n\nm\n\n\nk\n\n\n\n, \n\n\n\ny\n\n\nk\n\n\n=\n\u2207\nJ\n\n(\n\n\nm\n\n\nk\n+\n1\n\n\n)\n\n\u2212\n\u2207\nJ\n\n(\n\n\nm\n\n\nk\n\n\n)\n\n\n, and \n\n\n\n\u03c1\n\n\nk\n\n\n=\n\n\n\n(\n\n\ny\n\n\nk\n\n\nT\n\n\n\n\ns\n\n\nk\n\n\n)\n\n\n\n\u2212\n1\n\n\n\n.\nIn the L-BFGS method, the Hessian approximation is more efficient than in the original BFGS method. At a given iteration \nk\n, suppose that the current solution is \nm\n and the vector pairs of the previous \np\n iterations are \n\n{\n\n\ns\n\n\ni\n\n\n,\n\n\ny\n\n\ni\n\n\n}\n\n for \n\ni\n=\nk\n\u2212\np\n,\n\u2026\n,\nk\n\u2212\n1\n\n with associated matrices \n\n\nV\n\n\ni\n\n\n and scalars \n\n\n\u03c1\n\n\ni\n\n\n. We choose an initial \n\n\nH\n\n\nk\n\n\n\u2218\n\n\n and compute \n\n\nH\n\n\nk\n\n\n as \n\n(11)\n\n\n\n\nH\n\n\nk\n\n\n=\n\n\n\n\nV\n\n\nk\n\u2212\n1\n\n\nT\n\n\n.\n.\n.\n\n\nV\n\n\nk\n\u2212\np\n\n\nT\n\n\n\n\n\n\nH\n\n\nk\n\n\n\u2218\n\n\n\n\n\n\nV\n\n\nk\n\u2212\np\n\n\n.\n.\n.\n\n\nV\n\n\nk\n\u2212\n1\n\n\n\n\n\n+\n\n\n\n\u03c1\n\n\nk\n\u2212\np\n\n\n\n\n\n\nV\n\n\nk\n\u2212\n1\n\n\nT\n\n\n.\n.\n.\n\n\nV\n\n\nk\n\u2212\np\n+\n1\n\n\nT\n\n\n\n\n\n\ns\n\n\nk\n\u2212\np\n\n\n\n\ns\n\n\nk\n\u2212\np\n\n\nT\n\n\n\n\n\n\nV\n\n\nk\n\u2212\np\n+\n1\n\n\n.\n.\n.\n\n\nV\n\n\nk\n\u2212\n1\n\n\n\n\n\n+\n\n\n\n\u03c1\n\n\nk\n\u2212\np\n+\n1\n\n\n\n\n\n\nV\n\n\nk\n\u2212\n1\n\n\nT\n\n\n.\n.\n.\n\n\nV\n\n\nk\n\u2212\np\n+\n2\n\n\nT\n\n\n\n\n\n\ns\n\n\nk\n\u2212\np\n+\n1\n\n\n\n\ns\n\n\nk\n\u2212\np\n+\n1\n\n\nT\n\n\n\n\n\n\nV\n\n\nk\n\u2212\np\n+\n2\n\n\n.\n.\n.\n\n\nV\n\n\nk\n\u2212\n1\n\n\n\n\n\n+\n\n\u22ef\n\n+\n\n\n\n\u03c1\n\n\nk\n\u2212\n1\n\n\n\n\ns\n\n\nk\n\u2212\n1\n\n\n\n\ns\n\n\nk\n\u2212\n1\n\n\nT\n\n\n\n\n\n We then define a recursive procedure to efficiently calculate the product \n\n\n\nH\n\n\nk\n\n\n\u2207\n\n\nJ\n\n\nk\n\n\n\n, as shown in Algorithm 1. \n\n\n\n\n\n\n\n\n3\nApplications\nWe present numerous numerical examples of synthetic and field datasets used to validate the proposed inversion using the traditional convolutional model. We then extend the application to a synthetic dataset generated using the FD method. Two synthetic subsurface profiles, namely example 1 and example 2, are generated from well logs using the convolutional model. Then the inversion is extended to synthetic 2D data generated from an elastic model developed for the Edvard Grieg oil field located in the North Sea.\nIn example 1, we use a multilayered convolution model to simulate the synthetic angle gathers from a set of synthetic well log data. S-wave velocity and density are computed from P-wave velocity using Castagna\u2019s relation\u00a0(Castagna et al., 1985) and Gardner\u2019s equation\u00a0(Gardner et al., 1974) respectively. The true model variables and the initial guesses are shown in Fig.\u00a01. Synthetic AVO seismic gathers up to the maximum angle of incidence \n\n30\n\u00b0\n\n are generated by convolving a 25\u00a0Hz Ricker seismic wavelet with PP reflectivity series calculated with the linearized Aki and Richards equation. The pre-stack angle gather profiles without and with seismic random noise levels (S\/N = 50) are shown in Fig.\u00a02. Fig.\u00a03 represents the inverted P wave velocity, S wave velocity, and density from noise-free AVO seismic gather. The inverted variables are in good agreement with the true model variables. Fig.\u00a04 shows the corresponding seismic response of the true, initial and inverted models as well as the root mean square (RMS) error between true and predicted seismic gathers. The inversion results prove the accuracy of this inversion approach. The AVO inversion results with a signal-to-noise ratio (S\/N) of 50 are shown in Fig.\u00a05. The inversion results show a good agreement and are consistent with real models, despite some little instability of the solution at quite a few points, especially for S-wave velocity. We speculate that the local instability of the solution might be due to the discrete nature of the model and the band-limited nature of the data. The local instability can be mitigated using a regularization method such as the total variation regularization.\nWe then test the inversion using a synthetic seismic dataset generated from high-frequency acoustic and shear sonic logs and density data, measured in the Edvard Grieg oil field. These reference elastic properties \n\n\nV\n\n\nP\n\n\n, \n\n\nV\n\n\nS\n\n\n, and \n\u03c1\n along with initial models are shown in Fig.\u00a06. The presented true well log measurements (Fig.\u00a06) are upscaled to estimate the model variables at the seismic scale (i.e.\u00a0seismic wavelet scale) from the higher frequency properties (i.e.\u00a0sonic log). The reservoir zone is located between 1867 and 1888\u00a0ms. The convolution-based seismic forward modeling is used to generate pre-stack seismic AVA gathers (Fig.\u00a07) without noise and with S\/N = 50. The range of angles of the incident for seismic waves is from \n\n0\n\u2013\n30\n\u00b0\n\n with the interval of \n\n5\n\u00b0\n\n. Figs.\u00a08 and 9 show the AVA inversion results without and with noise. In the noise-free case, the comparison between true and inverted models shows that the results are accurate (Fig.\u00a08). Similarly, in the noise-added case, the results also show very good agreement with the true models (Fig.\u00a09) and are consistent with the real models. However, the inverted results presented in Figs.\u00a08 and 9 are after applying small weight Tikhonov regularization to the density and the shear wave velocity.\nWe then apply the inversion approach to a synthetic dataset built using the Edvard Grieg measured data along a section including horizontal and inclined stratigraphy and faulted geological layers. We adopt an elastic model obtained in previous studies (referred to as the true model in the following) to generate a synthetic seismic line. The seismic line comprises 38 common depth points (CDP) and the AVO inversion is applied trace by trace. The time window is from 500 to 2500\u00a0ms. Figs.\u00a010\u201312 show the inversion results for P- and S-wave velocity and density, respectively. Each figure shows the comparison between the initial model, true model, and inverted results for all CDPs. For each model variable, we compute the RMS error between true and predicted models. The inversion shows accurate results for P-wave velocity, whereas density values are slightly under-predicted in the bottom part of the interval and S-wave velocity shows mismatched in the reservoir region. Fig.\u00a013 shows the full stack of the seismic dataset, obtained after stacking the AVO gathers generated at each trace location, and the seismic response of the inverted model, showing an overall agreement between data and predictions. In some of the inversion results, the correlation between the predicted elastic properties is overestimated, possibly due to the linearization in the inverse problem. This is a common effect in seismic and petrophysical inversion, due to the correlation of the seismic angles introduced in the processing which reduces the degrees of freedom of the solution and makes the problem underestimated.\nWe extend the inversion to a seismic dataset generated using the staggered-grid finite difference model of elastic waves. We first generate the synthetic seismic by computing the forward model for the entire section from the surface and extracting the reservoir layer between 1700 and 1888\u00a0ms. We perform normal moveout (NMO) correction offset-to-angle transformation and apply an amplitude scaling factor to remove the effects of offset-dependent geometrical spreading. The wavelet with a dominant frequency of 25\u00a0Hz is used for the FD model. We assumed a layered isotropic elastic medium with elastic properties given in Fig.\u00a01 and simulate an OBC seismic survey. The number of receiver points are 1001 with a constant distance of 5\u00a0m, for a 5000\u00a0m maximum offset. The recording interval is 0.002\u00a0s. The synthetic seismograms generated by the finite difference simulations are shown in Fig.\u00a014. The overburden layers extend to 1750\u00a0m, whereas the reservoir layer is between 1750 and 1940\u00a0m, such that the overburden reflections and associated interbedded multiples are subtracted from the model. The FD results at the top of the reservoir are shown in (Fig.\u00a014c). As the staggered-grid finite difference modeling considers all the wave propagation effects, in order to implement a convolutional model-based inversion scheme for FD synthetic angles gathers, we have defined an amplitude scaling factor that compensates for the effects of geometrical spreading for FD seismic profiles and provides the best calibration between synthetics of both methods. The derivation of the amplitude scaling factor is described in Appendix\u00a0B. The comparison between convolution and FD models at the reservoir zone level, after the application of the scaling factor, is shown in Fig.\u00a015. We then run the inversion scheme assuming the FD model until incident angle 18\n\n\n\n\u2218\n\n\n. The results of the proposed adjoint-state-based inversion applied to the staggered-grid FD model are shown in Figs.\u00a016 and 17. Overall, the inverted P- and S- wave seismic velocities and density show good agreement with the true models, despite some discrepancies. Fig.\u00a017 shows the synthetic angle gathers for the initial, true, and inverted models plotted trace by trace up to 18\n\n\n\n\u2218\n\n\n, showing a good match.\n\n\n4\nDiscussion\nIn the gradient descent-based optimization algorithms, it is necessary to compute the gradient equations of the least-square objective function with respect to unknown elastic variables. The efficiency of the algorithm relies on the accuracy and the effectiveness of the computation of the gradient. To efficiently and accurately compute the gradient, we adopt the adjoint-state technique. The adjoint-state variables do not depend on the perturbations of the model variables. We apply the method under the non-linear constraints and derive the set of gradient equations for the objective function. The advantage of using the adjoint-state method is that it only requires solving one additional linear system, which makes the inversion more efficient than the Fr\u00e9chet derivative approach\u00a0(Plessix, 2006). The optimization problem is then solved by using the L-BFGS optimizer of the non-linear quasi-Newton class based on the gradient computed with the adjoint-state method. The non-linear L-BFGS approximates the inverse Hessian matrix by using a few previous iterations (\n\nl\n<\n10\n\n) and consequently reduces computational load and makes the algorithm more efficient than the traditional Newton\u2013Raphson and Gauss\u2013Newton methods. The proposed approach is tested on several examples based on two different forward models: the traditional convolutional approach and the FD method. The adjoint-state-based inversion method for the elastic properties can be theoretically applied to other properties e.g.,\u00a0seismic impedances, velocity ratio, or petrophysical parameters, by calculating the gradients with respect to the selected parameterization using the chain rule of derivative and by incorporating adequate rock-physics relations. The proposed inversion method can also be applied to other angle-dependent reflectivity operators such as Zoeppritz equations or their approximations\u00a0(Shuey, 1985; Fatti et al., 1994)\n\n\n5\nConclusions\nWe presented a non-linear seismic AVO inversion method based on a deterministic approach for the minimization of the objective function. The objective function is based on the convolutional model where the reflectivity is obtained using linearized Aki and Richards approximation. The gradient of the objective function is calculated by using the adjoint-state technique. The adjoint-state is computationally fast and more effective than traditional numerical methods. The minimization problem is iteratively solved by using L-BFGS, a non-linear optimization algorithm that approximates the inverse of Hessian and improves the convergence rate for inversion results. The applications of the proposed inversion scheme show accurate results for synthetic seismic data computed using the convolutional model as well as the staggered-grid finite difference method. The estimated synthetic angle gathers computed from the inversion results match the true data in both cases. The adjoint-state-based AVO inversion method can be applied to the different parameterizations of the model by using the chain rule of differentiation.\n\n\nCRediT authorship contribution statement\n\nNisar Ahmed: Defined and developed the methodology, Implemented the code, Contributed to the writing. Wiktor Waldemar Weibull: Defined and developed the methodology, Contributed to the writing and coding. Dario Grana: Revised the methodology, Contributed to the writing.\n\n","2":"","3":"","4":"","5":"\n\n1\nIntroduction\nMany subsurface applications in porous media ranging from groundwater and contaminant hydrology to \n\n\nCO\n\n\n2\n\n\n sequestration rely on physics-based models (Middleton et al., 2015; Choo and Sun, 2018; Yu et al., 2020; Kadeethum et al., 2019; Bouklas and Huang, 2012; Newell et al., 2017; Kadeethum et al., 2021b). These models often seek the solution (or approximation) of the governing partial differential equations (PDEs) in domains of interest. For instance, coupled hydro-mechanical (HM) processes in porous media could be described by Biot\u2019s equations (i.e.,\u00a0linear poroelasticity)\u00a0(Biot, 1941). These PDEs are often solved numerically using various techniques such as finite volume or finite element methods\u00a0(Wheeler et al., 2014; Deng et al., 2017; Liu et al., 2018), which is referred to as full order model (FOM) approaches. However, computational methods to handle field-scale systems require substantial computational resources, especially when discontinuities or nonlinearities arise in the solution (Hansen, 2010; Hesthaven et al., 2016). Therefore, in some instances, the FOM is not directly suitable to handle large-scale inverse problems, optimization, or even concurrent multiscale calculations in which an extensive set of simulations are required to be explored\u00a0(Ballarin et al., 2019; Hesthaven et al., 2016).\nA reduced order model (ROM) could be an alternative to handling field-scale inverse problems, optimization, or real-time reservoir management because it uses a low-dimensional representation of FOM, which requires less computational resources while maintaining an acceptable accuracy\u00a0(Schilders et al., 2008; Amsallem et al., 2015; Choi et al., 2019, 2020a; McBane and Choi, 2021; Yoon et al., 2009b,a). The ROM methodology primarily relies on the parameterization of a problem (i.e.,\u00a0repeated evaluations of a problem depending on parameters)\u00a0(Ballarin et al., 2019; Hesthaven et al., 2016; Hoang et al., 2021; Choi et al., 2020b, 2021). These parameters could correspond to physical properties, geometric characteristics, or boundary conditions\u00a0(Venturi et al., 2019; Copeland et al., 2021; Carlberg et al., 2018). Coupled processes such as HM processes commonly involve complex subsurface structures\u00a0(Flemisch et al., 2018; Jia et al., 2017; Chang and Yoon, 2020; Chang et al., 2020; Kadeethum et al., 2020b; Chang and Yoon, 2021) where the corresponding spatially distributed material properties can span several orders of magnitude and include discontinuous features (e.g.,\u00a0fractures, vugs, or channels). However, it is difficult to parameterize heterogeneous spatial fields of PDE coefficients such as heterogeneous material properties by a few parameters as previously shown by\u00a0Kadeethum et al. (2021d). Consequently, traditional projection-based ROM approaches might not be suitable for this type of problem as they commonly employ a proper orthogonal decomposition (POD) approach, which in turn will require a high dimensional reduced basis to capture most of the information at the expense of the computational cost\u00a0(Kadeethum et al., 2021d).\nDeep learning (DL), in particular, neural network-based supervised learning approaches, have been recently investigated for subsurface flow and transport problems\u00a0(Zhu and Zabaras, 2018; Mo et al., 2019; Wen et al., 2021a,b; Xu et al., 2021; Kadeethum et al., 2022; Wei et al., 2021). These DL approaches train various DL algorithms using training data generated by FOMs to map heterogeneous PDEs coefficients (i.e.,\u00a0heterogeneous permeability and\/or porosity fields) and injection scenarios (e.g.,\u00a0injection rates and a number of wells) into state variables such as pressure and CO2 saturation. During the online phase (i.e.,\u00a0prediction), these trained models are used to predict state variables, evaluate uncertainty quantification as fast forward models, or estimate material properties as inverse models\u00a0(Kadeethum et al., 2021d). In most reservoir problems, these DL models can also account for time-dependent PDEs, but the output of trained models is limited to the same time interval as in the input of training data and mostly flow and transport problems. The incorporation of physical constraints (e.g.,\u00a0equations, relationships, and known properties) into the learning process is actively studied to improve the accuracy and training efficiency of data-driven modeling.\n\nKadeethum et al. (2021d) presented a data-driven generative adversarial networks (GAN) framework that can parameterize heterogeneous PDEs coefficients (i.e.,\u00a0heterogeneous fields), which has been demonstrated with steady-state cases for both forward and inverse problems. This GAN-based work could be considered as an extension of regression in subsurface physics through GAN model such as\u00a0Zhong et al. (2019), Laloy et al. (2019) and\u00a0Lopez-Alvis et al. (2021), in which heterogeneous fields are also parameterized through GAN model\u00a0(Chan and Elsheikh, 2020; Hao et al., 2022; Guan et al., 2021) and subsequently used to predict the state variables (pressure and displacement responses). In\u00a0Kadeethum et al. (2021d), the conditional GAN (cGAN) approach\u00a0(Mirza and Osindero, 2014; Isola et al., 2017) was extended to the heterogeneous field for both generator and critic (i.e.,\u00a0discriminator) where usage of Earth mover\u2019s distance through Wasserstein loss (W loss) and gradient penalty\u00a0(Arjovsky et al., 2017; Gulrajani et al., 2017) improved the model accuracy and training stability compared to the traditional GAN approach. This improvement contributed to the Earth mover\u2019s distance enforcing the cGAN framework to approximate the training data distribution rather than a point-to-point mapping. However, the framework developed by\u00a0Kadeethum et al. (2021d) is limited to only steady-state solutions of given PDEs.\nRecently,\u00a0Ding et al. (2020) developed continuous cGAN (CcGAN) to condition the GAN model with continuous variables such as quantitative measures (e.g.,\u00a0the weight of each animal) rather than categorical data (e.g.,\u00a0cat or dog). For PDE problems, the concept of CcGAN can also be extended to quantify continuous variables (e.g.,\u00a0time domain), enabling the solution of time-dependent PDEs. In this work, we extend our previous work\u00a0(Kadeethum et al., 2021d) by extending the CcGAN concept to solve for time-dependent PDEs. This new framework is developed by utilizing element-wise addition or conditional batch normalization\u00a0(De\u00a0Vries et al., 2017) to incorporate the time domain in both training and prediction processes. As presented in Fig.\u00a01, our model treats the time domain as a continuous variable. Therefore, this model can handle the training data that contains different time-step resolutions. Furthermore, we can predict the continuous-time response without being limited to time instances that correspond to the training data. This design also provides flexibility to incorporate other parameterized continuous variables (e.g.,\u00a0Young\u2019s modulus, boundary conditions) as parameters to our framework.\n\nThe CcGAN approach to solving time-dependent PDEs in this work is uniquely employed to develop a data-driven surrogate model given highly heterogeneous permeability fields generated from two known distributions. The CcGAN performance will be evaluated by comparing the CcGAN-based results with FOM-based solutions, highlighting the computational accuracy and efficiency in heterogeneous permeability fields. The rest of the manuscript is summarized as follows. We begin with the model description and CcGAN architecture in Section\u00a02. The two variants of the framework, including element-wise addition or conditional batch normalization to incorporate the time domain, are also discussed. We then illustrate our ROM framework using three numerical examples in Section\u00a04. Lastly, we conclude our findings in Section\u00a05.\n\n\n2\nMethodology\n\n2.1\nGeneral governing equations\nWe first present a general framework with a parameterized system of time-dependent PDEs and then as a demonstration case we focus on the linear poroelasticity to represent a coupled solid deformation and fluid diffusion in porous media with highly heterogeneous permeability fields. A parameterized system of time-dependent PDEs are as following \n\n(1)\n\n\n\nF\n\n\n\n\nt\n\n\nn\n\n\n,\n\n\n\u03bc\n\n\n\n(\ni\n)\n\n\n\n\n\n=\n0\n\u00a0\u00a0in\u00a0\u00a0\n\u03a9\n,\n\n\n\n\n\n\nX\n=\n\n\nf\n\n\nD\n\n\n\u00a0\u00a0on\u00a0\u00a0\n\u2202\n\n\n\u03a9\n\n\nD\n\n\n,\n\u2212\n\u2207\nX\n\u22c5\nn\n=\n\n\nf\n\n\nN\n\n\n\u00a0\u00a0on\u00a0\u00a0\n\u2202\n\n\n\u03a9\n\n\nN\n\n\n.\n\n\n\n\n\n\nX\n=\n\n\nX\n\n\n0\n\n\n\u00a0\u00a0in\u00a0\u00a0\n\u03a9\n\u00a0at\u00a0\n\n\nt\n\n\nn\n\n\n=\n0\n,\n\n\n\n\nwhere \n\nF\n\n\n\u22c5\n\n\n\n corresponds to the system of time dependent PDEs, \n\n\u03a9\n\u2282\n\n\nR\n\n\n\n\nn\n\n\nd\n\n\n\n\n\n (\n\n\n\nn\n\n\nd\n\n\n\u2208\n\n{\n1\n,\n2\n,\n3\n}\n\n\n) denotes the computational domain, \n\n\u2202\n\n\n\u03a9\n\n\nD\n\n\n\n and \n\n\u2202\n\n\n\u03a9\n\n\nN\n\n\n\n denote the Dirichlet and Neumann boundaries, respectively. \n\n\nf\n\n\nD\n\n\n and \n\n\nf\n\n\nN\n\n\n are prescribed values on \n\n\u2202\n\n\n\u03a9\n\n\nD\n\n\n\n and \n\n\u2202\n\n\n\u03a9\n\n\nN\n\n\n\n, respectively. \n\n\nX\n\n\n0\n\n\n is an initial value of \nX\n. The time domain \n\nT\n=\n\n\n0\n,\n\u03c4\n\n\n\n is partitioned into \n\n\nN\n\n\nt\n\n\n subintervals such that \n\n0\n\u2255\n\n\nt\n\n\n0\n\n\n<\n\n\nt\n\n\n1\n\n\n<\n\u22ef\n<\n\n\nt\n\n\nN\n\n\n\u2254\n\u03c4\n\n, We denote \n\n\n\nt\n\n\nn\n\n\n\u2208\nT\n\n as \nn\nth time-step, \n\nn\n\u2208\n\n[\n0\n,\nN\n]\n\n\n. \nX\n is a set of scalar (\n\nX\n\u2208\nR\n\n) or tensor valued (e.g.\u00a0\n\nX\n\u2208\n\n\nR\n\n\n\n\nn\n\n\nd\n\n\n\n\n\n\nor\n\n\n\n\nR\n\n\n\n\nn\n\n\nd\n\n\n\n\n\u00d7\n\n\nR\n\n\n\n\nn\n\n\nd\n\n\n\n\n\n) generalized primary variables. For the parameter domain \nP\n, it composes of \nM\n members, i.e.,\u00a0\n\n\n\u03bc\n\n\n\n(\n1\n)\n\n\n\n, \n\n\n\u03bc\n\n\n\n(\n2\n)\n\n\n\n, \n\n\u2026\n\n\n, \n\n\n\u03bc\n\n\n\n(\nM\u22121\n)\n\n\n\n, \n\n\n\u03bc\n\n\n\n(\nM\n)\n\n\n\n, and \n\n\n\u03bc\n\n\n\n(\ni\n)\n\n\n\n could be any instances of \n\u03bc\n given \n\ni\n=\n1\n,\n\u2026\n,\nM\n\n. \n\n\n\u03bc\n\n\n\n(\ni\n)\n\n\n\n could be scalar (\n\n\n\n\u03bc\n\n\n\n(\ni\n)\n\n\n\n\u2208\nR\n\n) or tensor valued (e.g.\u00a0\n\n\n\n\u03bc\n\n\n\n(\ni\n)\n\n\n\n\u2208\n\n\nR\n\n\n\n\nn\n\n\nd\n\n\n\n\n\n\nor\n\n\n\n\nR\n\n\n\n\nn\n\n\nd\n\n\n\n\n\u00d7\n\n\nR\n\n\n\n\nn\n\n\nd\n\n\n\n\n\n) generalized parameters. We want to emphasize that \nX\n is an exact solution of \n\nF\n\n\nX\n;\n\n\nt\n\n\nn\n\n\n,\n\n\n\u03bc\n\n\n\n(\ni\n)\n\n\n\n\n\n\n and \n\n\nX\n\n\nh\n\n\n is an approximation obtained from FOM. In general, \n\n\n\u03bc\n\n\n\n(\ni\n)\n\n\n\n could correspond to physical properties, geometric characteristics, or boundary conditions at any given time \nt\n. In this study, we limit our interest in approximate primary variables \n\n\nX\n\n\nh\n\n\n for a solution of the system of PDEs given the generalized parameters \n\u03bc\n such as heterogeneous permeability fields that are constant through time \nt\n.\n\n\n2.2\nFramework development\nAs in a conceptual schematic (Fig.\u00a02), we train our framework using \n\n\nX\n\n\nh\n\n\n obtained from FOM to deliver \n\n\n\n\nX\n\n\nh\n\n\n\n\n\u0302\n\n\n with acceptable accuracy and high computational efficiency. The proposed framework consists of (1) the offline phase starting from data generation of permeability fields and \n\n\nX\n\n\nh\n\n\n to training of our proposed CcGAN and (2) the online phase of predicting \n\n\n\n\nX\n\n\nh\n\n\n\n\n\u0302\n\n\n as presented in Fig.\u00a02b.\n\n\nRemark 1\n\n\nWe note that the use of critic is essential in our framework. We have tested in our previous study\u00a0(Kadeethum et al., 2021d) and shown that the model without critic delivers higher relative root mean square errors, approximately three to four times greater than the model with critic. Please refer to\u00a0Kadeethum et al. (2021d) - Supplementary Section 4.6 for more details. Furthermore, we also have illustrated that the skip connections inside U-Net are critical as demonstrated in\u00a0Kadeethum et al. (2021d) - Supplementary Section\u00a04.5 where the model with skip connections (U-Net generator) outperforms the model without skip connections (deep convolutional autoencoder generator) by a factor of 2 \n\u223c\n 3.\n\n\n\n2.2.1\nOffline stage\nThe first step is an initialization of training, validation, and test sets of parameters (\n\n\n\u03bc\n\n\ntraining\n\n\n, \n\n\n\u03bc\n\n\nvalidation\n\n\n, and \n\n\n\u03bc\n\n\ntest\n\n\n) of cardinality \n\n\nM\n\n\ntraining\n\n\n, \n\n\nM\n\n\nvalidation\n\n\n, and \n\n\nM\n\n\ntest\n\n\n, respectively. We illustrate only \n\u03bc\n and \n\n\n\u03bc\n\n\ntest\n\n\n in Fig.\u00a02 and omit a subscript of training in \n\n\n\u03bc\n\n\ntraining\n\n\n and \n\n\nM\n\n\ntraining\n\n\n hereinafter for the sake of brevity. We want to emphasize that \n\u03bc\n\n\n\u2229\n\n\n\n\n\u03bc\n\n\nvalidation\n\n\n = \n\u2205\n, \n\u03bc\n\n\n\u2229\n\n\n\n\n\u03bc\n\n\ntest\n\n\n = \n\u2205\n, and \n\n\n\u03bc\n\n\nvalidation\n\n\n\n\n\u2229\n\n\n\n\n\u03bc\n\n\ntest\n\n\n = \n\u2205\n. These \n\u03bc\n, \n\n\n\u03bc\n\n\nvalidation\n\n\n, and \n\n\n\u03bc\n\n\ntest\n\n\n here represent any physical properties, but it could also serve as geometric characteristics or boundary conditions. In this work, we follow\u00a0Kadeethum et al. (2021d) and focus on using \n\u03bc\n, \n\n\n\u03bc\n\n\nvalidation\n\n\n, and \n\n\n\u03bc\n\n\ntest\n\n\n to represent collections of spatially heterogeneous scalar coefficients \u2014 more specifically heterogeneous permeability fields as described in\u00a0Section\u00a03 for data generation.\nIn the second step, we query the FOM, which can provide a solution in a finite-dimensional setting for each parameter \n\u03bc\n (i.e.,\u00a0\n\n\n\u03bc\n\n\n\n(\ni\n)\n\n\n\n) in the training set. Throughout this study, for the sake of simplicity, we use a uniform time-step which leads to each query of the FOM having the same number of \n\n\nN\n\n\nt\n\n\n. However, as presented in Fig.\u00a01, our framework could handle cases where adaptive time-stepping is required, for instance, advection\u2013diffusion problems. The same operations follow for each \n\n\n\u03bc\n\n\nvalidation\n\n\n and \n\n\n\u03bc\n\n\ntest\n\n\n in the validation and test sets. This work focuses on the linear poroelasticity equations and demonstrates our proposed framework with highly heterogeneous permeability fields. The FOM is used to approximate primary variables \n\n\nX\n\n\nh\n\n\n, which correspond to bulk displacement (\n\n\nu\n\n\nh\n\n\n) and fluid pressure (\n\n\np\n\n\nh\n\n\n) fields at each time-step \n\n\nt\n\n\nn\n\n\n given the field of parameters \n\n\n\u03bc\n\n\n\n(\ni\n)\n\n\n\n, in this case \u2014 permeability field, as input. Please find the detailed description in Appendix\u00a0A.\nIn the third step, ROM is constructed by training the data generated from the FOM where the inputs to the model are \n\n\nt\n\n\nn\n\n\n and \n\n\n\u03bc\n\n\n\n(\ni\n)\n\n\n\n, and the output is \n\n\nu\n\n\nh\n\n\n or \n\n\nX\n\n\nh\n\n\n with given \n\n\nt\n\n\nn\n\n\n and \n\n\n\u03bc\n\n\n\n(\ni\n)\n\n\n\n. In this study, we build a separate model for each primary variable (\n\n\nu\n\n\nh\n\n\n and \n\n\np\n\n\nh\n\n\n). Although both primary variables can be trained together with a single model, construction of each separate model could provide flexibility and save time. However, the reasoning behind this choice is the limitation of the graphics processing unit\u2019s (GPU) memory; as we increase our input and\/or output space, we also require more memory to hold the incremental data as well as the model\u2019s parameters. A key aspect of this work is to apply the CcGAN image-to-image translation framework for time-dependent PDEs by adapting the concept of a naive label input (NLI) and an improved label input (ILI) proposed by\u00a0Ding et al. (2020) to the framework developed by\u00a0Kadeethum et al. (2021d). The proposed framework in this work consists of a generator and critic where two types of architecture for the generator with a similar critic architecture are presented (Fig.\u00a03).\nThe first one uses the NLI concept (i.e.,\u00a0NLI model) by introducing a temporal term (\n\n\n\nt\n\n\nn\n\n\n\u2208\nT\n\n) to the generator\u2019s bottleneck using element-wise addition. The details of the architecture can be found in Table\u00a0B.1 and Listing B.1. The second one adopts the ILI concept (i.e.,\u00a0ILI model) by injecting the temporal term to all layers inside the generator through conditional batch normalization\u00a0(De\u00a0Vries et al., 2017). However, in contrast to\u00a0Ding et al. (2020) and\u00a0De\u00a0Vries et al. (2017), our \n\n\nt\n\n\nn\n\n\n is not categorical data (i.e.,\u00a0not a tag of number ranging from zero to nine), but continuous variable (i.e.,\u00a0\n\n\n\nt\n\n\nn\n\n\n\u2208\nT\n\n). Hence, we replace embedded layers with an artificial neural network (ANN). To elaborate, each conditional batch normalization composes of one ANN and one batch normalization layer. Each ANN composes of one input, three hidden, and one output layers. Each hidden layer and the input layer are subjected to the hyperbolic tangent (tanh) activation function. In this way, we can inform each \n\n\nt\n\n\nn\n\n\n to each layer of our generator through a conditional batch normalization concept, see Listing B.3 for the implementation. The details of the ILI architecture can be found in Table\u00a0B.2 and Listing B.2.\nThe critic, similar for both NLI and ILI, uses time \n\n\nt\n\n\nn\n\n\n, parameter \n\n\n\u03bc\n\n\n\n(\ni\n)\n\n\n\n, and primary variable (\n\n\nu\n\n\nh\n\n\n or \n\n\np\n\n\nh\n\n\n) as its inputs. The output is a patch score added with an inner product calculated using two linear layers and output from the last contracting block (\n\n\n4\n\n\nth\n\n\n contracting block) of the critic. To elaborate, parameter (\n\n\n\u03bc\n\n\n\n(\ni\n)\n\n\n\n) and primary variable (\n\n\nu\n\n\nh\n\n\n or \n\n\np\n\n\nh\n\n\n) are fed into the \n\n\n1\n\n\nst\n\n\n convolutional layer of the critic while time (\n\n\nt\n\n\nn\n\n\n) is injected into the model using \n\n\n2\n\n\nnd\n\n\n linear layer shown in Fig.\u00a03. The output from the \n\n\n4\n\n\nth\n\n\n contracting block of the critic is then passed through the \n\n\n1\n\n\nst\n\n\n linear layer shown in Fig.\u00a03 and performed an inner product operation with the output from the \n\n\n2\n\n\nnd\n\n\n linear layer. The result of this inner product is then added (element-wise) to the patch score presented in Fig.\u00a03. The architecture of the critic can be found in Table\u00a0B.3 and Listing B.4.\nTo train both generator and critic, we normalize \n\n\nt\n\n\nn\n\n\n, \n\n\n\u03bc\n\n\n\n(\ni\n)\n\n\n\n, and primary variables (\n\n\nX\n\n\nh\n\n\n or, in this paper, \n\n\nu\n\n\nh\n\n\n and \n\n\np\n\n\nh\n\n\n) to be in a range of \n\n[\n0\n,\n1\n]\n\n. The Wasserstein (W) loss (Arjovsky et al., 2017; Gulrajani et al., 2017) is used since it has been shown to provide the best result when dealing with building data-driven frameworks for PDEs as shown in\u00a0Kadeethum et al. (2021d). In short, this implementation enforces the model to approximate the training data distribution instead of aiming to do the point-to-point mapping. This improves our model generalization, which is essential as we deal with heterogeneous permeability fields. The W loss is expressed as \n\n(2)\n\n\n\n\nmin\n\n\nG\n\n\n\n\nmax\n\n\nC\n\n\n\n\n\n\n\u2113\n\n\na\n\n\n+\n\n\n\u03bb\n\n\nr\n\n\n\n\n\u2113\n\n\nr\n\n\n+\n\n\n\u03bb\n\n\np\n\n\n\n\n\u2118\n\n\np\n\n\n\n\n.\n\n\n\nHere, \nG\n and \nC\n are short for generator and critic, respectively, and \n\n\n\u2113\n\n\na\n\n\n is the Earth mover\u2019s distance defined as \n\n(3)\n\n\n\n\n\u2113\n\n\na\n\n\n=\n\n\n1\n\n\nB\n\n\n\n\n\u2211\n\n\ni\n=\n1\n\n\nB\n\n\nC\n\n(\n\n\nt\n\n\ni\n\n\n,\n\n\n\u03bc\n\n\n\n(\ni\n)\n\n\n\n,\n\n\n\n\nX\n\n\nh\n\n\n\n\ni\n\n\n)\n\n\u2212\n\n\n1\n\n\nB\n\n\n\n\n\u2211\n\n\ni\n=\n1\n\n\nB\n\n\nC\n\n\n\n\nt\n\n\ni\n\n\n,\n\n\n\u03bc\n\n\n\n(\ni\n)\n\n\n\n,\n\n\n\n\n\n\nX\n\n\nh\n\n\n\n\ni\n\n\n\n\n\u0302\n\n\n\n\n,\n\n\n\nwhere \n\nC\n\n\n\u22c5\n\n\n\n is the final score of the critic with given \n\n\nt\n\n\ni\n\n\n, \n\n\n\u03bc\n\n\n\n(\ni\n)\n\n\n\n, and \n\n\n\n\nX\n\n\nh\n\n\n\n\ni\n\n\n or \n\n\n\n\n\n\nX\n\n\nh\n\n\n\n\ni\n\n\n\n\n\u0302\n\n\n shown in Fig.\u00a03, and \nB\n is a batch size, which is set as \n\nB\n=\n4\n\n. The Earth mover\u2019s distance is used to measure the distance between output of the model and the training data. This way helps our model to better generalize its prediction. Additionally, \n\n\n\u03bb\n\n\nr\n\n\n is a user-defined penalty constant that we set at \n\n\n\n\u03bb\n\n\nr\n\n\n=\n500\n\n. This constant is normally used as a hyper-parameter to balance the importance of the reconstruction error term (\n\n\n\u2113\n\n\nr\n\n\n) in relative to the other two terms (\n\n\n\u2113\n\n\na\n\n\n and \n\n\n\u2118\n\n\np\n\n\n). We pick \n\n\n\n\u03bb\n\n\nr\n\n\n=\n500\n\n according to the trail and error performed by\u00a0Kadeethum et al. (2021d). \n\n\n\u2113\n\n\nr\n\n\n as a reconstruction error term is given by \n\n(4)\n\n\n\n\n\u2113\n\n\nr\n\n\n=\n\n\n1\n\n\nB\n\n\n\n\n\u2211\n\n\ni\n=\n1\n\n\nB\n\n\n\n|\n\n\n\n\n\n\nX\n\n\nh\n\n\n\n\ni\n\n\n\n\n\u0302\n\n\n\u2212\n\n\n\n\nX\n\n\nh\n\n\n\n\ni\n\n\n|\n\n.\n\n\n\n\n\n\n\n\n\n\u03bb\n\n\np\n\n\n denotes a gradient penalty constant set to 10 throughout this study. \n\n\n\u03bb\n\n\np\n\n\n is a hyper-parameter to regularize the importance of the Lipschitz continuity enforcement (\n\n\n\u2118\n\n\np\n\n\n). It has been selected as 10 by the recommendation of\u00a0Gulrajani et al. (2017). We note that the selection of \n\n\n\u03bb\n\n\nr\n\n\n and \n\n\n\u03bb\n\n\np\n\n\n should be done through hyper-parameter optimizations. In this study, however, we select these values according to previous studies\u00a0(Gulrajani et al., 2017; Kadeethum et al., 2021d). \n\n\n\u2118\n\n\np\n\n\n is the gradient penalty regularization. The latter is used to enforce Lipschitz continuity of the weight matrices (\nW\n), i.e.,\u00a0Euclidean norm of discriminator\u2019s gradient is at most one, and it reads as \n\n(5)\n\n\n\n\n\u2118\n\n\np\n\n\n=\n\n\n1\n\n\nB\n\n\n\n\n\u2211\n\n\ni\n=\n1\n\n\nB\n\n\n\n\n\n\n\n\n\u2016\n\u2207\nC\n\n(\n\n\n\n\nt\n\n\ni\n\n\n,\n\u03bc\n\n\ni\n\n\n,\n\n\n\n\n\n\nX\n\n\nh\n\n\n\n\u0304\n\n\n\ni\n\n\n)\n\n\u2016\n\n\n2\n\n\n\u2212\n1\n\n\n\n\n2\n\n\n,\n\n\n\nwhere \n\n\n\u2016\n\u22c5\n\u2016\n\n\n2\n\n\n is L\n\n\n\n2\n\n\n or Euclidean norm. This term helps to improve the stability of the training by limiting the step we can take in updating our trainable parameters (weight matrices (\nW\n) and biases (\nb\n). The term \n\n\n\n\n\n\nX\n\n\nh\n\n\n\n\u0304\n\n\n\ni\n\n\n is an interpolation between \n\n\n\n\n\n\nX\n\n\nh\n\n\n\n\n\u0302\n\n\n\n\ni\n\n\n and \n\n\n\n\nX\n\n\nh\n\n\n\n\ni\n\n\n, which is defined by \n\n(6)\n\n\n\n\n\n\nX\n\n\nh\n\n\n\n\u0304\n\n=\n\n\n\u03f5\n\n\ni\n\n\n\n\nX\n\n\nh\n\n\n+\n\n(\n1\n\u2212\n\n\n\u03f5\n\n\ni\n\n\n)\n\n\n\n\n\n\n\nX\n\n\nh\n\n\n\n\n\u0302\n\n\n\n\ni\n\n\n.\n\n\n\n\n\nWe randomly select \n\n\n\u03f5\n\n\ni\n\n\n for each \n\n\n\n\n\n\np\n\n\nh\n\n\n\n\u0304\n\n\n\ni\n\n\n from a uniform distribution on the interval of \n\n[\n0\n,\n1\n)\n\n. We use the adaptive moment estimation (ADAM) algorithm\u00a0(Kingma and Ba, 2014) to train the framework (i.e.,\u00a0updating a set of weight matrices (\nW\n) and biases (\nb\n)). The learning rate (\n\u03b7\n) is calculated as\u00a0(Loshchilov and Hutter, 2016) \n\n(7)\n\n\n\n\n\u03b7\n\n\nc\n\n\n=\n\n\n\u03b7\n\n\nmin\n\n\n+\n\n\n1\n\n\n2\n\n\n\n\n\n\n\u03b7\n\n\nmax\n\n\n\u2212\n\n\n\u03b7\n\n\nmin\n\n\n\n\n\n\n1\n+\ncos\n\n\n\n\n\n\nstep\n\n\nc\n\n\n\n\n\n\nstep\n\n\nf\n\n\n\n\n\u03c0\n\n\n\n\n\n\n\nwhere \n\n\n\u03b7\n\n\nc\n\n\n is a learning rate at \n\n\nstep\n\n\nc\n\n\n, \n\n\n\u03b7\n\n\nmin\n\n\n is the minimum learning rate (\n\n1\n\u00d7\n1\n\n\n0\n\n\n\u2212\n16\n\n\n\n), \n\n\n\u03b7\n\n\nmax\n\n\n is the maximum or initial learning rate (\n\n1\n\u00d7\n1\n\n\n0\n\n\n\u2212\n4\n\n\n\n), \n\n\nstep\n\n\nc\n\n\n is the current step, and \n\n\nstep\n\n\nf\n\n\n is the final step. We note that each step refers to each time we perform back-propagation, including updating both generator and critic\u2019s parameters (\nW\n and \nb\n).\n\n\n2.2.2\nOnline stage\nFor the fourth step, we use the trained generator to predict \n\n\n\n\nX\n\n\nh\n\n\n\n\n\u0302\n\n\n given \n\n\nt\n\n\nn\n\n\n and \n\n\n\u03bc\n\n\nvalidation\n\n\n or \n\n\n\u03bc\n\n\ntest\n\n\n for the instances of the parameters belonging to the validation and test sets. To avoid over-fitting, we first use \n\n\n\u03bc\n\n\nvalidation\n\n\n to evaluate our framework as a function of epoch. Subsequently, we select the model (fixed \nW\n and \nb\n at a certain epoch) that provides the best accuracy for \n\n\n\u03bc\n\n\nvalidation\n\n\n to test the \n\n\n\u03bc\n\n\ntest\n\n\n. To elaborate, we train our model for 50 epochs. We then test our model against our validation set and observe the model performance as a function of the epoch. We then select a set of \nW\n and \nb\n at the epoch that has the best accuracy. Other hyper-parameters including a number of convolutional neural network (CNN) layers, a number of hidden layers, and CNN parameters, and initialization of the framework are used based on the study in\u00a0Kadeethum et al. (2021d).\n\n\nRemark 2\nAs presented in Fig.\u00a01, by treating the time domain as a continuous variable, our framework could be trained using training data that contains different time-step. Furthermore, during our online inquiry, we simply inquire at the time of interest within the time domain provided during the training phase, which may or may not exist in the training data (see Section\u00a04.3.1 for the illustration). This way we do not introduce any additional interpolation error to our approximation. This characteristic is an asset of our framework because our framework is not bound by a time-stepping scheme that traditional numerical analysis or other data-driven framework has\u00a0Zhu and Zabaras (2018), Mo et al. (2019), Wen et al. (2021a), Xu et al. (2021). Our framework can evaluate quantities of interest at any time required. For instance, we may be interested in a pressure field at one, two, and three hours with a given permeability field. To achieve that using FOM, one may need to go through many intermediate steps in between to satisfy, but our framework could evaluate any particular time-step immediately within training data we evaluate.\n\n\n\n\nRemark 3\nWith time domain as a continuous variable, we do not need to treat (\n\n\nt\n\n\nn\n\u2212\n1\n\n\n, \n\n\n\u03bc\n\n\n\n(\ni\n)\n\n\n\n) and (\n\n\nt\n\n\nn\n\n\n, \n\n\n\u03bc\n\n\n\n(\ni\n)\n\n\n\n) dependently. It means that we do not feed (\n\n\nt\n\n\nn\n\u2212\n1\n\n\n, \n\n\n\u03bc\n\n\n\n(\ni\n)\n\n\n\n) and (\n\n\nt\n\n\nn\n\n\n, \n\n\n\u03bc\n\n\n\n(\ni\n)\n\n\n\n) to our model sequentially. In fact, we shuffle our training set (\n\n\nt\n\n\nn\n\n\n, \n\n\n\u03bc\n\n\n\n(\ni\n)\n\n\n\n where \n\nn\n=\n0\n,\n\u2026\n,\n\n\nN\n\n\nt\n\n\n\n and \n\ni\n=\n1\n,\n\u2026\n,\n\n\nM\n\n\ntrain\n\n\n\n) independently. For the aspect of computational time, as we stack our parameter domain \nP\n and time domain \nT\n (i.e.,\u00a0our total training snapshots are \n\n\n\nN\n\n\nt\n\n\n\n\nM\n\n\ntrain\n\n\n\n, we have similar training burden compared to POD framework (see\u00a0Kadeethum et al., 2021a). In a typical POD operation, one may not be able to compress all snapshots simultaneously because of the computational resource limitations, and therefore, resort to nested POD algorithm (or hierarchical approximate POD) to mitigate this issue by sacrificing some accuracy\u00a0(Audouze et al., 2009; Rap\u00fan and Vega, 2010; Audouze et al., 2013; Ballarin et al., 2016; Himpe et al., 2018; Wang et al., 2019). We use batch training as an alleviation for our framework (or machine learning models in general). To make sure of a stable training, batch normalization is also utilized in our framework; see Appendix\u00a0B for more information.\n\n\n\n\n\n\n3\nData generation\nWe utilize a discontinuous Galerkin finite element (FE) model of linear poroelasticity developed in\u00a0Kadeethum et al. (2021c, 2020a) to generate training, validation, and test sets, see Fig.\u00a02 - initialization. The geometry, boundary conditions, and input parameters are similar to that used in\u00a0Kadeethum et al. (2021d) where a steady-state solution of linear poroelasticity is studied, but, in this work, the temporal output of pressure and displacement are investigated, resulting in the dynamic behavior of pressure (\n\n\np\n\n\nh\n\n\n), displacement (\n\n\nu\n\n\nh\n\n\n) as well as pore volume. The mesh and boundary conditions over the square domain used in this work are presented in Fig.\u00a0A.1. We enforce constant pressures of 0 and 1000 Pa at the top and bottom boundaries, respectively, to allow fluid to flow from the bottom to the top while no-flow boundary on both left and right sides. Furthermore, we compress the medium with normal traction of 1000 Pa applied at the top boundary. We fix the normal displacement to zero m for the left, right, and bottom boundaries. The initial pressure is 1000 Pa, and initial displacement is calculated based on the equilibrium state.\nTo obtain a set of parameters \n\u03bc\n corresponding to heterogeneous \nk\n fields, we focus on two types of highly heterogeneous \nk\n fields generated using: (1) a Zinn & Harvey transformation\u00a0(Zinn and Harvey, 2003), and (2) a bimodal transformation\u00a0(M\u00fcller and Sch\u00fcler, 2020). The details of generation of \nk\n fields is available in\u00a0Kadeethum et al. (2021d). Briefly, the \nk\n field from the Zinn & Harvey transformation has a wider range of \nk\n values with thinner high permeability pathways. This feature represents highly heterogeneous sedimentary aquifers with preferential flow pathways, such as the MADE site in Mississippi\u00a0(Zinn and Harvey, 2003) and the Culebra dolomite developed for the Waste Isolation Pilot Plant (WIPP) project in New Mexico\u00a0(Yoon et al., 2013). In contrast, the \nk\n field from the bimodal transformation has narrow range \nk\n values with wider high permeability pathways, which is a good representation of sandstone reservoirs with an iron inclusion, for example, Chadormalu reservoirs in Yazd province, Iran\u00a0(Daya, 2015). A few examples of \nk\n fields from both transformations are shown in Figs. 4.\nIn this work, three examples of \nk\n fields are used. Two examples are from Zinn & Harvey (Example 1) and bimodal (Example 2) distributions. For Example 3, these two \nk\n fields are used together. Note that we employ unstructured grids in the finite element solver. However, our framework in this study requires a structured data set. Thus, we interpolate the FE result \n\n\np\n\n\nh\n\n\n to structured grids using cubic spline interpolation. We then replace the FOM dimension \n\n\nN\n\n\nh\n\n\np\n\n\n, associated with the unstructured grid, with a pair \n\n\n(\n\n\n\n\nN\n\n\n\u02dc\n\n\n\n\nh\n\n\np\n\n\n,\n\n\n\n\nN\n\n\n\u02dc\n\n\n\n\nh\n\n\np\n\n\n)\n\n=\n\n(\n128\n,\n128\n)\n\n\n, corresponding to the structured grid. The same procedures are carried out for the displacement field \n\n\nu\n\n\nh\n\n\n.\nFor simplicity, the FE solver employs a fixed number of \n\n\nN\n\n\nt\n\n\n for all \n\n\n\nk\n\n\n\n(\ni\n)\n\n\n\n\u2208\nk\n\n. Our time domain is set as \n\nT\n=\n\n\n0\n,\n250\n\n\n\n seconds, and \n\n\n\nN\n\n\nt\n\n\n=\n10\n\n, which leads to \n\n\u0394\nt\n=\n25\n\n seconds. The total size of data set is calculated by \n\n\n\nM\n\n\ni\n\n\n\n\nN\n\n\nt\n\n\n\n where \ni\n is the number of training, validation, or test sets. While we investigate the effect of \nM\n on the data-driven model accuracy, \n\n\n\nM\n\n\nvalidation\n\n\n=\n\n\nM\n\n\ntest\n\n\n=\n500\n\n is fixed. The samples of the test set of \n\n\nk\n\n\n\n(\ni\n)\n\n\n\n, \n\n\n\np\n\n\nh\n\n\n\n\n\n\nt\n\n\nn\n\n\n,\n\n\nk\n\n\n\n(\ni\n)\n\n\n\n\n\n\n, and \n\n\n\nu\n\n\nh\n\n\n\n\n\n\nt\n\n\nn\n\n\n,\n\n\nk\n\n\n\n(\ni\n)\n\n\n\n\n\n\n are presented in Fig.\u00a04. We note that the difference (DIFF) between solutions produced by the FOM (FEM in this case) and ROM (CcGAN in this case) is calculated by \n\n(8)\n\n\n\n\nDIFF\n\n\nX\n\n\n\n(\n\n\nt\n\n\nn\n\n\n,\n\n\n\u03bc\n\n\ntest\n\n\n\n(\ni\n)\n\n\n\n)\n\n=\n\n|\n\n\nX\n\n\nh\n\n\n\n(\n:\n,\n\n\nt\n\n\nn\n\n\n,\n\n\n\u03bc\n\n\ntest\n\n\n\n(\ni\n)\n\n\n\n)\n\n\u2212\n\n\n\n\nX\n\n\n\u0302\n\n\n\n\nh\n\n\n\n(\n:\n,\n\n\nt\n\n\nn\n\n\n,\n\n\n\u03bc\n\n\ntest\n\n\n\n(\ni\n)\n\n\n\n)\n\n|\n\n.\n\n\n\nTo reiterate, \n\n\nX\n\n\nh\n\n\n represents \n\n\np\n\n\nh\n\n\n and \n\n\nu\n\n\nh\n\n\n, \n\n\n\n\nX\n\n\n\u0302\n\n\n\n\nh\n\n\n represents \n\n\n\n\np\n\n\n\u02c6\n\n\n\n\nh\n\n\n and \n\n\n\n\nu\n\n\n\u02c6\n\n\n\n\nh\n\n\n, and \n\n\n\u03bc\n\n\ntest\n\n\n\n(\ni\n)\n\n\n\n is \n\n\nk\n\n\ntest\n\n\n\n(\ni\n)\n\n\n\n field. We also use the relative root mean square error (relative RMSE) between \n\n\nx\n\n\ni\n\n\n (FOM) and \n\n\n\n\nx\n\n\n\u02c6\n\n\n\n\ni\n\n\n (ROM) to evaluate the model performance as \n\n(9)\n\n\nrelativeRMSE\n=\n\n\n\n\n\n\n\n\n\u2211\n\n\ni\n=\n1\n\n\nM\n\n\n\n\n\n\n\n\nx\n\n\ni\n\n\n\u2212\n\n\n\n\nx\n\n\n\u02c6\n\n\n\n\ni\n\n\n\n\n\n\n2\n\n\n\n\nM\n\n\n\n\n\n\n\n\n\n\n\n\n\u2211\n\n\ni\n=\n1\n\n\nM\n\n\n\n\nx\n\n\ni\n\n\n2\n\n\n\n\nM\n\n\n\n\n\n\n,\n\n\n\nx\n\n\ni\n\n\n\u2208\n\n\nX\n\n\nh\n\n\nand\n\n\n\n\nx\n\n\n\u02c6\n\n\n\n\ni\n\n\n\u2208\n\n\n\n\nX\n\n\nh\n\n\n\n\n\u0302\n\n\n.\n\n\n\n\n\n\n\n\n\n\n4\nResults and discussion\n\n4.1\nExample 1: Zinn & Harvey transformation\nThe first Example test cases from the Zinn & Harvey transformation are shown in Figs. 4a\u2013b, e\u2013f, including \nk\n fields, FOM and ROM results, and DIFF fields for pressure and displacement fields, respectively. The box plots of relative RMSE values of pressure (\n\n\n\n\np\n\n\n\u02c6\n\n\n\n\nh\n\n\n) during training for different training samples are presented for NLI and ILI models with the validation set in Figs.\u00a0C.2 and C.3, respectively. As expected, the relative RMSE values improve over epochs during training, but reaches a plateau around epochs \n\u2248\n 32\u201334. The model performance is improved with increasing the number of training samples (\nM\n). Figs.\u00a0C.2 and C.3 show that the ILI model performs slightly better than the NLI model. The behavior of the results of \n\n\n\n\nu\n\n\n\u02c6\n\n\n\n\nh\n\n\n is similar to that of \n\n\n\n\np\n\n\n\u02c6\n\n\n\n\nh\n\n\n (results not shown).\nThe best-trained model is tested against the test set. The distributions of \n\n\n\n\np\n\n\n\u02c6\n\n\n\n\nh\n\n\n and \n\n\n\n\nu\n\n\n\u02c6\n\n\n\n\nh\n\n\n for the selected test cases are shown in Fig.\u00a04a\u2013b and e\u2013f, respectively. The DIFF values are very low (i.e.,\u00a0less than one percent of the average field values). The relative RMSE results of both \n\n\n\n\np\n\n\n\u02c6\n\n\n\n\nh\n\n\n and \n\n\n\n\nu\n\n\n\u02c6\n\n\n\n\nh\n\n\n of the test set are provided in Table\u00a01, which are very close to those of the validation set (Figs.\u00a0C.2 and C.3). As in training, model performance improves with increasing \nM\n. Besides, ILI always performs better than NLI. The relative RMSE of displacement (\n\n\n\n\nu\n\n\n\u02c6\n\n\n\n\nh\n\n\n) is generally lower than that of pressure (\n\n\n\n\np\n\n\n\u02c6\n\n\n\n\nh\n\n\n), which attributes to the relatively uniform response of displacement fields, compared to the pressure field as shown in Fig.\u00a04. Hence, the ROM can learn the solution easier.\n\n\n\n\n\n4.2\nExample 2: bimodal transformation\nThe second Example presents the model performance using \nk\n fields from the bimodal transformation, which has a narrow range of \nk\n values with wider high permeability pathways. As in Example 1, selected test cases of \nk\n fields, FOM and ROM results, and DIFF fields are presented in Fig.\u00a04c\u2013d and g\u2013h. The box plot of relative RMSE values of the validation set is presented in Figs.\u00a0C.4 and C.5. Similar to Example 1, the model performance improves with increasing \nM\n. Moreover, the higher the number of epochs, the model tends to provide more accurate results. Although there are some fluctuations of the relative RMSE values at a later training stage, the model accuracy tends to improve as the training progresses. Except for the case where \n\nM\n=\n1250\n\n, the ILI model provides better results than the NLI.\nFor the testing set, the ILI model provides better accuracy than the NLI except \n\nM\n=\n1250\n\n for \n\n\n\n\np\n\n\n\u02c6\n\n\n\n\nh\n\n\n (Table\u00a01). It is noted that ILI always performs better than NLI for the state variable \n\n\n\n\nu\n\n\n\u02c6\n\n\n\n\nh\n\n\n. Moreover, the relative RMSE results of \n\n\n\n\nu\n\n\n\u02c6\n\n\n\n\nh\n\n\n is always lower than those of \n\n\n\n\np\n\n\n\u02c6\n\n\n\n\nh\n\n\n, which are similar to Example 1.\nThe relative RMSE of Example 2 is slightly lower than that of Example 1 (Table\u00a01), however, the trend of the relative RMSE values between NLI and ILI is similar in both Examples 1 and 2. We will discuss the performance of NLI and ILI in the next sections. Since \nk\n fields from the bimodal transformation have a narrower range and wider permeable pathways with less contrast compared to those from the Zinn & Harvey transformation (see Fig.\u00a04), the corresponding pressure field may have similar features. This can be seen in the DIFF distribution where the DIFF values are larger along high-pressure gradient regions in all pressure cases (Fig.\u00a04a\u2013d). At \n\nt\n=\n25\n\n in Example 2 (Fig.\u00a04c), high DIFF values are mostly located near the top boundary where the pressure boundary is set to zero after the initial pressure of 1000 Pa. Over time the DIFF distribution propagates as the pressure contrast migrates along the high permeability regions (e.g.,\u00a0the DIFF fields at \n\nt\n=\n250\n\n in Fig.\u00a04d). Compared to Example 1 where pressure gradients tend to be slightly gradual (e.g.,\u00a0wider transition) and the high DIFF values are distributed in larger areas, Example 2 cases show the higher contrast of pressure values along with the pressure transition, and the high DIFF values tend to be distributed more locally (Fig.\u00a04a\u2013d).\nAlthough this comparison qualitatively shows the dependency of the model performance on input and output characteristics, it is also well-known that deep neural networks often need complex neural network architecture to extract and learn high-frequency features such as high permeability contrast and high-pressure gradients in this work\u00a0(Xu et al., 2019). A recent work by\u00a0Kim et al. (2021) transformed physical connectivity information of the high contrast drainage network into multiple binary matrices, which improved the network generation using deep convolutional GAN. However, the success rates of the drainage network with proper connectivity were relatively low. CcGAN developed in this work shows that although it is still challenging to improve the prediction accuracy, the increase in the training data sets may provide a potential solution to this challenging problem. However, the increase of training data sets also increases required computational resources. This aspect will be discussed later. For displacement results (Fig.\u00a04e\u2013h) the relative RMSE results in Example 2 (Table\u00a01) follow the same trend in the pressure results. The lower relative RMSE values of displacement than pressure also stem from the smooth displacement fields compared to pressure fields. It is noted that the relative RMSE values of the test set are similar to those of the validation set.\n\n\n4.3\nExample 3: Combined Zinn & Harvey and bimodal transformations\nIn Example 3, permeability fields from both Zinn & Harvey and bimodal transformations are used to test the generalization ability of the proposed approach (i.e.,\u00a0Figs. 4a-h). As discussed earlier, Example 3 can represent different types of heterogeneity with high permeable pathways. The relative RMSE of the validation set for pressure fields (\n\n\n\n\np\n\n\n\u02c6\n\n\n\n\nh\n\n\n) is presented in Figs.\u00a0C.6 and C.7. Similar to Examples 1 and 2, the model accuracy improves with increasing \nM\n, and we did not observe any over-fitting behavior.\nAs presented in Table\u00a01, for both pressure and displacement fields, the ILI model performs better than the NLI, and the model with a higher \nM\n (i.e.,\u00a0more training data) provides better accuracy. Note that Example 3 has two times higher \nM\n than the other two cases. Overall, the relative RMSE values in Example 3 are closer to those in Example 1 rather than Example 2 for the same number of \nM\n. This indicates that more challenging fields for ML training predominantly govern the model performance with combined fields. In addition, Example 3 tends to have higher relative RMSE values than Examples 1 and 2 for the lower number of \nM\n (e.g.,\u00a0\nM\n = 2500 and 5000 for pressure and displacement, respectively). As the number of \nM\n increases, however, Example 3 has lower RMSE values than Example 1 and gets closer to Example 2, indicating that more training data sets improve the ML model to a certain degree. Although there may be more optimal hyperparameters to train both fields better, these results demonstrate the general learning capability of the proposed models. The inclusion of a more challenging data set will increase the generalization capability of the trained model.\n\n4.3.1\nPrediction at timestamps different from the training set\nOne of the main benefits of the CcGAN framework is that with parameters as a continuous variable, we can inquire about the parameters\u2019 values that are not in the training set. To illustrate this capability, we use the same trained model and test permeability fields from Example 3, however, we change the time domain from \n\nT\n=\n\n\n0\n,\n250\n\n\n\n seconds and \n\n\n\nN\n\n\nt\n\n\n=\n10\n\n (i.e.,\u00a0\n\n\u0394\nt\n=\n25\n\n seconds) to \n\nT\n=\n\n\n0\n,\n245\n\n\n\n and \n\n\n\nN\n\n\nt\n\n\n=\n7\n\n (i.e.,\u00a0\n\n\u0394\nt\n=\n35\n\n seconds). Hence, the timestamps for pressure and displacement fields in the test set do not coincide with those in the training set. We use \n\nM\n=\n10000\n\n (5000 samples from each field in Example 3). The results of the ILI model tested at new timestamps are shown in Fig.\u00a05b. Comparing the testing results at the same timestamps as in the training set (Fig.\u00a05a) with those at different timestamps shows that the mean values of relative RMSE are 1.61% and 1.85%, respectively, and the range of relative RMSE distribution lies within a similar range. Interestingly, the testing results at different timestamps have fewer outliers than those at the same timestamps, indicating that the CcGAN model proposed in this work could deliver good predictions at timestamps different from the training set.\n\n\n\n\n\n\n4.4\nComputational costs\nA summary of the wall time used for each operation (i.e.,\u00a0steps 2 to 4 in Fig.\u00a02) is presented in Table\u00a02 where the time for step 1 or initialization was negligible compared to the other steps. The FE model (FOM) was run using a single AMD Ryzen Threadripper 3970X, while training and testing of CcGAN or ROM models were carried out with single GPU (NVIDIA Quadro RTX 6000). With the fixed number of \n\n\n\nN\n\n\nt\n\n\n=\n10\n\n throughout this study, each FOM simulation (per \n\n\n\u03bc\n\n\n\n(\ni\n)\n\n\n\n) takes about 40\u00a0s. Consequently, if we select \n\nM\n=\n10000\n\n as an example, it would take about 400,000\u00a0s (\n\u2248\n111\u00a0h) to build the training set. To train the model using \n\nM\n=\n10000\n\n, it takes approximately 30\u00a0h. We note that the training time between NLI and ILI is relatively similar and the trainable parameters of the generator for each model are also at a similar order (184,555,201 and 194,026,225 for NLI and ILI, respectively). Besides, as we discussed previously, both models share similar critic with the trainable parameters of 303,161. With similar numbers of trainable parameters training time is relatively similar and the model\u2019s performance or training dynamics are not much different. During the online or the prediction phase, however, the trained model could provide its result within 0.001\u00a0s per testing \n\n\n\n\nt\n\n\nn\n\n\n,\n\n\n\u03bc\n\n\n\n(\ni\n)\n\n\n\n\n\n.\nIt should be noted that the ROM as the trained model in this work is not required to have the number of \n\n\nN\n\n\nt\n\n\n in which FOM uses. Assuming the ROM also uses \n\n\n\nN\n\n\nt\n\n\n=\n10\n\n, it still provides a speed-up by 10,000 times. One advantage of the CcGAN framework compared to the FOM is that it can also deliver the solution at any time, including times different from the training snapshots without sacrificing the accuracy (see Section\u00a04.3.1). This nonlinear interpolation characteristic in output space is an asset of the ROM in this work because it is not bound by any time-stepping constraints and can evaluate quantities of interest at any time required. For instance, we may be interested in pressure and displacement fields at one, two, and three hours with given different \n\u03bc\n. To achieve this with FOM, it may need to go through many steps in between. However, ROM enables us to evaluate it at those three times only.\n\n\n\n\n\n4.5\nPrediction accuracy and subsurface physics applications\nWith three examples presented, we observe that the ILI model performs better than the NLI for all cases except one particular case (Table\u00a01). This finding is in good agreement with classification problems in\u00a0Ding et al. (2020).\u00a0Ding et al. (2020) speculates that the ILI overcomes the label inconsistency of the classification problems while the NLI could not. Our reasoning for the outperformance of the INI stems from continuous conditional batch normalization, which can carry out temporal information more consistently than the element-wise addition in the NLI model. In addition to the skip connections that are essential in transferring multiscale features between input (permeability fields) and output (pressure and displacement fields) (see\u00a0Kadeethum et al., 2021d), CcGAN provides the framework to account for temporal features over time, resulting in better representation of temporal patterns.\nOne key aspect of many subsurface energy applications is a coupled process that involves hydrogeology and geomechanics. Although ML-based data-driven modeling has been increasingly studied for reservoir modeling, most are still limited to uncoupled processes (e.g.,\u00a0Lange and Sippel, 2020; Miah, 2020) or relatively homogeneous fields (e.g., Zounemat-Kermani et al., 2021; Zhao, 2021). The CcGAN approach proposed in this study demonstrates its capability to handle coupled hydro-mechanical processes with relative RMSE less than 2% of the transient pressure and displacement responses in the worst case. The results also show that the relative RMSE of the ILI model can be improved to a scale of about 1% with more training data sets, which can be acceptable given the uncertainty and observational error in the subsurface systems. Additionally, our framework for model prediction (i.e.,\u00a0online stage) achieves up to \n\u2248\n10,000x speed-up compared to the FE solver. Note that the computational advantage of ML-driven ROM models will increase further with increasing degree of freedom in the FE solver (e.g.,\u00a0three-dimensional and longer transient problems). This computational advantage and accuracy will enable us to achieve real-time reservoir management and robust uncertainty quantification even for vast parameter space. At the same time, the ROM can be updated offline as necessary. It should be noted that the method presented here can be extended to incorporate more continuous variables into the system. For instance, besides the time domain, we could also add Young\u2019s modulus and Poisson ratio into the CcGAN model. Furthermore, since this model is data-driven, it is not limited to only coupled hydro-mechanical processes presented in this manuscript but also applicable to other coupled processes such as thermal\u2013hydrological\u2013mechanical\u2013chemical (THMC) processes.\nThis work illustrates our framework\u2019s performance using two- dimensional (2-D) examples. However, subsurface applications usually involve a 3-D domain. Several approaches can be used to extend our framework to 3-D modeling. For an example, 3-D convolutional layers, batch normalization, and feature-map-block can be used for both generator and critic. As discussed in Section\u00a04.4, the training of our model has to be performed using GPU. Hence, extension to 3-D problems would require a certain amount of GPU VRAM to be available. If there is a limit in computational resources, we could slide the \nz\n-directional component into multiple layers and subsequently use a z-coordinate of each layer as a continuous input to the model (treating the z-coordinate similarly to the time domain). We note that this way might not be applicable to problems with a periodic unit, which is needed to accurately represent the characteristics of the microstructure. We will evaluate these methods as a part of extension to 3-D problems in the future.\n\n\n\n5\nConclusions\nThis work presents a data-driven framework for solving a system of time-dependent partial differential equations, more explicitly focusing on coupled hydro-mechanical processes in heterogeneous porous media. This framework can be used as a proxy for time-dependent coupled processes in heterogeneous porous media, which is challenging in classical model order reduction. Our framework is developed upon continuous conditional generative adversarial networks (CcGAN) composed of the U-Net generator and patch-based critic. The model has two variations: (1) the time domain is introduced to only the generator\u2019s bottleneck using element-wise addition (i.e.,\u00a0NLI), and (2) the time domain is injected into all layers inside the generator through conditional batch normalization (i.e.,\u00a0ILI). The critic is similar for both models. Our approach is desirable because it does not require any cumbersome modifications of FOM source codes and can be applied to any existing FOM platforms. In this regard, the CcGAN approach to solve time-dependent PDEs is uniquely employed to develop a data-driven surrogate model given highly heterogeneous permeability fields. We illustrate that our framework could efficiently and accurately approximate finite element results given a wide variety of permeability fields. Our results have a relative root mean square error of less than 2% with 10,000 samples for training. Additionally, it could speed up at least 10,000 times compared to a forward finite element solver. ILI delivers slightly better results than NLI without any observable additional computational costs. To this end, this framework will enable us to do a large-scale operation of real-time reservoir management or uncertainty quantification with complex heterogeneous permeability fields, which are practically very difficult to do with FOM and traditional model order reductions.\n\n\n6\nData and code availability\nWe will release our CcGAN scripts and training and testing data to reproduce all results in the manuscript through the Sandia National Laboratories software portal \u2014 a hub for GitHub-hosted open source projects (https:\/\/github.com\/sandialabs) after the manuscript will be accepted. For review purpose we provide the training, validation, and testing data or the scripts used to generate them at https:\/\/codeocean.com\/capsule\/2052868\/tree\/v1\u00a0(Kadeethum et al., 2021e). FOM source codes were already made available here: https:\/\/github.com\/teeratornk\/jcp_YJCPH_110030_git as well as a tutorial (see tutorial number 9) for multiphenics package https:\/\/github.com\/multiphenics.\n\n\nCRediT authorship contribution statement\n\nT. Kadeethum: Conceptualization, Formal analysis, Software, Validation, Writing \u2013 original draft, Writing \u2013 review & editing. D. O\u2019Malley: Conceptualization, Formal analysis, Supervision, Validation, Writing \u2013 review & editing. Y. Choi: Conceptualization, Formal analysis, Supervision, Validation, Writing \u2013 review & editing. H.S. Viswanathan: Conceptualization, Supervision, Writing \u2013 review & editing. N. Bouklas: Conceptualization, Formal analysis, Funding acquisition, Supervision, Writing \u2013 review & editing. H. Yoon: Conceptualization, Formal analysis, Funding acquisition, Supervision, Writing \u2013 review & editing.\n\n","6":"","7":"","8":"","9":"","10":"","11":"\n\n1\nIntroduction\nIn the era of \u2018big data\u2019 from satellite Earth observation (EO), a growing number of downstream applications require fully automated processing chains of satellite data. In the past decade, machine learning algorithms have been used increasingly to turn satellite imagery from optical or Synthetic Aperture Radar (SAR) systems into information that is relevant to the users. For example, the European Union's Copernicus initiative and its Sentinel satellite missions provide free daily images globally (Malenovsk\u1ef3 et al., 2012). The unprecedented data availability is particularly important for applications to forest change detections after deforestation, logging and forest fires and the use of deforestation alerts can lead to substantial reductions in deforestation in Africa (Moffette et al., 2021). At the COP26 climate conference in Glasgow in 2021, 141 nations committed to ending deforestation by 2030 \u2013 together covering over 90% of the world's forest area. This clearly demonstrates the need for rapid access to information on forest change. Current forest detection methods process satellite images at daily or weekly time scales. This short time-scale requires an automatic method to download and process satellite images to manage workloads by human interpreters.\nThe Pyeo Python package (Roberts et al., 2020) provides near-real-time analyses of satellite images in less than 24\u00a0h of data acquisition. \u2018Near-real-time\u2019 in this context means that the software automatically queries a satellite data hub using its API and enters into the satellite image processing chain every time a new image has been acquired. The Sentinel-2 satellites provide a new free-and-open image acquisition every 5 days over all global land areas. Planet imagery is acquired daily over the whole world but it is only available in near-real-time-mode at a prize. Pyeo uses the last available cloud-free pixel before the current acquisition date to detect spectral changes in comparison with the base layer composite. The majority of satellite images obtained in this way are likely not detecting any changes, but the few images that do make a big difference to the users as they enable them to take rapid action in areas of unlicensed logging. Pyeo has been applied in Kenya (described in this paper) but also in Colombia and Mexico (Pacheco-Pascagaza et al., 2022).\nGoogle Earth Engine provides a very widely used processing platform for satellite images, but it is limited in terms of its machine learning capability compared to the Python library Scikit-Learn, which allows far more parameter customisation than Google Earth Engine. In addition, where operational change detections from satellite imagery are required for national monitoring schemes, governments often do not want to depend on a private company such as Google for its continuity. Pyeo is implemented in Python, which was not fully supported on Google Earth Engine at the time we developed the software. Google Earth Engine also does not support the integration of the Sen2Cor atmospheric correction software, but pyeo does. While Google Earth Engine provides some functionality for machine learning, its programming interface allows only a limited range of customisation options for its random forest classification algorithm, whereas pyeo (inherited from Scikit-Learn) allows a full customisation. The performance of pyeo depends on the computing platform it runs on. We did not compare its performance to the Google Earth Engine platform because it is not possible to choose the exact same processing options for atmospheric correction from L1C to L2A products and the same machine learning options.\nThe development of Pyeo was motivated by the co-design of an operational deforestation alert system with stakeholders in Kenya in the REDD\u00a0+\u00a0Round Table, with a view to improving the national forest monitoring system (Kenya Ministry of Environment and Natural Resources, 2016). Pyeo uses machine learning for image change detection, automation of image queries and downloads and processing of new satellite image acquisitions for the delivery of information to end users. Pyeo currently enables the query, download and processing of Sentinel-2, Landsat and Planet images and it can easily incorporate other satellite data. Although several operational applications of forest monitoring from Landsat images are currently available (most notably Global Forest Watch; Hansen et al., 2013), few operational applications make use of the new high-resolution images from Sentinel-2. With the launch of the Sentinel-2 satellites, land cover change can in principle be monitored in near-real-time if the processing can be fully automated. With Sentinel-2A and 2B, the Sentinel-2 satellites provide multispectral imagery at up to 10\u00a0m spatial resolution every 5 days, an improvement over the combined Landsat 8 and 9 revisit time every 8 days at 30\u00a0m resolution.\nMachine learning algorithms are increasingly being used to analyse satellite imagery (Fern\u00e1ndez-Delgado et al., 2014). review 179 machine learning classifiers from 17 families of algorithms including Bayesian neural networks, support vector machines, random forests, generalized linear models and other methods. They evaluate these algorithms with 121 datasets and conclude that random forests (Breiman, 2001) generally performed best (Fern\u00e1ndez-Delgado et al., 2014).\nPyeo has been designed to give a number of options of machine learning methods based on the widely-used Scikit-Learn library (Pedregosa et al., 2011). Scikit-Learn does not have any specific functionality to handle geographic data that are characterised by metadata describing the location of the dataset, its map projection and pixel size, in the case of rasters. Generic Python packages to support the processing of geographic data include RasterIO (Gillies et al., 2013), which is a higher-level abstraction of GDAL's raster functions (GDAL\/OGR contributors, 2021) and Fiona for vector data processing (Gillies et al., 2013) based on GDAL functionality.\nExisting software libraries for the analysis of remote sensing images include RSGISLib (Bunting et al., 2014), which contains C and Python algorithms for image segmentation, object-based classification, image-to-image registration, image filtering, zonal statistics, and general raster and vector processing functionality. At the time of developing Pyeo, RSGISLib did not provide machine learning functionality and specific functions for Sentinel-2 imagery.\nThe BFAST (\u2018Breaks For Additive Season and Trend\u2019) package for R integrates the decomposition of a time series into trend, season, and residual terms for the purpose of change detection (Verbesselt et al., 2010); http:\/\/bfast.r-forge.r-project.org). There is also a spatial version of BFAST for processing time series of raster data, called \u2018bfastSpatial\u2019 for R (Dutrieux and DeVries, 2014);\nhttp:\/\/github.com\/loicdtx\/bfastSpatial). TIMESAT (J\u00f6nsson and Eklundh, 2004; http:\/\/web.nateko.lu.se\/timesat) is another software package for analysing time series of satellite images. It was developed for analysing the seasonality of satellite time series data and their relationship with vegetation phenology, originally from AVHRR Normalized Difference Vegetation Index (NDVI) data but has since then been expanded to include other satellite missions. Pytorch (Ketkar, 2017) is a Python library for artificial intelligence (AI) applications to images of any kind and was developed for convolutional neural network modelling (Afzaal et al., 2021) and other AI techniques.\nThe Pyeo package introduced in this paper provides an open-source Python toolbox for change detection and image processing for satellite data from Sentinel-2, Landsat and Planet in its current form, although future releases may include other missions. Contributions to the software made via Github are welcome. The innovative value of this software library lies in its integration of the functionality of previous Python libraries and the Sen2Cor atmospheric correction software to create a fully automated image processing chain for change detection. This enables the near-real-time application of machine learning methods to a continuous stream of satellite remote sensing images for rapid change detection in forestry and for other applications. The software does not attempt to supersede any pre-existing libraries but it rather presents an innovative way of integrating them.\n\n\n2\nMethodology\nPyeo was designed to provide a library of generic Python functions that can be used to build operational satellite image processing chains to produce updatable thematic change maps for a defined area of interest (AOI). In its current version, Sentinel-2, Landsat and Planet satellite images are supported. In applications of satellite remote sensing data streams to geoscientific applications for the purpose of change detection, several challenges have to be overcome. First, the software needs to be capable of automating the querying of the data server for new image acquisitions over the area-of-interest and download new images for processing. Second, it needs to choose and apply appropriate pre-processing steps to the new image to make it radiometrically consistent with the previously processed images. Third, appropriate image pairs have to be combined (or stacked) in order to detect changes. And fourth, a trained machine learning model or other change detection algorithm needs to be applied to the stack of images.\nThere are three layers of abstraction available in Pyeo: a program that will download, stack and classify images for change detection, a set of shorter scripts that perform single steps in the downloading\/preprocessing\/image stacking\/classification processing chain (Fig. 1\n), and finally a set of Python modules and functions for integrating into existing scripts, building new applications and training new machine learning models.\n\n2.1\nSoftware design choices\nPyeo functions have been written such that users can put together their own processing chains by copying and pasting functionalities as required. Most functions take paths to geospatial objects (raster or vector data) in the file system instead of passing data between them internally. Similarly, most geoprocessing functions take an out-path argument that writes the final product to a raster file. Though this approach causes more data read\/write operations, it is more intuitive for users who are more experienced in Graphical User Interface (GUI) Geographic Information Systems (GIS) software than Python, and this permits straightforward examination of intermediate raster products at each stage of processing for the purpose of quality-checking and debugging own processing chains.\nTo reduce memory overhead when working with large rasters, Pyeo takes two approaches. Where possible, a raster that is opened for processing will be memory-mapped using GetVirtualMemArray() over the entire raster. This permits numpy-accelerated vectorised operations by band, row or column depending on context, removing the need to calculate offsets - the virtual mapping keeps memory use to the minimum needed for a given process. In the instance that Pyeo is being run on an OS that does not support the mmap system call (such as Windows), there is an automatic patch in place that replaces gdal.GetVirtualMemArray() with an equivalent using numpy's mmap class. Pyeo also implements the design ideal that a large object (such as a raster) should not exist past its required context, writing to the file system at the end of each major function call as described above. In the current version, Pyeo does not implement tiling at memory level; very large datasets should be tiled at the file system level prior to processing.\nThe software package organization is described in Appendix 1.\n\n\n2.2\nData acquisition\nAt present, Pyeo has a fully mature module for the acquisition of Sentinel-2 data from the Copernicus Open Access Hub and the Synergise-provided AWS buckets on Sentinel Hub. There are also modules in progress for downloading data from Landsat and Planet; however, these are still in the prototype phase.\n\n2.2.1\nSentinel 2\nFor all processing, Pyeo takes the product catalogue supplied by the Copernicus Open Access Hub (https:\/\/scihub.copernicus.eu\/) as the canonical list of available image acquisitions. Queries are performed against this list using user-supplied credentials, and can be filtered by date range, area of interest (AOI), maximum cloud cover (according to the Hub) and processing level (L1C or L2A in the case of Sentinel-2). Once acquired, a list of images can be further filtered before being passed to a downloader.\nThe available downloaders are the following:\n\n\u25cf\nCopernicus Open Access Hub This is the default option for downloading Sentinel-2 images. Images are downloaded in.zip format, and then automatically unzipped. Users are required by the Hub to register with a username and password before downloading, and there is a limit to no more than two concurrent downloads per username at a time. The Copernicus Open Access Hub is entirely free to access, but imagery is moved to the Long-Term Archive after a certain period of time, where data quota restrictions exist for users.\n\n\n\u25cf\nAmazon Web Services (AWS) via Sentinel HubSentinel data are also publicly hosted on Amazon Web Services on Sentinel Hub. This storage is provided by Sinergise, and is normally updated a few hours after new products are made available. There is a small charge associated with downloading this data. To access the AWS repository, users are required to register for an Amazon Web Services account (including providing payment details) and obtain an API key for that account (see https:\/\/aws.amazon.com\/s3\/pricing\/for pricing details). There is no limit to the concurrent downloads for the AWS bucket.\n\n\n\n\n\n2.2.2\nLandsat\nAll Landsat images (Wulder et al., 2019) are at present downloaded from the United States Geological Survey.\n\n\n\n2.3\nData preparation\nThe objective of the pre-processing step is to extract the values of interest from downloaded data and convert them into a uniform internal format for comparison. For Pyeo, the internal format for raster processing is consistent with the GDAL convention; a single multiple-band array of order [bands, y, x], with associated affine geotransform, coordinate system and resolution; the latter two of these specified by the user.\nThe Sentinel-2 pre-processingbegins with the user specifying the spectral bands of interest and the desired spatial resolution of the raster layers.Sen2Cor atmospheric correction is applied to the L1C imagery to estimate the L2A bottom-of-atmosphere reflectance. If available, the bands of interest are retrieved from the available rasters at native spatial resolution (if the user has requested 10\u00a0m RGB values, for instance). If a band of interest is not available at the requested resolution, the closest available resolution of raster will be resampled to the finer spatial resolution, with nearest-neighbour resampling as the default method. All selected bands are then stacked into a single raster containing all selected bands from both acquisition dates. A cloud mask is then produced to exclude cloudy pixels from the classification and change detection (section 2.4.1). The stacked raster for change detection is reprojected from its native map projection (typically the appropriate UTM projection for that Sentinel-2 granule) to the desired output map projection for the chosen application. All interim products are stored as temporary geotiff files and cleaned up after processing. All resampling and reprojection is performed using GDAL. Image stacking is carried out using the Pyeo function stack_images() (Roberts et al., 2020). There are further functions for clipping raster files to specific areas-of-interest.\n\n2.3.1\nProcessing steps\nCloud masking: If additional cloud-masking is required, Pyeo provides tools for producing cloud-masks from the Sentinel-2 scene classification layer (SCL), the cloud probability layer or the fmask algorithm (Zhu and Woodcock, 2014). The default cloud mask is a conservative combination of the thematic layer and fmask, with a user-definable buffering parameter. It is also recommended that users of Pyeo include a cloud class in the classification of their maps, or generate an intermediate cloud-trained model.\nImage compositing: If needed, Pyeo provides functions for compositing multiple images for large area coverage, comparison and base-layer production.\nMosaicking: Pyeo allows mosaicking of multiple images if the AOI intersects with several image footprints if large area monitoring is required. This will increase computing time though, so it is recommended to adopt a tile-based processing strategy.\nBand stacking: Pyeo contains functions that permit multiple raster layers to be stacked into a single file, e.g. image bands from subsequent acquisition dates. Unlike the similar function in GDAL, the Pyeo stacking routine can handle images with a different band order. In the case that images to be stacked do not overlap perfectly, the user can specify whether the intersection or union of all provided images is used; in the case of the union, the gaps are filled with a user-specified no-data value, defaulting to 0. Once preprocessed, rasters are prepared for exposure to the classification module for either feature extraction or classification.\nMasked composite production: A feature designed for cloud-free compositing, this function produces a composite of last-seen-unmasked pixels from a list of images with associated masks, with an optional secondary raster of the date the last pixel was seen.\n\n\n2.3.2\nFeature extraction\nThe purpose of this processing is to produce a set of labelled training data that can be processed by Scikit-Learn and similar libraries. The feature extraction process requires two components: a Pyeo-preprocessed raster and a set of non-overlapping labelled training polygons. It also requires the user to indicate a training attribute.\nThe feature extraction process is as follows:\n\n1.\nReproject the polygons to match the Pyeo-processed raster.\n\n\n2.\nRasterize the polygons. Any pixel that corresponds to a polygon will take the value of that polygon's training attribute; every other pixel will be set to zero.\n\n\n3.\nStack the training label raster on top of the Pyeo-preprocessed raster, producing a raster of band ordering, e.g. [label, pixel_value_band0, pixel_value_band1, \u2026].\n\n\n4.\nExtract each pixel that does not have a label of zero to a table.\n\n\n\nThe table can then be passed directly to the model creation functions, cast to a dataframe or dumped to a.csv for further inspection and processing.\n\n\n2.3.3\nModel creation\nPyeo can natively create trained models from using a default set of parameters; a Scikit-Learn ExtraTreesClassifier with the following parameters:\n\nImage 1\n\n\n\n\nThis model was arrived at using the Tpot library (Le et al., 2020) for a change classification dataset; other papers have also recommended random forest based methods for image classification (e.g. Balzter et al., 2015; Breiman, 2001; Gibson et al., 2020), indicating that this is an appropriate standard model for image classification work.\nPyeo permits both the dumping of data to the Scikit-Learn accepted label-feature format and the loading of trained models. This allows other modules to create and tune classification models for use with Pyeo.\n\n\n2.3.4\nClassification\nModels in Pyeo are transferred using the Python native joblib module (Joblib Development Team, 2020). A Pyeo-compatible model is a pickled instantiation of a class that implements at least a.predict() method that takes a GDAL-compliant array (shape=(bands, y, x)) as its only augment. If probabilities are required, it may also optionally implement a.predict_proba() method and an.n_classes_ attribute. Model outputs from sklearn follow this schema, but it would be possible for a user to implement their own classifier (based on static thresholding, for example) and pass that to Pyeo for use. Pyeo presently only implements pixel-by-pixel raster classification.\n\n\n\n\n3\nAlgorithm and implementation\nAfter the stacked band rasters from two satellite image acquisition dates have been processed as described in section 2, the core change detection algorithm can be run by Pyeo. Because Pyeo integrated Scikit-Learn, all available machine learning models from that library can in principle be applied. We have used a Random Forest and an Extra Trees Classifier, which tends to be computationally more efficient than a Random Forest and is based on a similar approach. In the Extra Trees Classifier, each decision tree will be built using all available training data. To create a new node in the decision tree, the best split is determined by searching a subset of randomly selected features.\nPyeo reads in the spectral reflectance or bottom-of-atmosphere image bands from the chosen satellite mission from a \u2018before\u2019 and \u2018after\u2019 acquisition date. The bands are stacked in a single raster file and the change classes between the two dates are determined with the Extra Trees Classifier. Optionally, a probability layer can be created that gives the confidence level in the class of each pixel.\n\nAlgorithm 1 below shows a simple change detection application of Pyeo, assuming a set of training data exists, and the folders s2_l1, s2_l2, preprocessed and classified have been created.\nAlgorithm 1\nSimple image classification with Pyeo.\n\nImage 2\n\n\n\n\nThis is a minimal working example of Pyeo. In the case of Sentinel-2, it will download L1C and where available L2A Sentinel-2 granules over the specified AOI, preprocess them to the default 10\u00a0m bands as specified in the \u2018data preparation\u2019 section and classify them using the default Pyeo Extra Trees Classifier model. This script can be expanded and the interim products inspected as desired; it is also a very small modification to replace the dates with a derived range based on the current date and make a simple monitoring system.\nThe applicability of Pyeo in its current version is primarily aimed at operational ongoing change detection between land cover or forest types from satellite images that are acquired regularly. However, the software can be adapted to any other application that requires automated image classification with a trained machine learning model. Examples of other applications in future may include desertification, urbanization and flood extent monitoring.\nThe strengths of Pyeo are that it combines the machine learning functionality from Scikit-Learn with the image and raster processing capabilities of GDAL and atmospheric correction software such as Sen2Cor for this purpose and that it automates the updating of the change maps all the way from searching for new images down to producing the machine leaning classifications. Limitations currently include the need for visual quality assurance of the change detections, the radiometric properties of the rolling image composite, which can become very heterogeneous over time as new pixel values are replacing old ones due to seasonality of vegetation in certain ecosystems, and residual cloud contamination of images after the cloud-masking stage.\n\n\n\n\n4\nApplication and results\n\n4.1\nIntroduction and context\nThe cooperation between the University of Leicester and the REDD\u00a0+\u00a0Round Table of forestry stakeholders in Kenya began in 2014 through a Global Prospects Fund grant by the University of Leicester, followed in 2016 by the project \u201cREDD\u00a0+\u00a0Monitoring Services with Satellite Earth Observation - Community Forest Monitoring Pilot\u201d funded by the National Environmental Research Council (NERC). The UK Space Agency project \u201cForests 2020\u201d funded the operationalisation of the prototype into a fully useable processing chain. The Government of Kenya wanted to establish a more robust forest monitoring system and build more capacity for using new Earth Observation data. It is estimated that Kenya's economy is losing US$68, 000, 000 annually from deforestation. Kenya has enshrined an ambitious afforestation and reforestation goal in law that aims to increase its forest cover from 6% to 10% of its land area by 2030 (Kenya Ministry of Environment and Natural Resources, 2016). Through a series of stakeholder workshops with the REDD\u00a0+\u00a0round table in Nairobi, we co-developed the system based on the user needs of the different organisations involved with forest monitoring, including representatives of government agencies, ministries, community forestry associations and non-governmental organisations.\nIn 2020, Pyeo was installed on the computing infrastructure at the Kenya Forest Service, with the aim of enabling the institution to create new monitoring applications over forest areas of concern. A capacity building event trained local staff in using, maintaining and developing the system and it now runs on the in-house computing systems at the Kenya Forest Service. Deforestation alerts delivered from this system are being used to inform the actions of forest rangers in protected forest areas.\nThis section describes the application of Pyeo in the Kwale Protected Forest Area, where the Sentinel-2 forest cover loss information is used in conjunction with user-submitted mobile phone alerts to inform the rangers of the Kenya Forest Service of any disturbances of the forest cover. The deforestation alerts and the Sentinel-2 forest cover loss detections are used together in a GIS database. Pyeo is used to create the models, construct the initial cloud-free base layer and carry out the automated downloads, preprocessing and classifications (Fig. 1).\n\n\n4.2\nMachine learning model\nThe Extra Trees Classifier models used in this project were created and trained by the technical staff in the Kenya Forest Service. Unlike in many other applications, the classification identifies change classes between land cover types rather than the land cover types themselves (Fig. 2\n). As forest cover change is being identified, the model is trained on 8 features, which are the 4 spectral bands in red, green, blue and near-infrared from Sentinel-2 at 10\u00a0m resolution from two stacked images acquired on different dates between which the change is detected. As image choice is important, the images are downloaded, quality-checked and stacked by an interpreter at this stage. Sentinel-2 images are selected that show appropriate examples of deforestation, which is a relatively rare event despite its aggregated importance at national scale.\nThe model used in this project trains 12 change classes using 8 features: 9 classes are derived from the transitions between three basic land cover types, and the remaining 3 classes are cloud, cloud shadow and water. When a new area is ingested into the KFS system for further monitoring, KFS staff download appropriate images and create a new model to cover that area. For this project, the default model detailed in the modelling section was sufficient during development with KFS.\n\n\n4.3\nBaseline image composite creation\nImages of forested areas of Kenya often have cloud cover in excess of 70%, especially in the wet season (April to June). For an effective change detection, a baseline image composite layer of cloud-free pixels is required. On instantiation of new detection areas, an appropriate date range is supplied and the function pyeo.raster_manipulation.composite_images_with_mask is used along with the automatically generated cloud mask to create a cloud-free or nearly cloud-free baseline image composite layer of the last seen cloud-free pixels at a given date. This is used as the initial raster for the change detection.\n\n\n4.4\nNRT change detection\nOnce an initial machine learning model had been created, a simple interface creates a batch script to call rolling_s2_composite.py. When it is executed, this will perform the processing chain shown in Fig. 3\n, downloading any recently acquired Sentinel-2 images since last change detection was carried out. Its outputs are a classified change map with the 12 classes, a corresponding probability map and an updated cloud-free image composite in which the latest cloud-free pixels are replacing the previous pixels in the composite. During this process, a forest mask based on the SLEEK land cover map for the year 2016 (Kenduiywo et al., 2020) is used to filter out any non-forest locations. Fig. 4\n illustrates the individual processing steps with real examples. A second program then takes the Pyeo-produced change map, extracts and polygonises any deforestation area, produces images of areas-of-interest, creates a KML file (Fig. 5\n) and distributes this to subscribed emails.\nWhen the Pyeo forest cover loss alerts are received by the Forest Information Centre of the Kenya Forest Service in Nairobi, an interpreter checks the quality of the alerts for any erroneous detections. These can occur because of residual cloud, cloud shadow or atmospheric haze effects in the Sentinel-2 images. Any alerts that pass the visual quality-checking are transmitted to the forest rangers in the protected forest area in Kwale for further investigation. Feedback from the rangers has shown that the inclusion of satellite-based deforestation detections adds value to their operations in the field. In particular, they are sometimes alerted to forest cover loss in areas that are hard to reach or that are not regularly being inspected. Forest cover loss polygons that pass the visual quality assurance can be used for updating the national forest inventory GIS database.\n\n\n\n5\nConclusions\nThis paper describes the architecture of Pyeo, a Python package for creating satellite data processing chains for automated change detections with machine learning algorithms. It introduces an example of a simple processing chain and demonstrates Pyeo's present use in an operational environment in the Kenya Forest Service. As part of the Forests 2020 project, Pyeo has also been introduced to other countries and is presently being used to manage Sentinel-2 downloads for monitoring deforestation-free supply chains in the Ghanaian cocoa industry. Its heritage goes back to the Earth and Sea Observing System (EASOS) funded by the UK Space Agency.\nThough the majority of platform-specific processing tools in Pyeo are oriented towards Sentinel-2, tools for downloading Landsat and Planet images exist in the codebase; in addition, any raster that has been appropriately preprocessed can be passed to the generic raster functions and machine learning sections. It is intended that further collaborations both inside and outside academia will increase the feature set of Pyeo to cover more platforms and further image classification techniques, permitting automated Earth observation techniques to be applied to ever widening fields.\nPyeo provides a scalable solution for change detection from regularly acquired satellite imagery. It can be run for a small area-of-interest defined by a shapefile extent, or it can be run for multiple granules or footprints, e.g. we are currently running it for over 100 Sentinel-2 satellite image granules over Mato Grosso in Brazilin theEuropean Space Agency project ForestMind, which is developing deforestation-free supply chain monitoring systems and it is the intention to incorporate the Pyeo functionality.\n\n\nAuthorship contribution statement\nJ.F. Roberts: software development and writing the first draft of the manuscript; R. Mwangi, F. Mukabi, J. Njui and K. Nzioka: implementation and operation of the software and interpretation of the change detections at the Kenya Forest Service, J.K. Ndambiri: team leader at Kenya Forest Service, conception of the application and stakeholder liaison; P. C.Bispo: satellite data analysis; F.D.B. Espirito-Santo: critical revision of the manuscript; Y. Gou: satellite data analysis; S.C.M. Johnson: critical revision of the manuscript; V. Louis: conception and application of the software; A.M. Pacheco-Pascagaza: satellite data analysis; P. Rodriguez-Veiga: satellite data analysis; K. Tansey: critical revision of the manuscript; C. Upton: leading a user study in Kenya on the uptake of the software; C. Robb: conception and implementation of the first prototype; and H. Balzter: conception of the idea for the software and supervision of its implementation, team leader at the University of Leicester. All authors contributed to drafting and revising the manuscript.\n\n\nCode availability section\nName of the code\/library: Pyeo.\nContact: hb91@le.ac.uk; +44\u2013116\u00a0252 3820.\nHardware requirements: Windows or Linux OS (Linux recommended).\nProgram language: Python.\nSoftware required: Packages specified in the.yml file.\nProgram size: 10\u00a0MB (zipped).\nThe source codes are available for downloading at the link: https:\/\/github.com\/clcr\/pyeo.git.\n\n","12":"","13":"","14":"","15":"","16":"","17":"","18":"","19":"","20":"\n\n1\nIntroduction\nLandslides are one of the most prevalent dangers in the world (Emberson et al., 2020; Kirschbaum and Stanley, 2018; Stanley and Kirschbaum, 2017). Since 1900, an estimated 130,000 people have died in landslides, with economic losses of over $ 50 billion\u00a0(Guha-Sapir et al., 2017; Nadim, 2017). Slope stability analysis is important for assessing and mitigating this risk. In geotechnical engineering, slope stability is typically assessed with the methods of slices, in which the equilibrium of a sliding soil mass is considered, which is discretised into vertical slices. Several versions of the method exist based on assumptions regarding the forces between the slices\u00a0(Bishop, 1955; Fellenius, 1936; Janbu, 1954; Morgenstern and Price, 1965; Spencer, 1967). Regardless of the method, a slip surface must be initially assumed. Among the available computer programs, SLOPE\/W\u00a0(GEO-SLOPE International, Ltd., 2017) and Slide\u00a0(Rocscience, Inc., 2021) are the most widely adopted ones for slope stability analysis. These programs calculate the degree of utilisation (the stability metric) along various slip surfaces to determine the critical one. Although circular slip surfaces are a viable assumption, non-circular slip surfaces are preferable and recommended by geotechnical standards\u00a0(European Committee for Standardization, 2004) when dealing with layered soils. Among the various methods of slices available in the literature, the\u00a0Janbu (1954) or\u00a0Morgenstern and Price (1965) methods handle non-circular surfaces efficiently. Au contraire, the\u00a0Bishop (1955) method is not well suited because the position of the moment centre affects the results\u00a0(Fredlund et al., 1992).\nIn SLOPE\/W\u00a0(GEO-SLOPE International, Ltd., 2017), the centre and radius of the critical circular slip surface are initially found by grid search. Subsequently, the portions of the surface are incrementally altered according to the Monte Carlo optimisation method\u00a0(Greco, 1996; Husein\u00a0Malkawi et al., 2001). The points of the slip surface are moved back and forth randomly until the non-circular surface with the highest degree of utilisation is found. Slide2\u00a0(Rocscience, Inc., 2021) uses hybrid algorithms to find the critical slip surface. First, random slip surfaces are created in the search domain. These can be iteratively modified with stochastic processes inspired by nature, such as the Cuckoo Search method\u00a0(Wu, 2012), Simulated Annealing\u00a0(Su, 2009) and Particle Swarm Search\u00a0(Cheng et al., 2007), inter alia. Alternatively, the Path Search\u00a0(Siegel et al., 1981; Sharma, 2008) can be chosen, which simply generates random slip surfaces. Further optimisation is achieved with the Surface Altering optimisation method\u00a0(Mafi et al., 2021), which tentatively modifies the geometry of the slip surface to improve the results. The previous techniques in conjunction with Surface Altering are used in our study for comparison purposes.\n\n\n\n\n\nContrary to the aforementioned optimisation approaches, we propose a method to determine the critical slip surface with reinforcement learning. Within the realm of artificial intelligence, reinforcement learning is a generic framework for representing and solving control tasks in which the actions performed by the algorithm at one time are influenced by past decisions. Reinforcement learning algorithms are trained by incentivising them to accomplish a goal. Although reinforcement learning research is still in its infancy, there have been recently some exciting developments, most notably those showcased by the Google\u2019s DeepMind research group\u00a0(Mnih et al., 2015; Schrittwieser et al., 2020; Silver et al., 2016, 2017, 2018), which demonstrated the ability to master Atari video games and popular board games such as chess and go. The transition of reinforcement learning from academia to industry is gaining momentum. Noteworthy examples are dynamic job shop scheduling\u00a0(Shahrabi et al., 2017), memory control optimisation\u00a0(Ipek et al., 2008; Martinez and Ipek, 2009), personalised web services\u00a0(Li et al., 2010; Theocharous et al., 2015), self-driving cars\u00a0(Kiran et al., 2020), algorithmic trading\u00a0(Gao, 2018), natural language summaries\u00a0(Paulus et al., 2017) and healthcare applications such as dynamic treatment regimes and automated medical diagnosis\u00a0(Yu et al., 2019). Reinforcement learning industrial products include Google Translate, Apple\u2019s Siri and Bing Voice Search\u00a0(Mousavi et al., 2018).\nTraditional machine learning has been widely applied to geotechnical engineering in general\u00a0(Ebid, 2021; Wang et al., 2020; Zhang et al., 2020) and slope stability in particular. The support vector machine has been applied on a database of slope cases to predict the factor of safety\u00a0(Samui, 2008) and to aid reliability analyses\u00a0(Zhao, 2008). The performance of decision trees, random forest, gradient and extreme gradient boosting has been assessed on a wide database of case histories\u00a0(Bharti et al., 2021; Zhou et al., 2019). The Gaussian process regression in combination with the Latin hypercube sampling has been utilised to predict the failure probability based on a small set of deterministic slope stability analyses\u00a0(Kang et al., 2015). Artificial neural networks have been used for back-calculations of slope design\u00a0(Ding and Zhang, 2004), predict slope displacement\u00a0(Lu and Rosenbaum, 2003), aid limit analysis\u00a0(Qian et al., 2019) and predict the factor of safety\u00a0(Rukhaiyar et al., 2018; Sakellariou and Ferentinou, 2005). Despite these numerous studies, no attempts exist on the application of reinforcement learning to slope stability analysis.\nWe study the capability of reinforcement learning to find the critical slip surface by adapting the Deep-Q learning algorithm used by the DeepMind research group\u00a0(Silver et al., 2016), by testing it on benchmark slope stability problems\u00a0(Arai and Tagyo, 1985; Giam and Donald, 1989) and by comparing the results with the slope stability software SLOPE\/W and Slide. This approach is particularly valuable in an uncertain and complex environment.\nIn the following, the proposed method is presented (Section\u00a02). Although the performance of the method is slightly lower in one of the three benchmark problems considered, Section\u00a03 shows that it outperforms the existing approaches in the remaining cases, albeit with less iterations. Section\u00a04 proves the robustness of the method; its novelty, limitations and possible performance enhancement are discussed in Section\u00a05.\n\n\n2\nMethodology\nIn the following, the construction and optimisation of the slip surfaces is described. The critical slip surface is located within a predefined search domain at a certain distance from the slope toe and crest. A coordinate system with the origin of the axes at the slope toe is chosen (Fig.\u00a01). The tentative slip surfaces are constructed as follows. A random entry point with coordinates \n\n(\n\n\nx\n\n\n0\n\n\n,\n0\n)\n\n is chosen with a predefined spacing \n\n\u0394\n\n\nx\n\n\n0\n\n\n\n. The pseudorandom number generator is initialised with a constant seed so that the results can be replicated. A first random slip surface is generated from \n\n(\n\n\nx\n\n\n0\n\n\n,\n0\n)\n\n with the predetermined slice width \nb\n. The initial slice base angle is defined as \n\n\n\n\u03b1\n\n\n0\n\n\n=\n\u2212\n45\n\u00b0\n+\n\n\n\n\n\u03c6\n\n\n0\n\n\n\u2032\n\n\n\n\n2\n\n\n\n. The slice base angle of the first slice is \n\n\n\n\u03b1\n\n\ni\n\n\n=\n\n\n\u03b1\n\n\n0\n\n\n+\n\u0394\n\n\n\u03b1\n\n\ni\n\n\n\n, where the angular increments \n\n\u0394\n\n\n\u03b1\n\n\ni\n\n\n\n are randomly chosen in the interval of natural integers \n\n(\n0\n\u00b0\n,\n6\n\u00b0\n)\n\n. The condition \n\n\u0394\n\n\n\u03b1\n\n\ni\n\n\n\u2265\n0\n\n gives rise to a convex slip surface and the upper bound of the interval \n\n\u0394\n\n\n\u03b1\n\n\nmax\n\n\n=\n6\n\u00b0\n\n is a hyperparameter. The maximum slice base angle is \n\n\n\n\u03b1\n\n\nmax\n\n\n=\n45\n\u00b0\n+\n\n\n\n\n\u03c6\n\n\n0\n\n\n\u2032\n\n\n\n\n2\n\n\n\n. As described in the next section, after the first tentative slip surface is generated, the algorithm starts to learn the optimal angular increments.\nThe Fellenius degree of utilisation (the inverse of the factor of safety FoS) is calculated for every slice according to \n\n(1)\n\n\n\n\n\u03bc\n\n\n\n\nF\n\n\ni\n\n\n\n\n=\n\n\n1\n\n\n\n\nFoS\n\n\n\n\nF\n\n\ni\n\n\n\n\n\n\n=\n\n\n\n\nN\n\n\n\n\nF\n\n\ni\n\n\n\n\n\u22c5\nsin\n\n\n\u03b1\n\n\ni\n\n\n\n\n\n\nc\n\n\ni\n\n\n\u2032\n\n\n\u22c5\n\n\nl\n\n\ni\n\n\n\u22c5\ncos\n\n\n\u03b1\n\n\ni\n\n\n+\n\n\nN\n\n\n\n\nF\n\n\ni\n\n\n\n\n\u22c5\ntan\n\n\n\u03c6\n\n\ni\n\n\n\u2032\n\n\n\u22c5\ncos\n\n\n\u03b1\n\n\ni\n\n\n\n\n\n\n\nwhere \n\n(2)\n\n\n\n\nN\n\n\n\n\nF\n\n\ni\n\n\n\n\n=\n\n\nW\n\n\ni\n\n\n\u22c5\ncos\n\n\n\u03b1\n\n\ni\n\n\n\n\n\nis the normal force at the base of the slice and \n\n(3)\n\n\n\n\nW\n\n\ni\n\n\n=\n\u03b3\n\u22c5\nb\n\u22c5\n\n\nh\n\n\ni\n\n\n\n\n\nis the weight of the slice. \n\n\n\u03b1\n\n\ni\n\n\n is the slice base angle, \n\n\nc\n\n\ni\n\n\n\u2032\n\n\n and \n\n\n\u03c6\n\n\ni\n\n\n\u2032\n\n\n are the cohesion and friction angle at the midpoint of the slice base and \n\n(4)\n\n\n\n\nl\n\n\ni\n\n\n=\n\n\nb\n\n\ncos\n\n\n\u03b1\n\n\ni\n\n\n\n\n\n\n\nis the slice base length. Whenever the slip surfaces reach the ground level the degree of utilisation for all the \n\n\nn\n\n\ns\n\n\n slices of the slip surface according to the Janbu simplified method\u00a0(Janbu, 1954) is calculated according to \n\n(5)\n\n\n\n\n\u03bc\n\n\nJ\n\n\n=\n\n\n1\n\n\n\n\nFoS\n\n\nJ\n\n\n\n\n=\n\n\n\n\n\u2211\n\n\ni\n=\n1\n\n\n\n\nn\n\n\ns\n\n\n\n\n\n\nN\n\n\n\n\nJ\n\n\ni\n\n\n\n\n\u22c5\nsin\n\n\n\u03b1\n\n\ni\n\n\n\n\n\n\n\u2211\n\n\ni\n=\n1\n\n\n\n\nn\n\n\ns\n\n\n\n\n\n(\n\n\nc\n\n\ni\n\n\n\u2032\n\n\n\u22c5\n\n\nl\n\n\ni\n\n\n\u22c5\ncos\n\n\n\u03b1\n\n\ni\n\n\n+\n\n\nN\n\n\n\n\nJ\n\n\ni\n\n\n\n\n\u22c5\ntan\n\n\n\u03c6\n\n\ni\n\n\n\u2032\n\n\n\u22c5\ncos\n\n\n\u03b1\n\n\ni\n\n\n)\n\n\n\n\n\n\nwhere \n\n(6)\n\n\n\n\nN\n\n\n\n\nJ\n\n\ni\n\n\n\n\n=\n\n\n\n\nW\n\n\ni\n\n\n\u2212\n\n\nc\n\n\ni\n\n\n\u2032\n\n\n\u22c5\n\n\nl\n\n\ni\n\n\nsin\n\n\n\u03b1\n\n\ni\n\n\n\u22c5\n\n\n\u03bc\n\n\nJ\n\n\n\n\n\n\nm\n\n\n\u03b1\n,\ni\n\n\n\n\n\n\n\nis the normal force at the base of the slice and \n\n(7)\n\n\n\n\nm\n\n\n\u03b1\n,\ni\n\n\n=\ncos\n\n\n\u03b1\n\n\ni\n\n\n+\n\n\n\u03bc\n\n\nJ\n\n\nsin\n\n\n\u03b1\n\n\ni\n\n\ntan\n\n\n\u03c6\n\n\n\u2032\n\n\n\n\n\nis a coefficient. Since \n\n\nN\n\n\n\n\nJ\n\n\ni\n\n\n\n\n (and \n\n\nm\n\n\n\u03b1\n,\ni\n\n\n) depend on \n\n\n\u03bc\n\n\nJ\n\n\n, the solution is determined iteratively with the starting value \n\n\n\n\u03bc\n\n\nJ\n\n\n=\n\n\n\u03bc\n\n\nF\n\n\n\n, where \n\n(8)\n\n\n\n\n\u03bc\n\n\nF\n\n\n=\n\n\n1\n\n\n\n\nFoS\n\n\nF\n\n\n\n\n=\n\n\n\n\n\u2211\n\n\ni\n=\n1\n\n\n\n\nn\n\n\ns\n\n\n\n\n\n\nN\n\n\n\n\nF\n\n\ni\n\n\n\n\n\u22c5\nsin\n\n\n\u03b1\n\n\ni\n\n\n\n\n\n\n\u2211\n\n\ni\n=\n1\n\n\n\n\nn\n\n\ns\n\n\n\n\n\n(\n\n\nc\n\n\ni\n\n\n\u2032\n\n\n\u22c5\n\n\nl\n\n\ni\n\n\n\u22c5\ncos\n\n\n\u03b1\n\n\ni\n\n\n+\n\n\nN\n\n\n\n\nF\n\n\ni\n\n\n\n\n\u22c5\ntan\n\n\n\u03c6\n\n\ni\n\n\n\u2032\n\n\n\u22c5\ncos\n\n\n\u03b1\n\n\ni\n\n\n)\n\n\n\n\n\n\n\n\nSo far, this process resembles the Irregular Surface Search of the slope stability program XSTABL\u00a0(Sharma, 2008; Siegel et al., 1981) that inspired the Path Search algorithm of Slide2\u00a0(Rocscience, Inc., 2021). However, the Path Search algorithm always generates random slip surfaces, whereas our method relies on a single fully-random slip surface, namely the very first one, and learns the location of the critical slip surface based on the rewards collected. We elucidate how the two search methods diverge in the following sections.\n\n\n\n\n\n2.1\nReinforcement learning\nOne of three basic machine learning paradigms alongside supervised and unsupervised learning, reinforcement learning involves learning optimal actions that maximise a numerical reward\u00a0(Andrew, 1998). In other words, reinforcement learning deals with the way (the policy) an intelligent agent (an algorithm) takes the actions that maximise a user-defined reward in a particular setting (the environment).\nThe policy defines how the algorithm behaves in a certain situation. More precisely, it connects the states of the environment to the actions to be taken. As such, the policy is the core of a reinforcement learning agent, given that it determines its behaviour\u00a0(Andrew, 1998). The well-established \u201cepsilon greedy strategy\u201d, one of the oldest policies\u00a0(Andrew, 1998), is selected in this study. An action is considered \u201cgreedy\u201d if it is expected to return the maximum reward for a given state. However, since the environment is unknown a priori, an initial exploration of the environment is necessary to determine these actions. As described in the previous section, this exploration begins with the first, completely random, tentative slip surface. The random exploration decreases over time as the environment knowledge is exploited.\n\nIn the present problem, the action to be optimised is the choice of the optimal angular change within subsequent slices of the slip surface that maximise the degree of utilisation and thus determine the critical slip surface based on the epsilon greedy policy. At each point of the slip surface \ni\n, the agent chooses a random action \n\n\nn\n\n\na\n\n\n with probability \n\u025b\n and the action associated with the highest expected reward \n\n\n\nQ\n\n\nmax\n\n\n\n(\n\n\nS\n\n\ni\n+\n1\n\n\n,\na\n)\n\n\n with probability \n\n1\n\u2212\n\u025b\n\n. \n\u025b\n is initialised at 1, which corresponds to a completely random selection, and decremented by \n\n1\n\/\nN\n\n after each episode, where \nN\n is the total number of tentative surfaces, until \n\u025b\n reaches 0.1. We choose \nN\n = 2000 episodes, so that \n\u025b\n decreases by 0.0005 per episode until it reaches 0.1 (Fig.\u00a02). In mathematical terms, let \n\nr\n\u2208\nR\n\u2229\n\n(\n0\n,\n1\n)\n\n\n be a random number and \n\n\u025b\n=\nmax\n\n(\n1\n\u2212\nj\n\/\nN\n,\n0\n.\n1\n)\n\n\n at episode \nj\n, the agent takes the action \n\n\nA\n\n\ni\n\n\n at the state \ni\n according to Eq.\u00a0(9), where \n\n\n\nn\n\n\na\n\n\n\u2208\nN\n\u2229\n\n(\n0\n,\n6\n)\n\n\n is a random number. \n\n(9)\n\n\n\n\nA\n\n\nt\n\n\n:=\n\n\n\n\n\n\n\nQ\n\n\nmax\n\n\n\n(\n\n\nS\n\n\ni\n+\n1\n\n\n,\na\n)\n\n\n\n\nif\u00a0\nr\n\u2265\n\u025b\n\n\n\n\n\n\nn\n\n\na\n\n\n\n\n\nif\u00a0\nr\n<\n\u025b\n\n\n\n\n\n\n\n\nHence, the random choice of the angular increment is initially the dominant pattern that slowly, but steadily abandoned over time as the agent gains some experience of the environment in which it operates. In other words, the angular increment \n\n\u0394\n\n\n\u03b1\n\n\ni\n\n\n\n becomes more of a \u201cconscious\u201d choice based on the rewards collected and represented by the value function \n\nQ\n\n(\ns\n,\na\n)\n\n\n, which returns the reward expected for the action \na\n in the state \ns\n.\nThe expected rewards are the local Fellenius and multiple of the global Janbu degrees of utilisation as well as the penalty for slip surfaces that reach the search boundaries. A reward corresponding to the Fellenius degree of utilisation \n\n\n\u03bc\n\n\n\n\nF\n\n\ni\n\n\n\n\n is collected at each slice. If the slip surfaces cross the search boundaries (Fig.\u00a01) a \u2212100 reward is assigned, if they reach the ground level, a reward of \n\n100\n\u22c5\n\n\n\u03bc\n\n\nJ\n\n\n\n is collected. Each slip surface defines an episode in reinforcement learning parlance. The rewards are listed in Table\u00a01 and are chosen to achieve the objective of maximising the degree of utilisation whilst trying to avoid slip surfaces that lie too deep or exit the slope far away from the crest. The actual number that the function returns is irrelevant, as long as the algorithm can maximise it, given the input data. The relative weight of the rewards listed in Table\u00a01, however, has an impact on the results.\nThe \u201cplayground\u201d of the agent is named environment\u00a0(Singh, 2021). The actions are chosen and the rewards are received by the agent in the environment. In the present case, the environment is defined by the slope stability problem simulated with the interpreted high-level general-purpose programming language Python\u00a0(Van\u00a0Rossum and Drake, 2009). By moving across states of this environment as depicted in Fig.\u00a01, the agent collects the rewards and stores them in the value function \n\nQ\n\n(\ns\n,\na\n)\n\n\n. In doing so, only some essential rules are prescribed, which are held constant in the case studies considered (Section\u00a02.2), and the critical slip surface is freely determined by the learning algorithm.\nThe state is a representation of the information necessary and relevant to take an action. It is not the physical state of the environment, but rather a representation of the information for the algorithmic decision-making\u00a0(Schuck et al., 2018). In this study, the coordinates of the slice bases \n\n(\n\n\nx\n\n\ni\n\n\n,\n\n\ny\n\n\ni\n\n\n)\n\n and their angles \n\n\n\u03b1\n\n\ni\n\n\n are selected as state variables. It is worth mentioning that different state representations were studied, such as by only considering the abscissae \n\n\nx\n\n\ni\n\n\n or by defining a sequentially numbered grid of points. The choice of the continuous state variables \n\n\nx\n\n\ni\n\n\n, \n\n\ny\n\n\ni\n\n\n and \n\n\n\u03b1\n\n\ni\n\n\n, however, provided the best results.\n\n\n\n\nAs anticipated in Section\u00a02, the rewards are determined by the outcome of the actions in the environment, namely the local degree of utilisations \n\n\n\u03bc\n\n\n\n\nF\n\n\ni\n\n\n\n\n, the multiple of the global degree of utilisation \n\n100\n\u22c5\n\n\n\u03bc\n\n\nJ\n\n\n\n or simply \u2212100 if the slip surfaces reach the search boundaries (Fig.\u00a03). Given that the possible states are infinite, we always have incomplete knowledge of the value function \n\nQ\n\n(\ns\n,\na\n)\n\n\n. This is where the Deep Q-Network (DQN) comes into play.\nDQN refers to the use of deep neural networks to approximate the value function \n\nQ\n\n(\ns\n,\na\n)\n\n\n\u00a0(Li, 2017). The DQN algorithm was developed by DeepMind, the Google research group, and was able to play six Atari games at record levels\u00a0(Mnih et al., 2013, 2015; Silver et al., 2016, 2017, 2018). DQN is a particular approach to Q-learning, a method of learning optimal action values, whose main idea is to predict the value of a state\u2013action pair, compare this prediction to the observed rewards and update the parameters of the algorithm to improve future predictions. Q-learning algorithms are formally described by Eq.\u00a0(10). \n\n(10)\n\n\nQ\n\n(\n\n\nS\n\n\ni\n\n\n,\n\n\nA\n\n\ni\n\n\n)\n\n=\nQ\n\n(\n\n\nS\n\n\ni\n\n\n,\n\n\nA\n\n\ni\n\n\n)\n\n+\n\u03bb\n\n[\n\n\nR\n\n\ni\n+\n1\n\n\n+\n\u0393\n\u22c5\n\n\nQ\n\n\nmax\n\n\n\n(\n\n\nS\n\n\ni\n+\n1\n\n\n,\na\n)\n\n\u2212\nQ\n\n(\n\n\nS\n\n\ni\n\n\n,\n\n\nA\n\n\ni\n\n\n)\n\n]\n\n\n\n\n\n\nIn the present example, the DQN is trained to choose the best angular increment solely based on the state variables. In other words, we simulate the behaviour of a crack that, randomly initiated, propagates through a material along the path of least resistance. The DQN receives the state vector \n\n\nS\n\n\ni\n\n\n as the input and delivers the vector of the expected rewards associated with every action as the output. After the action \n\n\nA\n\n\ni\n\n\n is taken and the agent moves to the new state \n\n\nS\n\n\ni\n+\n1\n\n\n, the reward \n\n\nR\n\n\ni\n\n\n is observed. The algorithm is run again with \n\n\nS\n\n\ni\n+\n1\n\n\n as the input and returns the action with the highest value \n\n\n\nQ\n\n\nmax\n\n\n\n(\n\n\nS\n\n\ni\n+\n1\n\n\n,\na\n)\n\n\n. Finally, the learning algorithm is updated to reflect the actual reward, using the mean squared error as loss function and minimising the difference between the predicted and target prediction of \n\nQ\n\n(\n\n\nS\n\n\ni\n\n\n,\n\n\nA\n\n\ni\n\n\n)\n\n+\n\u03bb\n\n[\n\n\nR\n\n\ni\n+\n1\n\n\n+\n\u0393\n\u22c5\n\n\nQ\n\n\nmax\n\n\n\n(\n\n\nS\n\n\ni\n+\n1\n\n\n,\nA\n)\n\n\u2212\nQ\n\n(\n\n\nS\n\n\ni\n\n\n,\n\n\nA\n\n\ni\n\n\n)\n\n]\n\n\n.\nIn Eq.\u00a0(10), \n\u03bb\n and \n\u0393\n are the hyperparameters that influence the algorithm learning process. \n\n\u03bb\n=\n1\n\n\n0\n\n\n\u2212\n3\n\n\n\n is the learning rate. The algorithm makes only small updates at each step with a low value of \n\u03bb\n and large ones with a high value. The parameter \n\u0393\n = 0.9 is the discount factor and controls how much our agent discounts future rewards when making a decision. Given that the discount factor is less than 1, we discount future rewards (the global degree of utilisation \n\n\n\u03bc\n\n\nJ\n\n\n) more than immediate rewards (the local degrees of utilisation \n\n\n\u03bc\n\n\n\n\nF\n\n\ni\n\n\n\n\n). Default values of the hyperparameters \n\u03bb\n and \n\u0393\n are chosen and Section\u00a04 studies their impact on the results.\nThe Deep Neural Network is implemented using the PyTorch library\u00a0(Paszke et al., 2019) of the Facebook AI Research lab and its higher-level interface nn, as explained in\u00a0Zai and Brown (2020). Fig.\u00a04 shows the general architecture of the neural network. The input layer is the state vector with the coordinates \n\n\nx\n\n\ni\n\n\n, \n\n\ny\n\n\ni\n\n\n and the slice base angle \n\n\n\u03b1\n\n\ni\n\n\n. Since the angular increment \n\n\u0394\n\n\n\u03b1\n\n\ni\n\n\n\n is comprised in the discrete interval of natural integers \n\n(\n0\n,\n6\n\u00b0\n)\n\n, the output layer is a vector with seven elements, one for each angular increment. The first and second hidden layers have each 50 neurons. This architecture is the result of a trade-off between learning capabilities and computational effort.\nUnfortunately, DQNs are generally prone to training instability\u00a0(Mnih et al., 2015). Hence, the following sections describe two common techniques used to stabilise the DQN, namely experience replay\u00a0(Lin, 1992) and target memory\u00a0(Mnih et al., 2015).\n\n\n\n\n\n\n\n\n2.1.1\nExperience replay\nTo some extent, reinforcement learning is a trial-and-error method, thus potentially resulting in unstable solutions. This instability is particularly evident if learning is slow, which occurs when the rewards are sparse. Experience replay can be used to accelerate the learning process\u00a0(Lin, 1992).\nSince each slip surface propagates from a random entry point, the algorithm cannot simply memorise a sequence of angular increments \n\n\u0394\n\n\n\u03b1\n\n\ni\n\n\n\n to take. It needs to be able to find the critical path to the ground level and avoid crossing the search boundaries regardless of the location of the entry point. This problem is deemed \u201ccatastrophic forgetting\u201d\u00a0(McCloskey and Cohen, 1989) and is a common issue of gradient descent-based training methods in online training, i.e.\u00a0when backpropagation is performed after each state. The essence of catastrophic forgetting is the push\u2013pull between very similar state\u2013action pairs that results in the inability to train the algorithm. To overcome catastrophic forgetting we implement experience replay, which adds batch updating to the online learning scheme, as follows:\n\n\n\n1.\nIn state \ni\n, the algorithm takes action \na\n, and observes the new state \n\n\ns\n\n\ni\n+\n1\n\n\n and reward \n\n\nr\n\n\ni\n+\n1\n\n\n,\n\n\n2.\nit stores this as a tuple \n\n(\ns\n,\na\n,\n\n\ns\n\n\ni\n+\n1\n\n\n,\n\n\nr\n\n\ni\n+\n1\n\n\n)\n\n in a list and\n\n\n3.\ncontinues to store each experience until the list is filled to a specific length, called \u201cmemory size\u201d, equal to 20.\n\n\n4.\nOnce the experience replay memory is filled, a subset with a predefined batch size (equal to 10 in this study) is randomly selected.\n\n\n5.\nThe algorithm iterates through this subset and calculates the value updates for each subset; it stores these in the target array \nY\n and the state \ns\n of each memory in \nX\n.\n\n\n6.\nFinally, it uses \nX\n and \nY\n as a mini-batch for training and overwrites the old values in the experience replay memory when the array is full.\n\n\n\nThus, in addition to learning the value of the actions, a random sample of past experiences is also used for training. A further refinement is represented by the target memory.\n\n\n2.1.2\nTarget memory\nOne issue of the Deep Q-Network is that by updating its parameters after each action, instabilities can arise. This instability is caused, inter alia, by the correlation between subsequent observations and can be overcome by updating the value function \n\nQ\n\n(\ns\n,\na\n)\n\n\n only after a prescribed number of episodes\u00a0(Mnih et al., 2015).\nIn the present problem, the rewards are sparse; they are significantly higher at the end of each episode than after every action. To solve this problem, the Q-network is duplicated so that we have its copy, the target \n\n\nQ\n\n\n\u02c6\n\n\n-network, whose parameters are not up-to-date and lag behind the regular Q-network\u00a0(Mnih et al., 2015). The \n\n\nQ\n\n\n\u02c6\n\n\n-network is implemented as follows:\n\n\n\n1.\nThe Q-network is initialised with parameters \n\n\n\u03b8\n\n\nQ\n\n\n and\n\n\n2.\nthe \n\n\nQ\n\n\n\u02c6\n\n\n-network is a copy of the Q-network with distinct parameters \n\n\n\u03b8\n\n\nT\n\n\n. At first, \n\n\n\n\u03b8\n\n\nT\n\n\n=\n\n\n\u03b8\n\n\nQ\n\n\n\n.\n\n\n3.\nThe epsilon-greedy strategy is used with the Q-values of the Q-network to select the action \na\n.\n\n\n4.\nThe reward and new state \n\n\nr\n\n\ni\n+\n1\n\n\n and \n\n\ns\n\n\ni\n+\n1\n\n\n are observed.\n\n\n5.\nThe \n\n\nQ\n\n\n\u02c6\n\n\n-values of the \n\n\nQ\n\n\n\u02c6\n\n\n-network are set to \n\n\nr\n\n\ni\n+\n1\n\n\n at the end of the episode or to \n\n\n\nr\n\n\ni\n+\n1\n\n\n+\n\u03bb\n\u22c5\n\n\n\n\nQ\n\n\n\u02c6\n\n\n\n\nmax\n\n\n\n(\n\n\nS\n\n\ni\n+\n1\n\n\n)\n\n\n otherwise.\n\n\n6.\nThe \n\n\nQ\n\n\n\u02c6\n\n\n-value is backpropagated through the Q-network (not the \n\n\nQ\n\n\n\u02c6\n\n\n-network).\n\n\n7.\nAfter a certain number of iterations, called synchronisation frequency (equal to 10 in this study), \n\n\n\u03b8\n\n\nT\n\n\n is again set equal to \n\n\n\u03b8\n\n\nQ\n\n\n.\n\n\n\nUp to this point, we have introduced all the features of the algorithm. In the next section, we present the testing campaign.\n\n\n\n2.2\nCase studies\nWe test our algorithm against three typical verification examples. The first example is the homogeneous slope introduced by\u00a0Arai and Tagyo (1985). The second is a layered slope with a layer of low resistance interposed between two layers of higher strength\u00a0(Arai and Tagyo, 1985). The third is the non-homogeneous three-layer slope ACADS 1(c) by\u00a0Giam and Donald (1989). The point coordinates of the three examples are depicted in Fig.\u00a05. Compared to the original examples, the coordinates are shifted so that the slope toe is the origin and the search domain is narrowed. The soil parameters of the three case studies are resumed in Table\u00a02. In the next section, the performance of the proposed reinforcement learning algorithm is tested on these three slopes and compared to SLOPE\/W\u00a0(GEO-SLOPE International, Ltd., 2017) and Slide2\u00a0(Rocscience, Inc., 2021). SLOPE\/W searches 400 circular surfaces first, optimises the critical one and obtains one single non-circular surface after 2000 iterations. The default number of iterations of the Slide2 search methods is adapted so that the number of generated slip surfaces approaches 2000. Other than this, the default parameters of the search methods of Slide2 are used.\n\n\n\n3\nResults\nThe results of the three case studies are presented in the next sections and are obtained with the maximum the angular increment \n\n\u0394\n\n\n\u03b1\n\n\nmax\n\n\n\n\n\n\n=\n6\n\u00b0\n\n, the discount factor \n\n\u0393\n=\n0\n.\n9\n\n and learning rates \n\n\u03bb\n=\n1\n\n\n0\n\n\n\u2212\n3\n\n\n\n. Two other hyperparameters, namely the slice base width \nb\n and the entry point spacing \n\n\u0394\n\n\nx\n\n\n0\n\n\n\n are chosen as 2\u00a0m and 0.50\u00a0m, respectively, for Case 1, and 1\u00a0m and 0.25\u00a0m for the remaining cases. Hence, the slice base width is comprised between \n\nB\n\/\n30\n\n and \n\nB\n\/\n15\n\n (or \n\nH\n\/\n20\n\n and \n\nH\n\/\n10\n\n), where \nB\n and \nH\n are the slope width and height, respectively, and the entry point spacing \n\n\u0394\n\n\nx\n\n\n0\n\n\n\n is a quarter of \nb\n. The sensitivity analysis performed in Section\u00a04 studies the impact of \n\u03bb\n, \n\u0393\n and \nb\n on the results.\n\n3.1\nCase 1\nCase 1 simply presents a homogeneous soil slope. The 2000 tentative slip surfaces are shown in Fig.\u00a06. Although some of these slip surfaces reach either the bottom or the right boundaries of the search domain, no surface crosses the slope face. A critical band of slip surfaces, where the degree of utilisation is close to its maximum, can be identified for the slip surfaces that emerge from the crest approximately in the interval of \nx\n-coordinates \n\n(\n30\n,\n55\n)\n\n. We discovered that centring the critical slip surface within this critical band is a proxy for the successful calibration of \n\n\u0394\n\n\n\u03b1\n\n\nmax\n\n\n\n and \nb\n. The effect of \n\n\u0394\n\n\nx\n\n\n0\n\n\n\n on the results is, on the contrary, less pronounced.\nHowever, Fig.\u00a06 alone does not indicate whether the algorithm is learning the location of the critical slip surface. This is illustrated in Fig.\u00a07 where the reward obtained by each slip surface and its moving average are shown. As expected, the results are quite erratic, since the abscissae of the entry points \n\n\nx\n\n\n0\n\n\n are randomly chosen at each episode, as previously mentioned. The algorithm is solving the optimisation problem each time from different initial conditions, finding local maxima. Yet, the moving average of the reward significantly increases, proof that the algorithm is learning and the solution converges to the global maximum. For instance, in the first 750 episodes and around episode 1750, there is a considerable number of slip surfaces that deliver negative rewards, while this outcome occurs remarkably less in the remaining episodes. This is due to the slip surfaces that cross the bottom and the right boundary of the search domain, delivering a \n\n\u2212\n100\n\n reward, as shown in the next figure.\n\n\n\n\n\n\nFig.\u00a08 displays the number of slices of the slip surfaces that tends to decrease with the episodes, evidence that the algorithm develops a proclivity for shorter surfaces. Deviations from this trend are present in the aforementioned intervals, namely in the first 750 episodes, where utterly short slip surfaces are constructed that cross the bottom boundary and around episode 1750, where the long surfaces cross the right boundary. So far we demonstrated that the algorithm optimises the reward and, therefore, the degree of utilisation (Fig.\u00a09), by choosing shorter slip surfaces over time.\nThe moving average of the degree of utilisation increases rapidly from about 0.3 to 0.75 between episodes 200 and 1750. The fact that this diagram is very similar to Fig.\u00a07 is a good indication that the reward and the degree of utilisation are well aligned. In the following, we compare the performance of the proposed method to the traditional approaches.\n\nFig.\u00a010 shows the critical slip surfaces obtained by the proposed method, SLOPE\/W and Slide2. The results are similar, hinting at the correctness of the proposed solution. Our slip surface is interposed between those of SLOPE\/W and Slide2 at the slope toe, above them at the crest, where SLOPE\/W and Slide2 almost coincide. Hence, we can prove that our algorithm performs fairly well, albeit based on a simple case. Case 2 is more tantalising.\n\n\n3.2\nCase 2\nCase 2 tests whether the algorithm is capable of identifying the Layer 3 of higher shear strength by keeping the slip surfaces within the weaker Layer 2. Since the general conclusions from Case 1 also apply to Case 2, we concentrate here on the discrepancies only, the first one being evident in Fig.\u00a011. Here, no slip surfaces cross the bottom and a negligible amount of surfaces (only 5% of the total) cross the right boundary of the search domain. Instead, many of the tentative slip surfaces reach the slope face. The slip surfaces can be subdivided into two groups, those that penetrate Layer 3 (and have thus a lower degree of utilisation because of its higher shear strength) and those that do not.\nThe reward plot (Fig.\u00a012) shows a more steady, less asymptotic increase than Case 1 (Fig.\u00a07), since only a handful of slip surfaces cross the boundaries of the search domain. Hence, the algorithm is not concerned with \u201cdeep\u201d or \u201clong\u201d slip surfaces, focussing, instead, on the optimisation of the slip surface within the search domain. This is confirmed by the diagram of the number of slices that shows a decreasing tendency as the model learns to keep the slip surfaces within Layer 2 (Fig.\u00a013).\nThe degree of utilisation (Fig.\u00a014) mirrors the reward with very low values when the slip surfaces penetrate the Layer 3 or exit the search domain.\n\nFig.\u00a015 shows the distinct behaviour of the proposed method compared to SLOPE\/W and Slide2. The critical slip surfaces of SLOPE\/W and Slide2 lie generally deeper towards the crest than the surface obtained with the proposed method. The Path Search critical surface of Slide2 lies above ours, entering the slope above the toe and reaching the ground level more to the left. The Slide2 surfaces show a scattered shape, especially as they exit Layer 2. All methods except the Path Search propagate towards the crest along the boundary between Layers 1 and 2.\nAlthough our model returns a slightly lower degree of utilisation than the SLOPE\/W and Slide2 in this case, it delivers the smoothest curve. The next case is probably the benchmark par excellence.\n\n\n3.3\nCase 3\nThe slope studied in Case 3 is a well-known benchmark example in the literature\u00a0(He et al., 2020; Rocscience, Inc., 2021). The proposed method predicts a certain number of \u201cdeep\u201d and \u201clong\u201d slip surfaces (approximately 10% of the total) in Case 3, because the search domain is squeezed against the slope (Fig.\u00a016). The algorithm, however, learns over time how to avoid crossing the boundaries of the search domain (Fig.\u00a017), a fact which is also witnessed by the decreasing trend in the number of slices (Fig.\u00a018). Many of the tentative slip surfaces cross the slope face. The reward and degree of utilisation plots (Figs.\u00a017 and 19) are almost identical. As in Case 1, our critical slip surface enters the slope between those put forward by SLOPE\/W and Slide2, except for the surface obtained with the Path Search method (Fig.\u00a020). Opposite to Case 1, however, the critical slip surface generally reaches the ground level more towards the right than the two software packages, the Path Search surface being the exception to this rule.\nThe sensitivity of the results to the hyperparameters is discussed in the next section.\n\n\n\n4\nSensitivity analysis\nThe proposed method is sensitive to some user-defined hyperparameters. Some of them, such as the discount factor \n\u0393\n and the learning rate \n\u03bb\n, are inherent in the reinforcement learning algorithm adopted. Others, such as the maximum angular increment \n\n\u0394\n\n\n\u03b1\n\n\nmax\n\n\n\n and the slice base width \nb\n, are due to this specific application. Case 3 is selected to test the sensitivity of the proposed algorithm, by variating the original values of the hyperparameters to lower and higher values according to Table\u00a03, resulting in 81 combinations.\nThe algorithm failed to learn within the prescribed number of episodes for the combination \n\u0393\n = 0.99, \n\u03bb\n = 0.01, \n\n\u0394\n\n\n\u03b1\n\n\nmax\n\n\n=\n5\n\u00b0\n\n and \nb\n = 1.5\u00a0m. This is mainly due to the high value of the slice base width \nb\n that causes the slip surfaces to constantly hit the boundaries of the search domain, thus leading to the end of the episode and negative total reward. Despite this fact, the mean degree of utilisation of all 81 combinations is 0.789 and the maximum degree of utilisation is 0.811, higher than the utilisation achieved with the original values of the hyperparameters (Fig.\u00a016).\n\nThe sensitivity of the degree of utilisation to the hyperparameters is depicted in Figs.\u00a021 to 24. For sake of clarity, the combination that failed to converge is excluded from these diagrams. The results are not very sensitive to the discount factor (Fig.\u00a021) and the original value of 0.9 appears to be an optimum. Similar conclusions apply to the learning rate (Fig.\u00a022). The degree of utilisation is more concentrated around its mean for smaller values of the maximum angular increment. However, it is evident from Figs.\u00a023 and 24 that there are a few outliers for which the algorithm converged to a low value of the degree of utilisation. These occur for low \n\n\u0394\n\n\n\u03b1\n\n\nmax\n\n\n\n and high \nb\n. Apart from these deviations, that can be controlled by carefully selecting the maximum angular increment and the slice base width, the degrees of utilisation achieved with various values of the hyperparameters are mostly concentrated within the range \n\n[\n0\n.\n790\n,\n0\n.\n811\n]\n\n, implying the robustness of the proposed method.\n\n\n\n\n\n\n\n\n5\nDiscussion and conclusions\nThe results from all three cases are resumed in Table\u00a04 in which the degree of utilisation, factor of safety and the number of slip surfaces generated by all methods are listed. The performance of our method, defined in terms of the maximum degree of utilisation obtained with a certain number of tentative slip surfaces, seems comparable to SLOPE\/W and Slide2 in Cases 1, slightly inferior in Case 2 and superior in Case 3. The added value provided by our method compared to the Path Search is evident in Table\u00a04. The performance of our model could be further boosted by increasing the number of neurons in the hidden layers beyond 50, albeit at a computational cost.\nAlthough there are three main limitations in our work, namely the use of the Janbu simplified method instead of exact methods such as Morgenstern and Price, the absence of the phreatic surface from the analyses and the selection of the Deep Q-Network instead of more refined reinforcement learning algorithms, we demonstrate the capability of our approach to locate the critical slip surface. We believe that this application is the first necessary step to embed reinforcement learning in the search of the critical slip surface of slopes and reserve to address both limitations in future papers. Our model can determine the critical slip surface within an environment simulated in Python, similarly to the reinforcement learning agents that master video and board games. The correctness of the results is cross-checked with benchmark tests carried out with SLOPE\/W and Slide2. Albeit with fewer iterations, our algorithm equalises or outperforms the search methods implemented in the commercial software in two of the three cases examined. The analysis of the model sensitivity to the hyperparameter values shows that, by carefully controlling the maximum angular increment and the slice base width, the performance can be further enhanced.\n\n\n\n\n\nCode availability section\nName of the codes: 211124_DQN_1.py, 211124_DQN_2.py, 211124_DQN_3.py\nContact: enrico.soranzo@boku.ac.at and +4314765487312\nHardware requirements: Intel Atom or Intel Core i3 processor; 1\u00a0GB disk space; Windows 7 or later, macOS or Linux\nProgram language: Python 3.8.12\nSoftware required: Python distribution\nProgram size: 24\u00a0KB\nThe source codes are available for download at the link: https:\/\/github.com\/soranz84\/210812_SlopeStab\n\n\n\nCRediT authorship contribution statement\n\nEnrico Soranzo: Conceptualization, Code, Article writing. Carlotta Guardiani: Methodology, Manuscript review. Ahsan Saif: Manuscript review. Wei Wu: Funding, Supervision.\n\n","21":"\n\n1\nIntroduction\nThe stratospheric polar vortex is a cyclonic circulation that forms every winter in the stratosphere over the winter pole. The polar vortex can also be identified by a region of low geopotential height and high potential vorticity (PV, a measure of rotation and stratification of the air parcel)\u00a0(Waugh and Polvani, 2010). The edge of the polar vortex is characterized by sharp PV gradients and a core of strong westerly jet, which roughly corresponds to what is known as polar night jet (PNJ). Indeed, steeper PV gradients are linked to a stronger jet. The origin of the polar vortex and PNJ is the large temperature difference between the pole and lower latitudes during winter, due to the lack of sunlight over the winter pole\u00a0(Waugh and Polvani, 2010). Consequently, it forms in fall, intensifies in mid-winter and disappears during spring, when the sunlight returns to the pole. The PNJ also acts as an air-barrier in such a way that polar air is isolated from air at lower latitudes.\nThe stratospheric polar vortex and PNJ are not only controlled by radiative processes, but dynamical effects also play a relevant role in its characterization, particularly in the Northern Hemisphere. Large-scale (Rossby) waves propagate upward from the troposphere and dissipate in the stratosphere, depositing energy and easterly momentum and so, perturbing and weakening the westerly winds\u00a0(Waugh and Polvani, 2010). The Rossby wave breaking takes place in the PNJ area, colocated with steep PV gradients, since PV gradients are involved in the restoring mechanism for the wave propagation. Thus, the location of the PNJ is of extreme relevance for the vortex, and by extension, for the polar stratosphere dynamics. The wave activity is substantially higher in the Northern Hemisphere than in the Southern Hemisphere, explaining the large differences in climatological intensity of the polar vortex between both hemispheres and hence, their different behavior\u00a0(Andrews et al., 1987). Bearing in mind these differences, from now on we will focus on the Northern Hemisphere.\nThe wave activity is not constant during winter, but it undergoes a large variability that results in high variability in the polar vortex too\u00a0(e.g. Waugh and Polvani, 2010). This has relevant consequences not only in other parts of the stratosphere but also in other atmospheric layers such as the troposphere. For instance, an extraordinary amplification of certain Rossby waves can lead to extreme polar vortex events called sudden stratospheric warmings (SSWs), which consist of a reversal of meridional temperature gradient, as well as a replacement of a westerly by an easterly jet\u00a0(Labitzke, 1981; McInturff, 1978; Pomerantz, 1963; Baldwin et al., 2021). In these cases, apart from a weakening of the polar vortex, the vortex either shifts out of the pole or splits into two pieces of similar size\u00a0(Baldwin et al., 2021). SSWs influence near-surface weather, increasing the likelihood of cold air outbreaks over North America and Eurasia, and modifying jet streams, temperatures, rainfall and storm tracks\u00a0(e.g Karpechko et al., 2018; Baldwin et al., 2021; Lu et al., 2021).\nThe stratospheric polar vortex and PNJ are also influenced by anthropogenic changes in greenhouse gases concentrations (GHGs). However, there is still high uncertainty in the stratospheric response to these changes across studies due to the different involved processes\u00a0(Wu et al., 2019; Ayarzag\u00fcena et al., 2020; Rao and Garfinkel, 2021). On one hand, increasing GHGs leads to a radiative cooling in the stratosphere\u00a0(Fels et al., 1980). This is particularly intense in the winter pole given the absence of sunlight, and so, would lead to a stronger polar vortex in the future. On the other hand, tropospheric wave activity entering into the stratosphere is also affected by GHGs changes in such a way that it is enhanced under anthropogenic climate change conditions\u00a0(McLandress and Shepherd, 2009). This would result in a weaker and more disturbed polar vortex and consequently, an increase in SSWs frequency\u00a0(Bell et al., 2010). The combination of these opposing effects leads to a weak amplitude of the stratospheric response to increasing GHGs\u00a0(Mitchell et al., 2012). In addition, the overestimation or underestimation of any of these processes in individual climate models might result in a change of the vortex of opposite sign, explaining the lack of consensus across models in this topic. This large spread in future projections of the stratospheric polar vortex does not only impact the projections of the rest of the stratosphere, i.e.\u00a0other stratospheric regions and pressure levels. Given the mentioned coupling between the stratosphere and other atmospheric layers, it also affects the future predictions of near-surface circulation\u00a0(Simpson et al., 2018).\nThe characterization of the Northern Hemisphere polar vortex and in particular, the PNJ in different datasets and climate scenarios is, therefore, of high relevance for the whole climate community. Some methods already exist in the literature, described hereafter. The standard method for characterizing the intensity of the PNJ is to average the zonal wind speed (\nu\n) along the 60 degrees north (60\u00b0N) at 10 hPa, used in many studies such as\u00a0Charlton and Polvani (2007),\u00a0Hardiman et al. (2019) or\u00a0Ayarzag\u00fcena et al. (2020). This location is close to the core of the PNJ and matches well with where higher changes of the winds were identified in observations\u00a0(Waugh and Polvani, 2010). This method has several advantages, particularly its simplicity. It only requires zonal mean zonal wind data and can be applied straight-forward to several datasets such as observations and the output of model simulations. However, the precise location of the PNJ is important too and might change in time or be different in climate models with respect to observations. For instance, Rossby wave sources can change in location during different SSWs\u00a0(e.g. Shi et al., 2020), leading to variations in the location of the polar vortex, PNJ or their features. Conversely, the location of the polar vortex and PNJ also modulates the propagation of Rossby and gravity waves into the stratosphere, which in turn affects the polar vortex. However, all these changes in the latitude of the PNJ would not be fully captured by using the 60\u00b0\nN\n standard algorithm. Indeed, it may provide misleading results in some cases. For instance, the polar vortex can sometimes be slightly shifted out of the pole and so, the PNJ would separate from being circumpolar and its core would move away of 60\u00b0N. However, all this does not always imply that the PNJ intensity has drastically changed, but the standard method would probably identify it as a weak PNJ event.\nSome of the disadvantages of the zonal mean perspective can be avoided with the two-dimensional (2D) analysis of the vortex edge. There are different 2D techniques. Some of them are related to the distribution of chemical tracers considering that the vortex air is an isolated air mass\u00a0(e.g McDonald and Smith, 2013; Kr\u00fctzmann et al., 2008). Others are based on Lagrangian methods following tracers trajectories and transport barriers\u00a0(Serra et al., 2017; Smith and McDonald, 2014). Another group of 2D diagnostics of vortex characteristics and edge are derived from the PV field\u00a0(e.g Waugh and Randel, 1999; Waugh, 1997). The vortex shape and variability can be characterized based on vortex moment analysis\u00a0(Waugh and Randel, 1999; Mitchell et al., 2011) and the edge is identified by seeking for steep PV gradients\u00a0(Nash et al., 1996). Although all these techniques provide a more accurate description of the vortex edge, the procedure is much more complex than the 60\u00b0\nN\n standard methodology. In most cases, they involve several computations such as coordinates changes, 2D integrations, trajectory calculations, which imply a high computational time, particularly in large databases. In addition, some of the required variables such as tracers concentrations or PV are not a common output of climate models or reanalysis data and should be first derived from other fields. In this sense, more recently, the 2D momentum method has been modified to be applied to a more common variable such as geopotential height at 10hPa\u00a0(Seviour et al., 2013; Rao et al., 2021). However, this method still requires other long computations. Further, a time mean value of the zonal mean geopotential height at a fixed latitude (60\u00b0N) is needed to identify the vortex area and assimilate to the vortex edge. However, as happens with the standard method, the location of this edge might be different in different scenarios or datasets, and these changes are not included in the diagnostics, leading to inaccurate results in polar vortex characterization.\nIn this paper, we investigate the application of machine learning techniques for the full characterization of the PNJ as a way to simplify this characterization. Machine learning techniques only recently have started to be applied at larger scales, instead of just very specific studies in climate science. Lately, interest has begun to increase in applying these techniques to climate change analysis\u00a0(Sebesty\u00e9n et al., 2021).\nConvolutional neural networks (CNNs) are some of the most common algorithms in the field of climate machine learning, and have been applied in tropospheric studies. A study has been able to predict the monthly mean temperature on Earth with an accuracy of 97% for one climate model (CRU TS 4.01,\u00a0Ise and Oba (2019)), by using data over 30\u00a0years to predict the subsequent 10\u00a0years with a CNN. Another algorithm using CNNs is presented by\u00a0Chattopadhyay et al. (2020), where an automatic labeling algorithm based on K-means clustering was designed in order to obtain the training dataset, leading to 94% accuracy in predicting North-American weather patterns. However, for these studies, there is a lack of expert-classified data. Therefore, training and testing is usually performed with autolabeling and historical data. This is due to the complexity and high number of datasets, which would escalate the amount of human effort for classification.\nIn stratospheric climate science, some machine learning algorithm studies have focused on stratospheric ozone depletion\u00a0(Nowack et al., 2018; Sedona et al., 2020; Keeble et al., 2021). A study has been made on polar stratospheric clouds, which play a key role in this topic. Techniques like random forest and support vector machines were found to be suitable to classify the composition of polar stratospheric clouds, training with infrared spectra data\u00a0(Sedona et al., 2020). Another study has predicted stratospheric column ozone, using Ridge regression, Lasso regression, random forests and extra trees techniques to train and classify the output of climate model simulations under different future climate scenarios, using greenhouse gases emissions as classification features (Keeble et al., 2021). Regarding geometric identification of climate structures in the stratosphere, computer vision techniques have been applied by\u00a0Lawrence and Manney (2018) in order to characterize the stratospheric polar vortex based on PV reanalysis data, and, more specifically, to detect onset dates for split vortex events. Their algorithm involves spatial overlap and some adaptive\u2013predictive techniques, while our method will focus on training a machine learning classifier. The stratosphere is an area with a lot of potential to be explored by means of machine learning techniques and this study is going to focus on that area.\nThe aim of this work is to present a new technique to detect and characterize the PNJ (intensity and location) by using artificial intelligence (AI) that can overcome some of the issues of the existing methodologies and spans several climate models and reanalysis data. The intention is that, by merely using a single simple field such as the zonal wind (u), more information about the PNJ can be obtained than the intensity provided by applying the 60\u00b0\nN\n standard methodology described in the previous paragraph. This will be also done without imposing assumptions based on observations such as the 60\u00b0\nN\n latitude and with simple fields (u) that are even common output of model simulations. This also results in less complexity and computational time than 2D moment analysis. More specifically, the method we will use is the development of a machine learning classifier that seeks for the grid points that form an area of high u values. A machine learning classifier is a black-box model to the end-user\u00a0(Kotsiantis et al., 2006). In this case, the input variables to the black-box model will be climate variables (\nu\n or at the most, geopotential height too) given on a grid and over time, obtained from available climate simulations. The outputs of this function will be the PNJ region and extension, averaged values over the region, and whether the polar night jet on each time step is under extreme event conditions or not. Speaking in terms of machine learning, the input variables to the function are called features, and the outputs are called class labels. The goal of the machine learning process is to model the outputs or class labels, given a distribution of features curated, initially, by a human, i.e.\u00a0the training process\u00a0(Muller et al., 2021). The resulting machine learning model, once trained, will be able to function in an unsupervised way by assigning labels based on features. The final product of this process, and this study, is a trained classifier, and all the end-user needs to do in order to obtain the PNJ extension and characterization features is to prepare the required inputs, as climate variables of the models to be classified. Apart from the mere description of the technique, we compare its performance with that of the standard method to first validate our algorithm and secondly, highlight the advantages of our technique. Further, we give evidence of the usefulness of our method by presenting different applications such as the analysis of the PNJ mean state and its variability in different datasets and under different climate conditions.\nThis article is divided into the following sections. The details of the different datasets together with the explanation of climate images used in the study are indicated in Section\u00a02. Section\u00a03 describes the methodology and artificial intelligence techniques applied to our scientific challenge. Section\u00a04 describes the results obtained. Finally, Section\u00a05 details the main conclusions.\n\n\n2\nData\nIn this study, datasets of two different kinds, i.e.\u00a0reanalysis and climate model simulations, are used.\n\n2.1\nReanalysis\nJapanese 55-year reanalysis (JRA-55) data\u00a0(Kobayashi et al., 2015) are considered in this study as observations. Though treated as observations, reanalyses are a combination of short-range weather forecasts with observational data. They provide a global dataset that is homogeneous in time for the latest several decades\u00a0(Kalnay et al., 1996). The JRA-55 dataset includes operational and delayed observations, digitized observations, as well as processed satellite data, in order to produce an homogeneous dataset that represents recent decades most realistically. The horizontal resolution of this dataset is 2.5\u00b0lon. \nx\n 2.5\u00b0lat. and our period of study extends from 1958 to 2015.\n\n\n2.2\nClimate models\nWe also employ the output of simulations performed with state-of-the-art climate models participating in the recent Coupled Model Intercomparison Project, Phase 6 (CMIP6)\u00a0(Eyring et al., 2016). These models provide a detailed representation of the climate system, including as many of Earth system processes as possible. In our analysis, we will focus on the three models shown in Table\u00a01. These three models have their model top in the stratopause or above, which means that they can have a good representation of at least the lower and middle stratosphere. This is important for our analysis as it is focused on the middle stratosphere. Other recent studies focusing their analysis in the mid-stratosphere are\u00a0(Martineau et al., 2018; Domeisen et al., 2019; Chun et al., 2019; Vargin et al., 2020). The choice of these specific models was done following results of previous work\u00a0(e.g. Liu et al., 2019; Ayarzag\u00fcena et al., 2020; Rao and Garfinkel, 2021) that analyzed the PNJ state based on the standard method. For instance, the CESM2-WACCM\u00a0(Danabasoglu, 2019; Gettelman et al., 2019) was selected because of the model\u2019s similarities with observational data, especially when simulating frequency of SSWs and mean PNJ intensity (Liu et al., 2019). CanESM5\u00a0(Swart et al., 2019a,b) and IPSL-CM6A-LR\u00a0(Boucher et al., 2018) were also chosen because of the different PNJ state under present climate conditions (i.e.\u00a0too strong for CanESM5 and too weak for IPSL-CM6A-LR with respect to observations) and the opposite response of the polar vortex to an extreme increase in CO2 concentrations according to\u00a0Ayarzag\u00fcena et al. (2020). This previous knowledge will allow for a validation of our algorithm and a better comparison of results with those derived from the standard method for PNJ characterization.\nIn terms of simulations, we use the output of only one ensemble member of the piControl and abrupt4xCO2 runs of the mentioned models. The piControl run consists of an evaluation simulation with prescribed preindustrial CO2 concentrations (Taylor et al., 2012). It extends several centuries, allowing the characterization of the internal stratospheric variability. The abrupt4xCO2 simulation is identical to piControl one, except for the CO2 concentrations that are instantaneously quadrupled from pre-industrial levels at the beginning of the simulation, and held fixed during the rest of the simulation. This simulation was firstly performed for climate sensitivity and feedback analysis to an abrupt and extreme increase in CO2 concentrations (Taylor et al., 2012).\n\n\n\n\n\n2.3\nClimate images\nWe will analyze monthly mean zonal wind (\nu\n) data at 10 hPa in winter months (December\u2013January\u2013February) of the previously described databases in all sections except for Section\u00a04.6 where daily data of \nu\n and geopotential height at the same level are used. The reason for choosing these data is that the PNJ is characterized by high values of \nu\n in the extratropics of the winter hemisphere and the 10hPa level corresponds to the middle stratosphere\u00a0(National Weather Service, 2021), where the PNJ is typically studied. Indeed, the standard method for characterizing the PNJ is based on the zonal mean zonal wind speed at 10 hPa\u00a0(e.g Charlton and Polvani, 2007; Hardiman et al., 2019), as previously indicated in Section\u00a01. The datasets described in Sections\u00a02.1 and 2.2 are originally available in netCDF (Network Common Data Form) format, which is a self-explained binary format for atmospheric data. For the purpose of this paper, they are converted into 2D images in a latitude\u2013longitude grid.\nA sample image is shown in Fig.\u00a01a, which shows, in the \nx\n axis, the longitude coordinates, and on the \ny\n axis, the latitude coordinates. In this image, we can see that the Northern Hemisphere contains a region of high zonal wind speeds, and it is where we will try to locate the PNJ. The training and testing methods described in Section\u00a03 are applied to a pool of 200 images in total, containing: 50 JRA-55 images, 50 CanESM5 piControl images, 50 CESM2-WACCM piControl images, and 50 IPSL-CM6A-LR piControl images, obtained from the first 50 winter months in each of the experiments. Since each dataset has a different horizontal resolution, each of these images has been mapped to the CESM2-WACCM grid (1.25\u00b0x 0.94\u00b0), which consists of a square grid of 192 latitude points and 288 longitude points. Final classification will be performed on the full datasets.\n\n\n\n3\nMethodology\nThis section describes the methodology based on artificial intelligence algorithms applied in this work.\n\n3.1\nRegion growing\nFirst, as a previous step to expert classification, a 8-connected region growing algorithm (Efford, 2000) was applied on each of the images representing the monthly mean \nu\n data at 10 hPa. This algorithm consists on comparing one pixel to its eight neighboring pixels, and appending those pixels to the region if they meet some condition. Its steps are described in detail below. \n\n\n\n\n\nIn region growing algorithms, it is either 4-connected or 8-connected regions that are typically used in image segmentation literature\u00a0(He et al., 2019). The reason for choosing one or the other depends on the specific use case\u2019s shape and wanted speed, as described in\u00a0Wu et al. (2008). In our case, an 8-connected region makes the final shape slightly closer to a continuous boundary, and also provides a faster region growth than four neighbors.\nThe growth boundary is established in an empirical way as 75% of the maximum zonal wind speed. The region growing algorithm is done to facilitate the work in Section\u00a03.2, and is only used as a starting point to build the ground truth. It is not used in any way by the final classifier. The region growing results obtained for one sample month are shown in Fig.\u00a01b with the corresponding contour. It is observed that the segmentation algorithm has selected the region of highest \nu\n without discontinuities.\n\n\n3.2\nGround truth: expert labeling\nAs a second step before classifier training, expert modification of the images was performed on the results provided by the region growing method in order to add or delete any area that was not consistent with the expert\u2019s vision. This work was performed by a climate expert, a researcher who manually inspects each of the available region growing results and modifies them according to their experience. The expert selected these regions on 50x4 images equally distributed in JRA-55, CanESM5 piControl CESM2-WACCM piControl, and IPSL-CM6A-LR piControl, as previously mentioned. More importantly, the expert also discarded any images which, in their view, contained zonal wind regions that are not consistent with the PNJ structure. An example of discarded image is shown in Fig.\u00a01d, since the maximum \nu\n and region detected by the region growing algorithm is located at very low latitudes, which is not compatible to an actual PNJ state.\n\nFig.\u00a01b shows an example of the results of the region growing algorithm and the final expert-modified PNJ field, which are identified over the zonal wind data field. In this case, the expert has only minimally modified the region produced by the region growing method. The expert contour on Fig.\u00a01b for this image and equivalent ones are the ones provided to the classification algorithm described later in this paper.\nThe images that the climate expert labeled make up the ground truth dataset, which is considered the most objective dataset in the scope of this study\u00a0(Muller et al., 2021). A ground truth dataset consists of correctly labeled data, which is a necessary requirement for training machine learning models\u00a0(Zhou et al., 2018). It is against this ground truth data that we will measure how accurate the classification algorithms are.\n\n\n3.3\nClassification\nA binary classification, with the goal of detecting the PNJ region, is performed for each point in the longitude\u2013latitude grid in order to obtain which of the grid points belongs to the PNJ region (labeled as 1), and which one does not (labeled as 0). The classifier will be specifically trained to label each grid point based on the ground truth developed in Section\u00a03.2, and will work independently of any fixed growth boundary, which is not possible when only applying region growing techniques (Section\u00a03.1).\nIn order to achieve an efficient and effective machine learning algorithm, two predictors as features of the classifier are selected. First, the normalized \nu\n at each grid point, which we will refer to as \n\n\nx\n\n\n1\n\n\n in this section, and secondly, the difference in latitude of each grid point with respect to the latitude of maximum \nu\n, which will be referred to as \n\n\nx\n\n\n2\n\n\n. Relative features have been selected because of possible variability between climate databases and over time.\nRegarding the type of classifier, we decided to try four: first, the Gaussian Naive Bayes classifier because of its simplicity and fast performance; then, we tried random forest and decision trees classifiers because they are among those used in recent stratospheric climate science machine learning literature, as mentioned in Section\u00a01; lastly, we tested the k-Nearest Neighbor classifier, since it is one of the most accurate statistical classifiers and is commonly used in image segmentation\u00a0(Bieniecki and Grabowski, 2004; Rajini et al., 2011). All these algorithms are available in the scikit-learn Python library\u00a0(Pedregosa et al., 2011). Each of them is described below.\nDeep learning algorithms were not applied because of the big training dataset size that would be needed, ranging from thousands to millions of images\u00a0(Sharma et al., 2020).\n\n\nThe Gaussian Naive Bayes classifier\u00a0(Salmi and Rustam, 2019) is based on the Bayes theorem and assumes strong independence between the predictors. The probability distribution for each feature is assumed to be Gaussian. The algorithm picks the output \ny\n with maximum probability for all possible values of the output (in our case, 0 and 1) and all possible values of predictors \n\n\nx\n\n\n1\n\n\n and \n\n\nx\n\n\n2\n\n\n: \n\ny\n=\na\nr\ng\nm\na\n\n\nx\n\n\ny\n\n\nP\n\n(\ny\n)\n\nP\n\n(\n\n\nx\n\n\n1\n\n\n|\ny\n)\n\nP\n\n(\n\n\nx\n\n\n2\n\n\n|\ny\n)\n\n\n. By calculating the maximum conditional probability out of \n\nP\n\n(\n\n\nx\n\n\n1\n\n\n|\n1\n)\n\nP\n\n(\n\n\nx\n\n\n2\n\n\n|\n1\n)\n\n\n and \n\nP\n\n(\n\n\nx\n\n\n1\n\n\n|\n0\n)\n\nP\n\n(\n\n\nx\n\n\n2\n\n\n|\n0\n)\n\n\n, we would obtain whether the grid point is or is not part of the PNJ region.\n\nDecision trees\u00a0(Daniya et al., 2020) are a rule based classification approach which split the dataset into branches, until smaller datasets belonging to each class (PNJ or no PNJ) have been isolated from each other. Scikit-learn Python library uses the CART (Classification and regression trees algorithm) by default for decision trees. This algorithm splits the dataset subsequently into two child nodes. For example, hypothetically, it would split the dataset into two categories, \n\n\n\nx\n\n\n1\n\n\n\u2264\nv\na\nl\nu\ne\n\n and \n\n\n\nx\n\n\n1\n\n\n>\nv\na\nl\nu\ne\n\n, and continue splitting the tree from there. The CART algorithm uses the Gini index as a quality measure of the purity of each potential split. The equation for the Gini index in this case would be \n\nG\ni\nn\ni\n=\n1\n\u2212\n\n\np\n\n\n0\n\n\n2\n\n\n\u2212\n\n\np\n\n\n1\n\n\n2\n\n\n\n, being \n\n\np\n\n\n1\n\n\n the probability of picking a data node from the PNJ region, and \n\n\np\n\n\n0\n\n\n the probability of picking a data node which does not belong to the PNJ. The split into child nodes would be made according to the smallest Gini index. In our use case, after several tests, we have set the maximum depth of the tree to 5.\n\nRandom forests\u00a0(Fawagreh et al., 2014) consist of averaging the predictions of several decision trees. If the majority of decision trees that form the forest agree that a grid point is PNJ, the data point will be labeled as such, and vice versa. Random forests use the bootstrap aggregation method in order to reduce overfitting. This method provides each of the decision trees in the ensemble equal vote toward the final labeling result, as well as drawing random samples of the dataset for training individual trees. In our use case, after several tests, we have set the maximum depth of the trees to 5, and the number of decision trees to 10.\n\nThe k-Nearest Neighbors algorithm (kNN)\u00a0(Alfeilat et al., 2019) classifies a test sample based on what the majority of \nk\n closest data points have been classified as. In this case, the distance to determine those \nk\n neighbors is the Euclidean distance, one of the most commonly used ones for kNN, and \nk\n has been set, after some tests, to 3. The Euclidean distance is calculated as the linear distance two points \na\n and \nb\n with their respective coordinates: \n\np\n=\n\n[\n\n\nx\n\n\n1\n\n\n,\n\n\nx\n\n\n2\n\n\n]\n\n\n: \n\nd\n=\n\n\n\n\n\n(\n\n\np\n\n\na\n\n\n\u2212\n\n\np\n\n\nb\n\n\n)\n\n\n\n2\n\n\n\n\n\n. Regarding our use case, if 2 out of the 3 closest points are classified as PNJ, the study point will also be tagged as PNJ. One of the disadvantages of the kNN algorithm is its high computational time (Trisal and Kaul, 2019).\n\n\n\n4\nResults\n\n4.1\nClassification\nThe classification algorithms described in Section\u00a03.3 have been applied to the ground truth dataset. The accuracy with respect to the ground truth and the elapsed training and classification times have been checked. In order to do this, the amount of images available is divided into hold-out subsets consisting of a 70\/30 train-test ratio (Awwalu and Ogwueleka, 2019). The accuracy results and elapsed time for this analysis are shown in Table\u00a02. The elapsed time corresponds to running the algorithms in the environment described in the Code availability section. The accuracy has been obtained by comparing predicted images with ground truth images.\nGiven the results in Table\u00a02, the decision tree classifier was selected going forward as a compromise between accuracy and training and testing computational complexity. Decision trees classifiers also have the advantage of being easier to interpret than random forests (Meng et al., 2020). The full expert dataset will from now on be used to retrain the algorithm and classify the rest of the images in the databases.\nThe classification algorithm results for one sample month are shown in Fig.\u00a01b. The figure shows, in yellow, the PNJ region identified by our classifier, on one of the images fed to the classifier as training data, as part of the ground truth dataset. The classified region is close to the expert contour, which is over the high \nu\n region on the Northern Hemisphere, the PNJ, visually validating the high accuracy of the tested classifiers.\nAfter training and testing, the classifier is applied to each remaining image in the climate databases. As an additional validation of the train-test process, we have compared the identified PNJ region with the \nu\n and geopotential field. To do so, we have used averaged fields along the time period of study because of two reasons (Fig.\u00a02). First, time-averaged fields summarize information and can be used instead of single month fields. In addition, these fields are smoother than that of a single sample month. In these figures we observe that the PNJ is located over the strongest \nu\n, which also coincides with the edge of a big geopotential (\n\n\nz\n\n\ng\n\n\n) drop, confirming the high skill of our algorithm to identify the PNJ.\n\n\n\n\n\n\n4.2\nCharacterization of climatological PNJ\nOnce classification of each latitude\u2013longitude grid point has been finished, the climatology of several variables has been obtained to characterize the PNJ in each dataset. In this first part, we analyze JRA-55 and piControl simulations. Firstly, we define three magnitudes averaged over the PNJ region: intensity (\n\n\nu\n\n\nc\n\n\n) and latitude (\n\n\n\u03d5\n\n\nc\n\n\n). Intensity consists of \nu\n weighted with the latitude cosine at each PNJ point. Latitude is weighted with \nu\n at each PNJ point. Secondly, these results are averaged over the time contained in each climate dataset, thus obtaining their climatology, named \n\n\n\n\nu\n\n\nc\n\n\n\n\u00af\n\n and \n\n\n\n\n\u03d5\n\n\nc\n\n\n\n\u00af\n\n. Obtaining these two magnitudes implies that the PNJ is perfectly described in extension, intensity, and latitudinal position, and this will be useful for comparisons between climate models and with the observational dataset. Apart from the relevance of the characterization of the PNJ itself, these magnitudes can also provide us a relevant information about the climatological state of the polar vortex. As previously mentioned, the intensity of the PNJ is intimately related to that of the vortex. \n\n\n\u03d5\n\n\nc\n\n\n denotes the location of the edge of the vortex and so, gives a hint of the vortex extension and its position either pole-centered or shifted out of the pole, when considering together with the PNJ intensity.\nThe characterization results are shown in Table\u00a03. They show that the obtained PNJ latitude is very close to 60\u00b0\nN\n in all datasets, suggesting once again the good performance of our algorithm and the consistency of using the standard 60\u00b0\nN\n method to calculate the strongest \nu\n at the edge of the polar vortex. Across datasets, only small differences in the PNJ intensity are observed. JRA-55 and CanESM5 have the strongest \n\n\n\n\nu\n\n\nc\n\n\n\n\u00af\n\n compared to CESM2-WACCM and IPSL-CM6A-LR models. In terms of PNJ intensity and latitude, CanESM5 is the closest climate model to observations, and both higher intensity and latitude are obtained in these two cases as compared to the remaining climate models.\n\n\n\n\n\n4.3\nCharacterization of monthly PNJ variability in climate databases\nAs indicated in Section\u00a01, the boreal polar vortex and so, the PNJ shows a large variability, ranging from a very strong polar vortex to extreme weak polar vortex events. In the latter cases, the PNJ might not exist or is highly perturbed and so, our algorithm has discarded those images from the analysis in Section\u00a04.2. We have divided these events into two types, with conditions set manually after iterating.\nThe first kind of events take place when the average obtained latitude is below 35\u00b0N, taken as the subtropical range limit (Ping et al., 2001). These events will be referred to from now on as type 1 events, and are in some way similar to displaced polar vortex events as the jet is out of its typical location at 10 hPa. Thresholding techniques like this one are common in machine learning both in image segmentation preprocessing and also for classification in multiple steps\u00a0(e.g. Sheeren et al., 2009). Composites of \nu\n and \n\n\nz\n\n\ng\n\n\n for these events are shown in Fig.\u00a03, where the good performance of our algorithm to detect the strong winds delimiting the polar vortex (region of low values of geopotential height at high latitudes) is confirmed, even in special cases. Further, during type 1 events, the intensity of the PNJ is weaker than the average of that of the climatological PNJ of Section\u00a04.2. Consistently, the polar vortex is also weak and displaced out of the pole (compare panels c and d of Figs.\u00a02 and 3. The displacement of the polar vortex towards Eurasia is typically associated with an intensification of the Aleutian high and an enhancement of wave activity, that would also weaken the polar vortex\u00a0(Labitzke, 1977). The frequency of these extreme events is evidently low (roughly 1%), being higher for the JRA-55 dataset (3%) than for climate models (Table\u00a04). Please note that these events cannot be directly identified as SSWs as we are using monthly mean data. Nevertheless, type 1 events correspond to months when a persistent SSW has taken place in its surroundings.\nWe have also observed a second type of weak events taking place over 35\u00b0\nN\n latitudes, which also show a very small classified area (type 2 events). These events have initially and tentatively been tagged as such when the intensity of the PNJ is as low as \n\n\n\nu\n\n\nc\n\n\n<\n\n\n\n\nu\n\n\nc\n\n\n\n\u00af\n\n\u2212\n2\ns\nt\nd\n\n(\n\n\nu\n\n\nc\n\n\n)\n\n\n, and the total number of PNJ grid points is below 10% of the total amount of image grid points. These events are described in more detail in Section\u00a04.6.\n\n\n\n\n\n\n4.4\nComparison of results with standard characterization method\nResults obtained from the 60\u00b0\nN\n characterization method have been included in Table\u00a05 for comparison with the classifier results. In general, \n\n\n\n\nu\n\n\u00af\n\n\n\nc\n\n\n results for PNJ events are lower, and the standard deviation is higher with respect to the classification results shown in Tables\u00a03 and 6. These differences are statistically significant at the 95% confidence level. The reason is that in many cases the core of the PNJ (the strongest \nu\n) might be displaced from the parallel and so, the standard method is not capturing, when taking the climatology at parallel 60\u00b0N. Similarly, by averaging at 60\u00b0N, we might be including points which do not correspond to the PNJ region, since the fact that the PNJ is characterized by a nearly zonal westerly circulation is an approximation. The most intense polar vortex corresponds, once again, to the JRA-55 dataset and CanESM5 climate model, versus CESM2-WACCM and IPSL-CM6A-LR.\n\nFig.\u00a04 shows all events obtained with standard and classification methods for JRA-55, marked with circle and \u201cx\u201d markers, respectively. Furthermore, type 1 events as tagged by the classifier have been labeled in red, versus blue color for PNJ events. We observe that the classifier captures a higher \n\n[\nu\n]\n\n than the standard method in cases of PNJ latitude south of 60\n\n\n\n\u2218\n\n\nN. Thus, the aforementioned fact that PNJ intensity is lower by applying the 60\n\n\n\n\u2218\n\n\nN characterization method is explained. This is due to the maximum intensity of the PNJ being located in a different latitude than 60\n\n\n\n\u2218\n\n\nN, information that can be seen on the \nx\n-axis of Fig.\u00a04. This result is particularly true and theoretically expected for red points, but it is also remarkable for events of PNJ latitude not that far from 60\n\n\n\n\u2218\n\n\nN. Indeed, while the classifier results of \n\n[\nu\n]\n\n overlap or are very similar to those of the standard method for episodes of PNJ latitude between 57.5\n\n\n\n\u2218\n\n\nN and 67.5\n\n\n\n\u2218\n\n\nN, there are events of PNJ latitude around 50-55 \n\n\n\n\u2218\n\n\nN that would have a very weak intensity based on the standard method but would show a PNJ intensity similar to 60\n\n\n\n\u2218\n\n\nN events from the classifier. This means that our classifier method is able to offer more information when the PNJ moves away from 60\n\n\n\n\u2218\n\n\nN, such as precise location and a more accurate value of the zonal wind speed, while preserving the ability to label type 1 events. This valuable information is lost when using the standard characterization method.\n\n\n\n\n\n\n4.5\nCarbon coupling: analyzing the future\nUnder increasing CO2 concentrations, the wind field at 10 hPa in extratropics (including 60\u00b0N) might be affected by the changes in tropical wind structures at lower levels\u00a0(e.g. Bunzel and Schmidt, 2013; Oberl\u00e4nder et al., 2013). These changes would not represent thus the changes in PNJ. Other studies such as\u00a0Zhang et al. (2016) and\u00a0Zhang et al. (2018) have documented a shift of the polar vortex towards Eurasia under climate change conditions. In all these cases, the analysis of future PNJ changes from a zonal mean perspective, and more specifically based on zonal mean \nu\n at 60\u00b0\nN\n and 10 hPa, might be misleading. Thus, the same characterization exercise with our AI technique has been applied to the abrupt4xCO2 simulations, in order to quantify the effect of carbon emissions on the PNJ intensity and latitude. A Student t-test has been performed on the results to check for statistically significant changes, on variables \n\n\nu\n\n\nc\n\n\n and \n\n\n\u03d5\n\n\nc\n\n\n. It has been concluded that the changes in these variables between Tables\u00a03 and 6 are significant at the 95% confidence level.\nResults point to the lack of consensus about climate models with respect to climate change that was expressed in Section\u00a01. The PNJ in CESM2-WACCM model in abrupt4xCO2 becomes weaker than for piControl (Table\u00a07), and presents a higher frequency of type 1 events than in piControl. In contrast, in CanESM5 and IPSL-CM6A-LR the PNJ becomes more intense with respect to piControl, and a lower frequency of type 1 events (i.e.\u00a0less disturbed) by extreme CO2 concentrations. These inconsistencies of PNJ response to climate change for CESM2-WACCM versus CanESM5 and IPSL-CM6A-LR are coherent with what is described by\u00a0Ayarzag\u00fcena et al. (2020), which presents the mean SSW frequency comparison between piControl and abrupt4xCO2 simulations for the climate models in this study, among others.\nRegarding the location of the PNJ core, latitude in Table\u00a06 is consistently lower than its respective piControl values. Within abrupt4xCO2 datasets, latitude is still the highest for CanESM5, which is also the most intense PNJ, maintaining the piControl tendency in this regard. This might imply a southward shift of the PNJ that would be consistent with the vortex movement out of the pole described by\u00a0Zhang et al. (2016). Interestingly, the trend is detected in models showing either a weakening and a strengthening of the PNJ.\nThus, conclusions derived from our method on the impact of increasing CO2 concentrations on the PNJ agree with those obtained with other methodologies, but we are able to capture all these aspects at once.\n\n\n\n\n\n\n4.6\nIdentification and classification of SSWs\nSo far, our algorithm has been applied to analyze the PNJ on a monthly time scale. However, a large part of the variance of the polar vortex is concentrated on a daily time scale. The most important instances of this variability are the SSWs as mentioned in Section\u00a01. In these cases, the PNJ is largely perturbed and weak. In addition, the polar vortex is either shifted out of the pole (displacement SSWs) or split into two pieces (split SSWs). Consequently, the PNJ would also move southward similarly to what happens in type 1 events detected in previous sections or split in several pieces (mainly two) in line with the type 2 events anticipated in Section\u00a04.3. We have thus applied our classifier to daily data of JRA-55 to investigate if our algorithm is able to both identify split and displacement SSWs. We have only done it for reanalysis data so that we can compare our results with those detected in literature by using other methodology.\nWhen applying the classifier trained with only monthly data to daily JRA-55 data in DJF, we are able to detect a high number of the displacement events identified by other methodologies in the literature\u00a0(Gerber and Martineau, 2022). Indeed, we retrieve more than 70% (6 out of 9) of the unanimously classified as displacement by different schemes based on PV field, geometric moment diagnostics of geopotential height or absolute vorticity as shown by\u00a0Gerber and Martineau (2022) and 11 out of 16 events classified as displacement SSWs by at least two of these methods. The onset dates of these events also match the first dates of the reversal of the wind with a difference of less than 6 days.\nAs for split events, these are associated with type 2 events mentioned in Section\u00a04.3, which, at that time, were pulled out of the main study for further investigation. These events are not common in monthly data in reanalysis and far infrequent in model simulations. As a result, the pool of data with a PNJ structure of type 2 is not enough to train the classifier. In order to correctly identify type 2 events, we have added additional training data to our classifier. These data are composed of 84 additional images from the JRA-55 daily simulation, containing extreme events. A new classification feature has also been implemented, the geopotential height. With these modifications, we have been able to detect type 2 events in daily JRA-55 data. We have also compared the initial dates of our daily results with\u00a0Charlton and Polvani (2007) and\u00a0Lawrence and Manney (2018), in Table\u00a08, which shows that our algorithm obtains 14 out of 17 split events (82%) of those other two studies combined. The splitting of the PNJ sequence is shown for 1978\u20131979 winter in Fig.\u00a06, in which two regions of interest are identified at the end days of February, associated with minimum geopotential height points. Lastly, the detected type 2 event regions on monthly JRA-55 data are shown on Fig.\u00a05(d), over the minimum geopotential height.\n\n\n\n\n\n\n\n\n5\nConclusions\nIn this study, we introduce a method for the characterization of the polar night jet (PNJ) that can be applied in both observations and climate models and under different climate conditions. This characterization is commonly performed based on the intensity of the zonal mean zonal wind field at 10 hPa and a fixed latitude (60\u00b0N), but this standard method presents some issues. It does not allow for a complete description of the PNJ state regarding its exact location and can even produce misleading results when the core of the PNJ is shifted away from 60\u00b0\nN\n parallel. On the other hand, the existing 2D methods for the analysis of the polar vortex and its edge usually require several variables that in many cases involve fields such as potential vorticity that are not direct output from model simulations. In addition, these 2D techniques imply several calculations and so, long computational time especially when working with long datasets. The method presented in this study seeks for achieving these challenges and is based on artificial intelligence. More specifically, we have trained a classifier with expert-labeled stratospheric monthly averaged \nu\n. The kind of ground truth we have built was not previously available, due to the high human effort that is required and the relatively recent introduction of machine learning to the field of climate science. The ground truth dataset binary-classified images did not previously exist in any public database, and were treated by the expert specifically for this study. In order to reduce the amount of human effort, we have helped the expert classify the images by preprocessing them with a region growing image segmentation method.\n\nThe validation of our classification algorithm reveals that it computes a climatological latitude of the PNJ in reanalysis very close to 60\u00b0\nN\n (the reference latitude of the PNJ standard characterization method). This is encouraging, but it can also mean that if we are only interested in assessing the intensity of PNJ, our method does not suppose any advantage with respect to the standard one. Moreover, the standard method is faster both computationally and in terms of human effort than applying a classification algorithm. A classifier requires high human effort for correct training, and more computational resources, which increase with dataset size. Thus, at a first sight the standard method would be more efficient than our method. However, the use of our classification algorithm does have several advantage:\n\n\n\n\u2022\nOnce trained, our classifier, as mentioned in Section\u00a01, is able to obtain, in one step, different parameters, from one image as an input: extension of the polar night jet, averaged magnitudes over the PNJ area, and type of PNJ event. Machine learning classifiers offer the possibility to obtain, at least, the combined accuracy of existing scientific methods and human labeling. In our case, we have reached an accuracy of 95.5%. Having been trained with a robust ground truth, machine learning classifiers can be trusted to function unsupervised in new datasets\u00a0(Muller et al., 2021).\n\n\n\u2022\nOur algorithm is able to characterize the PNJ in more detail with respect to area and location, preventing from undervaluing the intensity of the PNJ when its core is southward shifted as the standard method might do. This also implies that the classifier is not only able to detect the PNJ region and intensity, but it also identifies extreme episodes of the PNJ variability such as the aforementioned type 1 events, in which the PNJ is southward shifted. These kinds of events have been found by setting the analysis as a function of variables such as latitude and intensity, variables which would not be possible to be set as conditions if using the standard method. More variables could be added in the future, in order to refine the algorithm, as well as introducing optimization techniques to make a parametric analysis. This kind of postprocessing could also be useful for inferring which variables drive PNJ changes, and which ones do not, paving the way for causation or other studies\u00a0(Runge et al., 2019).\n\n\n\nKnowing where the PNJ (roughly the edge of the polar vortex) is located and its intensity, information that is provided by our algorithm, is important for the reasons described in Section\u00a01 and has many potential applications. In this study we have briefly explored some of them such as the analysis of the effects of climate on the boreal polar stratosphere or the study of the variability of the PNJ, in particular, the SSWs. In the first case, the application of our classifier results in a more accurate analysis of the impact of climate change on the PNJ and so, the polar vortex. We have observed the variability of climate change response of the PNJ, which highly depends on the climate model, both in terms of intensity and frequency of extreme events. We have confirmed the lack of consensus in this response among models already documented in the literature, which applied the 60\u00b0\nN\n characterization method for calculations. The fact that our algorithm can additionally provide the latitude of the PNJ, would, in a more extended analysis, allow to draw more conclusions about changes in intensity and extreme event frequency in relation to polar vortex location, both with and without a CO2 forcing. So far, our findings show that under the same climate conditions, a higher PNJ intensity corresponds to a relatively higher latitude. Interestingly, we also find that the PNJ latitude decreases with carbon emissions regardless of the sign of the change in the intensity. This last result should be investigated in more detail in future analysis. Thus, the application of our classifier to a multi-model analysis containing more climate models would allow us to draw more conclusions, which could potentially be used for making fast and simplified predictions about the future.\nAs for the variability of the PNJ, our algorithm is able to detect reasonably well the extreme weak PNJ events on a monthly and daily time scale, and even to characterize the type of these disturbances (vortex displacement or split). The classification of these events is relevant for the atmospheric community as there is still a large uncertainty regarding the influence of extreme polar vortex events on near-surface circulation, since not all extreme events are followed by a signal near surface\u00a0(Baldwin et al., 2021). Some authors\u00a0(e.g Mitchell et al., 2013; Hall et al., 2021) have suggested that this signal depends on the shape of the perturbed vortex in observations, but this was not further analyzed in detail in model simulations. In the case of our type 2 events (close to split vortex events), our classifier needs to be re-trained with daily fields and \n\n\nz\n\n\ng\n\n\n images and the performance has been proven to be very successful in comparison with existing algorithms in the literature. The good agreement between our results and those coming from the analysis by\u00a0Lawrence and Manney (2018) (CAVE-ART) is particularly important because CAVE-ART also applies computer vision techniques, but it involves more requirements that might imply more constrains for the detection and investigation of split events. For instances, while CAVE-ART requires scaled PV, temperature, geopotential height, and zonal and meridional wind as inputs, ours only uses zonal wind and occasionally geopotential height. Further, CAVE-ART usually needs sufficient time sampling as it uses spatial overlap between time steps to detect the split event. In contrast, our algorithm can work with one image on one time step, once it is trained. Lastly,\u00a0Lawrence and Manney (2018) uses subjective criteria in the final interpretation of results, in order to decide whether a split like event is a SSW, by observing the geometric evolution on the days surrounding each event. Our classifier directly outputs a label for different types of polar night jet extreme events, on the basis of both geometric and physical criteria.\nApart from the applications of our classifier explored in this work, there are many others. For instance, establishing an accurate contour of the polar vortex, and its spatial patterns, would be crucial for explaining the linkage between distribution of trace gases (particularly ozone) and polar vortex. Actually, studies have shown that the morphological changes of the polar vortex can impact the horizontal distribution of ozone in the Southern Hemisphere\u00a0(Zhang et al., 2017).\nFuture works are the use of more advanced machine learning algorithms, like CNNs, for the same aim of this study, something which would also require increasing the amount of expert-classified images, with procedures similar to the one explained in Section\u00a03.2. Millions of binary-classified images would be the ideal dataset size for training deep neural networks for image segmentation from scratch\u00a0(Sharma et al., 2020), and this type of classification has not been performed for stratospheric PNJ images as of now. Another extension of our algorithm might be its application to 3-D data, i.e.\u00a0fields in longitude\u2013latitude plane and at different pressure levels. This would be interesting for the analysis of the stratospheric polar vortex given that the core of PNJ also changes its vertical structure during different types of extreme PNJ events\u00a0(e.g., Serra et al., 2017; Matthewman et al., 2009). Finally, the methods in this paper might be adapted to be applied to other jets, and use with other climate variables for the identification of other atmospheric and stratospheric structures. Fig.\u00a07, for example, shows a first attempt of their use to detect the North Atlantic jet (NAJ) based on a map of the monthly climatology of zonal wind at 300 hPa (a tropopheric level) and when using the region growing algorithm described in Section\u00a03.1. Thus, the algorithm described in this study opens new opportunities to the analysis of atmospheric circulation structures, particularly in large datasets that would otherwise involve extensive computations and so, computational time.\n\n\nCRediT authorship contribution statement\n\nMar\u00eda Rodr\u00edguez-Montes: Data processing, Software, Drafting and revising the manuscript, Take intellectual responsibility for its content. Blanca Ayarzag\u00fcena: Climate advice, Climate problem design, Drafting and revising the manuscript, Take intellectual responsibility for its content. Mar\u00eda Guijarro: Artificial Intelligent design, Drafting and revising the manuscript, Take intellectual responsibility for its content.\n\n","22":"","23":"","24":"","25":"","26":"\n\n1\nIntroduction\nThe European Space Agency (ESA) Global Ocean Circulation Explorer (GOCE) mission (Drinkwater et al., 2007) provided a large set of Satellite Gravity Gradiometry (SGG) data, with many applications in various fields of the geosciences (Flechtner et al., 2021). GOCE, part of ESA's Living Planet program (ESA, 1999) was commissioned from 2009 to 2013 to study the Earth's gravity field and geoid with accuracies at the 1\u20132 mGal and 1\u20132\u00a0cm level, respectively (Drinkwater et al., 2007). The satellite followed a sun-synchronous near polar orbit with a 96.5\u00b0 inclination at a mean altitude of 250\u00a0km (Drinkwater et al., 2007), with its main observations carried out through a through a state of the art Electrostatic Gravity Gradiometer (EGG) comprised of six accelerometers ordered in pairs of two in the so-called diamond configuration (M\u00fcller, 2003). During the entre mission, the original Level2 gradiometric observations, satellite orbit data, and accompanying products have been processed by the GOCE High-Level Processing Facility (HPF) (Gruber et al., 2007) and released by ESA for scientific use.\nRecent improvements in GOCE data (Siemes, 2018) has been the motivation for developing a software that can reliably process GOCE Level2 SGG data, in their original format from ESA (ESA, 2014), without the prior need of in-depth geodetic knowledge, so that they can then be used directly in other studies. GeoGravGOCE is a new software with a simple-to-use and intuitive Graphical User Interface (GUI). The source code and the GUI were developed and designed in MATLAB App designer (MATLAB, 2019) with the latter being divided into four tabs following a logical, from the geodetic viewpoint of processing SGG data, sequence. The developed tabs refer to: (1) SGG pre-processing; (2) Transformation of a Global Geopotential Model (GGM) contribution from the Local North Oriented Frame (LNOF) to the Gradiometer Reference Frame (GRF), (3) Gradiometric data filtering, with Finite Impulse Response (FIR), Infinite Impulse Response (IIR), and Wavelet Multi-Resolution Analysis (WL-MRA); (4) Reference Systems (RSs) transformations from the LNOF to the GRF and vice-versa. The first and second tabs are connected, while the third and fourth tabs are independent and can be used at any stage to perform both a) filtering of input data that the user has and b) gradient reference frame transformation given availability of such data by the user. This sequential processing through the tabs, is designed to properly pre-process the SGG data, reduce the systematic trends of GOCE GGs, filter the reduced signals, and eventually transform the final gravity gradients of GOCE to the conventional reference frames in geodetic applications. Following this processing schema, the user will have the GOCE observations readily available for use and combination with other gravity field data. In GeoGravGOCE, the MATLAB Wavelet Toolbox (Peyre, 2022) is needed for the wavelet filtering, while it is recommended to install the M_Map package for MATLAB (Pawlowicz, 2020) to produce maps at various stages during the processing. Also, the MATLAB Parallel Computing Toolbox (PCT) (Moler, 2007) is needed if the user wishes to take advantage of such capabilities, but it is not mandatory for the program to be executed properly. If the MATLAB PCT is not available the program will run without any problems but it will require more time (see section 2.5 below) when processing many daily GOCE files in one step.\nSimilar efforts have been developed during the last years in order to make accessible specialized geodetic and gravity field data by new and\/or inexperienced users (Sinem Ince et al., 2019). Bucha and Jan\u00e1k (2013) developed GrafLab, a MATLAB-based GUI that allows the user to compute various functionals related to the Earth's gravity field from GGMs up to ultra-high degrees and orders of spherical harmonics expansion. Piretzidis and Sideris (2018) developed SHADE, which is a MATLAB GUI and toolbox to empirically decorrelate monthly GRACE solutions in the form of spherical harmonic expansions. As far as GOCE SGG data processing is concerned, no coherent and complete software exists until now to achieve the pre-processing and filtering of GOCE SGG data, so that they can be later used in geodetic, oceanographic and geophysical applications.\n\n\n2\nSGG pre-processing\nThe SGG Pre-Processing performs the necessary preprocessing of GOCE Level 2 products (ESA, 2014), provides visualization of the output files and offers additional output options to the user, such as computation of their statistics and plots of the derived outputs.\n\n2.1\nInput data\nThe input files of the GeoGravGOCE software refer to the GOCE Level 2 EGG_NOM and SST_PSO products, as provided by the European Space Agency (ESA). The EGG_NOM_2 product 2 (ESA, 2014; Gruber et al., 2007) is provided in daily files and contains the already calibrated and corrected GOCE gravity gradients and their errors, in E\u00f6tv\u00f6s [E]\n\n\n\n(\n1\nE\n=\n1\n\/\ns\ne\n\nc\n2\n\n)\n\n,\n\n at a 1\u00a0s sampling rate in the original Gradiometer Reference Frame (GRF). The SST_PSO_2 product (ESA, 2014; Gruber et al., 2007) contains the precise science orbit from the kinematic positions and the rotation matrices, known as quaternions, for the transformation between the Earth Fixed Reference Frame (EFRF) and the Inertial Reference Frame (IRF). The kinematic orbital elements of the satellite refer to the EFRF and are tagged in the Universal Time Coordinated (UTC) system, while the gravity gradients are time-tagged according to their GPS time. Note that the SST_PSO_2_product includes both the kinematic orbit with a sampling rate of 1\u00a0s and the reduced-dynamic orbit solution with a sampling rate of 10\u00a0s. In GeoGravGOCE, the kinematic orbit solution is used as it is of superior quality compared to the reduced-dynamic one (Bock et al., 2007). All GOCE gradiometric observations are available to the user community from the dedicated ESA GOCE Online Dissemination portal, which requires a simple user registration (https:\/\/goce-ds.eo.esa.int\/oads\/access\/collection\/GOCE_Level_2). For the software to start processing, both input files, i.e., the gradiometric and orbital data should be loaded after parsing them to ASCII format. This can be done with the GOCEPARSER (Arsov, 2012) provided by ESA, in order to extract the GOCE Level 2 data from native XML into a text file in columnar format. Finally, flags play an important role in interpreting the quality of GOCE SGG data (ESA, 2014), since they provide information about erroneous observations, special data treatment followed to fill-in gaps, external calibration and validation carried out, temporal corrections applied, etc.. Therefore, it is assumed that the user inputs in the GeoGravGOCE software SGG data that are not flagged.\n\n\n2.2\nOrbit referencing\nAs the EGG_NOM_2 and SST_PSO_2 refer to different time systems, a transformation between UTC and GPS time is required. This is necessary to reference the satellite orbit and accurately geolocate the gravity gradients. Within GeoGravGOCE, this is performed with the GPStoUTC library (Howat, 2022a) so that both GGs and the orbital elements are available in the same system. The aforementioned datasets are subject to outlier detection and screened for date-time gaps and disagreements in their fractional seconds. In case of differences, data gaps in the orbital elements are interpolated using spline interpolation (Brieden and M\u00fcller, 2014). The interpolation is applied to the Cartesian coordinates of STT_PSO_2 that the GOCE GGs fall in according to the GPS time of EGG_NOM_2. After this process, the datasets are correlated and synchronized.\n\n\n2.3\nCalculation of geodetic coordinates\nAfter the orbit referencing, the gravity gradients, i.e., the three-dimensional second derivatives of the gravitational potential are represented along the \n\nX\n\n, \n\nY\n\n, and \n\nZ\n\n axes of the orthogonal EFRF system. For practical reasons, such as the generation of maps and the combination of the SGG data with other geodetic data, a transformation to geodetic latitude, longitude, and ellipsoidal height \n\n\n(\n\n\u03c6\n,\n\u03bb\n,\nh\n\n)\n\n\n is needed. The computation of geodetic coordinates from the geodetic cartesian ones is straightforward for longitude and iterative for latitude and height, as (Torge and M\u00fcller, 2012):\n\n(1)\n\n\n\u03bb\n=\n\ntan\n\n\u2212\n1\n\n\n\n(\n\nY\nX\n\n)\n\n,\n\n\n\n\n\n\n(2)\n\n\n\u03c6\n=\n\ntan\n\n\u2212\n1\n\n\n\n(\n\n\nZ\n+\n\ne\n2\n\nN\n\nsin\n\n\u03c6\n\n\n\n\n\n\u03a7\n2\n\n\u2212\n\n\u03a5\n2\n\n\n\n\n)\n\n,\n\n\n\n\n\n\n(3)\n\n\nh\n=\n\nZ\n\nsin\n\n\u03c6\n\n\n\u2212\n\n(\n\n1\n\u2212\n\ne\n2\n\n\n)\n\nN\n\n\n\nwhere\n\n(4)\n\n\nN\n=\n\n(\n\n\u03b1\n\n\n1\n\u2212\n\ne\n2\n\n\n\nsin\n\n\n2\n\n\n\u03c6\n\n\n\n)\n\n\n\n\n\n\n\n(5)\n\n\n\ne\n2\n\n=\n\n\n\n\na\n2\n\n\u2212\n\nb\n2\n\n\n\n\na\n2\n\n\n\n\n\n\n\nWithin GeoGravGOCE the used reference ellipsoid is GRS80 with its defining parameters given in Moritz (2000).\n\n\n2.4\nExport of SGG data\nAt the end of the SGG pre-processing, the GUI exports the gradients and a respective report (see Fig. 1\n), with the included data referring to the latitude and longitude in decimal degrees, GOCE altitude in m, the GPS time in s, the GGs in E\u00f6tv\u00f6s, the quaternions, and the names of the processed files. The software offers additional options to the users in order to derive and report the statistics of the GGs in the GRF. Its dimensions are [Nx6] cells, where: N (rows) depict the number of the processed files, and the six (columns) present the six gravity gradients (\n\n\nV\n\nx\nx\n\n\n,\n\n\nV\n\ny\ny\n\n\n,\n\nV\n\nz\nz\n\n\n,\n\n\nV\n\nx\ny\n\n\n,\n\n\nV\n\nx\nz\n\n\n,\n\n\nV\n\ny\nz\n\n\n\n). Each interior cell includes the minimum, maximum, mean, standard deviation, and root mean square of the GGs. Moreover, within the export function, time-series plots of the GOCE GGs in the GRF can be generated (see Fig. 2\n), along with global maps presenting the GOCE orbit track and the satellite orbital height with time (see Fig. 3\n). All maps in the software are created via the m_map mapping toolbox (Pawlowicz, 2020) with the condition that it exists in the user's search path (m_map is available to download from https:\/\/www.eoas.ubc.ca\/\u223crich\/map.html).\nFurthermore, the GUI supports the use of the GrafLab software (Bucha and Jan\u00e1k, 2013) for the computation of the gravitational tensor components in the LNOF from a given GGM. This is carried out with the condition that the GrafLab M-file already exists in the user's search path. In order to compute the contribution of a GGM, an ASCII file is created by the software with the gravity gradient geodetic coordinates in an appropriate for GrafLab format. Then, the software can compute the gravitational tensor components \n\n\n(\n\nV\nx\nx\n,\nV\ny\ny\n,\nV\nz\nz\n,\nV\nx\ny\n,\nV\nx\nz\n,\nV\ny\nz\n\n)\n\n\n in the LNOF for the already processed data or other external input files.\n\n\n2.5\nGeoGravGOCE execution time testing\nA main aspect of any processing software, especially given the large amounts of GOCE SGG data, refers to the needed time for processing and multicore hardware capabilities. To validate the efficiency of the GOCE SGG pre-processing, several tests were performed in a computing system with the following configuration:\n\n\u2022\nProcessor CPU: Intel(R) Core(TM) i9-10940X @ 3.30\u00a0GHz,\n\n\n\u2022\nRAM: 256\u00a0GB,\n\n\n\u2022\nWindows edition: Windows 10 Education\n\n\n\u2022\nMATLAB version: 64-bit MATLAB, 2019b.\n\n\n\nThe aim was to check the execution time as it expresses the time until the user request is processed. For this purpose, single and multiple input files are loaded and tested using only one of the available cores. As expected, see Fig. 4\n and Table 1\n, the performance of the software developed is decreasing as more input files are loaded within the pre-processing procedure. The processing time, see Table 1, is doubled when requesting to process two instead of one file and reaches 1095\u00a0s (\u223c18\u00a0min) when 128 files are queued. The processing of many files at once, i.e., multi-day GOCE SGG data, is needed to avoid loading daily files individually. Therefore, the same experiment has been performed again, utilizing two, four, six, eight, and fourteen cores to improve the processing time. GeoGravGOCE has been developed in such a way that when multiple files are requested as input, parallel processing capabilities are enabled by default, so that no user intervention is needed. As it can be seen, in the extreme case of processing 128 files simultaneously, the selection of leveraging many cores within the multi-processing scheme provides improved processing times with respect to using only one, by 47.52%, 70.71%, 79.15%, 83.09% and 87.99% when employing two, four, six, eight and fourteen cores, respectively. The main improvement is found when using two and four cores instead of one, while for the six, eight and fourteen cores the improvement is smaller. \u0395ach test, between cores and processing files, was repeated fifteen times to safeguard the repeatability of the results and report average execution times. Fig. 4 and Table 1 present the mean values of the various execution times. It should also be mentioned that the parallel pool, a set of MATLAB workers on the local machine, was initialized from the beginning, since if it is turned off, an additional 10\u00a0s to start up was needed.\n\n\n\n3\nSGG and GGM transformations from LNOF to GRF\nIn geoid modeling and geoid determination, the use of the Remove-Compute-Restore technique (Omang and Forsberg, 2000) is a classical method to remove the contribution of long- and short-wavelength contributions from some input signal prior to prediction in order to have residuals that can be regarded as a stationary random signal. This is needed in order to then utilize methods like least-squares collocation and fast Fourier transforms to estimate e.g. a geoid model. Within the same concept, in order to filter the GOCE gradiometric observations one needs to remove the contribution to the gravity gradient tensor of a GGM. The latter, as derived by e.g. GrafLab are provided in the LNOF, hence a transformation to the GRF is needed. The GGM GGs are usually given in the LNOF which is a North-West Up system in GOCE standards (Gruber et al., 2007). The transformation of the GGM gradients from the LNOF to the GRF is necessary before their reduction from the original GOCE gravity gradients, since the latter are available in the GRF. To validate the appropriateness of the transformation, Laplace's equation for the GGM gradients in both the LNOF and the GRF is evaluated, with the statistics reported in Table 2\n. As it can be seen, the summation of the diagonal elements of the gradient tensor is practically zero, hence the transformation is applied correctly. Note that the original GOCE gravity gradients \n\n\nV\n\nx\ny\n\n\n\n and \n\n\nV\n\ny\nz\n\n\n\n are measured with an accuracy ten times worse than the other four, so their transformation from the GRF to the LNOF would propagate errors to all other tensor constituents (Bouman et al., 2016; M\u00fcller, 2003). Furthermore, as Rummel et al. (2011) point out, the measurement error of the \n\n\nV\n\nz\nz\n\n\n\n and \n\n\nV\n\nx\nz\n\n\n\n components is twice than the one expected (Brockmann et al., 2021). The full gravity gradient tensor to be transformed is (Hofmann-Wellenhof and Moritz, 2006)\n\n(6)\n\n\nV\n=\n\n[\n\n\n\n\nV\n\nx\nx\n\n\n\n\n\nV\n\nx\ny\n\n\n\n\n\nV\n\nx\nz\n\n\n\n\n\n\n\nV\n\ny\nx\n\n\n\n\n\nV\n\ny\ny\n\n\n\n\n\nV\n\ny\nz\n\n\n\n\n\n\n\nV\n\nz\nx\n\n\n\n\n\nV\n\nz\ny\n\n\n\n\n\nV\n\nz\nz\n\n\n\n\n\n]\n\n\n\n\n\n\nwith the sequel being dedicated to their transformation between the various reference frames (RFs) used in satellite gradiometry.\n\n3.1\nGravity gradient transformation between RFs\nThe first step is the transformation from the LNOF to the EFRF. This requires the computation of the rotation matrix \n\n\nR\n\nE\nF\nR\nF\n\u2192\nL\nN\nO\nF\n\n\n\n which uses the latitude (\n\n\u03c6\n)\n\n and longitude (\n\n\u03bb\n)\n\n of the SST_PSO_2 (.kin) file. This is computed with spline interpolation at each gravity gradient tensor measurement epoch as mentioned before. For the transformation, the computation of the rotation matrix is necessary, as\n\n(7)\n\n\n\nV\n\nE\nF\nR\nF\n\n\n=\n\nR\n\nE\nF\nR\nF\n\u2192\nL\nN\nO\nF\n\nT\n\n\nV\n\nL\nN\nO\nF\n\n\n\nR\n\nE\nF\nR\nF\n\u2192\nL\nN\nO\nF\n\n\n\n\n\n\n\n\n(8)\n\n\n\nR\n\nE\nF\nR\nF\n\u2192\nL\nN\nO\nF\n\n(\nN\nW\nU\n)\n\n\n\n=\n\n[\n\n\n\n\n\u2212\nsin\n\n\u03c6\n\ncos\n\n\u03bb\n\n\n\n\n\u2212\nsin\n\n\u03c6\n\nsin\n\n\u03bb\n\n\n\n\ncos\n\n\u03c6\n\n\n\n\n\n\nsin\n\n\u03bb\n\n\n\n\n\u2212\ncos\n\n\u03bb\n\n\n\n0\n\n\n\n\n\ncos\n\n\u03c6\n\ncos\n\n\u03bb\n\n\n\n\ncos\n\n\u03c6\n\nsin\n\n\u03bb\n\n\n\n\nsin\n\n\u03c6\n\n\n\n\n]\n\n\n\n\n\n\nThe transformation from the EFRF to the IRF and from the IRF to the GRF requires the use of the quaternions provided with the GOCE products, in order to determine the rotation matrices from the EFRF to the IRF (\n\n\nR\n\nE\nF\nR\nF\n\u2192\nI\nR\nF\n\n\n\n, see Eq. A.17) and then from the IRF to the GRF (\n\n\nR\n\nI\nR\nF\n\u2192\nG\nR\nF\n\n\n\n see Eq. A.18). The use of quaternions is analytically given in the Appendix, so that the frame transformation can then be applied as:\n\n(9)\n\n\n\nV\n\nI\nR\nF\n\n\n=\n\nR\n\nE\nF\nR\nF\n\u2192\nI\nR\nF\n\n\n\nV\n\nE\nF\nR\nF\n\n\n\nR\n\nE\nF\nR\nF\n\u2192\nI\nR\nF\n\nT\n\n.\n\n\n\n\n\n\n(10)\n\n\n\nV\n\nG\nR\nF\n\n\n=\n\nR\n\nI\nR\nF\n\u2192\nG\nR\nF\n\n\n\nV\n\nI\nR\nF\n\n\n\nR\n\nI\nR\nF\n\u2192\nG\nR\nF\n\nT\n\n\n\n\n\n\nA final note goes to the conversions between GPS and UTC times. This is carried out in the GUI using the function gps2utc.m for the conversion of GPS to UTC time (Howat, 2022a) and function utc2gps.m for the conversion from UTC to GPS time (Howat, 2022b).\n\n\n3.2\nExported transformed GGM gradients\nWithin the GeoGravGOCE software, the user can load gravity gradients from a GGM and transform them from the LNOF to the GRF (see Fig. 5\n). Since the GGM data need to be removed from the GOCE observations, the user can load the GGM files corresponding to the same daily files loaded in the pre-processing of the GOCE gradients (see Fig. 5). Then, the software can load the corresponding to the GOCE data SST_PSO_2 (.qat) files and perform the actual transformation of the gravity gradients from the LNOF to the GRF.\nAfter the GGM transformation the software can export and automatically save a mat file including, among other parameters, the transformed gravity gradients in the GRF and a corresponding report. If the user loads a GGM or SST_PSO_2 (.qat) file that does not correspond to the GOCE daily file already processed, a warning message will appear prompting to reload the correct data. In that sense, a set of checks have been implemented in the software to secure the correspondence of the data between the GOCE gravity gradients, the quaternions loaded and the GGM data.\nWith the transformation computation being completed, the software provides additional output options for users to select, as. e.g., the statistics and the figures of the gravity gradients in the GRF (see Fig. 6\n). One critical remark refers to the validation of the results of the transformation. This was done for the transformation process from LNOF to GRF by comparing our results with the ones presented by Tsoulis and Moukoulis (2019), Piretzidis (2014) and Piretzidis and Sideris (2017) and they were found to be consistent.\n\n\n3.3\nGG frame transformation execution time\nThe efficiency of the transformation was investigated by performing several tests in the same computing system as mentioned in Section 2.5. Parallel processing was selected in case that three or more files are selected by the user for simultaneous processing. In Fig. 7\n and Table 3\n the processing time needed for the transformations is presented, for files ranging from 1 to 128 and cores ranging from 1 to 14. Again, each set of tests was carried out fifteen times and the results are graphically reported in Fig. 7. Table 3presents the statistics of the average times for each core\/file combination. In the extreme case of processing 128 files simultaneously, the selection of utilizing fourteen cores provides the overall best results. In general, the processing time is smaller compared to the GOCE SGG pre-processing and reaches a maximum of \u223c224\u00a0s when 128 files are processed with 1 core. The improvement, compared to the latter, is at the level of 50,16%, 73.13%, 81.13%, 85.23% and 89.36% when employing two, four, six, eight and fourteen cores, respectively. The main improvement is found when employing two and four cores instead of one, and when more than four files are simultaneously processed, while for the six, eight and fourteen cores the improvement, despite substantial, is smaller.\n\n\n\n4\nFiltering of the GOCE SGG data\nAnother major aspect in GOCE SGG data processing refers to a need of spectral filtering before the data can be used in applications such as geoid and gravity field modeling, sea surface topography determination, inversion for geophysical prospecting. Due to the design of the gradiometer, GOCE can achieve its highest performance and measurement stability in the so-called measurement bandwidth (MBW), which is located from 0.005\u00a0Hz to 0.1\u00a0Hz (Yi et al., 2010). Outside the MBW, white noise increases as \n\n1\n\/\nf\n\n especially at low frequencies, i.e., frequencies lower than 0.005\u00a0Hz (ESA, 1999, 2014; Hugentobler et al., 2012; Jarecki et al., 2006; Krasbutter et al., 2014; Rummel et al., 2011). Therefore, filtering of the observed gravity gradients that will remove high frequency errors and long-wavelength correlated effects, i.e., preserving the signals within the MBW, are required (Piretzidis and Sideris, 2017).\nThe GeoGravGOCE software filtering allows users to choose from three different input data options which need to be filtered. The user can filter (i) already reduced by the user GGs, i.e., GOCE SGG data after removing the contribution of a GGM, (ii) the outputs of the pre-processing and referencing to a GGM (SGG_GRF.mat and GGM_LNOF_2_GRF.mat, respectively), and (iii) the output of the pre-processing (SGG_GRF.mat) and the computed GGM gravitational tensor components in GRF. Option (i) above is included to allow users who already have some reduced GOCE data, to use the filtering options. For options (ii) and (iii), the program automatically performs the reductions of the gravity gradients before any filtering process. Therefore, the GGM contribution which was estimated and transformed to the GRF, is removed from the initial GOCE GGs in the GRF. As pointed out by (Piretzidis and Sideris, 2017) any biases in the reduced GOCE SGG data should be removed prior to any filtering operation, therefore it is necessary for the user to inspect the spatial and spectral properties of the data at hand in order to guarantee that the signal to be filtered has a zero mean. This also implies, that the GOCE SGG flags (ESA, 2014) included in the EGG_NOM product are taken into account, since they provide information about erroneous observations.\nGeoGravGOCE offers three types of filtering options to be used and tested, namely Finite Impulse Response (FIR), Infinite Impulse Response (IIR), and Wavelet Multi-Resolution Analysis (WL-MRA). These filtering options are well established in the geodetic literature and GOCE related research, since FIR filters have been used by Wan et al. (2012) to account for the phase drift effect, i.e. for the zero-phase distortion in the recovered gravity field, FIR and IIR filters by Polg\u00e1r et al. (2013) to limit the GOCE measurements within the MBW, and Wavelet MRA by Grebenitcharsky and Moore (2014) to derive improved geoid models. Finally, as the filtering is performed in the GRF, a transformation from the GRF to the LNOF is available, while the filtered gravity gradients are provided in both the GRF and the LNOF.\nThe filtering process is performed separately for each tensor component \n\n\nV\n\ni\nj\n\n\n\n and SGG biases and drifts are automatically eliminated when applying the filters. The evaluation of each filter is performed in terms of the Power Spectral Densities (PSDs) of the unfiltered and filtered signals. Multiprocessing is no longer needed since the filtering procedure is very fast by itself and no significant delays have been evidenced. The scope of the present work is not to evaluate the performance of each filter and propose the best to use, but to rather offer different possible filtering options to the user to investigate. Nevertheless, based on the analysis below, and for each filter type, we propose the number of coefficients, for FIR and IIR, and the detail coefficients in WL MRA that provided the overall best results, in our analysis.\n\n4.1\nFinite Impulse Response filter implementation\nThe FIR filter is non-recursive, hence its output is computed using only the current and previous inputs and there is no feedback in the filter structure, so it is always stable (Grout, 2008; Lai, 2003a). In an FIR filter, the rational system function \n\nH\n\n(\nz\n)\n\n\n is a polynomial in \n\n\n\nz\n\n\u2212\n1\n\n\n\n, where \n\nz\n=\n\ne\n\ni\n\u03c9\n\n\n\n is a complex variable (Oppenheim and Schafer, 2009) and can be derived as (Hayes and Monson, 1999)\n\n(11)\n\n\nH\n\n(\nz\n)\n\n=\n\n\n\u2211\n\n\nn\n=\n0\n\nN\n\nb\n\n(\nn\n)\n\n\nz\n\n\u2212\nn\n\n\n\n\n\n\n\nFor a linear shift-invariant system, with a system response function \n\n\nH\n\n(\nz\n)\n\n\n, the input \n\nx\n\n(\nn\n)\n\n\n is related to the output time series \n\ny\n\n(\nn\n)\n\n\n through a linear constant coefficient difference equation in the time domain as:\n\n(12)\n\n\ny\n\n(\nn\n)\n\n=\n\n\n\u2211\n\n\nk\n=\n0\n\nN\n\nb\n\n(\nk\n)\n\nx\n\n(\nn\n\u2212\nk\n)\n\n\n\n\nwhere \n\nb\n\n(\nk\n)\n\n\n is the transfer function coefficient, \n\nN\n\n is the filter order and the first sample in the time is at \n\nn\n=\n0\n\n. In GeoGravGOCE, a Hamming window function (Tarr, 2018; Zhang and Moore, 2015) is applied to the forward and backward band-pass FIR filter (Rummel et al., 2011) to reduce the ripple effect (Gibb's phenomenon) and minimize the spectral leakage error in the boundaries of the determined MBW. Its cutting off frequency \n\n\nw\nn\n\n\n is given by (Hayes and Monson, 1999; Tarr, 2018):\n\n(13)\n\n\n\nw\nn\n\n=\n0.54\n\u2212\n046\n\ncos\n\n(\n\n\n2\n\u03c0\nn\n\n\n\nN\n\u2212\n1\n\n\n)\n\n,\n\n0\n\n\u2264\nn\n\u2264\n\nN\n\u2212\n1\n.\n\n\n\n\n\n\n4.1.1\nFIR filtered data outputs\nWithin the FIR filtering, the user can select and define the desired filter order, hence derive as many filtered gradients as required under different scenarios. In the frame of this paper, and after several tests with FIR filtering of the reduced GOCE signals, a value of 1500 as the filter's \n\n\nN\n\nt\nh\n\n\n\n order is recommended, as it provided a signal PSD within the GOCE MBW. The software exports a mat and a txt file, as shown in Fig. 8\n, with the filtered SGG data, while additional output options for users to select, such as to generate figures with the reduced filtered GOCE signals (see Fig. 9\n), their PSDs (see Fig. 10\n), and statistics are available. The software also allows users to display the \n\n\nN\n\nt\nh\n\n\n\n-order filter's impulse response (see Fig. 11\n) via the MATLAB interactive Filter Visualization Tool and display the magnitude, phase response, group delay, impulse response, step response, pole-zero plot, and coefficients of the filter. Especially for the PSDs, vertical dashed lines (see Fig. 10) are plotted to indicate the GOCE MBW and hence the cut-off frequencies of the band-pass filter (0.005\u20130.1Hz). As already mentioned, GeoGravGOCE is not limited to just one filter order, but the user can select any desired value, inspect the output results and conclude on the optimal to use.\n\n\n\n4.2\nInfinite Impulse Response filter implementation\nContrary to FIR, IIR is a recursive filter, so that the filter output is computed using the current and former inputs and former outputs (Grout, 2008). In other words, it estimates a weighted average of input samples, precisely as the FIR filter, and it adds to the estimated sample the sum of former output samples. The rational system function \n\nH\n\n(\nz\n)\n\n\n of an IIR filter (Oppenheim and Schafer, 2009), is determined as (Hayes and Monson, 1999)\n\n(14)\n\n\nH\n\n(\nz\n)\n\n=\n\n\n\n\n\u2211\n\n\nk\n=\n0\n\nM\n\nb\n\n(\nk\n)\n\n\nz\n\n\u2212\nk\n\n\n\n\n1\n+\n\n\n\n\u2211\n\n\nk\n=\n1\n\nN\n\na\n\n(\nk\n)\n\n\nz\n\n\u2212\nk\n\n\n\n\n\n\n\n\n\nFor the IIR the input \n\nx\n\n(\nn\n)\n\n\n is related to the output \n\ny\n\n(\nn\n)\n\n\n through the following difference equation (Hayes and Monson, 1999)\n\n(15)\n\n\ny\n\n(\nn\n)\n\n=\n\u2212\n\n\n\u2211\n\n\nk\n=\n1\n\nN\n\na\n\n(\nk\n)\n\ny\n\n(\nn\n\u2212\nk\n)\n\n+\n\n\n\u2211\n\n\nk\n=\n0\n\nM\n\nb\n\n(\nk\n)\n\nx\n\n(\n\nn\n\u2212\nk\n\n)\n\n\n\n\nwhere \n\nb\n\n(\nk\n)\n\n\n is the feed-forward coefficients, \n\na\n\n(\nk\n)\n\n\n is the feedback coefficients, and \n\nN\n\n and \n\nM\n\n are the filter order. For computing each output signal value \n\n(\nN\n+\nM\n+\n1\n)\n\n multiplies are required. In the software, the IIR filter is based on the equivalent Butterworth low pass filter (Lai, 2003b) which provides a flat response in the passband, so there is no ripple in the frequency response (Thompson, 2014), and its simple definition (transfer function) in the frequency domain is given as (Broughton and Bryan, 2009; Oppenheim and Schafer, 2009).where \n\n\u03c9\n\n denotes frequency, \n\n\n\u03c9\nc\n\n\n the cutoff frequency and \n\nn\n\n the order of the filter.\n\n4.2.1\nIIR filtered data outputs\nAs in the case of the FIR filter, within the IIR filter the user can select various orders for the filter and derive the corresponding results. After several tests with the reduced GOCE signals, the overall best results for IIR filtering were achieved for a value of 5 as the filter's \n\n\nN\n\nt\nh\n\n\n\n order. The output format is similar to the already described one for the FIR filter with corresponding mat and txt files, as shown in Fig. 12\n. Additional output options for users to select via pushbuttons are available and refer to figures of the filtered GOCE residuals in the GRF (see Fig. 13\n), figures of the PSDs of the filtered and the unfiltered residuals (see Fig. 14\n) and figures of the IIR filter (see Fig. 15\n).\n\n\n\n4.3\nFiltering based on wavelet multi-resolution analysis\nA third filtering option is based on a Wavelet MRA (Mallat, 1989) using a 1D Daubechies family wavelet with 10 vanishing moments (db10) (Daubechies, 1992) (pp. 194\u2013202) with soft thresholding (see Fig. 16\n), which is actually a good choice for potential field data based on an evaluation of GOCE-based GGMs against GNSS Levelling data (Peidou, 2016; Peidou and Vergos, 2016). Wavelet filtering is implemented through the MATLAB wavelet toolbox (Peyre, 2022), using a 1D wavelet as the analysis is performed along-track the GOCE orbit, while 12 levels of decomposition are used for filtering individually each of the GOCE residual gravity gradients. WL-MRA as a method is based on wavelets for representing functions, with the wavelet, denoted as \n\n\u03c8\n,\n\n carrying the detail coefficients of the signal (Liu, 2010), which usually encompasses the information that is useful and to be exploited. The scaling function \n\n\u03c6\n\n carries the approximation coefficients (see Fig. 17\n). In WL-MRA each level of decomposition consists of a detail and an approximation coefficient and corresponds to a specific spatial resolution, with the following level having twice lower the spatial resolution of the previous one. In GOCE data the first level of decomposition has a spatial resolution of 8\u201316\u00a0km (given the along-track spatial separation of the observables), the second one spans from 16 to 32\u00a0km, etc. The highest resolvable frequency of the first level corresponds to the GOCE observation rate of 1\u00a0s, which is approximately 8\u00a0km (ESA, 2014, 2006, 1999; Grebenitcharsky and Moore, 2014).\nThe choice of a maximum of twelve levels of decomposition dictates that the longest resolvable wavelengths match the perimeter of Earth in its spherical approximation. The reconstruction of the decomposed signal can be performed by summing all levels as well as the approximation coefficient of the last level of decomposition. As a result, twelve detail coefficients \n\n\nd\ni\n\n\n\n{\n\ni\n=\n1\n\u2212\n12\n\n}\n\n\n, one for each level of decomposition, and one approximation coefficient (\n\n\na\n12\n\n\n) for the last level of decomposition are computed.\nA signal reconstruction \n\ns\n\n is based on combining various detail coefficients, excluding some unwanted ones, and finally adding the approximation coefficient of the last level of decomposition, as:\n\n(17)\n\n\ns\n=\n\n(\n\nd\n1\n\n)\n\n+\n\n(\n\nd\n2\n\n)\n\n+\n\n\u2026\n+\n\n(\n\n\na\n12\n\n+\n\nd\n12\n\n\n)\n\n\n\n\n\n\nSince WL-MRA is performed in the along-track direction, orbit referencing and extraction is needed, hence a continuous dataset in time is needed, so that the orbits to be extracted will be filtered correctly. In the original GOCE data, the final orbit of each day is completed at the beginning of the next one, therefore within the software this matching is performed. When the first orbit of the data is loaded, GeoGravGOCE plots the detail and approximation coefficients, along with their PSDs, so that the user can select the coefficients to be retained or removed in the signal reconstruction. Once the reconstruction is over, the orbits are united again and given to the user in the original format that were initially loaded.\n\n4.3.1\nWavelet filtered data outputs\n\nFig. 18\n depicts the WL MRA panel of the program, where the various functionalities are seen. With the WL MRA decomposition the signal is decomposed in detail and approximation coefficients, where two different kind of outputs are available: first, the detail coefficients of each level of decomposition and the approximation coefficient of the last level of decomposition are plotted and then the PSDs of the aforementioned coefficients are depicted (see Fig. 19\n and Fig. 20\n, respectively). After that, the user can choose, which coefficients will be used for the signal reconstruction (see Fig. 18). Thirteen checkboxes are available, twelve for the detail coefficients and one for the approximation coefficient of the last (twelfth) level. Once the user-selected check boxes are chosen, the WL MRA reconstruction can be used.\nThe WL MRA synthesis reconstructs the signal using the selected coefficients and then, re-unites the orbits in their original loaded, daily, format. A.mat file including the filtered data as well as a corresponding report file format are saved automatically (see Fig. 18). With the reconstruction process being finished, the program can generate the statistics of the filtered data, figures of the reconstructed gravity gradients in their original daily format (see Fig. 21\n), and figures of the PSDs of the GGs after WL MRA (see Fig. 22\n). In the WL MRA it is interesting to notice the behavior of the reconstruction when using coefficients from levels 4 to 7 (see Fig. 22) and levels 3 to 7 (Fig. 23\n). Level 3 corresponds to spatial frequencies between 32\u00a0km and 64\u00a0km, which are marginally outside the GOCE targeting bandwidth. GOCE was expected to provide useful information for the Earth's gravity field to harmonic degrees of expansion (d\/o) 220\u2013250 which correspond to \u223c80\u00a0km. Nevertheless, the latest GOCE GGMs (Sinem Ince et al., 2019) have useful signal to d\/o 280\u2013300, which corresponds to spatial scales of \u223c60\u201366\u00a0km. As seen from the two reconstructions with WL MRA, excluding the signal of the detail coefficients from Level 3 can create steps in the PSD, since signal is lost. Retaining the Level 3 signal, some useful information to the limits of the GOCE MBW can be kept, but some signal is also left outside the MBW. This is the main advantage of WL MRA, since the signal from level 3 can be completely isolated and processed individually. For example, the level 3 detail coefficients can be filtered with an FIR or IIR filter within the GOCE MBW and then added back to the synthesis. This kind of selective filtering give various degrees of freedom when processing GOCE data in order to retain useful signal and omit signal outside the MBW.\n\n\n\n\n5\nGeneric reference system transformations\nThe final functionality of the software refers to the transformations from the GRF to the LNOF as well as the inverse one from the LNOF to the GRF (see Fig. 24\n). This generic transformation allows the user to input GGs from various sources (e.g., GGM, SGG, gravity, topography, etc.) in a given reference frame and transform them into another. This is a much-needed functionality in processing GOCE observations, as the mutual transformation from the GRF to the LNOF and vice versa, is necessary during many stages of the pre- and post-processing. For example, in order to reduce the filtered GOCE gradiometric observations with a GGM, one needs to compute the contribution of the GGM via, e.g., GrafLab (Bucha and Jan\u00e1k, 2013). The former GOCE gradients are in the GRF while the latter ones in the LNOF, hence the reference frame transformation is needed. Once reduced, the gradients of the residual disturbing potential, being in the GRF, need to be transformed to the LNOF, so that they can be later used in geoid and gravity field modeling.\nThe reference system transformation can be performed in two ways, i.e., from the GRF to the LNOF and the LNOF to the GRF. For each of them, and if the user inputs gravity gradients, the \n\n\nV\n\ni\nj\n\n\n\n in the GRF are depicted along with the transformed ones to the LNOF, and vice versa in the second case. For the transformations to be performed, the quaternions given by the user for the IRF to the EFRF and the EFRF to the IRF transformations (see Section 3) are considered to be already interpolated at the time of measurement of the gravity gradients, hence no additional interpolation is applied.\nTo perform the transformation from the GRF to the LNOF the input data need to be in a conventional format that is described in the GUI panel. Once the data are loaded, the transformation implies that the following intermediate steps are performed:\n\n(18)\n\n\n\nV\n\nI\nR\nF\n\n\n=\n\nR\n\nI\nR\nF\n\u2192\nG\nR\nF\n\nT\n\n\nV\n\nG\nR\nF\n\n\n\nR\n\nI\nR\nF\n\u2192\nG\nR\nF\n\n\n\n\n\n\n\n\n(19)\n\n\n\nV\n\nE\nF\nR\nF\n\n\n=\n\nR\n\nE\nF\nR\nF\n\u2192\nI\nR\nF\n\nT\n\n\nV\n\nI\nR\nF\n\n\n\nR\n\nE\nF\nR\nF\n\u2192\nI\nR\nF\n\n\n\n\n\n\n\n\n(20)\n\n\n\nV\n\nL\nN\nO\nF\n\n\n=\n\nR\n\nE\nF\nR\nF\n\u2192\nL\nN\nO\nF\n\n\n\nV\n\nE\nF\nR\nF\n\n\n\nR\n\nE\nF\nR\nF\n\u2192\nL\nN\nO\nF\n\nT\n\n\n\n\n\n\nAfter the transformation, an output file with the transformed gradients and a report are produced. There are then options providing further functionality in the GUI, such as the computation of statistics and the generation of plots. Fig. 25\n\n shows a typical exported file with GG transformed from the GRF to the LNOF and Fig. 26 form the GRF to the LNOF.\nThe same logic is followed in the reverse transformation from the LNOF to the GRF. After loading the data, the inverse steps outlined in Eqs. 18\u201320 are followed to go from the LNOF to the EFRF, then to the IRF and finally to the GRF. With the transformation procedure completed, an output file with the transformed gradients, a report and options to compute the statistics and generate plots become available.\n\n\n6\nSummary and conclusions\nWe present GeoGravGOCE, a new software for the processing and filtering of GOCE SGG data. GeoGravGOCE provides a friendly and easy-to-use GUI for the pre-processing of the original GOCE gravity gradients, precise transformation in the various reference frames, and spectral filtering with a passband FIR, a low-pass IIR, and a Wavelet MRA filter. The software delivers filtered GOCE gravity gradients and GGM gravity gradients in the GRF, which can then be readily employed in practical geodetic applications, such as geoid determination and gravity field modeling. With its current structure, the software is also suitable for educational purposes, as it provides all the necessary pre-processing of GOCE GGs, and can manage extensive experiments with multiple files. Therefore, the users can directly use the GUI and prepare data for their further processing, while they can also experiment with the various options offered to get intuitive information on the GOCE GGs. Experienced users can dive into the various reference frame transformations and filtering options tailoring the output to their specific requests, as well as they can modify the software source code to their custom needs. GeoGravGOCE produces also many supporting and final products, such as statistics reports and figures and complementary products, which can be of practical use for understanding and justifying the processing chain and results. A detailed flowchart with the sequential order process of GeoGravGOCE is given in Fig. 27\n, to summarize and clarify the program structure and the GUI functionalities. In the future, GeoGravGOCE can be further enhanced by implementing automatic data selection based on the SGG flags, additional filtering techniques, and custom-area section options.\n\n\nAuthorship contribution statement\nEM and EP are PhD candidates. EM designed the software GUI, the SGG pre-processing and the FIR and IIR filtering. EP designed the GGM transformation from LNOF to GRF, the reference system transformations from GRF to LNOF and the Wavelet filtering. EM, EP and DNA performed the numerical tests and wrote the first draft of the paper. GSV and INT provided comments, feedback and rewrote the paper in its final form. All authors contributed to the paper revision and have read and approved the final version of the paper.\nURL: http:\/\/gravlab.topo.auth.gr (GravLab).\n\n\nSoftware and computer code availability\nThe GeoGravGOCE software and documentation are included in the supplementary material of this paper, while they are also archived in GitHub (https:\/\/github.com\/gsvergos\/GeoGravGOCE) and the project webpage (http:\/\/olimpia.topo.auth.gr\/GeoGravGOCE\/). GeoGravGOCE was developed using MATLAB R2019b and MATLAB App Designer on a Windows 10 PC.\n\n","27":"","28":"","29":"","30":"","31":"","32":"","33":"","34":"","35":"","36":"","37":"","38":"\n\n1\nIntroduction\nThe continuously changing positions of continents and oceans throughout Earth history have played a fundamental role in shaping an enormous array of Earth processes. An important step toward an understanding of the long-term evolution of Earth is thus the quantification of past lithospheric plate motions. Following from Euler\u2019s rotation theorem, we may precisely describe plate motions in the form of finite rotations\u2014commonly called Euler vectors\u00a0(Cox and Hart, 1986). However, our ability to estimate Euler vectors of past plate motion from geological and geophysical data remains limited for much of Earth history. Improvements and innovations in the methods of Euler vector inference are therefore of broad importance.\nAn assumption common to all methods of Euler vector inference is that the motion of a given plate with respect to some arbitrary reference can be treated as stable over some nominal interval, such that it can be expressed by a single stage Euler vector\u00a0(Gordon et al., 1984; Cox and Hart, 1986). From the perspective of the system of reference, this stage Euler vector will be fixed and any point belonging to the plate will move along a plane perpendicular to that axis of rotation, tracing a circle centered on it. If an object passes from the domain of the reference to the plate (becoming a passive occupant of the latter), it too will follow the path of a circle during the stage interval, with a displacement proportional to its residence time on the plate. A series of such objects, passed from the reference domain to the plate progressively in time during the stage interval, would therefore form an arcuate array decorating the trace of a circle about the Euler pole. If a sufficient number of such objects could be identified, their distribution could be inverted to retrieve the location of the rotation axis, and their corresponding residence times on the plate could be used to determine its angular displacement.\nAt least two kinds of observational records present such age-progressive arcuate segments that can be used to infer stage Euler vectors: hotspot tracks and paleomagnetic apparent polar wander paths (APWPs) (Fig.\u00a01). Hotspot tracks record the motion of a plate with respect to the underlying mantle (neglecting any lateral plume motion), whereas APWPs record the motion of a plate relative to the Earth\u2019s magnetic field, which is aligned with the planetary rotation axis. The inversion of paleomagnetic data to retrieve stage Euler vectors \u2013 or paleomagnetic Euler pole (PEP) analysis \u2013 is of particular interest because it offers the possibility to recover full kinematic descriptions of past plate motion (i.e.\u00a0including relative paleolongitude change), in contrast to the conventional analysis of paleomagnetic data, which constrains only paleolatitude and paleoazimuth. However, despite being first conceptually introduced more than half a century ago\u00a0(Francheteau and Sclater, 1969), PEP analysis has seen limited application\u00a0(e.g. Gordon et al., 1984; Beck and Housen, 2003; Smirnov and Tarduno, 2010; Wu et al., 2017; Swanson-Hysell et al., 2019). This appears to be due, at least in part, to the fact that most prior approaches to PEP analysis have incorporated subjective decisions into its execution\u2014e.g.,\u00a0the subjective identification of change-points and distinct arcuate segments that together approximate an apparent polar wander path. To reduce this source of potential bias, our objective here is to introduce an unsupervised and objective framework with which to conduct PEP analysis\u2014although our methodology is applicable to stage Euler vector inference more generally. We furthermore seek to provide an accessible implementation of this framework for community use via Jupyter notebooks\u00a0(Kluyver et al., 2016) which are easily deployed using Binder\u00a0(Jupyter et al., 2018), an open-source tool that enables the creation and sharing of computing environments and code.\n\n\n\n\n\n2\nPaleomagnetic Euler pole analysis\nIf a given plate\u2019s motion relative to the planetary spin-axis is constant for some interval of time, the paleomagnetic poles acquired by the plate over that interval will trace an arc across the surface of the sphere, which can be described by either a small-circle or great-circle. The fundamental task in PEP analysis is to invert an assemblage of paleomagnetic data to retrieve the average instantaneous kinematics that were acting during such an interval \u2013 i.e.\u00a0the stage Euler vector \u2013 which can be concisely expressed by three parameters: the latitude and longitude of a pole of rotation \n\n(\n\u03b8\n,\n\u03d5\n)\n\n and an angular displacement \n\u03c8\n. However, because plate motions intermittently change according to changing torques upon them, APWPs comprise a discrete series of arcuate segments separated by change-points whose number and timing is unknown. Thus, PEP analysis also necessitates the division of paleomagnetic data into a number of temporal subsets \u2013 or groups \u2013 such that all the paleomagnetic poles of a given group represent the same (unknown) stage Euler vector. We are thus presented with two tasks that we may treat as distinct optimization problems:\n\n\n\n1.\nGrouping paleomagnetic data into subsets that manifest distinct stage Euler vectors.\n\n\n2.\nDetermination of the best-fit stage Euler vector for each paleomagnetic subset.\n\n\n\nHere we develop an unsupervised learning method to solve these problems in the reverse order by first determining the best-fit stage Euler vector to all possible APWP segments (all ordered subsets of the paleomagnetic data), and then identify the least-cost sequence of Euler vectors approximating the total APWP and the respective change-points. More generally, our methodology provides a tool to fit piecewise minimum circle segments to data on the sphere similarly to piecewise linear segmentation in one-dimensional time series\u00a0(Jekel and Venter, 2019).\n\n\n3\nMethodology\nWe start by considering that we have a series of \nN\n paleomagnetic poles that we call \n\n\np\n\n\ni\n\n\n, with \n\ni\n=\n1\n,\n2\n,\n\u2026\n,\nN\n\n. Each one of these paleomagnetic poles have an associated date \n\n\nt\n\n\ni\n\n\n and can be represented in Cartesian coordinates \n\n(\n\n\nx\n\n\ni\n\n\n,\n\n\ny\n\n\ni\n\n\n,\n\n\nz\n\n\ni\n\n\n)\n\n\u2014with \n\n\n\nx\n\n\ni\n\n\n2\n\n\n+\n\n\ny\n\n\ni\n\n\n2\n\n\n+\n\n\nz\n\n\ni\n\n\n2\n\n\n=\n1\n\n\u2014and also by their latitude and longitude, \n\n(\n\n\n\u03b8\n\n\ni\n\n\n,\n\n\n\u03d5\n\n\ni\n\n\n)\n\n. Without loss of generality, we assume that the paleomagnetic poles are time ordered, meaning \n\n\n\nt\n\n\ni\n\n\n<\n\n\nt\n\n\nj\n\n\n\n for \n\ni\n<\nj\n\n. Given two indices \ni\n and \nj\n with \n\ni\n<\nj\n\n, we denote by \n\n\n\nP\n\n\ni\n\u2212\nj\n\n\n=\n\n{\n\n\np\n\n\ni\n\n\n,\n\n\np\n\n\ni\n+\n1\n\n\n,\n\u2026\n,\n\n\np\n\n\nj\n\n\n}\n\n\n the subsequence of paleomagnetic points contained in the interval of time between \n\n\nt\n\n\ni\n\n\n and \n\n\nt\n\n\nj\n\n\n. We use \nq\n to refer to Euler poles, where \n\n\nq\n\n\ni\n\n\n is the instantaneous Euler pole at time \n\n\nt\n\n\ni\n\n\n. Lastly, we denote with \n\nR\n\n(\nq\n,\n\u03c9\n)\n\n\n the matrix describing a rigid counterclockwise rotation around the axis \nq\n by the angle \n\u03c9\n, which thus represents the average instantaneous rotation over the time interval \u2013 or stage \u2013 i.e.\u00a0a stage Euler vector.\n\n3.1\nEuler Vector optimization from paleomagnetic data\nIn this section we consider the problem of fitting an arc (whether belonging to a small- or great-circle) to a subsequence of paleomagnetic poles. Given \nk\n and \nm\n with \n\nk\n<\nm\n\n, we want to find the coordinates of the Euler pole \nq\n and an angle \n\n\u03c6\n\u2208\n\n[\n0\n,\n\u03c0\n\/\n2\n]\n\n\n such that the points in \n\n\nP\n\n\nk\n\u2212\nm\n\n\n are approximated by the trace of a circle with angular radius \n\u03c6\n, centered on Euler pole \nq\n\n (Fig.\u00a02a).\n\nWe infer the parameters of the circle by minimizing the sum of the squares of the geodesic distance from each data point to the circle trace. The geodesic distance can be computed as the absolute difference of the angle between the Euler pole \nq\n and each paleomagnetic pole \n\n\np\n\n\ni\n\n\n and the angle \n\u03c6\n: \n\n(1)\n\n\ngeod\n\n(\n\n\np\n\n\ni\n\n\n;\nq\n,\n\u03c6\n)\n\n=\n\n|\n\n\ncos\n\n\n\u2212\n1\n\n\n\n(\n\n\nx\n\n\ni\n\n\n\n\nx\n\n\nq\n\n\n+\n\n\ny\n\n\ni\n\n\n\n\ny\n\n\nq\n\n\n+\n\n\nz\n\n\ni\n\n\n\n\nz\n\n\nq\n\n\n)\n\n\u2212\n\u03c6\n|\n\n,\n\n\n\nwith \n\n(\n\n\nx\n\n\nq\n\n\n,\n\n\ny\n\n\nq\n\n\n,\n\n\nz\n\n\nq\n\n\n)\n\n the coordinates of the Euler pole \nq\n (see Fig.\u00a02b). Then, we consider an optimization problem that seeks to minimize the cost function given by \n\n(2)\n\n\n\n\nC\n\n\nk\n\u2212\nm\n\n\n\n(\n\u0398\n)\n\n=\n\n\n\u2211\n\n\ni\n=\nk\n\n\nm\n\n\ngeod\n\n\n\n(\n\n\np\n\n\ni\n\n\n;\nq\n,\n\u03c6\n)\n\n\n\n2\n\n\n,\n\n\n\nwhere the optimization is performed with respect to the three parameters of the circle that we write as the three-dimensional vector \n\n\u0398\n=\n\n(\n\n\n\u03b8\n\n\nq\n\n\n,\n\n\n\u03d5\n\n\nq\n\n\n,\n\u03c6\n)\n\n\n. In the noiseless case where all the points lie along a small- or great-circle and satisfy the equation \n\nx\n\n\nx\n\n\nq\n\n\n+\ny\n\n\ny\n\n\nq\n\n\n+\nz\n\n\nz\n\n\nq\n\n\n=\ncos\n\u03c6\n\n for all \n\np\n\u2208\n\n\nP\n\n\nk\n\u2212\nm\n\n\n\n, the quantity \n\n\n\nC\n\n\nk\n\u2212\nm\n\n\n\n(\n\u0398\n)\n\n\n achieves its minimum at zero. Since each \n\n(\n\n\nx\n\n\ni\n\n\n,\n\n\ny\n\n\ni\n\n\n,\n\n\nz\n\n\ni\n\n\n)\n\n is typically affected by random perturbations, there is no exact fit between the set of points and a small or great circle trace. This approach was also considered in\u00a0Gray et al. (1980) and\u00a0Fujiki and Akaho (2009), where the authors use a constrained optimization method to find the Cartesian coordinates of the pole.\nIn contrast to the ordinary least squares method, this optimization problem has no analytical solutions. Although an optimal solution can be found numerically via grid search\u00a0(e.g. Gordon et al., 1984), such an approach is very inefficient. Here we employ the nonlinear conjugate gradient (CG) method of\u00a0Polak and Ribiere (1969) to find an optimal solution for this nonlinear optimization problem due to its very low memory requirements and fast convergence rate. We take advantage of the solver provided in the open-source SciPy package\u00a0(Virtanen et al., 2020) to perform the computations. It is worth noting that even though we use the constraints \n\n\n\n\u03b8\n\n\nq\n\n\n\u2208\n\n[\n0\n,\n\u03c0\n]\n\n\n, \n\n\n\n\u03d5\n\n\nq\n\n\n\u2208\n\n[\n0\n,\n2\n\u03c0\n)\n\n\n and \n\n\u03c6\n\u2208\n\n[\n0\n,\n\u03c0\n)\n\n\n, the periodicity of the cosine and sine functions allow us to treat this problem as an optimization problem without constraints.\nThe final update yields a single Euler pole which best describes the subsequence of paleomagnetic poles (Fig.\u00a02b). Its cost function gives the sum of the residuals between the predicted and observed paleomagnetic poles, and thus, a measure of goodness of fit. Using the spherical law of cosines, from the geographic coordinates of the inverted Euler pole \n\n(\n\n\n\u03b8\n\n\nq\n\n\n,\n\n\n\u03d5\n\n\nq\n\n\n)\n\n we can infer a stage rotation angle \n\u03c8\n, that is, the rotation angle needed to translate the youngest paleomagnetic pole \n\n\np\n\n\nk\n\n\n to the oldest one \n\n\np\n\n\nm\n\n\n, at an average angular velocity \n\n\u03c9\n=\n\u03c8\n\/\n\n(\n\n\nt\n\n\nm\n\u2212\n1\n\n\n\u2212\n\n\nt\n\n\nk\n\n\n)\n\n\n. To illustrate the performance of our methodology, an example Jupyter notebook with synthetic data is provided via Binder (https:\/\/mybinder.org\/v2\/gh\/LenGallo\/PEPy\/HEAD).\n\n\n3.2\nIdentifying the least-cost sequence of Euler vectors approximating an APWP\nEquipped with a method to invert a given set of paleomagnetic data to recover the best-fitting Euler vector describing them, we now present a method that seeks to identify the unknown change-points (i.e.\u00a0plate-motion changes) that divide an APWP into discrete segments associated with distinct stage Euler vectors. This is accomplished by seeking that sequence of change-points and Euler vectors which maximizes the goodness of fit between the predicted and observed paleomagnetic poles (Fig.\u00a03).\n\nOur goal is to identify a subsequence of ordered indices (i.e.\u00a0change-points) \n\n\n\nj\n\n\n1\n\n\n,\n\n\nj\n\n\n2\n\n\n,\n\u2026\n,\n\n\nj\n\n\nm\n+\n1\n\n\n\n, with \n\n\n\nj\n\n\n1\n\n\n=\n1\n\n and \n\n\n\nj\n\n\nm\n+\n1\n\n\n=\nN\n+\n1\n\n, such that each one of the \nm\n segments \n\n\nP\n\n\n\n\nj\n\n\nk\n\n\n\u2212\n\n\nj\n\n\nk\n+\n1\n\n\n\n\n belongs to the same circle. This implies that \n\n\n\nq\n\n\n\n(\nk\n)\n\n\n\n=\n\n\nq\n\n\n\n\nj\n\n\nk\n\n\n\n\n=\n\u22ef\n=\n\n\nq\n\n\n\n\nj\n\n\nk\n+\n1\n\n\n\u2212\n1\n\n\n\n and \n\n\n\n\u03c6\n\n\n\n(\nk\n)\n\n\n\n=\n\n\n\u03c6\n\n\n\n\nj\n\n\nk\n\n\n\n\n=\n\u22ef\n=\n\n\n\u03c6\n\n\n\n\nj\n\n\nk\n+\n1\n\n\n\u2212\n1\n\n\n\n for all \n\nk\n=\n1\n,\n2\n,\n\u2026\n,\nm\n\n, where we define by \n\n\nq\n\n\n\n(\nk\n)\n\n\n\n and \n\n\n\u03c6\n\n\n\n(\nk\n)\n\n\n\n the Euler pole and radius of the circle in the \nk\nth segment \u2013 or stage \u2013 of the APWP. In order to do so, we seek to minimize the following cost function \n\n(3)\n\n\n\n\nmin\n\n\n\n\nm\n,\n\n\n(\n\n\nj\n\n\n1\n\n\n,\n\n\nj\n\n\n2\n\n\n,\n\u2026\n,\n\n\nj\n\n\nm\n+\n1\n\n\n)\n\n,\n\n\n\n\n\n(\n\n\nq\n\n\n\n(\nk\n)\n\n\n\n,\n\n\n\u03c6\n\n\n\n(\nk\n)\n\n\n\n)\n\n\n\nk\n=\n1\n\n\nm\n\n\n\n\n\n\n\n\n\n\u2211\n\n\ni\n=\n1\n\n\nN\n\n\ngeod\n\n\n\n(\n\n\np\n\n\ni\n\n\n;\n\n\nq\n\n\ni\n\n\n,\n\n\n\u03c6\n\n\ni\n\n\n)\n\n\n\n2\n\n\n\n+\n\n\n\n\u2211\n\n\nk\n=\n1\n\n\nm\n\n\ngeod\n\n(\n\n\np\n\n\n\n\nj\n\n\nk\n\n\n\n\n;\n\n\nq\n\n\n\n\nj\n\n\nk\n\u2212\n1\n\n\n\n\n,\n\n\n\u03c6\n\n\n\n\nj\n\n\nk\n\u2212\n1\n\n\n\n\n)\n\n\n+\n\n\u03bb\nm\n,\n\n\n\nwhere the optimization is performed over all possible increasing paths \n\n(\n\n\nj\n\n\n1\n\n\n,\n\n\nj\n\n\n2\n\n\n,\n\u2026\n,\n\n\nj\n\n\nm\n+\n1\n\n\n)\n\n of unknown length \nm\n with \n\n\n\nj\n\n\n1\n\n\n=\n1\n\n, \n\n\n\nj\n\n\nm\n+\n1\n\n\n=\nN\n\n; and with respect to the Euler pole \n\n\nq\n\n\n\n(\nk\n)\n\n\n\n and rotation angle \n\n\n\u03c6\n\n\n\n(\nk\n)\n\n\n\n associated to each one of the \nm\n segments \n\n\nP\n\n\n\n\nj\n\n\nk\n\n\n\u2212\n\n\nj\n\n\nk\n+\n1\n\n\n\n\n. The second term in\u00a0Eq.\u00a0(3) was added in order to account for the contribution of the change points to both the previous and consecutive segments in the APWP.\nThe last term \n\n\u03bb\nm\n\n in the cost function (3) corresponds to a regularization added to avoid overfitting, with \n\n\u03bb\n\u2265\n0\n\n being a hyperparameter of the model. Large values of \n\u03bb\n will increase the value of the cost function, such that a good-fitting APWP with fewer segments will be favored. Note that\u00a0Eq.\u00a0(3) can be rewritten as \n\n(4)\n\n\n\n\nmin\n\n\nm\n,\n\n\n(\n\n\nj\n\n\n1\n\n\n,\n\n\nj\n\n\n2\n\n\n,\n\u2026\n,\n\n\nj\n\n\nm\n+\n1\n\n\n)\n\n\n\n\n\n\u2211\n\n\nk\n=\n1\n\n\nm\n\n\n\n\n\n\nmin\n\n\n\n\n\u0398\n\n\n\n(\nk\n)\n\n\n\n\n\n\n\nC\n\n\n\n\nj\n\n\nk\n\n\n\u2212\n\n\nj\n\n\nk\n+\n1\n\n\n\n\n\n(\n\n\n\u0398\n\n\n\n(\nk\n)\n\n\n\n)\n\n+\n\u03bb\n\n\n,\n\n\n\nwith \n\n\n\u0398\n\n\n\n(\nk\n)\n\n\n\n the parameters of the minimum circle for the segment \n\n\nP\n\n\n\n\nj\n\n\nk\n\n\n\u2212\n\n\nj\n\n\nk\n+\n1\n\n\n\n\n. Based on the method introduced in Section\u00a03.1, we can find the Euler pole and amplitude for each one of the possible segments \n\n\nP\n\n\ni\n\u2212\nj\n\n\n for each possible value of \ni\n and \nj\n, and then compute the minimum inside the parentheses in\u00a0Eq.\u00a0(4).\nOur approach to minimize the cost function of\u00a0Eq.\u00a0(4) proceeds with an exploratory analysis to characterize all possible pathways \n\n\nP\n\n\ni\n\u2212\nj\n\n\n, and then employs a graph representation of the APWP to discover the optimal sequence of change-points \n\n(\n\n\nj\n\n\n1\n\n\n,\n\n\nj\n\n\n2\n\n\n,\n\u2026\n,\n\n\nj\n\n\nm\n+\n1\n\n\n)\n\n and the number \nm\n which yields the minimum sum of squared misfits between the set of paleomagnetic poles and the modeled arc segments describing them.\nGraphs are compact mathematical structures that amount to a set of connected objects, where objects are referred to as nodes and connections are referred to as arrows (or edges for undirected graphs). Arrows can have an associated weight that indicates the cost of traversing them. An APWP can then be considered as a graph where the nodes correspond to paleomagnetic poles and pairs of nodes are connected by arrows, to which a numerical weight can be assigned (Fig.\u00a04). We may thus model an APWP \n\nP\n=\n\n{\n\n\np\n\n\n1\n\n\n,\n\n\np\n\n\n2\n\n\n,\n\u2026\n,\n\n\np\n\n\nN\n\n\n}\n\n\n where the arrow from \n\n\np\n\n\ni\n\n\n to \n\n\np\n\n\nj\n\n\n exists for all \n\ni\n,\nj\n\n such that \n\n\n\nt\n\n\nj\n\n\n>\n\n\nt\n\n\ni\n\n\n\n and spans at least \n3\n paleomagnetic poles (at least 3 non-colinear points are required to define a plane). The weight of each arrow is given by the optimal value of the function \n\n\nC\n\n\ni\n\u2212\nj\n\n\n plus \n\u03bb\n. In this context, a candidate approximation of the APWP can be seen as the directed path \n\n(\n\n\np\n\n\n\n\nj\n\n\n1\n\n\n\n\n,\n\n\np\n\n\n\n\nj\n\n\n2\n\n\n\n\n,\n\u2026\n,\n\n\np\n\n\n\n\nj\n\n\nm\n+\n1\n\n\n\n\n)\n\n with \n\n\n\np\n\n\n\n\nj\n\n\n1\n\n\n\n\n=\n\n\np\n\n\n1\n\n\n\n and \n\n\n\np\n\n\n\n\nj\n\n\nm\n+\n1\n\n\n\n\n=\n\n\np\n\n\nN\n\n\n\n, where each pair of consecutive nodes in the path \n\n(\n\n\np\n\n\n\n\nj\n\n\nk\n\n\n\n\n,\n\n\np\n\n\n\n\nj\n\n\nk\n+\n1\n\n\n\n\n)\n\n is connected by an arrow. The total cost of each alternative series of directed paths is given by the sum of the individual weights of the arrows in the path, which coincides with Eqs.\u00a0(3) and (4). The characterization of all possible pathways \n\n\nP\n\n\ni\n\u2212\nj\n\n\n is accomplished by a moving window searching algorithm, where each possible \n\n\nP\n\n\ni\n\u2212\nj\n\n\n is analyzed.\n\nIn graph theory, Dijkstra\u2019s algorithm allows identification of the shortest path between two given nodes, such that the sum of the weight of its constituent edges is minimized\u00a0(Dijkstra, 1959). We perform shortest path searches through the open-source package SciPy\u00a0(Virtanen et al., 2020). The hyperparameter \n\u03bb\n is adjusted to avoid overfitting, namely, by preventing an over-estimation of the number of segments, \nm\n. The statistical literature offers several different approaches to select the optimal value of the hyperparameter \n\u03bb\n. We apply the elbow method to find the \n\u03bb\n value that yields the optimum value of \nm\n\u00a0(Bholowalia and Kumar, 2014). This is achieved by plotting the total cost against the number of segments \nm\n and evaluating their tradeoffs: small values of \nm\n will add large total costs but at some threshold value of \nm\n the cost will be observed to decrease sharply. Above this threshold, further increasing \nm\n will not yield significant further reductions to the total cost. The graphical \u2018elbow\u2019 in the plot thus points to this optimal value of \nm\n. Other methods to select the optimal number of segments include the Akaike information criterion (AIK) and Bayesian information criterion (BIC)\u00a0(Friedman, 2017).\nWe summarize our algorithm to find the optimal sequence of Euler vectors to approximate a given sequence of paleomagnetic poles \nP\n as follows:\n\n\n\n1.\nFor each time-ordered pair of paleomagnetic poles \n\n\np\n\n\ni\n\n\n and \n\n\np\n\n\nj\n\n\n, find the circle in \n\n\n\nP\n\n\ni\n\u2212\nj\n\n\n=\n\n{\n\n\np\n\n\ni\n\n\n,\n\u2026\n,\n\n\np\n\n\nj\n\n\n}\n\n\n that minimizes the value of \n\n\nC\n\n\ni\n\u2212\nj\n\n\n.\n\n\n2.\nConstruct a directed graph and select \n\u03bb\n through the elbow method to avoid overfitting.\n\n\n3.\nUse Dijkstra\u2019s algorithm to find the shortest path between the first pole \n\n\np\n\n\n1\n\n\n and the last pole \n\n\np\n\n\nN\n\n\n.\n\n\n4.\nFrom the optimal path, recover the parameters of the stage Euler vectors for each APWP segment.\n\n\n\n\n\n\n4\nResults\nTo illustrate the application of our methodology, we execute a simple test involving idealized synthetic paleomagnetic data. We start by generating a stochastic model of the drift of a plate over 200 Ma by randomly generating stage Euler vectors \u2013 represented in the matrix form as \n\nR\n\n(\nq\n,\n\u03c9\n)\n\n\n \u2013 which remain constant during a temporal stage of given duration. The geographic coordinates of each Euler vector \n\n(\n\n\n\u03b8\n\n\nq\n\n\n,\n\n\n\u03d5\n\n\nq\n\n\n)\n\n are randomly selected from a uniform distribution on the unit sphere and their angular velocities \n\u03c9\n are chosen from a uniform distribution constrained between 0.5\u00b0\/Ma and 2.5\u00b0\/Ma (which yields plate velocities up to \u00a028\u00a0cm\/yr). The corresponding duration of each Euler vector is randomly selected from a uniform distribution between 10 and 30 Ma, and Euler vectors are drawn until their summed durations reach or exceed 200 Ma. Using this stochastic kinematic model, we assemble a synthetic APWP populated by paleomagnetic poles every 1 Ma (Fig.\u00a05a). To demonstrate the ability to incorporate noise, we perturb the geographic coordinates of the paleomagnetic poles by Gaussian random noise up to 0.1\u00b0. Note that the latter step is only taken to demonstrate the capacity of our method to handle noise; a more comprehensive consideration of the feasibility of PEP analysis given standard paleomagnetic noise has yet to be addressed.\nNext, we proceed to invert the synthetic paleomagnetic data according to the methodology outlined above. First, for each candidate APWP segment (all ordered subsets of the paleomagnetic data) we compute the best-fitting Euler vector by minimizing Eq.\u00a0(4). We then organize the computed costs as a graph in the form of an adjacency matrix, which can be rendered visually as a heatmap (Fig.\u00a05b). Visual inspection of this heatmap reveals strong contrasts in the magnitude of the cost function, which tends to increase sharply with the passage of a change-point. Using the regularization introduced in\u00a0Eq.\u00a0(4) and the elbow method (Fig.\u00a05c and Fig.\u00a05d), we identify the shortest path through this graph following Dijkstra\u2019s algorithm, which yields the number of segments \nm\n and a discrete set of change-points \n\n(\n\n\nj\n\n\n1\n\n\n,\n\n\nj\n\n\n2\n\n\n,\n\u2026\n,\n\n\nj\n\n\nm\n+\n1\n\n\n)\n\n. From this sequence of change-points we retrieve the parameters of the minimum circle \n\n\n\u0398\n\n\n\n(\nk\n)\n\n\n\n for each segment \n\n\nP\n\n\n\n\nj\n\n\nk\n\n\n\u2212\n\n\nj\n\n\nk\n+\n1\n\n\n\n\n, enabling us to infer the complete kinematic history through the series of stage Euler vectors \n\n\n\nR\n\n\n\u02c6\n\n\n\n(\nq\n,\n\u03c9\n)\n\n\n. This workflow can be run interactively in a web browser via Binder (https:\/\/mybinder.org\/v2\/gh\/LenGallo\/PEPy\/HEAD) with a synthetic example in the Jupyter Notebook (APWP.ipynb).\n\nTo measure the performance of our graph-based PEP methodology in terms of accuracy, we generate 1000 stochastic models of the drift of a plate and invert the paleomagnetic data from each for the series of underlying stage Euler vectors \n\n\n\nR\n\n\n\u02c6\n\n\n\n(\nq\n,\n\u03c9\n)\n\n\n. Comparisons between the pairs of true \n\nR\n\n(\nq\n,\n\u03c9\n)\n\n\n and estimated \n\n\n\nR\n\n\n\u02c6\n\n\n\n(\nq\n,\n\u03c9\n)\n\n\n stage Euler vector series are then assessed by a variety of metrics. Using the inverted kinematic model, we assemble a synthetic APWP and compute the time-dependent geodesic distance (i.e.\u00a0residual) between it and the known APWP. In Fig.\u00a06a we show the ensemble median of this residual, along with the 0.25\u20130.75 and 0.16\u20130.84 percentile ranges representing standard confidence intervals. As could be expected, we observe that average misfits tend to accumulate backward in time, but we note that they never exceed 3 degrees. In 6b\u20136c we further dissect the discrepancies in the parameters of the Euler vectors: in panel b we show the ensemble median of the time-dependent geodetic distance between Euler poles, while panel c shows the difference in angular velocities. These results reveals that the average misfit between the true and estimated stage Euler vectors of any given simulation is low. As another way to evaluate differences between the true kinematics and our inferences, we can quantify the discrepancies in the drift of a given point or points after rotation by the series \nR\n vs. \n\n\nR\n\n\n\u02c6\n\n\n. To consider a range of misfits arising from different observation points, we start by generating 5 nodes evenly distributed in latitude (along the central meridian) and then calculate the time-dependant geodesic distance between the inferred drift and the true drift for each point. The mean of these distances is then computed for each time in order to estimate the time-dependant average drift (i.e.\u00a0total motion) misfit. Fig.\u00a06d shows that the median (of the ensemble of means) of the drift misfit never exceeds 7.5 degrees. As a final metric, we measure the surface velocity misfit associated with our kinematic inferences. Using the same points tracked above, we calculate the surface velocity at each point at each time by computing the cross-product between the point locations and the true and inferred Euler vectors (6e).\nConsidering all these ensemble medians to be representative of the misfit associated with the workflow in the absence of significant noise, we conclude from these metrics that our methodology can robustly retrieve the kinematics of a plate from its APWP.\n\n\n\n\n\n5\nDiscussion\nConceptually, in providing a means to retrieve the full kinematic histories of tectonic plates from paleomagnetic data, PEP analysis presents an enormously powerful tool. And yet, despite its foundations being laid more than half a century ago\u00a0(Francheteau and Sclater, 1969), lingering doubts about the rigor and viability of PEP analysis have left it largely stranded in obscurity.\nOne of the most significant and persistent shortcomings in PEP analysis has been the subjective determination of which APWP segments to invert, and which inversions to retain. As noted by previous studies\u00a0(e.g. Tarling and Abdeldayem, 1996; Wu and Kravchinsky, 2014), the optimal fits to real paleomagnetic data tend toward short segments associated with proximal circles of small-radii, which often imply improbable rates of plate motion change, unrealistic angular velocities and\/or geologically dubious kinematics. As a consequence, the selection of which segments to fit has been done a priori by visual inspection\u00a0(e.g. Gordon et al., 1984; May and Butler, 1986; Wu and Kravchinsky, 2014), undercutting the rigor of the analysis and making it prone to subjective human bias. In some instances the best-fitting small-circle solutions have also been dismissed in favor of either subjective assessments of more feasible solutions\u00a0(Tarling and Abdeldayem, 1996) or forced great-circle fits\u00a0(Smirnov and Tarduno, 2010). Although\u00a0Smirnov and Tarduno (2010) consider the use of great-circle fits to be a conservative measure, we note that approach confines all stage Euler vectors to the equatorial plane.\u00a0Rose (2016) presented a significant step forward in introducing a Bayesian approach to PEP analysis (later applied by\u00a0Swanson-Hysell et al. (2019)), which addressed some of these issues of subjectivity and presented a pathway toward a more comprehensive treatment of uncertainty in such analyses. However, in its present formulation, that approach still regularizes the problem through specification of the number of change-points.\nOur methodology addresses several of these deficiencies, and notably the problem of objectively selecting change-points, by identifying the least-cost solution without supervision. The least-cost solution is a full approximation of the APWP, such that the location of the change-points and the parameters of the stage Euler vectors defined between them are fully and automatically determined by the algorithm. Although the method employs a regularization parameter to avoid overfitting, the elbow method allows a more objective selection of this parameter. Other parameters that can be modified by the user (number of inversion seeds, maximum duration of a given stage) \u2013 while capable of affecting the outcome if set to too low values \u2013 are used to improve efficiency and so can be set to arbitrarily high values. Admittedly, our experiments here have been conducted on idealized synthetic data, and it can be anticipated that experiments with real (noisy) data may introduce circumstances in which the least cost solution can otherwise be shown to be unrealistic (e.g.\u00a0requiring impossibly high rates of rotation). However, the flexibility of our methodology is such that minor adaptions to our optimization framework (e.g.\u00a0specifying constraints on rotation rates in the optimization problem) allow such issues to be tackled systematically.\nDespite the advancements presented herein, there remain a number of significant challenges associated with the application of PEP analysis to real paleomagnetic data. The first is the feasibility of the methodology in light of the noise accompanying paleomagnetic data, arising from both intrinsic (geomagnetic secular variation) and extrinsic (e.g.\u00a0measurement errors, tectonic rotations, inclination shallowing, erroneous age assignments) uncertainties. Although the cost function can be further constrained to avoid unrealistic solutions, there exists some level of noise above which the uncertainty in the solution space becomes so vast as to render the optimal solution meaningless. However, a systematic exploration of these noise thresholds, as well as the trade-offs between noise level and the rate of plate motion, has yet to be conducted. Furthermore, we note that current APWPs\u00a0Torsvik et al. (2012, e.g.) describe plate motion in 10 Ma, yielding only small quantities of data to be fitted by our algorithm. However, advancements in the construction of APWPs from VGP-level data\u00a0(e.g. Vaes et al., 2022) provide promising data-rich alternatives that can enable PEP analysis to reach higher levels of precision.\nA second challenge concerns the phenomenon of true polar wander (TPW). TPW is a rotation of the entire solid Earth that occurs in response to changes in the planetary moment of inertia following changes to Earth\u2019s internal mass distribution\u00a0(Goldreich and Toomre, 1969). Because TPW always acts to restore the largest inertial axis to the planetary rotation axis, TPW always occurs about an equatorial axis. Thus, during a TPW event, the entire tectonic plate system (together with the mantle) moves relative to the spin axis by a common angular displacement. Paleomagnetic data, in recording the movement of the lithosphere relative to the magnetic field (which is not rotated by TPW) therefore represents some composite signal of TPW and differential plate motion. Before PEP analysis can be applied to any given paleomagnetic dataset, the TPW signal needs to be removed from it, if not otherwise assumed to be negligible. Unfortunately, time-dependent estimates of TPW are themselves uncertain and often controversial. More significantly, prior to Cretaceous time, TPW estimates are derived from plate kinematic models\u00a0(Steinberger and Torsvik, 2008); any results from PEP analysis employing these TPW estimates would thus undermine their own foundation wherever they challenged the underlying kinematic model. Future progress on this front will likely necessitate a joint modeling approach.\nWe end with a reminder that the methodology and tools presented herein, while motivated by the desire to improve upon existing approaches in PEP analysis, are not restricted to paleomagnetic applications. They could, for example, be equally applied to hotspot tracks. There are also broader applications of the small-circle fitting method (which is intentionally presented as a standalone tool), as arise, for example, in structural geology. In particular, the identification and analysis of conical folds is achieved by the best-fitting small circle to a set of poles to folded layers\u00a0(e.g. Ramsay, 1964; Cruden and Charlesworth, 1972; Pastor-Gal\u00e1n et al., 2012; Mulchrone et al., 2013).\n\n\n6\nConclusions\nOwing to the axial symmetry of the Earth\u2019s magnetic field, paleomagnetic data only directly record the latitudinal and azimuthal positions of crustal blocks in the past, and paleolongitude is not constrained. However, full paleo-kinematic information can be retrieved from assemblages of paleomagnetic data with use of PEP analysis. Here we introduced an optimization approach to PEP analysis that enables it to be conducted in a fully data-driven way. Our methodology follows the solution of two distinct optimization problems. First, the parameters of the best-fitting circle to a set of paleomagnetic poles are found through a gradient-based Euler vector optimization method. Then, through an exploratory data analysis, all the possible segments at every point of the APWP are assessed in order to transform the data into a weighted, undirected graph. This graph representation of the APWP allows the identification of that sequence of stage Euler vectors which together present the least cost (shortest path of the graph) description of an observed APWP. As demonstrated by a set of simple experiments with synthetic data drawn from stochastic kinematic histories, our algorithm is able to invert paleomagnetic data to recover kinematic parameters that closely approximate the known (true) values. However, while the tools presented herein mark an important step forward, a number of significant challenges remain for the application of PEP analysis to real paleomagnetic data.\n\n\nCRediT authorship contribution statement\n\nLeandro C. Gallo: Conceptualization, Formal analysis, Coding, Debugging of code, Visualization, Writing \u2013 original draft, Writing \u2013 review & editing, Supervision. Facundo Sapienza: Conceptualization, Formal analysis, Coding, Writing \u2013 review & editing. Mathew Domeier: Conceptualization, Formal analysis, Coding, Visualization, Writing \u2013 original draft, Writing \u2013 review & editing.\n\n","39":"","40":"","41":"\n\n1\nIntroduction\nThe Earth's electromagnetic cavity consists of the good dielectric atmosphere, limited by two conducting boundaries: the planet surface and the lower ionosphere. The main electromagnetic source of this cavity is the lightning activity, mainly produced by storms. The Earth's electromagnetic cavity has eigenfrequencies around 8, 14, and 21\u00a0Hz, for the first three modes, respectively. Lightning produces a broadband electromagnetic spectrum that reaches the MHz band, but, due to the conductivity of the atmosphere at these frequencies, only Extremely Low Frequency (ELF) components, up to \u223c50\u00a0Hz, are capable of completing various turns around the planet before they almost completely attenuate and become undetectable. Multiple lightning sources are active at any time around the globe, exciting ELF modes which are known as Schumann Resonances (SRs) (Schumann, 1952). SRs were predicted in 1952, and first detected by Balser and Wagner (1962). A historical review on the progress of SR studies can be found in (Besser 2007).\nThere is plenty of specialized scientific literature on Atmospheric Electrodynamics in general and on SRs in particular. Worthy examples are the books (Nickolaenko and Hayakawa, 2002, 2014) and the reviews (Price, 2016; Sim\u00f5es et al., 2012; or S\u00e1tori et al., 2013). The SRs constitute the AC part of the Global Electric Circuit (GEC) (Rycroft et al., 2008) and their variations have been linked to the average temperature of the surface near tropical latitudes (E. R. Williams, 1992) and to the global lightning activity (F\u00fcllekrug and Fraser-Smith, 1997; Pr\u00e1cser et al., 2019; Toledo-Redondo et al., 2010; Williams et al., 2014). SRs are also a powerful tool to monitor global climate phenomena, such as El Ni\u00f1o (Williams et al., 2021) and also to study fluctuations in the ionosphere at the scale of years (e.g., (Boz\u00f3ki et al., 2021; Koloskov et al., 2020; Nickolaenko et al., 2015).\nSRs on Earth are a very faint signal, of the order of a few pT for the magnetic field components, i.e., roughly six orders of magnitude below the geo-magnetic field strength at the Earth's surface. Therefore, in order to measure and characterize them, the detectors must have high sensitivity and must be optimized to provide a good signal to noise ratio (Fornieles-Callej\u00f3n et al., 2015). Most ELF stations devoted to measure SRs typically comprise two magnetometers which lie in the plane of the surface, plus one vertical electric antenna (e.g., Nickolaenko and Hayakawa, 2014). Nowadays, there are several SR stations spread all around the globe and various studies benefit from multi-point observations (Roldugin et al., 2004; Schlegel and F\u00fcllekrug, 2000; Sentman and Fraser, 1991; Shvets and Hayakawa, 2011). Despite the interest in ELF station global networks to share data and findings, the actual situation is that each research group usually works with their own data, since details on the data measured, the signal processing techniques and the codes to implement them are not usually available for the scientific community.\nIn this sense, two contributions are introduced in this work with regards to previous studies on SR measurements which may be of interest for those research groups that are working or want to start research on SRs and related problems such as the GEC. It can also be useful for those research groups in Atmospheric Sciences and Space Weather to make correlations between different measurements. The first contribution is that the scientific community is granted access to four years of raw and processed data from the Sierra Nevada ELF station measurement records. It seems reasonable to think that providing the data is of high interest to pursue a common goal set by the SR research community in particular, and by the atmospheric electrodynamics research community, in general, as it is the creation of a shared database with records of the different worldwide ELF stations. Our intention is not to set a standard for the processing and the format of the data but to propose a starting point. The second contribution of this work is concerned with the presentation of the codes for obtaining the SR parameters and other related results from these data. The analysis of the four-year long records may demand a huge amount of code lines, which can make it difficult to fully understand the algorithms and, therefore, may hinder their use by other researchers. For this reason, the Jupyter notebooks (Granger and Perez, 2021) are presented together with this paper in the supplementary material, containing the Python code, as well as the explanations needed to understand the processing of the data. Providing the data and the self-explanatory Jupyter notebooks focuses on the concepts and tools behind reporting modern data analyses in a reproducible manner: any supposed scientific achievement must be verified and must serve as a basis for building new achievements. In this sense, the notebooks from Jupyter project allow to publish data analyses in a single document that permit other researchers to easily carry out the same analysis to obtain the same results for the provided data and comparable results for their own measurements.\nThis paper is structured as follows. Section 2 describes the main characteristics of the Sierra Nevada ELF station and the structure of the data in the different repositories. Section 3 gives the skeleton of the programs and the blocks in which they are structured. In Sections 4 to 7, a detailed description of each block mentioned above is presented. Finally, Section 8 summarizes the main conclusions of this work.\n\n\n2\nThe station and the data\n\n2.1\nThe station\nThe ELF measurement station is located at the heart of the Sierra Nevada National Park, at 2500\u00a0m above sea level, in the area surrounding the mountain hut, Refugio del Poqueira (37\u00b002\u2032N, 3\u00b019\u2032W) and it has two magnetometers, North-South (NS) and East-West (EW) oriented. The magnetometers have been optimized with the specific purpose of measuring the first three SRs with a very high signal-to-noise ratio (28\u00a0dB for a time-varying signal of 1\u00a0pT in the band from 6\u00a0Hz to 25\u00a0Hz).\nThe station data acquisition system works at a sampling frequency of f\n\nm\n\u00a0=\u00a0256\u00a0Hz and generates a raw data file for each sensor and hour. The sampling time interval is 3906 \u03bcs, which is slightly less than 1\/f\n\nm\n. In 1\u00a0h file, a total of 921,600 samples are taken, so the time length of measurements of each file is 3599.7696\u00a0s, instead of 3600\u00a0s, which implies a delay of 0.2304\u00a0s every hour. For this reason, the time of the first date of each hour file is different to the next hour, and so on.\nThe measurements correspond to the output of the sensor, including the amplification and digitization system. Measurements are taken between \u00b110\u00a0V limits, greater values produce saturation. The digitization system employs 16 bits equally distributed. The amplification system uses a Butterworth filter of tenth order to dampen the effect of the 50\u00a0Hz network signal. Due to the specific characteristics of the designed filter, it is very inaccurate to obtain the phase of the transfer function. That's why the scaling function shown below only defines the amplitude.\nThe magnetometers have been calibrated for frequencies between 6\u00a0Hz and 25\u00a0Hz. Considering the equivalent circuit of the magnetometers, (Fornieles-Callej\u00f3n et al., 2015), their frequency response corresponds to an RLC circuit that exhibits an almost flat response for frequencies below 25\u00a0Hz, approximately, which defines the upper bandwidth limit. In order to avoid the 1\/f noise, the lower bandwidth limit has been stablished to 6\u00a0Hz. To convert the voltage induced in the sensors into a magnetic field, the calibration function must be defined for each frequency of the spectrum. Previously, it is necessary to pass the measurements from the time domain to the frequency domain using the Fourier transform. From the theorical RLC model of the magnetometers, the calibrated magnetic field is given by:\n\n(1)\n\n\nB\n\n(\nf\n)\n\n=\n\nS\nc\n\n\n(\nf\n)\n\nV\n\n(\nf\n)\n\n,\n\n\n\nwith f in the (6\u00a0Hz, 25\u00a0Hz) interval. B(f) is the magnetic field (in T\/ \n\n\n\nH\nz\n\n\n\n), V(f) is the transformed voltage amplitude and S\n\nc\n(f) is the calibration coefficient for each frequency f. As mentioned above, the specific design of the Butterword filter presents difficulties in accurately measuring the phase, therefore, we only apply the calibration function to the amplitude of the Fourier transformed voltage and all the quantities in (1) are to be understood as the amplitude of the corresponding quantity.\nThe function S\n\nc\n(f) is given by:\n\n(2)\n\n\n\nS\nc\n\n\n(\nf\n)\n\n=\n\n1\n\n2500\n\n\n\n\n\n1\n\n\n1.9\n\n\u00b7\n\n\n10\n6\n\n\n\n\n\n\n1\n\n\nf\n\n\n\n\n\n\n(\n\n1\n\u2212\n\n\n(\n\nf\n\nf\n0\n\n\n)\n\n2\n\n\n)\n\n2\n\n+\n\n\n(\n\n2\n\u03b4\n\nf\n\nf\n0\n\n\n\n)\n\n2\n\n\n\n,\n\n\n\nwhere 1\/2500 is the amplification factor, 1\/(1.9\u00b7106) is the magnetometer sensitivity, 1\/f is given by Faraday's induction law. Finally, f\n0 and \u03b4 are the resonance frequency and dumping factor of the equivalent RCL of the measurement system, respectively, and they can be calculated from:\n\n(3)\n\n\n\nf\n0\n\n=\n\n1\n\n2\n\u03c0\n\n\nL\n\nC\nT\n\n\n\n\n\n,\n\n\n\n\n\u03b4\n=\n\nR\n2\n\n\n\n\nC\nT\n\nL\n\n\n,\n\n\n\nwhere R\u00a0=\u00a0320\u00a0k\u03a9 is the magnetometer resistance, L\u00a0=\u00a0298\u00a0kH is the magnetometer inductance and C\n\nT\n\u00a0=\u00a040\u00a0pF is the magnetometer capacitance. The technical characteristics of the station are described in more detail in (Fornieles-Callej\u00f3n et al., 2015).\n\n\n2.2\nThe data\nThe Sierra Nevada ELF station was deployed on July 17, 2012 and has been operating almost continuously since March 2013. The measurement period made available with this paper runs from March 2013 to February 2017. The days for which no measurements are available are: 2014, July 2 and 3 and November 5 to 18; 2016, from July 4 to 6.\nThe raw data of the station corresponding to the aforementioned measurement periods can be downloaded from the following Zenodo repositories (Salinas et al., 2022a, b, c, d). Each repository corresponds to one year of measurements except the first one that contains the data for the years 2013 (from March) and 2017 (January and February).\nEach data folder is named with the four digits corresponding to the specific year, e.g., 2014. Each year contains folders for each month, named with the four digits corresponding to the year and month, e.g., 1412 stands for the raw data during December 2014. Within each month folder, the data and information files for each sensor and each hour are available. Data are stored in files containing measurements corresponding to a time period of approximately 1\u00a0h. The filenames begin with a common part, \u201csmplGRTU1_sensor_\u201d, followed by a specific part to denote the sensor used (0 for the NS orientation and 1 for the EW orientation), the date and the initial time of the measurements recorded. For example, smplGRTU1_sensor_0_1412010430 stands for data measured by the NS-oriented magnetometer, during the year 2014, month 12, day 01, hour 04, and starting minute 30. The information file has the same name but ends with _info.txt. Each information file, among other data, contains the specific time in which the first sample of the corresponding data file is taken. The starting time for each file is determined using GPS signals and has a resolution of milliseconds. Since this time of the first sample in each file is variable and part of the filename, the filenames in each month folder are not completely determined a priori. For this reason and to ease the task of the different programs used, each month folder includes two files, 'ficheros0\u2032 and 'ficheros1\u2032, for sensor NS and EW respectively, that contain the set of filenames for that specific month. Therefore, for each normally measured day, we have 24 data files and also 24 information files for each sensor. Each hour data file occupies 1.8\u00a0MB, so each month has roughly a data volume of 2.6\u00a0GB.\nThe data in each 1\u00a0h data file is in 16-bit binary format and its conversion to voltage involves multiplying by a factor defined by 10\/215, to describe the \u00b110\u00a0V saturation limits and the use of a 16-bit of the A\/D system, one bit for the sign and 15 bits for amplitude.\n\n3.\nThe code: an overview\n\n\n\nThe software codes to extract the SR parameters presented with this work are programmed in Python (version 3.8) using the modules Numpy,\n1\n\n\n1\n\nhttps:\/\/numpy.org\/.\n Matplotlib,\n2\n\n\n2\n\nhttps:\/\/matplotlib.org\/.\n Lmfit,\n3\n\n\n3\n\nhttps:\/\/lmfit.github.io\/lmfit-py\/.\n and Interact\n4\n\n\n4\n\nhttps:\/\/pypi.org\/project\/interact\/.\n (this one is only used in notebooks).\nThe main blocks and programs developed are the following:\n\n1.\n\u2018Reading_Fourier.py\u2019. It reads the raw data in the time domain and obtains the amplitude spectrum in the calibrated band of the measurement system.\n\n\n2.\n\u2018Anthropo.py\u2019. When present, this program eliminates the anthropogenic noise in the amplitude spectrum of the previous step.\n\n\n3.\n\u2018Lorentz_Lmfit.py\u2019. The program applies a non-linear fit using the \u2018Lmfit\u2019 module of Python to generate a Lorentzian fitted spectrum together with its corresponding error parameters.\n\n\n4.\n\u2018Packaging.py\u2019. This program carries out a final storage of the relevant data. The program generates a file for each month and sensor that allows a direct reading and recovering of the structure (arrays of the Numpy package of Python) of all the processed data generated by the previous packages for subsequent analysis purposes.\n\n\n\nThe full set of four programs may be of interest to those researchers who are interested in the direct measurements of the Sierra Nevada station or in processing their own raw data. They are welcome to use the program of block 1 using the ELF station raw data that can be downloaded from the Zenodo repositories (Salinas et al., 2022a, b, c, d) as mentioned above. But the code is developed in modular form, thus, as regards those researchers interested in working with the different phases of the spectrum, with or without anthropogenic noise, with the spectrum fitted or not, they can use the output files from programs in block 2 or 3. There are several repositories that can help in this task. The University of Granada repositories http:\/\/hdl.handle.net\/10481\/73978 and http:\/\/hdl.handle.net\/10481\/73978 contain all the files of the complete process (blocks 1 to 4) corresponding to December 2014 and March 2015 respectively. They have the same structure of years and months as the raw data folder. Within each month, output files for each step of the processing can be found. The code files, notebooks and one day (March 1, 2015) input\/output files can be downloaded from the GitHub repository https:\/\/github.com\/asalinas62\/Sierra_Nevada_SR.\nIf the researcher's interest is exclusively concentrated in the use of the final results of the analysis, they can directly use the files resulting from block 4. These files can be downloaded from the University of Granada repository https:\/\/digibug.ugr.es\/handle\/10481\/71563. This folder stores the final SR results of the raw data processing from the period of time between March 2013 to February 2017. These files have been used to carry out the Schumann resonance long-term analysis presented in (Rodr\u00edguez-Camacho et al., 2022), where a first analysis of the data provided with this work can be found.\nThe input and output files associated to the programs used for each intermediate processing stage are:\n\n1.\n\u2018Reading_Fourier.py\u2019: the amplitude spectra and related information generated by this program are stored in the following files (December 2014 is used as an example):\n\n\u2022\nSR1412_media_0,1: it contains the amplitude spectra within the calibrated band of the magnetometers (6\u00a0Hz\u201325\u00a0Hz), for the NS sensor (sensor 0) and for the EW sensor (sensor 1), respectively. The writing format is ASCII in one column.\n\n\n\u2022\nSR1412_satper_0,1: it stores the percentage of those 10\u00a0s intervals that show saturation within each 10\u00a0min interval. There is a file for each sensor. The writing format is ASCII in one column.\n\n\n\n\n\n2.\n\u2018Anthropo.py\u2019: the anthropogenic noise detected in the measurements is eliminated from the spectrum. The following output file is generated:\n\n\u2022\nSR1412_mediaNA_0,1: similar to SR1412_media_0,1 but with the anthropogenic noise filtered from the amplitude spectra. The writing format is ASCII in one column.\n\n\n\n\n\n3.\n\u2018Lorentz_Lmfit.py\u2019: a Lorentzian fitting of the amplitude spectrum is made. The output files for this step are:\n\n\u2022\nSR1412_mediaLO_0,1: it contains the parameters of the Lorentzian adjustment of the spectra in amplitude for each 10\u00a0min interval and sensor. The writing format is ASCII in one column.\n\n\n\u2022\nSR1412_mediaLOC_0,1: it includes the fitting error parameter of the \n\n\n\u03c7\n2\n\n\n function in each spectrum. The writing format is ASCII in one column.\n\n\n\u2022\nSR1412_mediaLOE_0,1: it contains the errors that the fitting program assigns to each calculated fitting parameter. The writing format is ASCII in one column.\n\n\n\n\n\n\nAs a summary of the whole process, for each month of measurements (December 2014 and NS sensor is used as an example) and each sensor the procedure is as follows. The raw data from the station is arranged in files 1\u00a0h long. The 'Reading_Fourier.py' program calculates the amplitude spectrum for the frequency band (6\u00a0Hz\u201325\u00a0Hz) and the saturation index for each 10\u00a0min interval and they are saved in two files, SR1412_media_0 and SR1412_satper_0, respectively. The first file is used to filter the anthropogenic noise with the 'Anthropo.py' program and the output spectra are stored in the file SR1412_mediaNA_0. This file in turn serves as input for the 'Lorentz_Lmfit.py' program, which returns the parameters of the Lorentzian fit. Finally, selected outputs of each block are stored.\nThe general procedure for processing data from an ELF station with the code presented in this work may have many particularities that must be taken into account if it is to be adapted to data from other stations. Nevertheless, we can establish the following general inputs to the programs that can be adapted to each particular ELF station:\n\n1.\nDefine the measurement interval generated by the station: in our case, one month is divided in 1\u00a0h data files.\n\n\n2.\nDefine the characteristics of the A\/D system: sampling frequency, measurement range and number of bits.\n\n\n3.\nDefine the calibration function to convert voltage into magnetic field. In the case of the Sierra Nevada ELF station and due to the characteristics of the sensors, this step can only be taken in the frequency domain.\n\n\n4.\nIn case there is anthropogenic noise with characteristics similar to those found in Sierra Nevada: define the frequency bands with possible anthropogenic noise.\n\n\n\nA more detailed description of the programs corresponding to this overview is carried in the following sections.\n\n\n\n3\nSignal processing step 1: the Reading_Fourier.py program\nThis program reads the raw data and obtains the amplitude spectrum for each component of the horizontal magnetic field. The code and details of this stage are described in the notebook 'Reading_Fourier_docu.ipynb'.\nThe data for each hour and sensor is processed in 10\u00a0min intervals. Since the noise level is large due to the low power of SRs, in order to get a natural stabilization of the SR spectra, we apply the Welch method to improve the signal to noise ratio. The 10\u00a0min time signal is divided into 10\u00a0s segments, overlapped by 5\u00a0s, and a Fast Fourier Transform (FFT) is calculated for each of them after applying a Hann window. Following Nickolaenko and Hayakawa, 2002, pg. 170, the resulting power spectra are then averaged, and the calibration function is applied to the obtained amplitude spectrum, see also (Price and Melnikov, 2004), providing physical units of pT\/ \n\n\nH\n\nz\n\n.\nTo apply Welch's method, the FFT of a set of n data is made, taken in groups of m data and with a shift of p data. The number of intervals generated from n, m and p is \n\n\n[\n\n\n(\n\nn\n\u2212\nm\n\n)\n\n\/\np\n\n]\n\n+\n1\n\n, where \u2018[ ]\u2019 stands for the integer part. The value of n corresponds to the amount of data in a 10\u00a0min interval. The value of m\u00a0=\u00a02560 corresponds to the number of measurements in a 10\u00a0s sub-interval and p corresponds to a 5\u00a0s shift interval, which means that consecutive FFTs have and overlap of 5\u00a0s.\nAs mentioned above, we use the Hann's window with m samples that is defined by:\n\n(4)\n\n\nh\n\n(\ni\n)\n\n=\n1\n\u2212\n\n\ncos\n\n\n2\n\n\n(\n\n\u03c0\n\n(\ni\n\u2212\n1\n)\n\n\/\nm\n\n)\n\n\n\n\nfor \n\ni\n=\n1\n,\n\u2026\n,\nm\n\n, with the normalizing factor \n\n\n\n\n\u2211\n\ni\n=\n1\n\nm\n\n\nh\n\n\n(\ni\n)\n\n2\n\n\/\nm\n\n\n\n\n. The m data window is expanded with zeros to 1023 samples. This new window size and the sampling frequency define a frequency increment of 31.25\u00a0mHz.\nThe FFT is done with the Numpy subroutine 'fft.rfft' that takes advantage of the real character of the data. The Hann window and Welch method have been easily implemented using Numpy arrays.\nIn the analysis of each 10\u00a0min interval through the FFT of the 10\u00a0s sub-intervals, each 10\u00a0s time span including saturated signals is eliminated and not considered in the average for its corresponding 10\u00a0min interval. According to the number of 10\u00a0s time portions eliminated, the percentage of saturation for each 10\u00a0min span is defined. This saturation level may be indicative of the presence of transient sources in the ELF band. Due to the large size of the Earth's electromagnetic cavity, it is likely that its influence in global quantities is not relevant but may be indicative and interesting to detect local phenomena. The study of the possible correlation of this percentage of saturation with global or local variations of the different parameters associated with the SRs, amplitudes, resonance frequencies and resonance widths is a pending task which is beyond the scope of this work.\nThe following directory tree is considered in this program. The input files for this code contain the raw data measured at the station, which are stored in a folder named \u2018S_N_Data\u2019 (Sierra Nevada Data). The output files are located in the folder named \u2018S_N_FD\u2019 (Sierra Nevada, Frequency Domain) which contains another sub-folder with the month under study in a scheme which resembles that of the \u2018S_N_Data\u2019 folder. As previously mentioned, the \u2018Reading_Fourier.py\u2019 code generates two output files: one stores the spectrum and the other includes the percentage of saturation for each 10\u00a0min interval in that specific month. There are independent files for each magnetometer. The output file is organized as one column data using ASCII format. As regards the spectrum, the file includes an amplitude value for each frequency between 6\u00a0Hz and 25\u00a0Hz with a frequency resolution of 31.25\u00a0mHz, while there is a unique value of the saturation level for each 10\u00a0min interval.\n\nFig. 1\n shows an example of the raw data measured by one of the sensors, at 256 samples\/s, which is the input for program \u2018Reading_Fourier.py\u2019. The ordinate axis represents volts at the magnetometers due to the fact that the calibration function Sc(f) required to translate to magnetic field values is defined in the frequency domain and the data are shown in time domain. Fig. 2\n shows an example of the amplitude spectra for the two sensors obtained as output of the program \u2018Reading_Fourier.py\u2019 for a specific 10\u00a0min interval spectrum, such as that shown in Fig. 1. The three SR peaks can be clearly distinguished at roughly 8, 14, and 20\u00a0Hz. In addition, there are fluctuations of the spectra due to the low energy of the signal (below 0.4 pT\/ \n\n\nH\n\nz\n\n for the whole 6\u00a0Hz\u201325\u00a0Hz band). As stated before, the electromagnetic field energy that can travel few turns around the cavity is very small, due to the losses caused by the conductivity of the Earth's atmosphere.\n\n\n4\nSignal processing step 2: elimination of the anthropogenic noise\nDespite the site to set the Sierra Nevada ELF station was carefully chosen to avoid human activity, an increase of this activity occurred in the vicinity of the station soon after its deployment, and, since then, the appearance of anthropogenic signals could be detected in the measurements at certain hours. This noise is not observed in the example of Fig. 2 but in other 10\u00a0min intervals, such as in the example shown in the top of Fig. 3\n, the amplitude spectrum of the signal includes clear narrow peaks with an amplitude much higher than that of the natural signal. These peaks are fundamentally observed in three bands, (14.7\u00a0Hz, 15.00\u00a0Hz), (15.00\u00a0Hz, 15.48\u00a0Hz), and (16.45\u00a0Hz, 16.90\u00a0Hz), which have an anthropogenic origin. The first two peaks are closely located and appear while a diesel generator that supplies electricity to a mountain hut near the station is running. The third peak contains the operating frequency of the Central European train network. The appearance of this anthropogenic noise does not follow a fixed hourly pattern, so its elimination requires a detection process to avoid filtering in time intervals for which there is no noise. Furthermore, the presence of noise in the different bands is neither simultaneous nor permanent. Thus, there are some 10\u00a0min intervals for which the three peaks can be observed, only one or two peaks are present in another intervals, while other periods do not present any anthropogenic signal at all. Due to the proximity of the two lower bands, the combined frequency band (14.55\u00a0Hz, 15.35\u00a0Hz) has been considered to deal with the anthropogenic noise originated by the diesel generator of the hut.\nIf an anthropogenic signal exists in the corresponding noisy band, a straight line is fitted between these limits. That is to say, for a set of n noisy measurements of the magnetic field in this band, B\n\nN\n (i), at a frequency f\n\nN\n (i), where i\u00a0=\u00a01, \u2026, n, with i\u00a0=\u00a01 and i\u00a0=\u00a0n define de lower and upper limits of the band, respectively, the filtering process consists of substituting the actual measurements, B\n\nN\n (i), by the filtered values, B\n\nFN\n (i) corresponding to f\n\nN\n (i) for the straight line connecting the noisy band limits, f\n\nN\n (1) and f\n\nN\n (n):\n\n(5)\n\n\n\nB\n\nF\nN\n\n\n\n(\ni\n)\n\n=\n\n\n\nB\nN\n\n\n(\nn\n)\n\n\u2212\n\nB\n\nN\ni\n\n\n\n(\n1\n)\n\n\n\n\nf\nN\n\n\n(\nn\n)\n\n\u2212\n\nf\nN\n\n\n(\n1\n)\n\n\n\n\n(\n\n\nf\nN\n\n\n(\ni\n)\n\n\u2212\n\nf\nN\n\n\n(\n1\n)\n\n\n)\n\n+\n\nB\nN\n\n\n(\n1\n)\n\n.\n\n\n\n\n\nThe problem raised is determining if the anthropogenic noise is present or not to avoid filtering a signal unaffected by this type of noise. To this aim, a neighbor band at which anthropogenic noise has never been detected is used as reference. At this band, the set of reference measured magnetic field, B\n\nR\n(i), at the reference frequencies, f\n\nR\n (i), i\u00a0=\u00a01, \u2026, n, helps in detecting the anthropogenic noise presence. To do so, interpolated values B\n\nFR\n(i) at the reference frequencies for the straight line connecting the reference band limits are calculated through the expression:\n\n(6)\n\n\n\nB\n\nF\nR\n\n\n\n(\ni\n)\n\n=\n\n\n\nB\nR\n\n\n(\nn\n)\n\n\u2212\n\nB\nR\n\n\n(\n1\n)\n\n\n\n\nf\nR\n\n\n(\nn\n)\n\n\u2212\n\nf\nR\n\n\n(\n1\n)\n\n\n\n\n(\n\n\nf\nR\n\n\n(\ni\n)\n\n\u2212\n\nf\nR\n\n\n(\n1\n)\n\n\n)\n\n+\n\nB\nR\n\n\n(\n1\n)\n\n.\n\n\n\n\n\nThen, quantities d\n\nN\n and d\n\nR\n to compare interpolated and original values are defined for both the anthropogenic noise band and the reference one in the following manner:\n\n(7)\n\n\n\nd\n\nN\n,\nR\n\n\n=\nMax\n\n{\n\n\n\nB\n\nN\n,\nR\n\n\n\n(\ni\n)\n\n\u2212\n\nB\n\nF\nN\n,\nF\nR\n\n\n\n(\ni\n)\n\n\n\n\nB\n\nF\nN\n,\nF\nR\n\n\n\n(\ni\n)\n\n\n\n}\n\n+\nMin\n\n{\n\n\n\nB\n\nN\n,\nR\n\n\n\n(\ni\n)\n\n\u2212\n\nB\n\nF\nN\n,\nF\nR\n\n\n\n(\ni\n)\n\n\n\n\nB\n\nF\nN\n,\nF\nR\n\n\n\n(\ni\n)\n\n\n\n}\n\n,\n\n\n\nwhere the N and R subscripts refers to the noise and the reference bands, respectively. The way in which d\n\nN\n and d\n\nR\n allow detecting the presence of anthropogenic noise is as follows. In the absence of anthropogenic noise in the considered band and due to the fact that the ELF signal detected at the station includes a natural noise component, measurements will be randomly distributed above and below the straight line and the sum of the positive maximum and negative minimum terms in (7) tend to compensate. This also applies in the reference band, which is defined close to the band with anthropogenic noise to preserve a similar behavior. In contrast, when anthropogenic noise is present, the maximum positive term in (7) will be considerable higher and will not be cancelled by the minimum negative value which will not be substantially affected. For this reason, detecting a value of d\n\nN\n clearly higher than that of d\n\nR\n indicates the presence of anthropogenic noise and the noise filter must be applied, and the actual measurements B\n\nN\n(i) are substituted by the filtered ones, B\n\nFN\n(i) at each frequency measured.\nMore detailed explanations are given in 'Antropo_Docu.ipynb'. The process is automatic, but the singularity of the noise advises including the possibility of a second and direct correction if the filtered signal is not as good as expected after this first automatic filtering process. To this end, the notebook allows carrying out a visual inspection of the signal to apply a second-stage filtering. To perform this task, the \u2018Interact\u2019 package within the Python module \u2018ipywidgets\u2019\n5\n\n\n5\n\nhttps:\/\/pypi.org\/project\/ipywidgets\/.\n is used.\n\nFig. 3 shows an example of the anthropogenic noise removal. The top panel shows the output of the program 'Reading_Fourier.py' for a 10\u00a0min period time with anthropogenic noise. Three large spikes can be clearly observed, two around 15\u00a0Hz and one around 16.5\u00a0Hz. The bottom panel shows the same spectra after being processed with the program 'Antropo.py'. The anthropogenic noise has been removed. As a final result of this step, we have a file with the signal amplitude spectrum for each sensor and month, which is free of anthropogenic noise within the calibrated band of the magnetometers. In Fig. 2 and for the EW sensor, a somehow regular spike sequence can be also observed but their amplitude is comparable to that of the measured signal. These peaks are a result of the noisy nature of the ELF signal received and they do not require the application of a filtering process as it is the case of the anthropogenic peaks of much higher amplitude observed at the top of Fig. 3.\nThe input files for \u2018Antropo.py\u2019 program are the output files of \u2018Reading_Fourier.py\u2019 program, i.e., the amplitude spectrum for each 10\u00a0min interval of the month under study and its percentage of saturation. The output of \u2018Antropo.py\u2019 is a file containing the spectrum without anthropogenic noise but maintaining the structure of the input file, that is to say, ASCII format and one column. In case this filtering stage is not required, it will suffice changing the name of the output file of program \u2018Reading_Fourier.py\u2019 or, alternatively, to change the name of the input file of program \u2018Lorenzt_Lmfit.py\u2019 of the next step.\n\n\n5\nSignal processing step 3: curve fitting using Lorentzian functions\nOnce the two previous steps have been applied, the amplitude spectrum of the horizontal magnetic field components, free of anthropogenic noise, is available for each 10\u00a0min interval. Unfortunately, this spectrum shows important fluctuations due to its low amplitude with respect to the amplitude of the environmental and instrumental noises, as it can be seen in Figs. 2 and 3. The bandwidth of the magnetometers at the Sierra Nevada station allows the measurement of the first three SRs. Each mode is quantitatively defined through three parameters: amplitude or local maximum value of the spectrum, frequency at which this maximum is observed, and bandwidth of each resonance. These parameters are difficult to define in the noisy spectra of stages 1 and 2. To overcome this difficulty, we can adjust the noisy spectrum to an analytical function that allows defining them with the least possible ambiguity. The resonant behavior expected is in the origin of the Lorentzian fit first proposed in (Sentman, 1987). Since then, a fit combining Lorentzian functions, plus a straight line to allow taking into account a possible slope in the spectrum, is often used by other authors (Mushtak and Williams, 2009; Nickolaenko and Hayakawa, 2002, and references therein). The comparison of Lorentzian fits with Gaussian ones is considered in (Rodr\u00edguez-Camacho et al., 2018), which confirms lower errors between the adjusted and experimental data for the case of the Lorentzian fit.\nTherefore, the goal of this part of the process is to fit the noisy spectrum provided by step 2 for each 10\u00a0min interval using a Lorentzian function for each peak in the frequency interval within the calibrated bandwidth of the magnetometers. The code and notebook associated with this step is \u2018Lorentz_Lmfit.py\u2019 and 'Lorentz_Lmfit_docu.ipynb' respectively.\nThe Lorentzian fit for adjusting the signal corresponding to three modes used in this stage consists in minimizing the mean square error between the experimental spectrum and the function defined by:\n\n(8)\n\n\nL\n\n(\nf\n)\n\n=\n\n\u2211\n\ni\n=\n1\n\n3\n\n\n\nA\ni\n\n\n1\n+\n\n\n(\n\n\nf\n\u2212\n\nf\ni\n\n\n\n\u03c3\ni\n\n\n)\n\n2\n\n\n\n+\nB\nf\n+\nC\n,\n\n\n\nwhere A\n\ni\n stands for the amplitude of the i-th mode, resonating at frequency f\n\ni\n and with a bandwidth of \u03c3\n\ni\n, while parameters B and C correspond to an added linear adjusting term in the frequency spectrum.\nIn addition to these 11 parameters, we can define a global mode amplitude, P\n\ni, as the value of the fitting function at the resonance frequencies, f\n\ni\n. Namely:\n\n(9)\n\n\n\nP\ni\n\n=\nL\n\n(\n\nf\ni\n\n)\n\n.\n\n\n\n\n\nThese values describe the amplitude of each resonance peak and takes into account the simultaneous effect of the three Lorentzian terms combined with the correcting effect of the linear term.\nThe procedure therefore consists in minimizing a function \n\n\n\u03c7\n2\n\n\n that accounts for the difference between the data and the model by varying the parameters of the fitting:\n\n(10)\n\n\n\n\u03c7\n2\n\n=\n\n\u2211\n\ni\n=\n1\n\nN\n\n\n\n(\n\nF\n\n(\n\nf\ni\n\n)\n\n\u2212\nL\n\n(\n\nf\ni\n\n)\n\n\n)\n\n2\n\n,\n\n\n\nwhere \n\nF\n\n(\n\nf\ni\n\n)\n\n\n is the value of the measured spectrum for the frequency f\n\ni\n, L is the fitting function, namely, the sum of Lorentzian functions and a linear term, also for the frequency f\n\ni\n, and N is the total number of samples of the spectrum to be analyzed. Therefore, the function \n\n\n\u03c7\n2\n\n\n is a scalar function that depends on 11 parameters and the optimization problem results in finding the vector with the values of the 11 parameters for which function \n\n\n\u03c7\n2\n\n\n has a minimum value.\nIn the developed code, we use the packages \u2018numpy\u2019, \u2018lmfit.minimize\u2019, and \u2018lmfit.Parameters\u2019. The module \u2018lmfit\u2019\n6\n\n\n6\n\nhttps:\/\/lmfit.github.io\/lmfit-py\/model.html.\n is not installed by default within Python. This module provides a high-level interface for performing non-linear adjustments in Python. It is an extension of \u2018scipy.optimize\u2019, the first optimization module developed for this language. We chose the Levenberg-Marquardt fit algorithm, which provides good results with less computing demands than other methods (Rodr\u00edguez-Camacho et al., 2018).\nIn order to carry out the fit, the method needs the estimation of a seed, or initial value, for each one of the 11 parameters. The seeds are chosen as follows for each 10\u00a0min interval:\n\n1.\nWe start from preset values for the central frequencies, width of each of the first three resonances and the linear term. This choice is based on averaged measurements of these quantities. Details can be found in (Rodr\u00edguez-Camacho et al. (2018).\n\n\n2.\nThe initial guesses for peak amplitudes are determined by averaging the month of data and taking the average amplitude at the predefined central frequency. This choice of initial amplitudes is a simplification of the process described in (Rodr\u00edguez-Camacho et al., 2018), but providing similar results.\n\n\n\nThe code executes the following steps:\n\n1.\nReading of the output spectra of the 'Anthopo.py' program of block 2, i.e., the calibrated frequency band for each 10\u00a0min interval.\n\n\n2.\nDefinition of the spectrum for the range of fitting frequency band that is contained in the range of calibrated frequency interval. The bandwidth (6.35\u00a0Hz\u00a0\u2212\u00a023.75\u00a0Hz) has been chosen because it produces a lower number of parameters outside the acceptable range described in Table\u00a01 in (Rodr\u00edguez-Camacho et al., 2018) when compared to other possible frequency regions defined inside the calibrated bandwidth of the station magnetometers.\n\n\n3.\nCall to \u2018lmfit\u2019 function and determination of 11 fitting parameters for each 10\u00a0min interval of the month.\n\n\n4.\nEvaluation of the three global mode amplitude.\n\n\n5.\nOutput of the results obtained.\n\n\n\nThe outputs of program 'Lorentz_Lmfit.py' for each 10\u00a0min interval of the month and sensor are:\n\n1.\nA list of 14 fitting parameters.\n\n\n2.\nA list of the 11 errors associated with the Lorentzian fitting.\n\n\n3.\nA list of the mean square errors (\n\n\n\u03c7\n2\n\n\n function) of the fitting.\n\n\n\nThe error parameters are outputs of program Lmfit. The study of the correlation of these errors with variations of the parameters associated with the SRs is still pending.\n\nFig. 4\n shows the results of fitting an amplitude spectrum using three Lorentzian fits. The signal in red line shows the anthropogenic noise filtered spectrum obtained after applying the program 'Anthropogenic.py' at stage two. This signal is the input for the program 'Lorentz_Lmfit.py' of stage three, which provides the fit shown with blue-dashed line.\n\n7.\nStep 4: Data packaging\n\n\n\nIn the final part of the process, a file is generated in numpy format (npz\n7\n\n\n7\n\nhttps:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.savez.html.\n) with all the information for the month. This is the file that could be shared between ELF stations, containing all the information necessary for the study of the SRs for each month. Information about the beginning in universal time of each 10\u00a0min interval of the month is also incorporated.\nFor each 10\u00a0min interval, the following readings from the previous steps are done:\n\n\u2022\nPercentage of saturation.\n\n\n\u2022\nAmplitude spectrum.\n\n\n\u2022\nParameters of the Lorentzian fitting.\n\n\n\u2022\nValue of the \n\n\n\u03c7\n2\n\n\n function.\n\n\n\u2022\nValue of the error in the fitting for each parameter.\n\n\n\nIn addition, six new parameters associated with the Lorentzian fitting are obtained from the fitted spectra. They correspond to the maximum amplitude values for each mode and corresponding frequencies for each resonance. The maximum values of this fitted function, the local maximum amplitudes, and their corresponding frequencies, local maximum frequencies, also could define the SR amplitudes and frequencies, respectively. This calculation has been included in this part in order to make it optional with respect to the Lorentzian fitting made in the previous step.\nThe output file has npz format for writing\/reading data. The file is in binary format, but it maintains the numpy array structure so that its reading and the recovery of the structure of each array is immediate. This makes that the size of each file is considerably reduced and, in addition, that no specific program is required to relate data with each output array. As a drawback, the reading depends on Python language and the Numpy package.\nThere is a file for each month of measurements and for each sensor. The filename makes reference to the year, month and sensor used. The name for December 2014 and sensor NS is SN_1412_0.npz, for instance. Each file has a size of 43.3\u00a0MB, therefore, all the months can be stored in the same directory in order to facilitate the reading for a later analysis.\nThe list of saved arrays is:\n\n\u2022\n\u2018sat\u2019: saturation percentage,\n\n\n\u2022\n\u2018fre\u2019: list of frequencies in the calibrated band,\n\n\n\u2022\n\u2018freajus\u2019: list of frequencies in the adjustment band,\n\n\n\u2022\n\u2018rs\u2019: spectra obtained from the measurements,\n\n\n\u2022\nrs_LO: spectra obtained from the fitting process,\n\n\n\u2022\nPARLO: parameters obtained from the adjustment (peaks, amplitudes, frequencies, widths, slope, and ordinate at the origin of the linear term),\n\n\n\u2022\nparLOC: \n\n\n\u03c7\n2\n\n\n function,\n\n\n\u2022\nparLOE: error in the parameters,\n\n\n\u2022\npr: maximum values for each resonance,\n\n\n\u2022\ntime: date and time of the beginning of each 10\u00a0min interval of the month.\n\n\n\n\n\n6\nConclusions\nMeasuring the natural ELF electromagnetic field in the atmosphere is a complicated task. The ELF stations around the Earth are scarce and the techniques to extract the SRs from them are also complex. Sharing these measurements, procedures and codes may help to spread the findings of a particular research group to the rest of scientific community and to stablish a global network to study SRs and their applications.\nIn this work, we present the algorithms that allow characterizing each SR mode starting from the raw data measured by the ELF station at Sierra Nevada, Spain, recorded between March 2013 and February 2017. This software is written in Python language (version 3.8), because it allows easy access to the different stages of the procedure and, therefore, it facilitates obtaining intermediate results as well as including modifications, adaptations and even improvements of the method. A detailed and comprehensive description of the code is given through several Jupyter notebooks that are provided as supplementary material. In addition, the time raw data series measured by the Sierra Nevada station during a four year period are provided in this paper as well as the outputs of the different parts of the procedure.\nThe procedure starts from the time series raw data measured by the ELF station and ends with the obtention of three parameters to quantitatively describe each resonance: peak amplitude, central frequency and resonance width. The code is divided into four stages, which can be run independently in order to optimize the results of each stage without affecting the other. Each block can be run on a personal computer requiring a CPU time of a few minutes.\nThe programs and basic tasks carried out in each stage are the following:\n\n1.\nFirst, 'Reading_Fourier' reads the raw data obtained by the stations and generates an initial spectrum for each 10\u00a0min period of each month.\n\n\n2.\nThe program 'Anthropo.py' is applied to filter the human noise present in some of the spectra obtained at the first stage.\n\n\n3.\nIn the third stage, a Lorentianz fit is obtained through the program 'Lorentz_Lmfit'.\n\n\n4.\nFinally, the last program, 'Packaging.py', generates an output file in 'npz' format with all the results.\n\n\n\n\n\nAuthorship contribution statement\nAll authors conceived the idea, provided software supervision and participated in essential manuscript reviews and editing. The code has been developed mainly by A. Salinas and J. Rodr\u00edguez- Camacho. The original manuscript has been written by A. Salinas and J. Port\u00ed. S. Toledo-Redondo, J. Fornieles-Callej\u00f3n and M.C. Carri\u00f3n provided project resources and contributed to data curation and analysis.\n\n\nCode availability section\nName of the code\/library: Sierra_Nevada_SR\nContact: asalinas@ugr.es; +34 958242312.\nHardware requirements: Laptop or PC running Linux, Mac or Windows operating systems.\nProgram language: Python (version 3.8).\nSoftware required: Python and Jupyter notebooks.\nProgram size: 27\u00a0kB (total for the four programs).\nThe source codes, Jupyter notebooks and one day of data (March 1, 2015) can be found at: https:\/\/github.com\/asalinas62\/Sierra_Nevada_SR.\nThe Sierra Nevada ELF raw data station from the period March 2013 to February 2017 can be downloaded from the Zenodo repositories [Salinas et al. a, b, c, d].\nThe final proceeded data from the previous raw data using the program \u2018Packaging.py\u2019 in npz format can be download from de University of Granada repository http:\/\/hdl.handle.net\/10481\/71563\n.\n\nThe raw data, intermediate proceeded and final data of the December 2014 and March 2015 can be download from the University of Granada repositories http:\/\/hdl.handle.net\/10481\/73978\nand\nhttp:\/\/hdl.handle.net\/10481\/73977 respectively.\n\n","42":"","43":"","44":"","45":"","46":"","47":"","48":"","49":"","50":"","51":"","52":"","53":"","54":"","55":"","56":"\n\n1\nIntroduction\nWell to seismic tying is a process of correlating the information obtained from well-logs to observed seismic data. Well ties are performed at various stages of the exploration phase: In seismic processing, where they help determine the amount of phase shift required to zero-phase the seismic data. The extracted wavelets\u2019 amplitude at various incidence angles can also be analysed to perform a proper offset-scaling of the dataset. In horizon interpretation, where stratigraphic markers from wells are matched to seismic reflectors. Finally, in seismic inversion, where the extracted wavelets are used together with the reflection coefficients to produce synthetic seismic, which is then compared against the measured seismic to optimize the elastic properties of the model. A wavelet with a reliable phase and amplitude estimation is often-times crucial for most seismic inversion approaches.\n\nSimm et al. (2014) and\u00a0White and Simm (2003) provide tutorials for classical well-ties and\u00a0Zabihi\u00a0Naeini et al. (2017) presents well-tie approaches on broadband seismic data. The initial step of well tying is quality control of the data\u00a0(White and Tianyue, 1998; White and Simm, 2003; Herrera et al., 2014; Gelpi et al., 2020). It is crucial to check the quality of the logs used for the generation of the synthetic seismograms, as any non-geological spikes in velocity or density may lead to erroneous events or affect the amplitudes of geologically induced events in the synthetic seismic\u00a0(Anderson and Newrick, 2008). Quality checking the seismic is also important to gauge the expected quality of the tie. If the seismic is perturbed by residual multiples or has a low signal-to-noise ratio, the synthetic seismic cannot be expected to exhibit a good match with the real data. Next, it is necessary to convert the well log information from measured depth into travel-time, as the seismic data is recorded in time. The ideal data for that are checkshots, vertical seismic profiles (VSP) data or integrated compressional velocity logs\u00a0(Simm et al., 2014; White and Simm, 2003). Following, one calculates the reflectivity coefficients using velocity and density information measured in the logs. Typically, one starts with coarsening the logs sample rate and averaging the values in the new coarser grid either using Backus averaging or time-average approaches\u00a0(White and Simm, 2003). This is followed by computing the time dependent reflectivity from contrasts in impedance using Zoeppritz equations. The main fundament of synthetic modelling is the convolutional model, which states that the seismic trace can be modelled by convolving the seismic pulse (a wavelet) with a reflectivity series\u00a0(Goupillaud, 1961). Computing synthetic seismograms with this model relies on specific assumptions such as wavelet stationarity, layer homogeneity or the multiple-free nature of the seismic\u00a0(Goupillaud, 1961). As summarized by\u00a0Anderson and Newrick (2008), the selected wavelet has a pronounced impact on the resolution and appearance of the synthetic seismogram. The simplest way to extract a seismic wavelet is to use the autocorrelation of the seismic traces to produce a zero-phase statistical wavelet. This wavelet can later be phase-rotated to obtain a satisfactory tie. This approach can be very effective for horizon matching, if the phase-characteristics of the seismic data is fairly linear across the seismic bandwidth\u00a0(White and Simm, 2003). A widely used approach for deterministic wavelet extraction is the approach by\u00a0Walden and White (1998), which extracts the wavelet from the data through a least-squares technique and treats the matching as a noisy input\u2013output problem. This approach allows for the estimation of both the frequency and phase-characteristics of the wavelet, as well as the amplitude scaling factor.\nMost of the well-tie pitfalls relate to the phase-timing ambiguity problem, which means that the data can be tied comparably well with wavelets that have different phase and timing. Moreover, for cost-saving reasons, velocity and density are often logged only around the formation of interest, especially for production wells. This often results in insufficient length of logged intervals for a stable deterministic wavelet extraction, especially at lower frequencies\u00a0(White and Simm, 2003; Zabihi\u00a0Naeini et al., 2017). In this scenario, the choice of the size and position of the wavelet extraction window can play a big role in the ability to extract a reliable wavelet. Another common pitfall is the manipulation of the time-depth relation. It is common to create a synthetic seismogram using a zero-phase statistical wavelet and perform a bulk-shift of the well logs to match the main reflectors in order to mitigate timing inaccuracies possibly originating from inaccurate datum, differences in frequency content, differences in wave propagation effects (anisotropy and attenuation) and migration effects\u00a0(Simm et al., 2014; Anderson and Newrick, 2008). Often-times, a bulk-shift alone is not sufficient to obtain a satisfactory fit, especially in cases where the wave propagation effects are located within the wavelet extraction window and are very local in nature, or in case of structural imaging issues. In this case, some interpreters perform stretching and squeezing of the synthetics to better match the real seismic.\nAs highlighted, manual well-tying is highly interpretative, involves many degrees of freedom and is prone to human errors or bias\u00a0(Gelpi et al., 2020; Mu\u00f1oz and Hale, 2012; White and Simm, 2003). To help alleviate those issues, approaches which automate some of the sub-procedures have been introduced. The majority of the automated well-tie approaches published so far primarily focus on determining the time-depth relation. Main contributions in this field are documented by\u00a0Herrera and van\u00a0der Baan (2012) and Herrera et al. (2014) who apply constrained dynamic time warping (DTW) to stretch and squeeze the synthetic seismic.\u00a0Mu\u00f1oz and Hale (2012) extended the DTW approach to a multi-well case.\u00a0Gelpi et al. (2020) propose an approach to automatically tie the synthetic seismogram to the real seismic, while preserving the waveform and simultaneously determine the wavelet phase. The method iteratively perturbs the velocity log using a perturbation function built by monotonic cubic spline interpolator with a predefined number of knots. The wavelet phase is perturbed simultaneously with the velocity log to maximize the correlation between the updated trace and the real seismic.\nIn recent years, machine learning has been increasingly employed in exploration workflows, to facilitate seismic and well data interpretation, seismic processing and reservoir characterization. In seismic interpretation,\u00a0Wu et al. (2019) and An et al. (2021) employ a neural network to automatically interpret faults,\u00a0Tschannen et al. (2020) present the use of a 3D convolutional neural network (CNN) for the task of horizon picking.\u00a0Pires de Lima et al. (2020), Dunham et al. (2020) and Ao et al. (2020) show applications of machine learning for well log and well core interpretation. In seismic processing,\u00a0Li et al. (2021) use a dilated CNN to remove noise from land seismic data,\u00a0Greiner et al. (2022) employ deep learning for seismic data reconstruction.\u00a0Liu and Grana (2019), Saikia et al. (2020), Kim and Nakata (2018) and\u00a0Grana et al. (2020) demonstrate the use of machine and deep learning to various inversion and reservoir characterization approaches.\nIn this work, we propose a novel machine learning based workflow to automate the well tie process. Our goal is to automate the entire tying process and provide uncertainty ranges for the predictions to enable users to quantify the confidence in the predicted wavelet. Additionally, we expand the method to tackle the multi-well tie and the prestack well-tie tasks. Our approach is based on two main technologies. First, we solve the wavelet extraction problem by training a deep neural network. Our network can handle arbitrary length input data and provides uncertainties in the spectrum of the predicted wavelet. Second, we resort to Bayesian optimization to tune the various key hyper-parameters, such as the optimal time-depth bulk shift. With this approach, the interpreter has control over the optimization space and can explicitly constrain the search in order to yield interpretable and trustworthy results. We showcase our method on two challenging real data applications, a joint tie between two wells of the same field and a prestack well-tie.\n\n\n2\nMethodology\n\n2.1\nWell tie approach\nIn this section we describe the recipe that we follow in order to tie a well to seismic, and emphasis on the various hyper-parameters that need tuning. Our particular sequence of operations is similar to the one described in\u00a0Simm et al. (2014). The input ingredients are the well trajectory, a time to depth relation table, the well logs (acoustic velocity, bulk density as well as the shear velocity for the prestack tie) and the seismic extracted along the well path. The main steps that need to be performed can be summarized as:\n\n\n\n1.\nConvert the logs from the measured depth domain (MD) to the two-way time domain (TWT) using the well trajectory and the time-depth table.\n\n\n2.\nCompute the reflectivity.\n\n\n3.\nExtract a wavelet.\n\n\n4.\nCompute a synthetic seismogram.\n\n\n5.\nEvaluate the similarity between the synthetic and the real seismic.\n\n\n\nWe use Zoeppritz equations to compute the reflectivity and choose the convolutional model to obtain the synthetic seismic. For the similarity score, we use the zero-lag cross-correlation coefficient between the real and synthetic seismic. It is possible to resort to different modelling methods and similarity measures without affecting the overall workflow. The precise sequence of operations with the hyper-parameters that need to be adjusted are described in Algorithm 1. The stopping criterion requires a threshold for the similarity. However in practice, the similarity is strongly dependent on the data quality and we cannot recommend a single value. In place, we set a maximum number of iterations. We use 30 in this work as it is a good compromise between computing time and parameter space exploration. \n\n\n\n\n\n\n\n2.2\nWavelet extraction with deep learning\nIn this section we consider the wavelet extraction independently from the rest of the tie process. Following the convolutional model, we assume that the seismic trace results from the convolution between a source wavelet and the reflection coefficient series\u00a0(Goupillaud, 1961). Given that both the seismic and the reflection series are known, the goal is to solve the inverse problem to estimate the shape of the wavelet. Since both the seismic and the logs are noisy and the source wavelet has a limited frequency support, the deconvolution can be unstable\u00a0(Wang, 2016).\nAt this stage, we consider that the timing between the seismic and the reflection series is correct, and we train a neural network to estimate the wavelet. For a seismic trace \n\ns\n\n(\nt\n)\n\n\n, a reflection coefficient series \n\nr\n\n(\nt\n)\n\n\n, a wavelet \n\nw\n\n(\nt\n)\n\n\n and random white noise \nn\n, all functions of time \nt\n, we have the following relationship: \n\n(1)\n\n\ns\n\n(\nt\n)\n\n=\nw\n\n(\nt\n)\n\n\u2217\nr\n\n(\nt\n)\n\n+\nn\n,\n\n\n\nwhere \n\u2217\n is the convolution operation. We design a neural network \n\n\nf\n\n\n\u03b8\n\n\n and train its parameters \n\u03b8\n such that: \n\n(2)\n\n\n\n\nf\n\n\n\u03b8\n\n\n\n(\ns\n\n(\nt\n)\n\n,\nr\n\n(\nt\n)\n\n)\n\n=\n\n\nw\n\n\n\u02c6\n\n\n\n(\nt\n)\n\n\u2248\nw\n\n(\nt\n)\n\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOur architecture is a convolutional variational auto-encoder\u00a0(LeCun et al., 1998; Kingma and Welling, 2014) shown in Fig.\u00a01. The first part of the network is a 1-dimensional convolutional neural network (CNN) that takes as an input the 1D seismic and the 1D reflection series concatenated along a second dimension referred to as the channels. After a sequence of convolutional and pooling layers, the data passes through a global average pooling (GAP)\u00a0(Gao et al., 2011). This layer takes the mean value along the time dimension and outputs a fixed length vector with a size equal to the number of filters of the last layer which is chosen as a hyper-parameter. As a consequence, the network supports arbitrary input shapes and can function with traces of variable lengths. In a second step, the data is sent to a variational decoder. The decoder is a standard multi-layer perceptron composed of a chain of fully connected layers\u00a0(Orbach, 1962). However, rather than having a deterministic link between the encoder and the decoder, we select a variational formulation that imposes a stochastic regularization of the model\u00a0(Kingma and Welling, 2014). In this setting, the network learns to map the input data to a zero-centred Gaussian distribution with unit diagonal variance in the latent space. To compute the output, the network samples a vector from the distribution and sends it to the decoder. In practice, using two fully connected layers, we can compute a mean vector \n\u03bc\n and a standard deviation vector \n\u03c3\n and the sampling of a vector \nz\n is obtained with the re-parametrization trick \n\nz\n=\n\u03b5\n\u2217\n\u03c3\n+\n\u03bc\n\n, where \n\u03b5\n is a noise vector sampled from the unit Gaussian. In this way, the sampling is compatible with the back-propagation algorithm used to train neural networks\u00a0(Werbos, 1974). In addition to having a regularization effect which is potentially beneficial to reduce over fitting, we also show in the following that the variational setting is useful to account for the uncertainties of the deconvolution process. In particular, this formulation helps dealing with the ambiguity in the exact phase of the wavelet. The final output is a vector of fixed length. We describe the exact architecture in Table\u00a01.\n\nThe network is trained with supervised learning. It is common practice to perform numerical modelling to create training data without the need to resort to a tedious manual preparation\u00a0(Wu et al., 2019; Tschannen et al., 2019). We generate 150 000 synthetic training samples. Each sample is composed of a random source wavelet, a random reflectivity, a random noise vector and the corresponding seismic trace computed with Eq.\u00a0(1). As base wavelets, we use both the Butterworth and the Ormsby filters\u00a0(Ryan et al., 1994). To incorporate diversity, for each realization, we randomly select a different frequency support and perform random constant and per frequency phase rotations. The reflectivity series are generated from random uniform and random correlated distributions\u00a0(Perlin, 2002) with varying signal-to-noise ratio and variation scales. With the random correlated distribution we aim at taking into account the fact that real geological series are themselves correlated. The noise vectors are drawn from a centred Gaussian distribution with varying standard deviation. We show some instances of the dataset in Fig.\u00a02. Exact details about the parameters used in the data generation can be found in the open source code published with this manuscript. As mentioned earlier, the output of the network is constrained to have a fixed length. For this reason we choose the model wavelets to all be 300\u00a0ms long with a sampling of 2\u00a0ms. This choice is general enough to represent most of the wavelet shapes that we might encounter in field data. In the case of a broadband acquisition, a second network with a larger output size would need to be trained. Input seismic and reflectivity series may be of arbitrary length. We choose a length of 464\u00a0ms with a sampling interval of 2\u00a0ms for the training data. We normalize all inputs between \u22121 and 1.\nWe train the network for 270 epochs with a momentum stochastic gradient descent optimizer using an initial learning rate of 0.003 decreasing by 10% every 20 epochs. The wavelet loss measures the reconstruction error between the true wavelet and the one estimated by the network. We use the mean absolute error: \n\n(3)\n\n\nw\na\nv\ne\nl\ne\nt\n\nl\no\ns\ns\n=\n\n\n1\n\n\n\n\nN\n\n\nt\n\n\n\n\n\n\n\u2211\n\n\nt\n\n\n\n|\n\n\nw\n\n\n\u02c6\n\n\n\n(\nt\n)\n\n\u2212\nw\n\n(\nt\n)\n\n|\n\n\n\n\nwhere \n\n\nN\n\n\nt\n\n\n is the number of time samples. The variational loss measures the statistical distance between the reference unit Gaussian distribution and the latent distribution computed by the encoder. As derived in\u00a0Kingma and Welling (2014) this distance is expressed as the Kullback\u2013Leibler divergence with a closed form solution: \n\n(4)\n\n\nv\na\nr\ni\na\nt\ni\no\nn\na\nl\n\nl\no\ns\ns\n=\n\u2212\n\n\n1\n\n\n2\n\n\n\u2211\n\n\n1\n+\nl\no\ng\n\n(\n\n\n\u03c3\n\n\n2\n\n\n)\n\n\u2212\n\n\n\u03bc\n\n\n2\n\n\n\u2212\n\n\n\u03c3\n\n\n2\n\n\n\n\n,\n\n\n\nwhere the sum is across the latent dimension size.\nBoth losses work against each other as the variational term introduces some noise in the process and therefore increases the mean error in the wavelet reconstruction. To find a correct balance between both terms we introduce a scalar hyper-parameter \n\n0\n\u2264\n\u03b1\n\u2264\n1\n\n such that: \n\n(5)\n\n\nt\no\nt\na\nl\n\nl\no\ns\ns\n=\n\n(\n1\n\u2212\n\u03b1\n)\n\n\u2217\nw\na\nv\ne\nl\ne\nt\n\nl\no\ns\ns\n+\n\u03b1\n\u2217\nv\na\nr\ni\na\nt\ni\no\nn\na\nl\n\nl\no\ns\ns\n.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe initially set \n\u03b1\n to 0.05 and increase it until 0.5 with a rate of 10% every 20 epochs. Starting with a small value allows the network to first focus on properly estimating the wavelet. As \n\u03b1\n increases it progressively gives more importance to the regularization term. The training and validation losses are plotted in Fig.\u00a03 and we show an example result on blind test data in Fig.\u00a04. The wavelet plotted in Fig.\u00a04 corresponds to the expected value obtained by setting the vector \n\u03c3\n to 0 at inference time. Uncertainties in the phase and amplitude spectrum are obtained by sampling 50 realizations from the variational network and computing the standard deviation associated with the posterior wavelet distribution.\n\n\n2.3\nParameter tuning with Bayesian optimization\nWe described above the steps that need to be performed for the tie. In particular, we highlighted some of the parameters that geoscientists have to adjust in order to maximize the quality of the tie. We define the parameter space as the set of all parameters that need tuning. To automate the search, one can utilize global optimization algorithms designed to find the optimum point in a parameter space\u00a0(T\u00f6rn and Zilinskas, 1989). Given a performance measure, the algorithms search for the point that yields the best score. Such approach faces the trade-off between exploration and exploitation. Exploration techniques, such as random search or grid search, are performant to find the global maximum of a space but are computationally very expensive as the number of iterations required for real world applications is usually very large. On the other end, exploitation methods, such as gradient ascent, focus on regions of high performance near the initial position. They are computationally much cheaper but might fail to find the best solution and remain stuck in a sub-optimal region. Bayesian optimization is a popular approach that was designed to efficiently tackle this trade-off\u00a0(Mockus, 2012). Given initial observations (i.e.\u00a0samples from the parameter space), the method builds a surrogate model that estimates the performance and uncertainties over the entire parameter space. This process is similar to the so called Kriging method in geostatistics, where the prior covariance matrix computed from sparse observations of a spatially varying field, is used to estimate the posterior distribution of the field at every position\u00a0(Chil\u00e8s and Desassis, 2018). Based on its current estimation of the space, the algorithm then relies on an acquisition function to decide which parameters should be evaluated next. The acquisition function is designed to focus with some trade-off on areas where the expected performance is large and on areas where the uncertainties are large, hence alternating between exploitation and exploration\u00a0(Mockus, 2012). After the new observation, the covariance matrix is updated and the operation is repeated until some convergence criteria is met or until a maximum number of iterations is reached.\nThe initial observations can be selected using a Sobol sequence. This sequence provides a quasi-random sampling of the parameter space, enforcing a more even spread of the points compared to a uniform sampling. A popular choice to build the surrogate model is Gaussian process (GP) regression\u00a0(Rasmussen, 2003). The GP is parameterized by a mean vector and a covariance matrix computed over the observations and it is continuously defined over the entire parameter space. For a new parameter of unknown performance, one can compute the expected performance and uncertainties using the GP. The approximation of the true function becomes better as the number of observations used to compute the mean and covariance increases. In order to select the next observation, an acquisition function such as the upper confidence bound\u00a0(Srinivas et al., 2009) is computed over the entire parameter space. Each point \n\n\nx\n\n\n\u2217\n\n\n approximated by the GP can be considered as a normal random variable with mean \n\n\u03bc\n\n[\n\n\nx\n\n\n\u2217\n\n\n]\n\n\n and variance \n\n\u03c3\n\n[\n\n\nx\n\n\n\u2217\n\n\n]\n\n\n, and the upper confidence bound is equal to: \n\n(6)\n\n\nu\nc\nb\n\n[\n\n\nx\n\n\n\u2217\n\n\n]\n\n=\n\u03bc\n\n[\n\n\nx\n\n\n\u2217\n\n\n]\n\n+\n\u03b2\n\u03c3\n\n[\n\n\nx\n\n\n\u2217\n\n\n]\n\n,\n\n\n\nwhere \n\u03b2\n is a positive parameter balancing the two terms. The function will either favour regions of large expected improvements (exploitation) or large uncertainties (exploration).\nWhen the number of parameters is not too large (typically less than 15) Bayesian optimization is often observed to find better configurations than local optimizers at a computational cost inferior to a random search\u00a0(Hutter et al., 2011; Snoek et al., 2012). Since the covariance matrix grows quadratically with the number of observations, it is important to rely on an efficient implementation such as the one proposed by\u00a0Balandat et al. (2020) that takes advantage of graphical processing units (GPUs).\nFollowing Algorithm 1, we define a simple 4-dimensional parameter space for the while loop. The first two parameters are the well logs despiking window length, constrained between 22\u00a0ms and 126\u00a0ms, and the despiking threshold constrained between 0.1 and 5 standard deviations. In the case where the geoscientist assesses that the spikes might not correspond to real geological events, despiking is performed by a sliding median filter and aims at removing isolated and large values in the reflectivity that might strongly influence the wavelet extraction. The third parameter is the Gaussian smoothing kernel\u2019s standard deviation, constrained between 0.5 and 5. The final parameter is the time-to-depth bulk shift constrained between \n\u00b1\n12\u00a0ms. We chose the bounds of the parameters such that the minimum value results in almost no effect on the data while the maximum value represents a limit beyond which it is probable that meaningful signal may be lost. All prior distributions are uniform over the specified ranges. At each iteration, we measure the goodness of the tie as the central cross-correlation coefficient between the real and the synthetic seismic. Using this value, the optimizer progressively reshapes the posterior likelihood to favour the interesting regions in terms of performance and uncertainty.\nAt this stage, we compute the expected wavelet with the variational network and enforce a zero-phase spectrum with a post-processing filter. As suggested by\u00a0Simm et al. (2014), using a zero-phase wavelet to find the time to depth bulk shift is a simplification in order to solve the coupling problem between the bulk shift and the phase of the wavelet. We set the number of iterations to 80 with a ratio of 60% of random search and 40% of Bayesian updates. The values we provide here are default parameters that we believe are sensible to most datasets.\n\n\n2.4\nAutomatic stretch and squeeze and final wavelet\nFollowing\u00a0Hale (2013) and\u00a0Herrera et al. (2014), we compute dynamic time lags between the synthetic and real data to correct the time-depth relationship. In a manner inspired by dynamic time warping\u00a0(M\u00fcller, 2007) we use a sliding cross-correlation to find the time lag at every points of the trace. The user can control the length of the correlation window and the maximum lag. The window length satisfies a trade-off between smoothness and accuracy and the maximum lag is an explicit restriction that avoids unrealistic stretches. This operation is optional and we let the user select the parameters manually as this process can easily lead to unphysical modifications if not performed by an interpreter. We show an example in Fig.\u00a05. Once the time-lags have been found they can be used to modify the time-depth table. The final seismic wavelet is extracted after the optional stretch and squeeze. To do so, we first convert the well logs to TWT with the updated time-depth table and compute the new reflectivity. We then sample 50 wavelets from the variational network and select among the top 5 that provide the highest zero-lag correlation similarity the wavelet with the minimum absolute phase. Finally, since we also care about the absolute amplitude of the wavelet we also need to optimize for the scaling factor. Using the un-normalized reflectivity and real seismic, we compute the synthetic seismic with the scaled wavelet and aim to match the energy of the real seismic. Here, we define the energy as the sum of the squared amplitudes. Using Bayesian search in the same fashion as described above, we optimize the scalar factor for 30 iterations.\n\n\n\n3\nResults\nWe evaluate our method on two field datasets. The open Volve data from the Norwegian North Sea and the open Poseidon dataset in offshore Australia. In both cases we assume that the preprocessing and migration of the seismic data, the editing of the well logs and the initial time-depth table are adequate so that one can reasonably perform a well tie.\n\n3.1\nWell tie\nWe first evaluate our algorithm on the well Torosa from the Poseidon dataset. In Fig.\u00a06 we compare the tie results between the non-optimized and optimized versions. We can see several problems in the non optimized tie. In particular there seems to be a slight timing issue between the reflectivity and the seismic that the neural network tries to compensate by predicting a wavelet with a strong negative phase, as seen in Fig.\u00a07(a). The optimized tie exhibits a better fit between the predicted synthetic seismic and the real trace, and the predicted wavelet in Fig.\u00a07(b) shows a more realistic phase. Since, at this stage of the interpretation flow, the migrated seismic data is supposed to be near zero-phase, one usually expects some symmetry of the wavelet around the amplitude axis. The selected wavelet in Fig.\u00a07(b) has a mean phase of around 4\n\u00b0\n whereas the mean phase is around \u22121\n\u00b0\n. These are not the same since our selection criteria is first influenced by the similarity measure and then by the absolute phase value, which might result in choosing a wavelet different from the mean expected value. While the overall fit is good, there remain some timing issues, especially with the deeper reflections after 2.8 s, where the synthetic seismic is slightly in advance. In order to further improve the timing we also perform automatic stretch and squeeze. We set the correlation window length to 60\u00a0ms and allow for a maximum lag of 10\u00a0ms. The final tie is shown in Fig.\u00a08 and the wavelet and time-depth table in Fig.\u00a09. There is a better match in the deeper reflectors and the final wavelet has a similar phase rotation as before the stretch, although the mean phase has increased. In the final time-depth relation we observe the bulk-shift computed in the first step of the optimization and we can also see the effect of the stretch and squeeze when the effective velocity is locally increased or decreased. The overall compute time is about 3\u00a0min on a modern laptop.\n\n\n3.2\nPrestack well tie\nNext, we apply our algorithm to perform a prestack well tie. The data we use come from the well 15\/9-19\u00a0A of the Volve dataset. The input seismic is a gather computed between the reflection angles 2\n\u00b0\n and 36\n\u00b0\n, extracted along the well path. The methodology is similar to the stack well tie, with some adjustments. The shear velocity log is also required as an input in order to compute an angle dependent reflectivity with Zoeppritz equations. Wavelets are extracted independently for each incidence angle using the same neural network as previously. The similarity measure used by the optimizer is the mean value, across angles, of the zero-lag correlation coefficient between the real and synthetic gather. Therefore, the time-depth table and logs processing is shared among all angles and the optimizer looks for the parameters that provide the best fit for the entire gather. We also perform a stretch and squeeze using a window of 40\u00a0ms and a maximum lag of 8\u00a0ms. The tie is shown in Fig.\u00a010 and the wavelets in Figs.\u00a011 and 12. Results for this well are noisier than in the previous experiment. It is first important to notice that the recording length of the logs is short, around 270\u00a0ms, which poses a challenge for wavelet extraction with traditional methods\u00a0(White and Simm, 2003; Zabihi\u00a0Naeini et al., 2017). The real gather might also not be free of issues, in particular residual multiples and amplitude stretches can interfere with the process. The wavelets show a consistent behaviour across angles, with a similar phase and a progressively narrowing frequency support. Our current implementation takes about 12\u00a0min on a laptop, but there is room for optimization by for instance resorting to a GPU and by partially parallelizing the process.\nWe also compare the results with a tie done manually using a commercial software. As shown in Figs.\u00a010, 13 and 15, the semi-automated pre-stack approach delivers a reasonable result, which is more stable than the tie achieved with a manual approach. The result shown in Fig.\u00a010 was obtained after few minutes of computing on a laptop. The instability of the manual approach when processing noisy data with short extraction windows requires a time consuming fine-tuning of numerous parameters such as the position of the extraction window, log despiking, shifting of the time-depth table and wavelet smoothing to achieve the result shown in Figs.\u00a013 and 14, which ended up costing over 3\u00a0h of manual work.\n\n\n3.3\nCross validation\nWe also perform an automatic tie of the well Boreas from the Poseidon dataset and compare with the wavelet obtained for the Torosa well. The tie is shown in Fig.\u00a016. The shallow part, before 2.9 s, seems quite noisy but the results on the stronger reflections after 3.1 s show a good fit on average. The wavelet displayed in Fig.\u00a017 has a phase about similar to the Torosa wavelet (see Fig.\u00a09(a)). Since the Boreas well logging interval is deeper than Torosa, the wavelet is dominated by lower frequencies. We also perform a cross-synthetic quality control in Fig.\u00a018. Besides the expected difference in vertical resolution, the match between both synthetics and the real seismic is good on average.\n\n\n\n4\nDiscussion\nDeep learning and Bayesian optimization are powerful methods to partially automate well tying. Our neural network is robust to noisy inputs and provides consistent results on real data. Since it is trained from synthetics, there is no need for a tedious and time consuming manual preparation of the examples. Once trained, the network is easy to use as practitioners do not need to tune any parameter and it is computationally very efficient. In our synthetic training set, we did not apply any specific averaging or blocking of the log values in order to account for the resolution difference between the logs and the seismic. This discrepancy is implicitly learnt by the network and, as shown in the results section, one can predict a trustworthy wavelet without resorting to any blocking technique. Introduction of global average pooling provides the possibility to apply the network on arbitrary reflectivity and seismic lengths. The variational aspect is used to estimate uncertainties in the deconvolution operation, in particular for the phase of the wavelet. Moreover, neural networks seem to be more stable than traditional methods to solve inverse problems when the frequency support of the data is limited\u00a0(Ovcharenko et al., 2019; Garg et al., 2019). As an illustration, we experimented with the Volve well 159-19\u00a0A that contains short logs (around 300\u00a0ms), and performed several wavelet estimations with a least squares filter\u00a0(Walden and White, 1998). As shown in Fig.\u00a019, the filter is quite sensitive to the time window chosen for the extraction. The lack of low frequencies seems to have a negative impact on the method. On the other hand, our approach provides stable results even at such short extraction windows as seen in Fig.\u00a020. We show the synthetic data obtained with both wavelet extraction methods in Fig.\u00a021. Additionally, once trained, there are no hyper-parameters to tune for the neural network, which makes it easier and faster to use than a traditional method. Using Bayesian search, we show how the lengthy and tedious process of iteratively searching for the best combination of parameters can be automated in a computationally efficient fashion. By designing the parameter space and selecting the appropriate bounds, the geoscientist also keeps control over the optimization and is not forced to resort to a purely black box approach. The deliverables of the method are the wavelet and the updated time-depth table. If the user is not yet completely satisfied, the values could be imported into a common software and be used as initial conditions to further refine the results. In this way, one could imagine an iterative procedure where the geoscientist progressively guides the tool to converge towards the desired solution, in a similar workflow as proposed by\u00a0Tschannen et al. (2020). The use of a Bayesian optimizer is general and our formulation of the well tie could be modified without changing the global methodology. It is indeed possible to implement a different recipe or to change some of the key elements such as the similarity measure of the synthetic modelling tool and to optimize the parameters in a similar manner as described in this paper.\nHowever, there also remain some challenges for a full automation. In this work, we assume that the seismic and well logs are reasonably processed and that one has access to a reliable first estimation of the time-depth table. Yet, in the presence of strong migration issues or positioning errors of the well, it is unlikely that the tool can converge towards an acceptable solution. Another limit concerns the poor extrapolation capabilities of neural networks. If the real wavelet is too different from the synthetic training wavelets, it is likely that the results of the network will be wrong. In this work we assume the wavelets to be non causal, and the network would certainly fail to recover a causal wavelet. In this case, synthetic data with causal wavelets would need to be generated to train the network. Similarly, estimating a broadband wavelet would require training the data using synthetics generated from a much wider frequency range. We have also assumed that the data has been zero-phased, thus recovering a wavelet with a phase shift beyond 30 degrees would require to adjust the tie recipe. Regarding the random generation of the reflectivity, a possible improvement could be to perform a physically meaningful modelling, as e.g described by\u00a0Dvorkin et al. (2014). We also started to explore how the tool can be employed to perform prestack and joint well ties. While the current results look encouraging, more experiments need to be performed to validate the method in more extreme cases. In particular, one could for instance think about joint optimization strategies to explicitly enforce some form of consistency between the different wavelets. Additionally, automatic stretch and squeeze can be performed to overcome small positioning errors between the logs and the seismic, but the method we use is solely based on signal processing and does not resort to a-priori knowledge of the geology of the field, which might eventually lead to erroneous modifications of the table. The well-tie recipe we employ also involves some simplifications, such as the use of a zero-phase wavelet as an intermediary product in order to optimize for the best bulk shift value. This assumption can however be counter productive if the expected phase is completely unknown and in particular has an extreme value such as 90\n\u00b0\n or 180\n\u00b0\n. In our experiments, we use data that were preprocessed prior to inversion studies, and we therefore assume that the absolute expected phase is not so extreme as zero-phasing filters were applied.\n\n\n5\nConclusion\nWe have introduced a practical and versatile approach to partially automate the seismic to well tie process and consequently reduce the amount of engineer-hours needed to perform this tedious task. We use a variational neural network to solve the wavelet extraction problem, and employ Bayesian optimization to automatically tune the hyper-parameters of the problem. Our trained network can robustly predict seismic wavelets from variable length reflectivity and seismic traces. It can also estimate uncertainties to quantify the ambiguity in the phase of the wavelet. The Bayesian optimizer iteratively searches for the best parametrization of the process, and geoscientists have control over the nature and the bounds of this space. After convergence, the tool yields an updated time depth table at the well location and a source wavelet. We also show how the tool can be employed to perform a prestack tie and offers possibilities for joint ties. Validation on field datasets shows the potential of the method, and in future work we plan to extend the search space and the joint optimization objectives, by for instance incorporating geological prior knowledge, in order to tackle challenging use cases.\n\nCode availability section\n\nName of the code\/library: wtie python package.\nContact: Valentin Tschannen, valentin.tschannen@pm.me\nLicense: GPL-3.0 License\nHardware requirements: Modern laptop.\nProgram language: Python 3.7\nSoftware required: No external software. Python dependencies are listed in the project. Installation tested on Ubuntu 20.04 and Windows, with the conda package manager.\nProgram size: Source files and data occupy 27.4 Mb of memory.\nData: Data used in this study is included in the repository. The Volve data was released by Equinor under the Equinor Open Data Licence. The Poseidon data was provided by Geoscience Australia under the CC-BY 4.0 license.\nThe source codes are available for downloading at the link: https:\/\/github.com\/dlseis\/dlseis_well_tie\n\n\n\nCRediT authorship contribution statement\n\nValentin Tschannen: Conception of the original idea, Development of the source code, Conduction of the experiments, Writing \u2013 original draft. Ammar Ghanim: Conception of the original idea, Collection of the field data, Experiments to compare the results with a traditional method, Code testing, Writing \u2013 original draft. Norman Ettrich: Conception of the original idea, Technical support and critical feedback, Supervision.\n\n","57":"","58":"\n\n1\nIntroduction\nDiscontinuities are essential elements that affect slopes stability, the rock mass strength, and permeability (Hudson, 2001; Brown and Hoek, 1980). There are several geomechanical classifications (Barton, 1978; Bieniawski, 1989; Romana, 1985; Hack et al., 2003; Pantelidis, 2009; Tom\u00e1s et al., 2012), and for all of them a detailed characterization of discontinuities and their properties is critical, being necessary to define parameters such as orientation, spacing, persistence, roughness, and aperture among others (Wang et al., 2003). Traditionally, surveys and studies of rock mass characterization were made by direct measurements of the spatial distribution of discontinuity planes (i.e., joints, faults, bed plane, foliations), which requires understanding the discontinuity sets that affect the outcrops. The analysis has to be defined by measuring the orientation (dip and dip direction) of many discontinuities, their spacing, and persistence, and translating these data into stereographic plots where the main discontinuity sets are determined based on pole density distributions. Once every set is identified, a statistical analysis is developed to determine their representative spacing and persistence. This approach is time-consuming, difficult to apply in places of dangerous or impossible access, and depends on the accuracy and the ability of the geologist or engineering geologist to take proper measurements. Thus, the use of alternative safe and fast surveying technologies improves the quality of the results. The International Society for Rock Mechanics (Barton, 1978) recommended photogrammetry to measure discontinuity, and recent advances have led to numerous studies developing semi-automated extraction and characterization of discontinuities based on analyses of 3D point clouds, commonly used in non-contact surveys such as Laser Imaging or LiDAR (Slob et al., 2005; Kemeny et al., 2006; Mah et al., 2011; Maerz et al., 2013; Deliormanli et al., 2014; Riquelme et al., 2015; Chen et al., 2016; Jianqin et al., 2016; Jord\u00e1 Bordehore et al., 2017; Drews et al., 2018).\nSimilarly, Unmanned Aerial Vehicle (UAV) photogrammetry appears as a method that offers a viable cost-effective alternative to traditional remote sensing systems such as laser scanners, photogrammetry from airplanes, or satellite imagery, (Barton, 1978) especially for analyzing small areas. The use of UAV allows rapid acquisition of high-resolution photographs from airborne cameras (Wang et al., 2020). Giordan et al. (2015) proposed UAV data acquisition and processing methods to monitor active small landslides affecting urban environments; Bonali et al. (2019) analyzed evidence of tectonic and volcano-tectonic activity in the Iceland rift, and DeBell et al. (2016) reviewed the use of UAVs to manage water resources; UAV techniques are also relevant in the analysis of geological hazards (Tu et al., 2021), and this technology appears as a safe and fast monitoring technique for geotechnical, geological, and other engineering applications, such as the estimation of errors in position or volume uncertainties in civil engineering projects (ie. quarries, roads, etc. Siebert and Teizer, 2014; Castiglioni et al., 2017; Tziavou et al., 2018) and the determination of variations of stockpile volumes in quarries (Raeva et al., 2016). Photogrammetry developed with information obtained from UAV has been used as well in previous works to create high resolution terrain models that are important sources of geographical and geological information for disciplines related to Geosciences (Ou\u00e9draogo et al., 2014; Vasuki et al., 2014; Madden et al., 2015), and to obtain terrain features (with cm accuracy) or to create 3D models from high resolution point clouds (Labourdette and Jones, 2007; James and Robson, 2012; Garc\u00eda-L\u00f3pez et al., 2018).\nAdditionally, outcropping planes represent a wide variety and geologically relevant features: sedimentation mechanisms, tectonic events, etc. In 3D environments and point clouds, planes appear as a rock geometrical feature that is easy to define. Planes are the simplest 3D geometric item, and currently there are many studies looking for methods to obtain them automatically (Riquelme et al., 2014; Vasuki et al., 2014; Dewez et al., 2016; Jianqin et al., 2016; Jord\u00e1 Bordehore et al., 2017).\nThe present work investigates the utility of using UAVs as a tool to generate an outcrop 3D model that provides a 3D point cloud of the terrain surface from which a rock mass characterization is developed by semi-automated extraction of discontinuities (Deb et al., 2008; Sturzenegger and Stead, 2009; Menegoni et al., 2019; Giordan, D. et al., 2020; Buyer et al., 2020; Kong et al., 2021). Mechanical characterization has been performed using a photogrammetric analysis in a limestone quarry in northern Spain, where every year students of the degree of Engineering Geology develop a field trip to learn about geological mapping cartography and rock mass characterization (Fig. 1\n). The UAV survey covers a 65\u00a0m long and 30\u00a0m high quarry bench. A multi-rotor DJI Phantom 3\u00a9 equipped with a digital camera was used to survey the rock outcrop. Data acquisition took approximately 20\u00a0min, and point cloud creation took 6\u00a0h. The reconstruction of the camera positions, the generation of a high-resolution point cloud, the 3D model, and the determination of different surface characteristics were obtained using Structure from Motion (SfM, AGISOFT PhotoScan LLC, 2014) computer vision techniques in multi-view photos with cm accuracy (Verhoeven, 2011; Westoby et al., 2012). The generated 3D point cloud was processed by the open-access software CloudCompare (Girardeau-Montaut, 2016), and discontinuities were extracted by the open software DSE (Discontinuity Set Extractor (Riquelme et al., 2014) and FACETS (Dewez et al., 2016). Finally, the results obtained from both software have been compared with data measured directly in the field during fieldwork campaigns by teachers and undergraduate students at University Geology Degrees.\n\n\n2\nGeological setting and aerial photographs\nThe analyzed outcrop is located within the Cantabrian Zone (Lotze, 1945), in northern Spain (Fig. 1a), which forms the western part of the European Variscan belt (Marcos and Pulgar, 1982; Gallastegui et al., 1997). This area belongs to a thin-skinned foreland thrust and fold belt (Julivert, 1978; P\u00e9rez-Esta\u00fan et al., 1988), characterized by a folded and fractured thick sedimentary paleozoic unit (Fig. 1b). The paleozoic succession consists of cambrian limestones and sandstones, followed upsection by ordovician and devonian quartzarenites and a carboniferous sequence of siliciclastic layers alternating with carbonate units on top (Aller, 1986). The Alpine North to South compression produced a thrust fault, and Variscan structures were reactivated and uplifted in the Bay of Biscay to the north and the Duero basin to the south (Alonso and Pulgar, 1993). Abundant veins and dikes indicate the existence of hydrothermal activity related to rift extensional events produced during the North-Atlantic opening, and appear related to Mississippi Valley Type mineralization (S\u00e1nchez et al., 2009).\nThe outcrop at which the rock mass characterization is performed is one of the benches of an open pit quarry in the Barcaliente Fm (Carboniferous in age), made up of greyish black, and well-bedded micritic limestones (Fig. 1c). In the outcrop, the strata, with centimeter to decimeter thickness, appears subvertical, slightly overturned, and presenting a NNW dipping. The fracture network results from the interaction of both orogenies, the Variscan and the Alpine. Therefore, this outcrop is interesting for teaching purposes due to the geological complexity of the area, which can be easily and clearly analyzed in the limestone quarry. A substantial part of the study area is inaccessible due to its height, which exceeds 30\u00a0m. To allow visualization of the quarry, it has been pictured using a UAV. Photogrammetric information has been used to 3D model the geological structure of the rock mass.\n\n\n3\nMethodology\nCharacterization and classification of rock discontinuities allow determining parameters such as the strength or permeability of rock masses as well as establishing the stability of surface and under-ground excavations (Brown and Hoek, 1980; Hudson, 2001). In this study, a quarry has been analyzed in detail by conducting several fieldwork campaigns, composed of 4 compass scanlines. A UAV flight has been performed in the same quarry to obtain a photograph dataset that has led to the development of a 3D model and establish a discontinuities analysis with two open access software such as DSE and FACETs. The results are finally compared with data obtained by direct measurements in the field. Therefore, by developing this type of analysis for teaching purposes, students learn both techniques, how to measure data in the field, and the use of different software packages (Fig. 2\n) and interpret and compare results.\n\n3.1\nAcquisition of photographs of the outcrop: UAV flight planning\nData acquisition in this study consists of georeferenced aerial photographs obtained from a DJI Phantom 3\u00a9 UAV flight. The UAV has four blades and is equipped with an intelligent flight battery, remote controller, camera, and a stereo Vision Positioning System (VPS). The former allows to fly safely even without satellite positioning (Yusoff et al., 2018). The digital camera mounted on the UAV was a Nikon D3200 mounted with a 28\u00a0mm lens, with a resolution of 12.4 megapixels.\nOnce in the air, the aircraft is programmed to automatically fly a grid pattern horizontally over the outcrop (autopilot mode), and takeoffs and landings were conducted by a licensed pilot from the ground. The UAV camera took a photograph every 1\u20132\u00a0s. Synchronization of the flight path with the camera was made before the flight using PIX4D\u00a9 software (Pix 4D mapper, 2018). The flight of the aircraft was previously defined in detail, including flight altitude, speed, number of strips, and overlap of the photographs. The resolution per pixel of the pictures was 5\u00a0cm.\nThe flight was conducted in six horizontal stripes, spaced approximately 10\u00a0m apart, resulting in 80% and 60% forward and side overlaps, respectively. This level of overlap is enough to generate accurate 3D models (AGISOFT PhotoScan LLC, 2014), at an elevation of 50\u00a0m above the terrain. The UAV survey was completed in 10\u00a0min (it was repeated several times) for the 4000\u00a0m2 study area and the 65\u00a0m long quarry.\n\n\n3.2\nDetermination of camera locations and production of the sparse point cloud\nPhotographs taken from the UAV were processed using the Structure from Motion (SfM) methodology, from which the 3D structure is generated from overlapping images (Blistan et al., 2016). Structure from Motion (SfM) was developed with the AGISOFT PhotoScan Professional Version 1.1 software environment (AGISOFT PhotoScan LLC, 2014. The program can determine the camera positions (Fig. 3\n) and ground characteristics by identifying and matching different features from the available photographs. In total, 77 images were used to create the sparse point cloud, resulting in 15767 points after 6\u00a0h of processing. Afterwards, a dense point cloud of 1188361 points was generated, with a resolution of 1.25\u00a0cm and the textured mesh containing the ground features (Fig. 4\n).\nThis 3D mesh was produced by the Poisson Surface Results (Nesbit et al., 2020), and the resulting information was then included as the graphical framework for geological structures (fault planes and other discontinuities) recognition. Finally, the open-source software CloudCompare (Girardeau-Montaut, 2016) was also used for 3D visualization and computation.\n\n\n3.3\nDiscontinuity Set Extractor (DSE)\nThe dense point cloud dataset obtained by the SfM approach has been used to perform a Discontinuity Set Extractor (DSE) statistical analysis of the data. DSE is an open software based on MATLAB and designed by the University of Alicante in Spain (Riquelme et al., 2014). The software detects structural discontinuities of rock masses from 3D point clouds that are generated with field data obtained from remote sensing methods and photogrammetry. Discontinuities are ordered in sets that define a plane, and similar planes constitute discontinuity clusters. Thus, the software detects different point sets and characterizes the clusters of discontinuities present in a rock mass by performing the following operations:\n\n1)\nLocal curvature calculation: DSE finds the discontinuities' orientation of the rock mass at every point of the 3D point cloud. Simultaneously, the software detects the k-nearest neighbors (knn) or nearest points to each data analyzed with the same orientation, creating sets of points. Riquelme et al. (2014) suggest ranging the knn parameter between 15 and 30, with the highest value being the most effective. Once the sets of points have been defined, DSE develops a coplanarity test to group each point of the 3D point cloud and their corresponding nearest neighbors into planes. Rencher (2012) proposes that the admissible variance or deviation in the orientation of the forming points of a set is 80%. Therefore, the tolerance value of the coplanarity test (\u03b7max) is commonly set as 20%. Thus, this 20% tolerance has been applied. Finally, DSE determines the best-fit plane for the point sets previously generated and calculates the normal vector for this plane. The operator defines the minimum deviation angle between the normal vectors of the sets of points and the principal vector of the best-fit plane (Angle min v ppal) to group different sets into planes (Riquelme et al., 2014, 2016).\n\n\n2)\nStatistical analysis of the planes: DSE projects the poles of the discontinuity planes or normal vectors previously defined in a density stereographic plot by using the Kernel density estimation KDE method (Silverman, 1986). In the resultant plot, the software locates the areas with maximum pole densities and assesses the clusters of discontinuities, applying two parameters controlled by the user, the cone filter angle and the maximum number of clusters (np). The cone filter angle is the minimum accepted distance between two poles of planes of discontinuities, to consider that these planes belong to a cluster. The maximum number of clusters is established by the user, and it depends on the number of discontinuities present in the rock mass previously observed in direct field observations (Riquelme et al., 2014, 2015; Jord\u00e1 Bordehore et al., 2017).\n\n\n3)\nCluster analysis: DSE characterizes discontinuity clusters by assessing their main pole dip and dip direction. It is possible to hand-edit the final number of clusters regarding the number of points per cluster (ppc). Riquelme et al. (2014) state that clusters with less than 100 points can be statistically considered as noise in the model.\n\n\n4)\nNormal and persistence calculations: Once the software has detected the different clusters in the model, it can assess the spacing and persistence of the different discontinuity sets present in the model.\n\n\n\nTherefore, the advantage of using DSE relays in the possibility of obtaining a substantial number of virtual compass measurements in a short period of time (Riquelme et al., 2014).\n\n\n3.4\nFACETS\nFACETS is an open plugin from CloudCompare software that extracts planar facets from 3D point clouds, calculates their dip and direction, and plots them in stereograms. To extract the planar facets from the model, the software segments the dense cloud of points in facets with a user-defined degree of coplanarity using two algorithms, the kd-tree method (KD) or the fast-marching method (FM) (Dewez et al., 2016). The KD algorithm covers the\u00b73D model by a lattice of cells obtained by repeatedly splitting the 3D point cloud into smaller flat patches, until these patches coincide with one of the best-fitting faces obtained by the Root Mean Square (RMS) threshold (maximum distance). In contrast, the FM algorithm divides the 3D point cloud into smaller patches that are regrouped. Hence, the patches will have similar sizes and will be equal or larger than a pixel (Dewez et al., 2016). Due to the geological complexity of the outcrop and the amount and type of discontinuities present in the analyzed quarry, the method selected in this paper is the KD algorithm.\nAfter the KD analysis and segmentation of the point cloud into coplanar elementary subdivisions, the software performs three different levels of clustering. At a first level, FACETS clusters the elementary subdivisions that belong to a small fragment of a plane. At a second clustering level, elementary planes are grouped into single planes that belong to the same overall plane. Finally, at a third clustering level, parallel planes are combined into plane sets sharing the same spatial attitude (Dewez et al., 2016).\n\n\n3.5\nField measurements\nEvery year, undergraduate students of the 4th year Engineering Geology Degree from the Complutense University of Madrid (Spain) visit the area to study the structural geology and develop a rock mass characterization analysis. For this purpose, teachers and students perform several scanlines with their compass in the studied outcrop (Barton, 1978).\nDue to the limited accessibility to the quarry face, in this work, teachers and students performed four scanlines with 15\u00a0m of length and covering a height of up to 2\u00a0m during the 2018\/2019 course. In addition to orientation measurements, spacing and persistence data were collected with a measure tape along the 4 scanlines.\n\n\n3.6\nPlatform for dissemination: web-based 3D mesh (sketchfab)\nVirtual outcrops (VO) and Digital Outcrop Models (DOM) provide photorealistic models for geosciences with high spatial precision and characteristics such as geometric relationships between geologic features can be easily observed Bellian et al. (2005); Jones et al. (2009); Herrero Fern\u00e1ndez et al., 2019; S\u00e1nchez Moya et al., 2020.\nThe generation of 3D outcrop models has greatly evolved during the last few years due to recent technological advances. Commonly, these models appear as large file sizes and the need for 3D programs (some requiring payment for their use), with difficulties in sharing and visualizing them. Within this context, there are platforms for model dissemination, such as Sketchfab (2020), which can be used either to improve students learning or to share results and interpretations in an intuitive and visually attractive environment (Nesbit et al., 2020), and even to obtain additional measurements from areas with dangerous, difficult, or impossible access, where traditional methods are inadvisable.\n\n\n\n4\nResults\nThe development of the DSE analysis, performed following up the steps defined in section 3.4, has led to identification of four discontinuity clusters (Fig. 5\n and Table 1\n).\nIn addition, the main discontinuities extracted by FACETS (section 3.5) are shown in Fig. 6\n\n and Table 2\n (see Fig. 7).\nFinally, field measurements reveal the existence of seven sets or discontinuity families (S1\u2013S7, Table 3\n). Sets 2 to 6 are fractures related to the different tectonic events that occurred in the area, while set 7 corresponds to the stratification of the limestones. Set 1 is the main fracture set, which is vertical to subvertical, close to the stratification dip.\n\n4.1\nComparison between field measurements and DSE and FACETS results\nThe results obtained by the measurements of the compass scanlines have been compared with the discontinuities extracted from the 3D point cloud obtained from the UAV photographs using DSE and FACETS software (Table 4\n). In the three applied methods, it is considered that a plane belongs to a discontinuity set if its dispersion is\u00a0\u2264\u00a030\u00b0 with respect to the mean plane of the set. Thus, the clusters have been grouped and correlated according to this admissible dispersion (Table 4).\nIt is observed that the DSE software has detected and calculated the orientation and properties of 2 sets (S3 and S6; Table 4). FACETS software has extracted and detected sets similar to S1, S2, S3, and S6 obtained from measurements in the outcrop (Table 4). It is noticeable that neither of the two software has been able to detect the stratification of the limestone (S7) or the joints S4 and S5, which are subvertical and perpendicularly oriented to the model. Therefore, their surfaces are reduced to lines that are not automatically extracted using these types of extraction methods. Depending on the orientation of the planes and the position from which the photographs are taken, the planes of a rock mass discontinuities can be observed as surfaces or traces and are not extracted. Both programs are only able to extract planes from surfaces in the 3D model.\nThe comparison of the planes obtained from field measurements, and the planes obtained by the DSE and FACETS software analyses lead to the identification of one plane that is coincident in the three methods, S6, J4, and F4 (Table 4). The orientation of the quarry face analyzed has been detected by both software (S3, J1 and F1, Table 4). On the contrary, FACETS has detected three planes identified in the field as well (S1 and F2; S2 and F3; S6 and F5). It is noticeable, though, that the similarity of hand-measured planes and those obtained with the FACETS software show a more accurate correspondence. Finally, both software has determined new sets that were not measured in the otucrop, named S8 (only DSE) and S9 (both software). These sets could correspond to joint that are inaccessible on the quarry face, and these should be considered in future field campaigns.\nThe spacing and persistence of the sets measured in the field and by DSE are also shown in Table 5\n. It can be observed that the spacing measured in the field in sets S1 and S6 is lower than 10\u00a0cm. This measurement is lower than the proper resolution of the created model (1\u00a0m). Therefore, the spacing obtained from the DSE analyses for the J1set appears to be of a higher value than the real one, and the same occurs with persistence.\n\n\n4.2\nLimitations of DSE and FACETS semi-automatic analysis in the case of study\nHigh resolution UAV photographs allow the development of 3D outcrop models and the identification of geological features such as stratification, discontinuity planes, or even boundary surfaces. This identification can be improved including additional field data, and permit to reach dangerous places or with difficult access. Limitations arise when trying to generate models in complicated settings: places with complex color backgrounds. In these cases, the result of the software can contain large unwanted areas in the point cloud. To solve this problem, outcrop reconstructions should ensure that the depth of the photograph field is focused on the outcrop.\nAnother limitation of the UAV-SfM photogrammetry technique is that it is unable to reconstruct surfaces in the case of studying vegetated or cloudy or shadow areas, while other methods such as LiDAR are able to achieve this. These limitations could be solved by flying during the season with less vegetal cover (i.e., in the summer), or by limiting the presence of shadows by taking the pictures with zenithal light.\nDiscontinuity semi-automated extractor software such as DSE and FACETS is primarily limited to the orientation of the pictures taken to create the 3D point clouds and the proper discontinuities. Vertical discontinuities, or those perpendicular to the camera, cannot be detected by these methods. Thus, manual analysis of traces in digital images or 3D models, using commercial software such as ShapeMetriX or the compass plugin in CloudCompare (Buyer and Schubert, 2018; Pack et al., 2021) or any other semi-automatic method for trace extraction (Vasuki et al., 2014; Li et al., 2016 or Guo et al., 2019) must be applied. Furthermore, the use of these software requires previous knowledge of the structural geology of the area to assess the correct number of clusters, as well as some knowledge of statistics to properly establish the clustering process. In addition, the data obtained from this study, especially those related to spacing and persistence calculations, are directly limited by the resolution of the model. In the present study, the model was designed for teaching purposes, and georeferenced images from the UAV were considered accurate enough to develop the 3D model and to show students the structure of the rock mass. However, although the accuracy of the model is not high enough to represent the real spacing and persistence of the discontinuities detected, this problem could also occur with more precise models with topography surveys implemented in areas with very close spacing.\nVirtual outcrops or 3D outcrop models can become an excellent means of studying terrain properties, although results show greater accuracy by combining them with field visits. The use of this methodology may serve to improve scientific visualizations, helping the analysis of data, increasing the number of field measurements, and providing new visual perspectives, even for teaching purposes of both geology and computing methods.\n\n\n4.3\nSketchfab web platform for dissemination of the 3D model\nThis Sketchfab web-based interface permits to upload models and establish different rendering options, include comments, etc. (Fig. 7). There are as well various texture resolutions and Virtual Reality (VR) environment, allowing to extend and amplify the sensory experiences of the user who can get many details and an almost complete immersive experience within the model (https:\/\/sketchfab.com\/3d-models\/villamanin-leon-spain-5874116f37964ddc8a62b156847d2330). Thus, this platform can be of great use to develop future virtual laboratories and field trips for Geosciences students. In addition, this platform can be used as an attractive repository of LiDAR and photogrammetry data as the one proposed by Lato et al. (2013). Sketchfab permits to stick annotations on the models. Annotations are small, clickable notes, which may include images, lines, or explanations. They have a title and description and can be added from the Edit 3DSettings menu on the platform.\n\n\n\n5\nConclusions\nThe development of 3D outcrop models from UAV outcrop mapping allows obtaining detailed characterization of geological outcrops. The results obtained prove that UAV photogrammetry can be used to identify discontinuities and thus enhance rock mass characterization studies in unreachable areas and seem a valuable solution in terms of safety, accuracy, cost, and efficiency. UAVs are inexpensive devices compared to other techniques for field surveying, and they have proved to be the most suitable source of photogrammetric information for small areas.\nDiscontinuity semi-automated extractor software are open-access and fast tools that can complement field data acquisition (compass data), especially for limited access outcrops. They can extract the main discontinuity planes and group them into different discontinuity sets with different orientations, providing relevant information of rock mass characterization analyses and improving the existing. DSE is also able to calculate other relevant properties for geomechanical purposes, such as the spacing and persistence of each set of discontinuities detected, although the FACETs results appear more similar to the outcrop data in this study. The resulting 3D models can be of use for teaching rock mass characterization, as well as the use of different software and web-platforms.\nSketchfab and other internet platforms provide a straightforward way to share data sets and 3D models with other scientists, students, and stakeholders with no need of programming or using expensive specialized software. The possibility of implementing the 3D model as a Virtual Reality (VR) environment permits to extend and amplify the sensory experiences of the end-users.\nFor the moment, 3D models will not replace field surveys because the information obtained by an experienced geologist's eye and by the rest of the senses cannot be replaced. There will appear cases where planar features of interest are not measurable surfaces, and in this case the resulting point clouds will not generate relevant information. In other cases, plane widths would not be sufficiently determined by the point cloud, or inaccurate results would be obtained, for example due to the wrong selection of the points of view of the survey. The study of virtual outcrops is a worthwhile technique for many applications in geology, although it will not replace fieldwork, but combining geoscientist knowledge with new technology domains will permit establishing future needs to achieve optimization and efficiency.\n\n\nCode availability section\nThe open software codes used in this manuscript are available in the following repositories:\nCloudcompare v. 2.10.2.https:\/\/github.com\/cloudcompare\/cloudcompare;\nFACETS (CloudCompare plugin). http:\/\/www.cloudcompare.org\/doc\/wiki\/index.php?title=Facets_%28plugin%29;\nDSE v. 3.01.https:\/\/github.com\/cloudcompare\/cloudcompare.\n3D model and data associated showed in this work are also available inhttps:\/\/sketchfab.com\/3d-models\/villamanin-leon-spain-5874116f37964ddc8a62b156847d2330.\nOther commercial tools are available for downloading in their respective websites: Pix 4D mapper fromhttps:\/\/www.pix4d.com\/es\/and Sketchfab fromhttps:\/\/sketchfab.com\/.\n\n\nAuthorship contribution statement\nMar\u00eda J. Herrero: Conceptualization, investigation, writing, methodology, supervision, software. Patricia P\u00e9rez-Fortes: Conceptualization, data curation, methodology, writing. Jos\u00e9 I. Escavy: Investigation, writing, and visualization. .Juan M. Insua-Ar\u00e9valo: Conceptualization, review. Ra\u00fal De la Horra: Investigation, review. Francisco L\u00f3pez-Acevedo: Software, validation. Laura Trigos: Writing-reviewing and editing.\n\n","59":"","60":"","61":"","62":"\n\n1\nIntroduction\nHydro-morphodynamic models are highly complex coupled models used to simulate hydrodynamics, sediment transport and bed morphology in both fluvial and coastal environments. They are often associated with a high degree of uncertainty in part due to incomplete knowledge of various physical, empirical and numerical closure related parameters in both the hydrodynamic and morphodynamic solvers.\nResearch on methods to assess this uncertainty is ongoing. A range of statistical methods has been applied to hydro-morphodynamic models, including Monte Carlo methods\u00a0(e.g. Villaret et al., 2016; Hieu et al., 2015; Kopmann et al., 2012) and ensemble methods\u00a0(e.g. Unguendoli, 2018; Tang et al., 2018). Both of these require multiple runs of very computationally expensive models to produce statistically robust results, e.g. Harris et al. (2018) require 240,000 runs of the complex hydro-morphodynamic model XBeach. This makes these methods often computationally unfeasible, particularly when simulating the long time periods required in many hydro-morphodynamic problems, and means that simplified models must be used: e.g.\u00a0in\u00a0Dissanayake et al. (2014), a 1D rather than 2D model is used.\nOther advanced numerical methods can be used to manage uncertainty. For example, a tangent linear approach has been implemented with the hydro-morphodynamic model Telemac-Mascaret\u00a0(Hervouet, 1999). This implementation is presented in\u00a0Naumann and Riehme (2008) and examples of its application in morphodynamic test cases are given in\u00a0Kopmann et al. (2012), Villaret et al. (2016), Hieu et al. (2015), Dalledonne et al. (2017) and Riehme et al. (2010). This first order method is computationally cheaper than statistical methods, but must be run at least once for each uncertain parameter of interest. This can become computationally expensive because\u00a0Villaret et al. (2016) state that each tangent linear model run in their implementation takes approximately three times more than a forward model run (i.e. a standard hydro-morphodynamic model run).\nIn this work, we use adjoint methods to manage model uncertainty to great advantage. We emphasise here that by managing model uncertainty, we mean dealing with the issues posed by having uncertain parameters including, but not limited to, making the value of uncertain parameters more certain. Adjoint methods are used in numerical modelling to compute gradients of model outputs with respect to input parameters and are thus a useful tool for local sensitivity analysis and for the calibration and inversion of uncertain parameter values. Their main advantage is that only one adjoint evaluation is required to compute the sensitivity of an output quantity irrespective of the number of uncertain parameters, or their dimension (e.g.\u00a0a scalar or a field of values). Therefore, any number of multi-dimensional uncertain parameters can be considered at the same time with almost no effect on the computational cost (see e.g.\u00a0Funke et al., 2017; Chen et al., 2014; Heemink et al., 2002 where adjoint methods are applied to hydrodynamic models).\nAdjoint methods have already been successfully applied to hydro-morphodynamic models for non-cohesive sediments, although not to a fully coupled 2D hydro-morphodynamic model. For example, they are applied to a simple 1D hydro-morphodynamic model for turbidity currents in\u00a0Parkinson et al. (2017) and to the morphodynamic component Sisyphe of Telemac-Mascaret in\u00a0Kopmann et al. (2012), Merkel et al. (2013) and Merkel et al. (2016). However, only three published test cases use the adjoint method with Sisyphe and recent research appears to be limited. Additionally, to the best of our knowledge, adjoint methods have never been applied to the fully coupled hydro-morphodynamic model in Telemac-Mascaret. This is significant because many parameters influence both the hydrodynamic and morphodynamic components and there are many feedback effects between these two components. Thus, only calculating the adjoint on the morphodynamic component reduces the accuracy of the results of the adjoint methods and limits the cases where they can be applied. Furthermore, the implementation of the adjoint method within the Telemac-Mascaret model is financially expensive because it requires the use of a commercial NAG FORTRAN compiler\u00a0(see Merkel et al., 2016).\nThere is thus a clear need for the fully flexible, free-to-use, and relatively computationally cheap adjoint framework that we present in this work. This flexibility is achieved by using the pyadjoint\u00a0(Farrell et al., 2013) library, which works with the code-generation framework Firedrake\u00a0(Rathgeber et al., 2017) to automatically derive adjoint equations using the high level abstraction of the finite element equations available within all Firedrake based models. Thus, we can assess the uncertainty of any parameter in the model with respect to any model functional, without further code implementation. A further advantage of using pyadjoint is that, for all test cases considered in this work, an adjoint run takes at most three times more than a forward model run. This can be contrasted with Telemac-Mascaret, where for the test case in\u00a0Merkel et al. (2016), the adjoint run is 135 times more computationally expensive than the forward model run.\nIn this work, in order to benefit fully from this Firedrake - pyadjoint framework, we use the 2D depth-averaged non-cohesive coupled hydro-morphodynamic model presented in\u00a0Clare et al. (2021), which has been developed using Firedrake within the finite element coastal ocean modelling system Thetis\u00a0(K\u00e4rn\u00e4 et al., 2018). Although the adjoint method has previously been used with the hydrodynamic component of Thetis, e.g.\u00a0in\u00a0Warder et al. (2021), this work is the first time pyadjoint is used in a coupled model. A further advantage of this hydro-morphodynamic model is that it is more accurate than industry-standard models such as Telemac-Mascaret, as shown in\u00a0Clare et al. (2021), partly because of the relatively novel use of a discontinuous Galerkin based finite element discretisation. This has several advantages, as discussed in\u00a0Clare et al. (2021), including being well-suited for advection-dominated problems such as those considered in this work\u00a0(K\u00e4rn\u00e4 et al., 2018).\nThe remainder of this paper is structured as follows: Section\u00a02 describes the adjoint method; Section\u00a03 outlines the hydro-morphodynamic model; Sections\u00a04 and 5 uses simple test cases to show how adjoint methods can be used for sensitivity analysis and to perform inversion and calibration; Section\u00a06 shows how adjoint methods can be used to invert for tsunami-like waves from sediment deposits and, finally, Section\u00a07 presents some concluding remarks.\n\n\n2\nAdjoint methods\nAdjoint methods can compute the gradient of a model functional with respect to a set of parameters and are thus useful for managing uncertainty\u00a0(Farrell et al., 2013). To establish notation, we first briefly present a derivation of these methods, following\u00a0Plessix (2006) and\u00a0Funke et al. (2017).\nThe hydro-morphodynamic forward model to be presented in Section\u00a03 can be written in the abstract form \n\nF\n\n(\nu\n\n(\nm\n)\n\n,\nm\n)\n\n=\n0\n\n where \nm\n is a set of uncertain parameters and \n\nu\n\n(\nm\n)\n\n\n is the model solution. Any given model functional, \n\n\nJ\n\n\n\u02c6\n\n\n, depends only on \nm\n and \nu\n, and the reduced functional \nJ\n can be defined as \n\n(1)\n\n\nJ\n\n(\nm\n)\n\n=\n\n\nJ\n\n\n\u02c6\n\n\n\n(\nu\n\n(\nm\n)\n\n,\nm\n)\n\n.\n\n\n\nThen \n\n(2)\n\n\n\n\nd\nJ\n\n\nd\nm\n\n\n=\n\n\n\u2202\n\n\nJ\n\n\n\u02c6\n\n\n\n\n\u2202\nu\n\n\n\n\n\u2202\nu\n\n\n\u2202\nm\n\n\n+\n\n\n\u2202\n\n\nJ\n\n\n\u02c6\n\n\n\n\n\u2202\nm\n\n\n.\n\n\n\nComputing the derivatives of \n\n\nJ\n\n\n\u02c6\n\n\n in this equation is simple because the reduced functional is usually provided via an analytic formula. However, computing and deriving \n\n\u2202\nu\n\/\n\u2202\nm\n\n is complex because it is only given implicitly by the model\u00a0(Funke, 2012). Therefore, we derive a formula for it by differentiating the forward model with respect to \nm\n, which yields \n\n(3)\n\n\n\n\n\u2202\nu\n\n\n\u2202\nm\n\n\n=\n\u2212\n\n\n\n\n\n\n\u2202\nF\n\n\n\u2202\nu\n\n\n\n\n\n\n\u2212\n1\n\n\n\n\n\u2202\nF\n\n\n\u2202\nm\n\n\n,\n\n\n\nand hence \n\n(4)\n\n\n\n\nd\nJ\n\n\nd\nm\n\n\n=\n\u2212\n\n\n\u2202\n\n\nJ\n\n\n\u02c6\n\n\n\n\n\u2202\nu\n\n\n\n\n\n\n\n\n\u2202\nF\n\n\n\u2202\nu\n\n\n\n\n\n\n\u2212\n1\n\n\n\n\n\u2202\nF\n\n\n\u2202\nm\n\n\n+\n\n\n\u2202\n\n\nJ\n\n\n\u02c6\n\n\n\n\n\u2202\nm\n\n\n,\n\n\n\nwhich can be evaluated using either the adjoint or the tangent linear approach. If adjoint methods are used, we evaluate \n\u03bb\n in \n\n(5)\n\n\n\u03bb\n=\n\n\n\u2202\n\n\nJ\n\n\n\u02c6\n\n\n\n\n\u2202\nu\n\n\n\n\n\n\n\n\n\u2202\nF\n\n\n\u2202\nu\n\n\n\n\n\n\n\u2212\n1\n\n\n,\n\n\n\nand substitute it into \n\n(6)\n\n\n\n\nd\nJ\n\n\nd\nm\n\n\n=\n\u2212\n\u03bb\n\n\n\u2202\nF\n\n\n\u2202\nm\n\n\n+\n\n\n\u2202\n\n\nJ\n\n\n\u02c6\n\n\n\n\n\u2202\nm\n\n\n,\n\n\n\nto find the derivative. Note, the adjoint has the advantage that only one linear solve is ever necessary to evaluate (5), independent of the number of uncertain parameters \nm\n or their dimension. The adjoint is not well suited though to computing the sensitivity with respect to uncertain input parameters of a large number of model outputs, multiple scalar values or entire spatially-varying fields, because an adjoint evaluation is required for each scalar output. However, the symbolic language employed in the Firedrake and pyadjoint framework (discussed later in this section) means that we can easily express any scalar functional, including ones which are dependent on multiple outputs, and can aggregate spatially or time-varying output fields in the form of integrals (see (10) and (12) for example). The automated adjoint then allows us to efficiently compute the sensitivity of that specified functional with respect to an arbitrary number of uncertain input parameters.\nIf the tangent linear approach is used, we evaluate \n\u03bc\n in \n\n(7)\n\n\n\n\n\u03bc\n\n\nT\n\n\n=\n\u2212\n\n\n\n\n\n\n\u2202\nF\n\n\n\u2202\nu\n\n\n\n\n\n\n\u2212\n1\n\n\n\n\n\u2202\nF\n\n\n\u2202\nm\n\n\n.\n\n\n\nand substitute it into \n\n(8)\n\n\n\n\nd\nJ\n\n\nd\nm\n\n\n=\n\n\n\u2202\n\n\nJ\n\n\n\u02c6\n\n\n\n\n\u2202\nu\n\n\n\n\n\u03bc\n\n\nT\n\n\n+\n\n\n\u2202\n\n\nJ\n\n\n\u02c6\n\n\n\n\n\u2202\nm\n\n\n,\n\n\n\nto find the derivative. Note this is equivalent to (3), meaning that the tangent linear approach is solving for \n\nd\nu\n\/\nd\nm\n\n. The expression for \n\u03bc\n can be solved using a variety of different methods including explicitly inverting the linearised PDE operator \n\n\u2202\nF\n\/\n\u2202\nu\n\n or performing an iterative solve for each dimension of \nm\n\u00a0(see Funke, 2012). For all methods, the computational cost of solving this expression scales linearly with the dimension of \nm\n. By contrast, once \n\u03bc\n is computed, then the derivative of the reduced functional \nJ\n can be efficiently computed for multiple different outputs. This makes the tangent linear approach well-suited to problems with a relatively small number of uncertain one-dimensional input parameters but a large number of uncertain outputs and\/or a functional which is a spatially-varying field. Conversely, the adjoint solution is ideally suited to problems where there are multiple uncertain input parameters (both scalar and multi-dimensional), which is the case for hydro-morphodynamic problems.\nAdjoint methods can be difficult to derive and implement, but we overcome this problem by using the hydro-morphodynamic model from\u00a0Clare et al. (2021), which is built within Thetis, a Firedrake-based model. This means we can use the pyadjoint library\u00a0(Farrell et al., 2013), which is constructed to work within the Firedrake environment. This library automatically derives the adjoint equations by first \u2018taping\u2019 (recording the sequence of numerical operations) a forward model run and then using this tape to construct the discretised adjoint equations. This means that the actual derivative of the discrete model is used (up to numerical truncation errors and solver tolerances), rather than a discrete approximation of a continuous derivative\u00a0(see Funke et al., 2017). A similar methodology can also be used to implement the tangent linear approach in pyadjoint. Although taping the model does mean that pyadjoint requires more memory than a hard-coded user implementation, this disadvantage is far outweighed by the fact that pyadjoint significantly simplifies both the calculation and the implementation of the adjoint model. In this work memory has not been an issue, but for very large problems there are intelligent checkpointing techniques that can be used to reduce memory requirements (see for example Griewank and Walther, 2000; Kukreja et al., 2018).\nThe adjoint method has already been used successfully with the hydrodynamic component of Thetis\u00a0(e.g. Warder et al., 2021), but we expand upon this here by using the adjoint method with a coupled model which requires extending the pyadjoint code to ensure that the coupling is correctly captured. In particular, the coupling between the components of the hydro-morphodynamic model relies on a split mechanism, which extracts the velocity and elevation from the hydrodynamic component so that both can be passed to the morphodynamic component. The new pyadjoint code tapes and calculates the adjoint of this operation, thus facilitating the use of pyadjoint for all Firedrake-based coupled models (full details of the code change are given in\u00a0Firedrake\u00a0Project, 2021). In this work, we do not show the adjoint equations since using pyadjoint means we do not have to manually derive them. However, for the interested reader,\u00a0Funke (2012) shows the derivation of the adjoint form of the shallow water equations, and the adjoint of the sediment concentration equation and Exner equation can be derived following a similar methodology.\nThe Taylor remainder convergence test (explained in more detail in\u00a0Funke, 2012) can be used to verify the adjoint implementation derived by pyadjoint and in particular the new adjoint implementation of our coupled model. This test checks whether the gradient \n\n\nd\nJ\n\n\nd\nm\n\n\n derived using the adjoint solution is correct by verifying that the Taylor residual converges at second order \n\n(9)\n\n\n\n|\nJ\n\n(\nm\n+\nh\n\u03b4\nm\n)\n\n\u2212\nJ\n\n(\nm\n)\n\n\u2212\nh\n\u03b4\nm\n\n\nd\nJ\n\n\nd\nm\n\n\n|\n\n=\nO\n\n(\n\n\nh\n\n\n2\n\n\n)\n\n\nas\u00a0\nh\n\u2192\n0\n,\n\n\n\nwhere \n\n\u03b4\nm\n\n is a random perturbation. This second-order convergence is very sensitive to implementation errors\u00a0(see Funke, 2012) and thus represents a strict code verification check. All test cases outlined in this work pass the Taylor remainder convergence test. Additionally, using the Trench test case in Section\u00a05, we also verified that the adjoint derivative of \nJ\n is consistent with that obtained via the direct finite difference approach applied to \nJ\n when each model parameter is perturbed independently. These two tests confirm that the adjoint implementation of our coupled hydro-morphodynamic model is correct.\n\n2.1\nUsing adjoint methods to manage uncertainty\nAdjoint methods can be used to manage the uncertainty of hydro-morphodynamic models with respect to particular parameters, through sensitivity analysis and calibration and inversion. We perform calibration by inverting our model for the uncertain parameters and using an optimisation algorithm to minimise the error between the model output and the desired output to improve model accuracy. Thus calibration can be seen as a sub-type of inversion and we use the same methodology for both. Note that adjoint methods can also be used in a Bayesian framework, for example, through performing inversion on an ensemble of prior realisations generated using the Randomized Maximum Likelihood (RML) technique\u00a0(see Alpak and Jennings, 2020), but such an exercise is beyond the scope of this work. To analyse the sensitivity of model outputs to particular uncertain parameters, we use adjoint methods to compute the derivative \n\nd\nJ\n\/\nd\nm\n\n. This reduced functional, \nJ\n, can take many forms and we denote the reduced functional used for sensitivity analysis as \n\n\nJ\n\n\nsen\n\n\n and define it as follows \n\n(10)\n\n\n\n\nJ\n\n\nsen\n\n\n\n(\n\n\nu\n\n\nmodel\n\n\n,\n\nm\n)\n\n\u2254\n\n\n1\n\n\n2\n\n\n\n\n\n\u2211\n\n\nj\n=\n1\n\n\n\n\nN\n\n\nout\n\n\n\n\n\n\n\n\u222b\n\n\n0\n\n\nT\n\n\n\n\n\u222b\n\n\n\u03a9\n\n\n\n\n\n\n\n|\n\n\nu\n\n\nj\n\n\nmodel\n\n\n|\n\n\n\n2\n\n\n+\n\u03b5\n\n\n\n\nd\nx\n\nd\nt\n,\n\n\n\nwhich analyses the local sensitivity of an integrated output to uncertain parameters (in Section\u00a04.2, this output is chosen to be the bedlevel). Here \n\u03a9\n is the domain of the model, \n\n\nN\n\n\nout\n\n\n the number of output variables used and \n\u03b5\n is a parameter set to 10\u22126 which we have added to smooth our results if \n\n\n\nu\n\n\nj\n\n\nmodel\n\n\n=\n0\n\n anywhere in the domain. It is important to note that this is a local sensitivity analysis rather than a global one and is dependent on the trajectory on which \n\nd\nJ\n\/\nd\nm\n\n is evaluated.\nFor inversion and calibration, we minimise the following problem using the default L-BFGS-B algorithm available via the SciPy library\u00a0(Jones et al., 2001) \n\n(11)\n\n\n\n\n\nminimize\n\n\nu\n,\nm\n\n\n\n\n\nJ\n\n\ninv\n\n\n\n(\nu\n,\nm\n)\n\n\n\n\n\n\n\nsubject\u00a0to\n\nF\n\n(\nu\n,\nm\n)\n\n=\n0\n.\n\n\n\n\n Here we use \n\n\nJ\n\n\ninv\n\n\n to denote the reduced functional used for inversion and calibration; its general form is defined as \n\n(12)\n\n\n\n\n\nJ\n\n\ninv\n\n\n\n(\n\n\nu\n\n\nmodel\n\n\n,\n\nm\n)\n\n\u2254\n\n\n\n\u2211\n\n\nj\n=\n1\n\n\n\n\nN\n\n\nout\n\n\n\n\n\n\n\n\u03b1\n\n\nj\n\n\n\n\n\u222b\n\n\n0\n\n\nT\n\n\n\n\n\u222b\n\n\n\u03a9\n\n\n\n\n\n|\n\n\nu\n\n\nj\n\n\ntrue\n\n\n\u2212\n\n\nu\n\n\nj\n\n\nmodel\n\n\n|\n\n\n\n2\n\n\n\nd\nx\n\nd\nt\n+\n\n\n\n\u2211\n\n\ni\n=\n1\n\n\n\n\nN\n\n\nin\n\n\n\n\n\n\n\n\u03b2\n\n\ni\n\n\n\n\n\u222b\n\n\n0\n\n\nT\n\n\n\n\n\u222b\n\n\n\u03a9\n\n\n\n\n\n|\n\n\nm\n\n\ni\n\n\n|\n\n\n\n2\n\n\n\nd\nx\n\nd\nt\n,\n\n\n\n\nwhere \n\n\nN\n\n\nin\n\n\n is the number of uncertain parameters and \n\n\n\u03b1\n\n\nj\n\n\n is a user-specified scaling factor. The first integral term in (12) is the difference between the model output and the true value, either experimental\/real-world data or, in the case of a dual twin experiment, synthetic data generated using a previous run of the model with known parameter values. Unlike with (10), we do not use a square root in the term, so as to more severely penalise large differences between the model and true values. The second integral term is a Tikhonov regularisation term, which aids in the solution of ill-posed problems and can be used to prevent the magnitude of the parameters becoming unphysical\u00a0(see Engl et al., 1996). The amount of regularisation is controlled by the scalar \n\n\n\u03b2\n\n\ni\n\n\n and can be different for each uncertain parameter (see for example Section\u00a05.2). Recall that our use of pyadjoint means we can utilise its symbolic language to express any functional which suits the problem. This makes it possible to, for example, set \n\n\n\u03b2\n\n\ni\n\n\n as a correlation matrix to account for correlations between parameters or to include spatially-varying weights in the functional to account for expected measurement noise.\n\n\n\n3\nHydro-morphodynamic model\nWe now briefly describe the 2D depth-averaged coupled hydro-morphodynamic model presented in\u00a0Clare et al. (2021) and used in this work. It is able to update the bed morphology as a result of both suspended sediment and bedload transport, while taking into account gravitational and helical flow effects. Fig.\u00a01 shows a diagrammatic representation of what our hydro-morphodynamic model simulates.\n\nFor reasons of stability, the time derivatives in the model equations are approximated using a fully-implicit backward Euler timestepping scheme. Clare et al. (2021) give full details on the original development of this model for a fully wet domain which is used in the test cases in Sections\u00a04 and 5 and the equations are summarised below:\n\nShallow water equations\n\n\n\n(13)\n\n\n\n\n\u2202\nh\n\n\n\u2202\nt\n\n\n+\n\u2207\n\u22c5\n\n(\nh\nU\n)\n\n=\n0\n,\n\n\n\n\n\n\n(14)\n\n\n\n\n\u2202\nU\n\n\n\u2202\nt\n\n\n+\nU\n\u22c5\n\u2207\nU\n+\ng\n\u2207\n\u03b7\n=\n\u03bd\n\n\n\u2207\n\n\n2\n\n\nU\n\u2212\n\n\n\n\nC\n\n\nh\n\n\n\n\nh\n\n\n\n|\n\n|\nU\n|\n\n|\n\nU\n,\n\n\n\nwhere \n\nh\n\n(\nx\n,\nt\n)\n\n\n is the total depth, \n\nU\n\n(\nx\n,\nt\n)\n\n\n the depth-averaged velocity, \n\n\u03b7\n\n(\nx\n,\nt\n)\n\n\n the free surface elevation, \ng\n the gravitational constant, \n\u03bd\n the viscosity parameter and \n\n\nC\n\n\nh\n\n\n the quadratic drag coefficient;\n\n\nNon-conservative sediment concentration equation\n\n\n\n(15)\n\n\n\n\n\u2202\nC\n\n\n\u2202\nt\n\n\n+\n\n\nF\n\n\ncorr\n\n\nU\n\u22c5\n\u2207\nC\n=\n\n\n\u03b5\n\n\ns\n\n\n\n\n\u2207\n\n\n2\n\n\nC\n+\n\n\n\n\nE\n\n\nb\n\n\n\u2212\n\n\nD\n\n\nb\n\n\n\n\nh\n\n\n,\n\n\n\nwhere \n\nC\n\n(\nx\n,\nt\n)\n\n\n is the depth-averaged sediment concentration, \n\n\n\u03b5\n\n\ns\n\n\n the diffusivity coefficient, \n\n\nE\n\n\nb\n\n\n the erosion flux, \n\n\nD\n\n\nb\n\n\n the deposition flux, and \n\n\nF\n\n\ncorr\n\n\n a correction factor;\n\n\nExner equation\n\n\n\n(16)\n\n\n\n\n\n(\n1\n\u2212\n\n\np\n\n\n\u2032\n\n\n)\n\n\n\n\n\nm\n\n\nf\n\n\n\n\n\n\n\u2202\n\n\nz\n\n\nb\n\n\n\n\n\u2202\nt\n\n\n+\n\u2207\n\u22c5\n\n\nQ\n\n\nb\n\n\n=\n\n\nD\n\n\nb\n\n\n\u2212\n\n\nE\n\n\nb\n\n\n,\n\n\n\nwhere \n\n\nm\n\n\nf\n\n\n is a morphological acceleration factor, \n\n\np\n\n\n\u2032\n\n\n the porosity, \n\n\nQ\n\n\nb\n\n\n the bedload transport and \n\n\n\nz\n\n\nb\n\n\n\n(\nx\n,\nt\n)\n\n\n the bedlevel (also known as the bathymetry). Note, throughout \n\n\nz\n\n\nb\n\n\n is defined such that the water depth, \n\nh\n=\n\u03b7\n\u2212\n\n\nz\n\n\nb\n\n\n\n.\nHere, \n\n\nF\n\n\ncorr\n\n\n in (15) accounts for the fact that depth-averaging the product of two variables is not equivalent to multiplying two depth-averaged variables; and the morphological acceleration factor, \n\n\nm\n\n\nf\n\n\n, in (16) artificially increases the rate of bedlevel changes compared with the underlying hydrodynamics, thus decreasing computational cost.\nIn Table\u00a01, we summarise the properties of the variables and parameters in Eq.\u00a0(13)\u2013(16), and the dependencies of the empirical variables on the model variables. Furthermore, in Sections\u00a04 and 5, we assess the uncertainty in the average sediment grain size, \n\n\nd\n\n\n50\n\n\n, the bed reference height \n\n\nk\n\n\ns\n\n\n and the sediment density \n\n\n\u03c1\n\n\ns\n\n\n, and thus Table\u00a01 also shows which quantities directly depend on these parameters. Full details on the parameters and the empirical formulae used in our model can be found in\u00a0Clare et al. (2021).\n\n\n\n\n\n3.1\nWetting-and-drying\nCoastal zone test cases (like that considered in Section\u00a06) often have a wetting-and-drying interface. Thus,\u00a0Clare et al. (2022) extends the hydro-morphodynamic model in\u00a0Clare et al. (2021) to deal with wet-dry domains by using the wetting-and-drying scheme detailed in\u00a0K\u00e4rn\u00e4 et al. (2011), where the depth, \nh\n, is replaced by \n\n(17)\n\n\n\n\nH\n\n\n\u0303\n\n\n\u2254\n\u03b7\n\u2212\n\n\nz\n\n\nb\n\n\n+\n\n\n1\n\n\n2\n\n\n\n\n\n\n\n\nh\n\n\n2\n\n\n+\n\n\n\u03b4\n\n\n2\n\n\n\n\n\u2212\nh\n\n\n,\n\n\n\nwhere \n\u03b4\n is a user-defined parameter set to be approximately equal to \n\nd\n\n|\n\n|\n\u2207\nh\n|\n\n|\n\n\n with \nd\n the mesh length scale. To avoid the sediment leakage that is observed when applying (15) in combination with wetting-and-drying, in the wetting-and-drying model we use the following conservative sediment concentration equation \n\n(18)\n\n\n\n\n\u2202\n\n\n\u2202\nt\n\n\n\n(\n\n\nH\n\n\n\u0303\n\n\nC\n)\n\n+\n\u2207\n\u22c5\n\n(\n\n\nF\n\n\ncorr\n\n\nU\n\n\nH\n\n\n\u0303\n\n\nC\n)\n\n=\n\n\n\u03b5\n\n\ns\n\n\n\n\n\u2207\n\n\n2\n\n\n\n(\n\n\nH\n\n\n\u0303\n\n\nC\n)\n\n+\n\n\nE\n\n\nb\n\n\n\u2212\n\n\nD\n\n\nb\n\n\n.\n\n\n\nFinally, the Exner Eq.\u00a0(16) is unchanged because it is not explicitly dependent on depth. Full details on the wetting-and-drying hydro-morphodynamic model are given in\u00a0Clare et al. (2022).\n\n\n\n4\nLocal sensitivity analysis for a meander\nAs an initial test case, we consider flow around the curved channel of a meander and use the configuration from experiment 4 of\u00a0Yen and Lee (1995). This test case has already been validated for our hydro-morphodynamic model in\u00a0Clare et al. (2021). Fig.\u00a02 is taken from the latter and shows that our model causes erosion at the outer bend (negative bedlevel evolution) and deposition at the inner bed (positive bedlevel evolution), as expected from physical intuition. Moreover, the figure shows good agreement between our Thetis model results and the experimental data. Note that in all meander figures the flow is from the bottom left to the bottom right. For this test case, we use the same set-up as in\u00a0Clare et al. (2021) and refer the reader there for the parameter values used.\n\n\n\n\n4.1\nTangent linear approach to local sensitivity analysis\nThis meander test case is also studied in\u00a0Riehme et al. (2010), where they analyse the local sensitivity of the bedlevel evolution to uncertain parameters in the hydrodynamic Telemac-2D and morphodynamic Sisyphe components of the Telemac-Mascaret model, hereafter Sisyphe. They use the First Order Reliability Method (FORM) to calculate the quantity \n\n\u03c3\n\n\nJ\n\n\nsen\n\n\n\n, which is referred to in\u00a0Riehme et al. (2010) as the standard deviation but we refer to as the scaled gradient to avoid confusion with the standard deviation of the parameter. The scaled gradient is defined as \n\n(19)\n\n\n\u03c3\n\n\nJ\n\n\nsen\n\n\n=\n\n\n\u03c3\n\n\nm\n\n\n\n\n\n\n\u2202\n\n\nJ\n\n\nsen\n\n\n\n\n\u2202\nm\n\n\n|\n\n\nm\n=\n\n\u3008\nm\n\u3009\n\n\n\n,\n\n\n\nwhere \n\n\nJ\n\n\nsen\n\n\n is the bedlevel evolution, a spatially-varying field (i.e.\n\n\n\n\nJ\n\n\nsen\n\n\n=\n\n\nz\n\n\nb\n\n\nfinal\n\n\n\u2212\n\n\nz\n\n\nb\n\n\ninitial\n\n\n\n). The derivative is calculated using the tangent linear approach and \n\n\u3008\nm\n\u3009\n\n and \n\n\n\u03c3\n\n\nm\n\n\n represent the mean and standard deviation of the uncertain parameter, respectively, which must be estimated. Note that in\u00a0Riehme et al. (2010), the standard deviation \n\n\n\u03c3\n\n\nm\n\n\n is assumed constant and thus multiplying by \n\n\n\u03c3\n\n\nm\n\n\n merely scales the sensitivity without altering its spatial pattern.\nTherefore, as a first verification step of our Thetis-pyadjoint framework, we compare the FORM analysis (19) using our model with that from using Sisyphe for the two scalar parameters of average sediment grain size, \n\n\nd\n\n\n50\n\n\n and bed reference height \n\n\nk\n\n\ns\n\n\n. Uncertainty quantification for these parameters is important because both are challenging to determine, particularly \n\n\nd\n\n\n50\n\n\n which is difficult to measure in offshore environments and may change seasonally\u00a0(see Jaffe et al., 2016). In addition, \n\n\nk\n\n\ns\n\n\n determines the bed friction, to which hydrodynamic models are highly sensitive\u00a0(see e.g. Merkel et al., 2013; Warder et al., 2021). Following\u00a0Riehme et al. (2010), we assume a mean of 1 \n\u00d7\n 10-3\nm for \n\n\nd\n\n\n50\n\n\n and 3 \n\u00d7\n 10-3\nm for \n\n\nk\n\n\ns\n\n\n, and a standard deviation of 1 \n\u00d7\n 10-4\nm for both.\n\nFig.\u00a03 shows the scaled gradient \n\n\u03c3\n\n\nJ\n\n\nsen\n\n\n\n of the bed evolution for the scalar parameters \n\n\nd\n\n\n50\n\n\n and \n\n\nk\n\n\ns\n\n\n computed using our model, whilst Fig.\u00a04 shows the same for Sisyphe taken from\u00a0Riehme et al. (2010). Their comparison reveals that the distribution of the scaled gradient is consistent for both parameters. Furthermore, at the outer bend, the magnitude of the scaled gradient determined by our model is very similar to that determined by Sisyphe for both \n\n\nd\n\n\n50\n\n\n and \n\n\nk\n\n\ns\n\n\n. At the inner bend, however, the Sisyphe model predicts a greater scaled gradient magnitude for both \n\n\nd\n\n\n50\n\n\n and \n\n\nk\n\n\ns\n\n\n. These magnitude differences are to be expected because the models are constructed slightly differently and the Thetis final bedlevel results are more accurate than Sisyphe\u2019s when compared against experimental data (see Figure 11 in\u00a0Clare et al., 2021). Moreover, the FORM analysis with Sisyphe is limited because the tangent linear approach is only applied to the morphodynamic component of the model (see\u00a0Riehme et al., 2010 for more details). In contrast, our Thetis model computes the gradient for the fully coupled hydro-morphodynamic model, hence arguably producing more accurate results.\nAddressing the uncertainty of the fully coupled model to \n\n\nk\n\n\ns\n\n\n is especially important because this parameter is key in determining both the sediment transport rate in the morphodynamic component and the bed friction in the hydrodynamic component. To investigate this further, we experimented with keeping \n\n\nk\n\n\ns\n\n\n constant in the hydrodynamic component of our Thetis framework. Note this still does not make the two models set-ups identical because the \n\n\nk\n\n\ns\n\n\n in the morphodynamics causes changes in the hydrodynamics not accurately captured by Sisyphe which then causes changes in the morphodynamics and so forth. However, this Thetis result agrees more closely with that from Sisyphe, although these results are not included here for brevity.\nIn summary, the spatial patterns of the scaled gradients are similar between Thetis and Sisyphe providing confidence in our Thetis-pyadjoint framework implementation.\n\n\n\n\n\n\n4.2\nAdjoint approach to local sensitivity analysis\nThe tangent linear analysis above calculates the local sensitivity of the bed everywhere to a single scalar parameter \nm\n evaluated at a specific point. In contrast, a key advantage of using the adjoint approach is that we can determine the local bed sensitivity to more than one scalar and\/or to a spatially-varying parameter using a single run. In this section, we choose the latter option and analyse the sensitivity of the meander bed to the now assumed to be spatially-varying parameters \n\n\nd\n\n\n50\n\n\n and \n\n\nk\n\n\ns\n\n\n, using (10) as the integrated functional where \n\n\nu\n\n\nmodel\n\n\n is the final bedlevel.\n\nFigs.\u00a05(a) and 5(b) show the local sensitivity of the bedlevel to the spatially-varying parameters \n\n\nd\n\n\n50\n\n\n and \n\n\nk\n\n\ns\n\n\n, respectively, where we have evaluated the derivative of the functional (10) at \n\n\n\nd\n\n\n50\n\n\n=\n1\n\u00d7\n10\n\n\n\n-3\n\n\n\nm\n\n everywhere and at \n\n\n\nk\n\n\ns\n\n\n=\n3\n\u00d7\n10\n\n\n\n-3\n\n\n\nm\n\n everywhere, since these are the values used in the test case\u00a0(see Clare et al., 2021). Given (10) and that the meander bedlevel is centred around zero, positive sensitivity means altering the uncertain value here causes more overall bed movement compared to the unperturbed final bedlevel, and vice versa. The figures show that the most positive sensitivity region for \n\n\nk\n\n\ns\n\n\n is at the centre of the channel, whereas for \n\n\nd\n\n\n50\n\n\n it is on the left, at the flow input.\n\nTo better understand the computed local sensitivities, we perturb both uncertain spatially-varying parameters from their original value in the direction of the derivative by adding their respective gradient fields (depicted in Figs.\u00a05(a) and 5(b)) multiplied by 10\u22126. We then calculate the difference between the original and perturbed final bathymetries as spatially varying fields. Note that a positive value indicates that perturbing the uncertain value results in more deposition and a negative value that it results in more erosion.\n\nFig.\u00a06(a) shows that perturbing \n\n\nd\n\n\n50\n\n\n causes increased deposition at the outer bend of the inflow. This can be explained physically: from Fig.\u00a05(a), the perturbation results in a larger sediment grain size at the inflow, which only the faster velocity at the outer bend can erode. This sediment also gets deposited quickly because of its mass, meaning the overall effect is increased deposition in this area. In the rest of the domain, the perturbation in \n\n\nd\n\n\n50\n\n\n accentuates the sediment transport patterns already present in the original final bedlevel (see Fig.\u00a02), which is a sensible result.\nPerturbing \n\n\nk\n\n\ns\n\n\n causes increased deposition at the inflow and increased erosion at the outflow and within this trend more deposition at the inner bend and more erosion at the outer bend, as shown in Fig.\u00a06(b). This has a physical explanation, as increasing the friction in a region decreases the velocity there, leading to increased deposition, with the inverse also true. The different behaviour at the inner and outer bend can be explained by two reasons: (i) from Fig.\u00a05(b), the friction perturbation is lower at the outer bend, meaning flow is pushed towards this region which leads to increased erosion; (ii) the velocity is naturally faster at the outer bend due to the helical flow effect, and thus is less affected by the friction increase.\n\nThus, the results in this section demonstrate that our Thetis- pyadjoint framework can accurately analyse the sensitivity of our model to uncertain spatially-varying parameters for a complex test case.\n\n\n\n5\nOptimum parameter calibration for a migrating trench\nAs discussed in Section\u00a02.1, adjoint methods can also be used to calibrate for uncertain parameters. We illustrate this by considering a migrating trench which has already been verified and validated for our hydro-morphodynamic model in\u00a0Clare et al. (2021) and for which experimental data exists in\u00a0Van Rijn (1980). Throughout this section, unless otherwise stated, we use the set-up of\u00a0Clare et al. (2021) with a mesh of \n\n\u0394\nx\n=\n0.25\n\nm\n\n and \n\n\u0394\ny\n=\n0.2\n\nm\n\n and a morphological acceleration factor, \n\n\nm\n\n\nf\n\n\n, of 100.\n\n5.1\nDual twin experiment\nWe first conduct a dual-twin experiment, where the \u2018true\u2019 output is generated by a previous model run. Thus, we know the value of the \u2018uncertain\u2019 parameter and can verify that our framework can reconstruct it. Given we are not trying to match with experimental data and for reasons of time, the simulation is only run for 5h instead of the full experimental time of 15h.\nTo be consistent between the dual twin experiment and the calibration in Section\u00a05.2, we assume here that the \n\n\nz\n\n\nb\n\n\ntrue\n\n\n profile is only known at certain locations, as is the case with real world data. Following\u00a0Saito et al. (2011), we extract the bedlevel at these locations by multiplying the model bedlevel and the \u2018true\u2019 \n\n\nz\n\n\nb\n\n\n by a Gaussian function centred at the experimental data locations, \n\n\nx\n\n\ni\n\n\n\n\n\n(20)\n\n\n\n\n\n\nz\n\n\n\u02c6\n\n\n\n\nb\n\n\n\n(\nx\n;\n\n\nx\n\n\ni\n\n\n)\n\n=\n\n\nz\n\n\nb\n\n\n\u00d7\n\n\n\n\nexp\n\n\n\u2212\n50\n\n\n\n(\nx\n\u2212\n\n\nx\n\n\ni\n\n\n)\n\n\n\n2\n\n\n\n\n\n\n,\n\n\n\nwhere the exponent is scaled by 50 to ensure the base of the Gaussian function is narrow around \n\n\nx\n\n\ni\n\n\n and we use our knowledge of the test case to assume no variation in the \n\ny\n\u2212\n\ndirection. Thus, the general functional (12) from Section\u00a02.1 becomes \n\n(21)\n\n\n\n\nJ\n\n\ninv\n\n\n\n(\n\n\nz\n\n\nb\n\n\n,\nm\n)\n\n=\n\n\n1\n\n\n2\n\n\n\n\n\u2211\n\n\ni\n=\n1\n\n\nk\n\n\n\n\n\u03b1\n\n\ni\n\n\n\n\n\n\n\n\u222b\n\n\n\u03a9\n\n\n\n\n\n|\n\n\n\n\nz\n\n\n\u02c6\n\n\n\n\nb\n\n\nmodel\n\n\n\n(\nx\n;\n\n\nx\n\n\ni\n\n\n)\n\n\u2212\n\n\n\n\nz\n\n\n\u02c6\n\n\n\n\nb\n\n\ntrue\n\n\n\n(\nx\n;\n\n\nx\n\n\ni\n\n\n)\n\n|\n\n\n\n2\n\n\n\nd\nx\n\n\n\n\n\u222b\n\n\n\u03a9\n\n\n\n\n\n|\n\n\nexp\n\n\n\u2212\n50\n\n\n\n(\nx\n\u2212\n\n\nx\n\n\ni\n\n\n)\n\n\n\n2\n\n\n\n\n|\n\n\n\n2\n\n\n\nd\nx\n\n\n,\n\n\n\nwhere \nk\n is the number of experimental data points and the Gaussian function has been normalised. Recall that \n\n\n\u03b1\n\n\ni\n\n\n is a user-defined scaling factor which here we set equal to 1000 for all \ni\n. The integral over time in (12) is unnecessary because the experimental data only exists at one point in time.\nOne of the advantages of adjoint methods is that the number of uncertain parameters has almost no effect on the computational cost and therefore here we choose to reconstruct multiple uncertain reference parameters at once. Note that unlike in the previous section, we consider them to be spatially-constant. For our uncertain parameters, we choose \n\n\nd\n\n\n50\n\n\n and \n\n\nk\n\n\ns\n\n\n because they are both key in determining sediment transport rate (see Section\u00a04), the sediment density \n\n\n\u03c1\n\n\ns\n\n\n because it follows that if sediment size is uncertain then sediment density may also be uncertain, and the diffusivity parameter \n\n\n\u03b5\n\n\ns\n\n\n because\u00a0Clare et al. (2021) show this test case is very sensitive to it. To generate the \u2018true\u2019 output, we use the values \n\n\n\n\u03c1\n\n\ns\n\n\n=\n2000\n\nkg\n\nm\n\n\n\n\u22123\n\n\n\n, \n\n\n\nd\n\n\n50\n\n\n=\n2\n\u00d7\n10\n\n\n\n-4\n\n\n\nm\n\n, \n\n\n\nk\n\n\ns\n\n\n=\n0.01\n\nm\n\n and \n\n\n\n\u03b5\n\n\ns\n\n\n=\n0.01\n\nm\n\n\n\n2\n\n\n\ns\n\n\n\n\u22121\n\n\n\n. We then use \n\n\n\n\u03c1\n\n\ns\n\n\n=\n2650\n\nkg\n\nm\n\n\n\n\u22123\n\n\n\n, \n\n\n\nd\n\n\n50\n\n\n=\n1.6\n\u00d7\n10\n\n\n\n-4\n\n\n\nm\n\n, \n\n\n\nk\n\n\ns\n\n\n=\n0.025\n\nm\n\n and \n\n\n\n\u03b5\n\n\ns\n\n\n=\n0.15\n\nm\n\n\n\n2\n\n\n\ns\n\n\n\n\u22121\n\n\n\n as the initial guesses to start the optimisation algorithm. Note, because these four parameters have different orders of magnitude, to ensure the optimisation algorithm works, we scale them by multiplying them by one over their order of magnitude so that the scaled parameters all have order of magnitude of 1. Naturally, within the forward model, these are then re-scaled to ensure physically correct results.\n\nFig.\u00a07(a) shows that the reduced functional (21) decreases at each iteration of the optimisation algorithm (11). This results in a general error reduction in the scaled \n\n\n\u03c1\n\n\ns\n\n\n, \n\n\nd\n\n\n50\n\n\n, \n\n\nk\n\n\ns\n\n\n and \n\n\n\u03b5\n\n\ns\n\n\n shown in Fig.\u00a07(b). However, this reduction is not always smooth, as is common in similar non-linear multi-parameter problems. A better metric to look at is the total error (i.e. the sum of the error from the four individual parameters) which does decrease more uniformly. Furthermore, at the final iteration, the error between the reconstructed and actual scaled values has an approximate order of magnitude of 10\u22123 or better for all four parameters, corresponding to an unscaled order of magnitude error of approximately \n\n1\n\u00d7\n10\n\n\n\n-2\n\n\n\nkg\n\nm\n\n\n\n\u22123\n\n\n\n for \n\n\n\u03c1\n\n\ns\n\n\n, \n\n1\n\u00d7\n10\n\n\n\n-9\n\n\n\nm\n\n for \n\n\nd\n\n\n50\n\n\n, \n\n1\n\u00d7\n10\n\n\n\n-5\n\n\n\nm\n\n for \n\n\nk\n\n\ns\n\n\n and \n\n1\n\u00d7\n10\n\n\n\n-5\n\n\n\nm\n\n\n\n2\n\n\ns\n\n\n\n\u22121\n\n\n\n for \n\n\n\u03b5\n\n\ns\n\n\n. A smaller error could be achieved by reducing the tolerance in the optimisation algorithm, but these errors are already much smaller than mesh or model error. Thus, they demonstrate that adjoint methods can be used to calibrate for multiple spatially-constant parameters in the hydro-morphodynamic model in one simulation.\n\n\n\n\n\n5.2\nCalibration of parameters for a laboratory test case\nFollowing the verification of our adjoint framework, we can now use the same method to perform uncertain parameter calibration with experimental data, for the four uncertain parameters \n\n\n\u03c1\n\n\ns\n\n\n, \n\n\nd\n\n\n50\n\n\n, \n\n\nk\n\n\ns\n\n\n and \n\n\n\u03b5\n\n\ns\n\n\n. We use the same migrating trench test case set-up as in Section\u00a05.1 but now run the simulation for the full experimental time (15h) and set the true values in (21) to be the experimental data in\u00a0Van Rijn (1980). As in Section\u00a05.1, we scale the parameters so that they all have an approximate order of magnitude of 1. Furthermore, to ensure the optimum values obtained are physical and do not blow up during the optimisation, we add Tikhonov regularisation to (21): \n\n(22)\n\n\n\n\n\nJ\n\n\ninv\n\n\n\n(\n\n\nz\n\n\nb\n\n\n,\nm\n)\n\n=\n\n\n\u2211\n\n\ni\n=\n1\n\n\nk\n\n\n\n\n\u03b1\n\n\ni\n\n\n\n\n\n\n\n\u222b\n\n\n\u03a9\n\n\n\n\n\n|\n\n\n\n\nz\n\n\n\u02c6\n\n\n\n\nb\n\n\nmodel\n\n\n\n(\nx\n;\n\n\nx\n\n\ni\n\n\n)\n\n\u2212\n\n\n\n\nz\n\n\n\u02c6\n\n\n\n\nb\n\n\ntrue\n\n\n\n(\nx\n;\n\n\nx\n\n\ni\n\n\n)\n\n|\n\n\n\n2\n\n\n\nd\nx\n\n\n\n\n\u222b\n\n\n\u03a9\n\n\n\n\n\n|\n\n\nexp\n\n\n\u2212\n50\n\n\n\n(\nx\n\u2212\n\n\nx\n\n\ni\n\n\n)\n\n\n\n2\n\n\n\n\n|\n\n\n\n2\n\n\n\nd\nx\n\n\n\n\n\n\n\n\n\n+\n\n\n\n\u2211\n\n\ni\n=\n1\n\n\n\n\nN\n\n\nin\n\n\n\n\n\n\n\n\u03b2\n\n\ni\n\n\n\n\n\u222b\n\n\n\u03a9\n\n\n\n\n\n|\n\n\nm\n\n\ni\n\n\n|\n\n\n\n2\n\n\n\nd\nx\n,\n\n\n\n\nwhere the \u2018true\u2019 value is taken to be the experimental data in\u00a0Van Rijn (1980), \n\n\n\u03b1\n\n\ni\n\n\n is equal to 1000, and \n\n\n\u03b2\n\n\ni\n\n\n is equal to 10\u22127 for \n\n\n\u03c1\n\n\ns\n\n\n, \n\n\nd\n\n\n50\n\n\n and \n\n\nk\n\n\ns\n\n\n, and equal to 5 \n\u00d7\n 10-8 for \n\n\n\u03b5\n\n\ns\n\n\n, because\u00a0Clare et al. (2021) shows \n\n\n\u03b5\n\n\ns\n\n\n has a large impact on the final result. For the initial guesses for the optimisation algorithm, we use the original parameter values for this test case from\u00a0Clare et al. (2021).\n\nClare et al. (2021) conduct parameter calibration for this test case for \n\n\n\u03b5\n\n\ns\n\n\n using trial-and-error, and estimate an optimum value of \n\n\n\n\u03b5\n\n\ns\n\n\n=\n0.15\n\nm\n\n\n\n2\n\n\n\ns\n\n\n\n\u22121\n\n\n\n. Therefore, as a first test, \n\n\n\u03b5\n\n\ns\n\n\n is considered uncertain. Fig.\u00a08(a) shows the functional decreases with each iteration, and Fig.\u00a08(b) shows how this leads to the convergence of \n\n\n\u03b5\n\n\ns\n\n\n to an optimal value of 0.183m2\ns\u22121, which is close to the value estimated in\u00a0Clare et al. (2021), giving further confidence in our adjoint framework.\n\nGiven this result, we now calibrate for the optimum values of all four parameters. Fig.\u00a09(a) shows that the functional value decreases with each iteration due to the error term value in the functional decreasing. Note that the regularisation term increases in value with each iteration but remains substantially lower than the error term throughout. Fig.\u00a09(b) shows that the decrease in the overall functional value leads to the convergence of all four parameters to optimal values. The parameters shown in the figure are the scaled ones and therefore the actual optimum value of \n\n\n\u03c1\n\n\ns\n\n\n is 2511kgm\u22123; of \n\n\nd\n\n\n50\n\n\n is 1.99 \n\u00d7\n 10-4\nm; of \n\n\nk\n\n\ns\n\n\n is 0.0341m and of \n\n\n\u03b5\n\n\ns\n\n\n is 0.321m2\ns\u22121. Note the optimum \n\n\n\u03b5\n\n\ns\n\n\n value here is much greater than its value when we optimised for \n\n\n\u03b5\n\n\ns\n\n\n on its own, showing a clear difference in results if parameters are optimised individually or in a group. To summarise, the optimum sediment is less dense, larger, erodes less easily and diffuses at a greater rate than assumed in the original simulation.\n\n\nFig.\u00a010(a) compares the final bedlevel obtained using either all four optimum parameters, just the optimum diffusivity parameter, or the original values from\u00a0Villaret et al. (2016). It shows that \n\n\n\u03b5\n\n\ns\n\n\n has the largest impact on accuracy, but that using optimum choices for all four parameters improves the accuracy further, in particular the gradient of the slope. In order to test the robustness of these optimum parameters we re-run the test case using a coarser mesh of \n\n\u0394\nx\n=\n1\n\nm\n\n (compared to the \n\n\u0394\nx\n=\n0.25\n\nm\n\n mesh used originally), although the initial trench profile is not well defined for this coarser mesh. Fig.\u00a010(b) shows the accuracy improvements with the coarser mesh are similar to before, including the improved gradient from using four optimal parameters. This suggests these optimum parameters are not resolution dependent and thus that our adjoint framework can accurately calibrate multiple uncertain scalar parameters in this test case.\n\n\n\n\n\n\n6\nTsunami inversion\nAs our final test case, we consider a tsunami-like event. Tsunami events are often difficult to simulate due to the large array of uncertain parameters, especially for historical scenarios where the only record is in the form of sediment deposits\u00a0(e.g. Tang et al., 2018; Dourado et al., 2021). These uncertain parameters are often estimated by using educated guesses in a forward model and adjusting their value accordingly by comparing the model results with the data\u00a0(see e.g. Dourado et al., 2021). A more sophisticated approach is to use tsunami inversion models such as TSUNFLIND\u00a0(see Tang and Weiss, 2015) which has been coupled with statistical methods in\u00a0Tang et al. (2018). However, these inversion models do not have the full capabilities of standard forward models, for example TSUNFLIND cannot model bedload transport. Tsunami events are therefore an ideal scenario on which to apply our hydro-morphodynamic adjoint framework. For our test case, we consider the experiment in\u00a0Kobayashi and Lawrence (2004), where a series of tsunami-like solitary waves break over a sloping beach.\n\n6.1\nForward model set-up and results\nFor this test case not all of the domain is wet, hence we use the wetting-and-drying version of the hydro-morphodynamic model detailed in Section\u00a03.1. The beach slope also requires the use of the sediment slide mechanism (detailed in\u00a0Clare et al., 2022).\nFollowing\u00a0Kobayashi and Lawrence (2004), the wave is simulated by imposing the following free surface elevation boundary condition at the open boundary \n\n(23)\n\n\n\u03b7\n\n(\nt\n)\n\n=\n\n\nH\n\n\nwave\n\n\n\n\nsech\n\n\n2\n\n\n\n\n\n\n\n\n3\n\n\nH\n\n\nwave\n\n\n\n\n4\nh\n\n\n\n\n\n\n\n\ng\n\n(\n\n\nH\n\n\nwave\n\n\n+\nh\n)\n\n\n\n\n\nh\n\n\n\n(\nt\n\u2212\n\n\nt\n\n\nmax\n\n\n)\n\n\n\n+\n\n\n\u03b7\n\n\ndown\n\n\n,\n\n\n\nwhich causes a tsunami-like solitary wave to travel into the domain. Here \n\n\nH\n\n\nwave\n\n\n is the average wave height, \nh\n the still water depth, \n\n\nt\n\n\nmax\n\n\n the arrival time of the wave crest at the open boundary and \n\n\n\u03b7\n\n\ndown\n\n\n the initial decrease of the elevation at the beginning of the simulation (also the initial elevation in the domain). Our model cannot currently simulate shoaling and breaking waves and thus a relatively high viscosity value of 0.8m2s\u22121 is used in the hydrodynamics to dissipate energy. This is standard practice, for example\u00a0Li and Huang (2013) view viscosity as a model calibration parameter for energy dissipation, rather than a physical parameter.\nThe remaining parameters, taken from\u00a0Kobayashi and Lawrence (2004) and\u00a0Li and Huang (2013), are summarised in Table\u00a02. Note\u00a0Li and Huang (2013) run the simulation for 40s with \n\n\n\nt\n\n\nmax\n\n\n=\n23.9\n\ns\n\n for each solitary wave, but the system is stationary for the first 20s. Therefore we only run our model simulation for 20s with \n\n\n\nt\n\n\nmax\n\n\n=\n3.9\n\ns\n\n for each solitary wave. Furthermore, we found that our final model results are fairly insensitive to the morphological acceleration factor, \n\n\nm\n\n\nf\n\n\n. Hence, here we set \n\n\nm\n\n\nf\n\n\n equal to four, meaning we only need to model two solitary waves to simulate the bed changes caused by the eight waves in the experiment.\n\n\nFig.\u00a011 shows there is good agreement between our forward model results and the experimental data obtained in\u00a0Kobayashi and Lawrence (2004). For comparison, this agreement is more than competitive with that shown between the results and the experimental data for a similar test case in\u00a0Kazhyken et al. (2021) which uses a dispersive wave model. Thus, our forward model is validated for this test case and we proceed to using the adjoint framework.\n\n\n\n\n\n6.2\nReconstructing reference wave from sediment deposits\nAs discussed in Section\u00a01, the adjoint method is ideally suited to cases where there are multiple uncertain parameters. In this section, we consider the inflow tsunami-like wave boundary condition to be the uncertain time-varying spatially-constant parameter, and use a dual twin experiment to verify our adjoint framework\u2019s ability to reconstruct it. For the dual twin, the \u2018true\u2019 data is the bedlevel generated by the reference wave (23) at every timestep. Given we are not matching with experimental data and for reasons of time, here we only run one wave in our simulation but still use a morphological acceleration factor of four, meaning this is equivalent to simulating four waves. For our initial guess for the optimisation algorithm, we assume the wave was caused by a sudden rupture in the Earth\u2019s crust causing a discontinuous wave profile \n\n(24)\n\n\n\n\n\u03b7\n\n\ninitial\n\n\n\n(\nt\n)\n\n=\n\n\n\n\n\n\n0.05\n\nm\n\n\n\nt\n<\n7.5\n\ns\n,\n\n\n\n\n0\n\nm\n\n\n\notherwise\n,\n\n\n\n\n\n\n\n\n\nas shown in Fig.\u00a013. Note, only the first 10s of the wave are considered to be uncertain because we know the \u2018true\u2019 free surface perturbation is only non-zero at the boundary between 2s and 6s, so considering the whole time region is unnecessary.\nFor the reduced functional, we use (22) with a time integral and \n\n\n\n\u03b2\n\n\ni\n\n\n=\n1\n\n\n0\n\n\n\u2212\n4\n\n\n\n in the regularisation term, and centre the Gaussian functions (20) at the experimental data locations in\u00a0Kobayashi and Lawrence (2004). Fig.\u00a012(a) shows that the reduced functional, and the error and regularisation terms within it, decrease as the number of iterations of the optimisation algorithm increases. Notably, even when the value of the regularisation term is larger than that of the error term, the latter is still decreasing. This minimisation of the functional results in the convergence of the L1 error norm between the reference input and model input waves, as shown in Fig.\u00a012(b). The effect of the minimisation on the model wave itself is shown in Fig.\u00a013, revealing the model wave has the correct general shape already by iteration 50. Finally, the figure shows the final iteration approximates the reference wave very well, with the only discrepancy being that our adjoint framework slightly under-predicts the waveheight. This is because as the waveheight approaches the right range, the impact of the waveheight on the underlying bed decreases. This can be seen in Fig.\u00a012(a) which shows that the difference between the error term value at iteration 50 and the value at the final iteration is small, despite the fact that Fig.\u00a013 shows that the difference in the waveheight of the two waves at these iterations is relatively large. To summarise, this test case has shown that our adjoint framework is capable of reconstructing an input solitary wave from sediment deposits, illustrating that we can apply our framework to real data.\n\n\n\n\n\n\n6.3\nFinding the optimum wave from sediment deposits\nWe now consider the inversion of the wave from the experimental sediment deposit data in\u00a0Kobayashi and Lawrence (2004). This data only exists for the final bedlevel and thus the problem would be ill-posed without regularisation terms in our reduced functional. Moreover, we add a regularisation term to enforce continuity in the wave because using the same reduced functional as in Section\u00a06.2 was observed to result in large jumps in waveheight. Therefore, the reduced functional is \n\n(25)\n\n\n\n\n\nJ\n\n\ninv\n\n\n\n(\n\n\nz\n\n\nb\n\n\n,\nm\n)\n\n=\n\n\n\u2211\n\n\ni\n=\n1\n\n\nk\n\n\n\n\n\u03b1\n\n\ni\n\n\n\n\n\n\n\n\u222b\n\n\n\u03a9\n\n\n\n\n\n|\n\n\n\n\nz\n\n\n\u02c6\n\n\n\n\nb\n\n\nmodel\n\n\n\n(\nx\n;\n\n\nx\n\n\ni\n\n\n)\n\n\u2212\n\n\n\n\nz\n\n\n\u02c6\n\n\n\n\nb\n\n\ntrue\n\n\n\n(\nx\n;\n\n\nx\n\n\ni\n\n\n)\n\n|\n\n\n\n2\n\n\n\nd\nx\n\n\n\n\n\u222b\n\n\n\u03a9\n\n\n\n\n\n|\n\n\nexp\n\n\n\u2212\n50\n\n\n\n(\nx\n\u2212\n\n\nx\n\n\ni\n\n\n)\n\n\n\n2\n\n\n\n\n|\n\n\n\n2\n\n\n\nd\nx\n\n\n\n\n\n\n\n\n\n\n+\n\n\n\n\u2211\n\n\ni\n=\n1\n\n\n\n\nN\n\n\nin\n\n\n\n\n\n\n\n\u03b2\n\n\ni\n\n\n\n\n\u222b\n\n\n\u03a9\n\n\n\n\n\n|\n\n\nm\n\n\ni\n\n\n|\n\n\n\n2\n\n\n\nd\nx\n+\n\n\n\n\u2211\n\n\ni\n=\n2\n\n\n\n\nN\n\n\nin\n\n\n\n\n\n\n\n\u03b3\n\n\ni\n\n\n\n\n\u222b\n\n\n\u03a9\n\n\n\n\n\n|\n\n\nm\n\n\ni\n\n\n\u2212\n\n\nm\n\n\ni\n\u2212\n1\n\n\n|\n\n\n\n2\n\n\n\nd\nx\n,\n\n\n\n\nwhere \n\n\nm\n\n\ni\n\n\n represents the input wave at time \n\n\nt\n\n\ni\n\n\n, the scalar parameters are \n\n\n\n\u03b1\n\n\ni\n\n\n=\n1\n\n, \n\n\n\n\u03b2\n\n\ni\n\n\n=\n5\n\u00d7\n10\n\n\n\n-6\n\n\n\n and \n\n\n\n\u03b3\n\n\ni\n\n\n=\n5\n\u00d7\n10\n\n\n\n-2\n\n\n\n, and the Gaussian functions are centred at the experimental data locations.\nTo ensure stability, we enforce the wave elevation to be 0m at both the start and end of the simulation, and only consider the wave to be uncertain during the middle of the simulation, where we initialise the optimisation algorithm using an initial elevation of 0.05m (see Fig.\u00a015(a)). This replicates the initial guess of a discontinuous wave profile from Section\u00a06.2. All model parameters are as described in Section\u00a06.1 and we simulate two solitary waves with a morphological acceleration factor of four because we are comparing against real data.\n\nFig.\u00a014 shows the reduced functional (25) decreases as the number of iterations of the optimisation algorithm increases, causing the value of the error term in (25) to decrease. The trend in the magnitude and continuity regularisation terms is less uniform, but can be loosely interpreted as follows: for the first 20 iterations the optimisation algorithm minimises the continuity term, from 20 to 30 iterations it minimises the error term at the expense of the magnitude term and after iteration 30 it again minimises the continuity term. This interpretation is confirmed by the differences between iterations 20, 30 and the final one in Fig.\u00a015(a).\n\nFig.\u00a015(a) compares the optimum wave found in this section with the theoretical solitary wave (23) that\u00a0Kobayashi and Lawrence (2004) used to describe the incoming wave observed in their experiment. It shows our optimum wave has a wider base and a smaller amplitude than (23). Consequently, Fig.\u00a015(b) shows the simulated bedlevel from the optimum wave has very good agreement with the experimental data and this agreement is much better than that with the theoretical wave, particularly in the area of deposition between 5m and 10m. Therefore, our adjoint framework can be used to invert accurately for tsunami-like waves from final bedlevel sediment deposits, which is a very promising result.\n\n\n\n\n\n\n\n7\nConclusion\nIn this work, we have developed the first freely available and fully flexible adjoint hydro-morphodynamic framework. By fully flexible we mean that the use of pyadjoint allows us to assess any parameter uncertainty in the hydro-morphodynamic model with respect to any functional. Hence, we have shown that our framework can perform calibration, inversion or sensitivity analysis of multiple uncertain parameters in a single model run and have verified these capabilities using dual-twin experiments. Moreover, we showed that these inversion and calibration capabilities can produce physically-sensible results with experimental data and that the optimum parameters obtained using these methods result in more accurate final bedlevels. Notably, we showed that our approach is capable of reconstructing the shape and magnitude of incoming waves from the resulting sediment deposits. The next stage of our work will be to apply our approach to historical tsunami sediment deposits to invert for the tsunami wave. This will contribute to a better understanding of historical tsunami events and help mitigate the impacts of future events.\nMoreover, as the dimension of the uncertain parameters has little effect on the computational cost, our framework is capable of managing the uncertainty of spatially-varying parameters. Thus, we showed that a single run of our adjoint framework can determine where changing the friction and sediment size causes the greatest bed level change, for example. Obtaining this type of information using other methods, such as Monte Carlo or via the tangent linear approach, is either impractical or much more computationally expensive. Therefore, the knowledge gained through our adjoint framework can be invaluable to a variety of users and stakeholders in understanding and mitigating the impacts of coastal and fluvial hazards.\n\n\nCRediT authorship contribution statement\n\nMariana C.A. Clare: Conceptualisation; Methodology; Software; Formal analysis, Investigation; Writing \u2013 Original Draft; Visualisation. Stephan C. Kramer: Software; Writing \u2013 Review & Editing; Supervision. Colin J. Cotter: Writing \u2013 Review & Editing; Supervision. Matthew D. Piggott: Conceptualisation; Writing \u2013 Review & Editing; Supervision.\n\n","63":"","64":"","65":"","66":"\n\n1\nIntroduction\nThe changing state of the biophysical environment through time and space can be simulated using computer models. Modelling frameworks\n1\n\n\n1\nWe use the term framework loosely, to mean software containing at least data types and algorithms, used for the development of individual models. This includes the case of a software library implementing these, but excludes integration frameworks used for coupling models.\n contain data structures and operations which can be used to develop simulation models in less time, by model developers who do not have to know about the details involved in implementing the data structures and operations. Given the continuous increase in temporal and spatial extent and resolution of datasets, and the subsequent increase of model complexity to incorporate more detailed environmental process descriptions, it is important that modelling frameworks support the development of models that perform and scale well over additional hardware. For some modelling operations good performance and scalability is easier to achieve than others. An important aspect of a modelling operation that potentially limits its performance and scalability when parallelized is the spatial dependency of output values on input values. Parallelizing modelling operations generally involves dividing the spatial domain into partitions, each of which is processed by a separate worker, like an OS thread on a CPU core. In case of spatial dependencies of output values on input values, data must be exchanged between workers. The performance and scalability of such an operation depends on how well workers are able to cooperatively carry out the total amount of work. Examples of modelling operations with spatial dependencies of output values on input values are spreading operations and flow routing operations (Burrough et al., 2015).\nThe current paper concerns routing of material over a D8 flow direction raster using flow accumulation operations. In a D8 flow direction raster, each cell is assigned a direction of one of its 8 neighbours to which it drains (O\u2019Callaghan and Mark, 1984). This results in a dense non-divergent directed acyclic graph of which the main branches correspond with the hydrologic network of streams and rivers. Flow accumulation operations are part of several Earth surface simulation models, examples of which are LISFLOOD (Burek et al., 2013), used for the European Flood Awareness System (EFAS), and the PCR-GLOBWB global water balance model (Sutanudjaja et al., 2017).\nFlow accumulation algorithms that can be found in the literature solve the problem of transporting all material in downstream direction (Ortega and Rueda, 2010; Sten et al., 2016; Barnes, 2017; Cordonnier et al., 2019; Zhou et al., 2019; Kotyra et al., 2021), but sometimes flow accumulation operations are required that use a criterion to split the total amount of material entering a cell (inflow) into an amount that is transported downstream (outflow) and an amount that remains in the cell (residue, Table\u00a01, (Karssenberg, 2006)). An example of such a criterion is a threshold representing the minimum amount of material that has to be received by a cell before excess material starts to be transported downstream. Using this operation, henceforth referred to as accu_threshold, the process of Hortonian overland flow (Hendriks, 2010) can be simulated, for example. Examples of other processes that can be simulated by these operations are loss of material while on transport (using accu_fraction), flow through a sewage system (using accu_capacity), and mass movements (using accu_trigger).\n\nThe various existing flow accumulation algorithms have different computational properties. We focus on flow accumulation algorithms that use CPU cores rather than GPU devices. Distributing work over multiple GPUs in multiple cluster nodes complicates the algorithms and makes the implementation less portable. In\u00a0Zhou et al. (2019) a review of serial algorithms is provided and a new algorithm is presented that offers better performance than those reviewed. Although this algorithm only considers the basic flow accumulation function, without using a criterion, it can be extended to support the flow accumulation operations that do use one. One limitation of the algorithm is that it is not capable of using multiple CPU cores, which limits its applicability to relatively small problems. In\u00a0Kotyra et al. (2021) it has been concluded that a parallel version of the algorithm by\u00a0Zhou et al. (2019) performs best compared to other parallel algorithms they tested; one limitation of their algorithm is that is not capable of using multiple nodes in a compute cluster. Barnes (2017) presents an approach for distributing flow accumulation computations over multiple processes. For this algorithm to work, the spatial domain is partitioned into rectangular partitions. Like in the case of the algorithm presented in\u00a0Zhou et al. (2019), this algorithm only considers flow accumulation without using a criterion, but this algorithm cannot be easily extended to support the other kinds of flow accumulation operations. The algorithm by\u00a0Barnes (2017) requires that there is a linear relation between the amount of material entering a partition and the amount leaving it. This allows the algorithm to calculate a final result efficiently, in a single concurrent step per partition, without having to iterate over partitions containing upstream parts of large scale streams to partitions containing downstream parts. Given our requirement of being able to use a criterion, we cannot use the final steps of this algorithm. The criteria used by accu_threshold, accu_capacity, and accu_trigger require that the total amount of inflow of material in a cell is known, before the amounts of residue and outflow of material from that cell can be calculated.\nThe fact that, in the general case, flow accumulation results for cells of streams that flow from spatial domain partition to domain partition must be calculated in order, going from upstream to downstream direction, implies that there is a temporal load imbalance between partitions. The larger the flow direction raster and the more partitions involved in calculating the flow accumulation result, the larger this load imbalance can become. This is important when flow accumulation is used in a calculation involving other operations as well, like in a simulation model or a GIS workflow. Performance and scalability of such calculations will be limited when subsequent calculations have to wait on the last partition of the flow accumulation operation to finish. In case of such a synchronization point, workers like CPU cores or even whole cluster nodes may be drained of useful work to do. Ideally, partitions for which flow accumulation operation calculations have finished should already participate in calculations of other operations.\nWe call a set of modelling operations composable when the time it takes the set to finish executing is shorter than the sum of their individual latencies. To the best of our knowledge no existing flow accumulation algorithm has been designed taking into account that the flow accumulation operation will be combined with other operations.\nThe problem we try to solve is the parallelization and distribution of a set of flow accumulation algorithms, some of which use a criterion for determining how much material flows downstream from each cell. As an additional requirement, we want the resulting operations to be composable with other operations. Our objective, therefore, is to design a general scheme for flow accumulation algorithms that enables them to perform well, scale well, and compose well with other operations.\nTo reach our objective, we make use of an approach for writing parallel and distributed software called asynchronous many-tasks (AMT), as implemented in the HPX C\uff0b\uff0b\u00a0library for parallelism and concurrency (Kaiser et al., 2020). One advantage of using AMT is that it allows the software developer to define tasks, representing an amount of work to be performed, to be asynchronously scheduled, potentially allowing work from multiple operations to be scheduled concurrently and executed in parallel. We designed and expressed our new algorithms in terms of AMT concepts and the HPX API, added prototype implementations to the LUE\n2\n\n\n2\nLUE stands for Life, the Universe and Everything, which is the title of one of the books in Douglas Adams\u2019 Hitchhiker\u2019s Guide to the Galaxy \u201ctrilogy\u201d. Here, it refers to the fact that in designing LUE we try to make it applicable in as many contexts as possible.\n modelling framework (de\u00a0Jong et al., 2021), and performed experiments to assess their strong and weak scalability, and their composability.\nOur results show that our AMT-based algorithms are capable of using additional hardware efficiently, and perform well when combined. The strong and weak scaling efficiencies when scaling a flow accumulation operation of six physical CPU cores in a NUMA node are 83% and 84% respectively. When scaling a case-study model over four cluster nodes containing 48 physical CPU cores each, the strong and weak scaling efficiencies are 73% and 84% respectively. Also, the new algorithms are composable: the latency of executing two flow accumulation operations combined is lower than the sum of their individual latencies.\nThe organization of this paper is as follows. We start with an introduction of AMT, HPX and the LUE modelling framework (Section\u00a02). We then describe our flow accumulation algorithms (Section\u00a03), and the experiments we performed (Section\u00a04). The results of the experiments can be found in Section\u00a05. We finish the paper with a discussion of the results and our conclusions (Section\u00a06).\n\n\n2\nAMT, HPX, And the LUE modelling framework\nWe start with a brief introduction of some major aspects of AMT, the HPX implementation thereof (Kaiser et al., 2020), and the use of AMT and HPX in the LUE environmental modelling framework (de\u00a0Jong et al., 2021).\nWith the AMT programming model the software developer defines relatively small tasks of work that need to be performed, and the dependencies between them. Once tasks have been created, the AMT runtime system is responsible for executing them in a correct order, using the available workers (e.g.\u00a0CPU cores). To increase the chance that an AMT program performs and scales well, it should create enough tasks that are ready to run to keep all workers busy. Tasks are therefore spawned asynchronously, and they must have as few dependencies as possible between them.\nHPX is an implementation of the AMT programming model and runtime. It is an open source software library written in portable C\uff0b\uff0b\u00a011\/14\/17\/20 code. With HPX, every system, ranging from laptops to compute clusters, is represented as a single abstract machine, containing one or more localities. For our purposes, localities are equal to operating system processes, so we will use the more familiar term process. Each process exposes plain actions and component actions. Plain actions are globally accessible free functions without state, and component actions are globally accessible member functions of objects with state. The software developer uses the HPX API to define these actions. An HPX task is a lightweight HPX thread. A task can call an action and can execute locally or remotely, in a different process. When an HPX task is spawned asynchronously, a future object to the result is returned immediately. This object represents a result that may not be computed yet, and it allows one or more continuations to be attached, which get called once the future they are attached to becomes ready. Futures can be composed to represent relations between tasks. HPX components are globally addressable (using an ID) instances of classes. A component server is the actual instance, located in a process. A component client is a lightweight object providing access to a possibly remote server instance. It is semantically equal to a shared future to the ID of the remote server instance. HPX channel components allow asynchronous communication between different tasks in different processes.\nLUE is a modelling framework targeted at domain experts, like hydrologists, soil scientists and biologists. The modelling operations are inspired by map algebra (Tomlin, 1990). LUE currently contains a set of local, focal, zonal, and global operations. In LUE models, time is typically discretized in time steps, and space in raster cells. For model developers LUE provides a Python language binding, which allows them to use the common procedural programming paradigm to implement models. Models run unchanged on laptops and compute clusters. To the modeller, LUE models look similar to models created with other map algebra implementations, like ArcGIS (Esri, 2021), GDAL (GDAL\/OGR\u00a0contributors, 2021), GRASS (Neteler et al., 2012), and PCRaster (van Deursen et al., 2019). The core data structure used in the current LUE API is the partitioned array. A partitioned array contains array partition clients referring to partition servers containing a rectangular section of the overall array. In the implementation, LUE modelling operations attach continuations to array partition clients. These continuations asynchronously spawn work and immediately return new array partition clients, to be used as input for other operations. Depending on the dependencies between the array partition clients, tasks from multiple modelling operations can be scheduled for execution at the same time, in parallel. LUE distributes array partition servers, containing the array data, evenly over the processes. Work generated by modelling operations translate input partitions to output partitions, and execute in the same processes as the partitions they operate on. In case of local and focal operations, an even distribution of partitions over processes results in an even distribution of computational load.\n\n\n3\nFlow accumulation\nOur flow accumulation algorithms combine and extend the efficient algorithm of\u00a0Zhou et al. (2019) and the distributed algorithm of\u00a0Barnes (2017). In this section we describe our algorithms and show how we applied AMT. We use the accu_threshold operation as an example. The other flow accumulation algorithms use the same approach. A call to this operation looks like this: \n\n\n\n\n\nLike the flow direction, the material and threshold arguments vary through space, and are represented by arrays.\n\n3.1\nOverview\nOur algorithm works with partitioned arrays (Section\u00a02). Cells in a partitioned flow direction array can be classified according to their location within the flow direction graph and within a partition (Figs.\u00a01 and 2). Cells that only receive material from upstream cells that are located in the same partition are called intra-partition stream cells. Cells that receive material from at least one upstream cell that is located in another partition are called inter-partition stream cells. A partition output cell provides material for a cell in a neighbouring partition.\n\n\nPer partition, flow accumulation calculations start with intra- partition stream cells and continue with inter-partition stream cells. Within the intra-partition stream cells, calculations start at ridge cells, which do not receive input from another cell, and terminate at inter-partition stream cells, sinks, or partition output cells. Within the inter-partition stream cells, calculations start at partition input cells and stop at sinks, or partition output cells.\nA flow accumulation calculation for a cell starts with adding the external material \u2013 passed in as an argument to the operation \u2013 to the amount of inflow material that the cell received from upstream, if any. Based on the threshold criterion also passed in, the total amount of material in the cell is then split into an amount of residue and an amount of outflow material.\nIn order to be able to visit all cells in the correct order, going from upstream to downstream in the flow direction graph, we first calculate the number of directly neighbouring cells that drain into each cell. In\u00a0Zhou et al. (2019) this is called the number of input drainage paths (NIDP). Cells with an NIDP of zero do not receive material from any neighbour. Most of these cells are ridge cells, but some may be positioned at the border of the raster and part of a large scale stream flowing into the area represented by the raster. For the purpose of our algorithm, this latter kind of cells can be treated as ridge cells. Cells with an NIDP of eight must be sink cells. Cells with an NIDP between one and seven are junction cells, some of which may be sink cells \u2013 surrounded by at least one no-data cell \u2013 but the majority will drain to a downstream cell.\nGiven the NIDP values of each cell, per partition, ridge cells can be found and used as starting points for flow accumulation calculations. Once calculations for a ridge cell have finished and assuming it has a downstream cell, the resulting outflow is added to the material of the downstream cell and its NIDP value is decreased by one. If the updated NIDP value of the downstream cell has become zero, the current cell is the last cell draining into it. In that case, the flow accumulation procedure is repeated for that cell. The procedure terminates when a downstream cell is encountered which is either a junction cell with an updated NIDP value that is larger than zero, a sink cell, or a partition output cell. Once all ridge cells in a partition have been used as starting points this way, flow accumulation calculations are finished for all intra-partition stream cells. All material \u2018produced\u2019 by these cells has been \u2018deposited\u2019 in inter-partition stream cells, sink cells, and partition output cells.\nNext, for each partition, the input cells for which material is available in the corresponding partition output cells in the neighbouring partition(s) are used as starting points for the same flow accumulation procedure as used during the calculations for the intra-partition stream cells. Once calculations for all input cells and the inter-partition stream cells downstream of them have finished, the flow accumulation calculations for the partition have finished.\nConcluding, our algorithm performs these three steps for each partition: (1) calculate the NIDP for each cell, (2) calculate the flow accumulation results for the intra-partition stream cells, and (3) calculate the flow accumulation results for the inter-partition stream cells.\nCompared to the algorithm by\u00a0Zhou et al. (2019), we have split the calculations in two steps: one to solve the flow accumulation for intra-partition stream cells, and one for solving the flow accumulation for inter-partition stream cells. This is necessary since we use a partitioned array, and in general, partitions contain inter-partition stream cells that can only be calculated once the flow accumulation calculations for all upstream cells have finished. Note that in general, partitions cannot be ordered according to their position along an inter-partition stream. A large scale stream may visit the same partition multiple times and multiple large scale streams may pass through the same partition (Fig.\u00a02).\nCompared to the algorithm by\u00a0Barnes (2017), we have changed the procedure for calculating the results for the inter-partition stream cells. As described in Section\u00a01, this cannot be done in a single concurrent step, but requires multiple steps, propagating material from partition input cells and through inter-partition stream cells as the material becomes available from upstream partition output cells.\n\n\n3.2\nParallelization\nA number of concurrent aspects can be identified in the above procedure. First, the NIDP values can be calculated in parallel for each partition. A small amount of information about which partition output cells flows into which partition input cells must be communicated. Second, the flow accumulation results for intra-partition stream cells can be calculated in parallel for each partition. Information about material reaching partition output cells must be communicated to allow this material to be used as input for partition input cells in a subsequent step. Third, propagating material from a partition input cell through a partition can be done in parallel for each partition.\n\n\n3.3\nApplication of AMT\nThe next list shows the steps of our flow accumulation algorithm in terms of the AMT approach.\n\n\n\n1.\nCreate channels for exchanging information about partitions between tasks. Each channel server instance is instantiated in the process of the partition for which information is sent.\n\n\n2.\nFor each partition, asynchronously spawn a task to calculate the results of the flow accumulation calculations. Each of the tasks performs these steps: \n\n(a)\nAsynchronously spawn a task that calculates the NIDP for each cell. Use channels to send information about which partition input cell in a neighbouring partition receives material from this partition, to a task monitoring the relevant channel for this neighbouring partition.\n\n\n(b)\nAsynchronously spawn a task that calculates the flow accumulation results for all intra-partition stream cells. Use channels to send information about material flowing into a partition input cell in a neighbouring partition, to a task monitoring the relevant channel for this neighbouring partition.\n\n\n(c)\nAsynchronously spawn a task that calculates the flow accumulation results for all inter-partition stream cells. Again, use channels to send information about material flowing into a partition input cell in a neighbouring partition, to a task monitoring the relevant channel for this neighbouring partition.\n\n\n\n\n\n3.\nReturn partitioned arrays for outflow and residue. Note that these arrays are returned before the flow accumulation calculations have finished. They may not even have started yet.\n\n\n\nAll tasks are spawned asynchronously and return futures to results immediately. Each next task depends on the results of the previous task(s) and will only be created and scheduled for execution by the HPX runtime after these results have become available. Within each algorithmic step, tasks performing work for a certain partition only depend on tasks performing work for directly neighbouring partitions. These dependencies are represented by the channels which are used to exchange information.\nWhen calculating the results for the inter-partition stream cells in a partition, work is only performed once material becomes available for a partition input cell. Until that is the case, the task is automatically suspended by the HPX runtime. It is important to note that the topology of the flow direction graph is not explicitly used to order tasks according to their relative position along the inter-partition streams. Once a task calculating the flow accumulation results for a partition has received and propagated material for all its partition input cells, the work for the partition is done. As soon as this happens, the corresponding outflow and residue result partition clients are marked as ready, and these partitions can be used in subsequent modelling operations.\nAdditional details on our algorithms can be found in Appendix\u00a0A.\n\n\n\n4\nExperiments\nWe performed various experiments to characterize the performance, scalability and composability of our new flow accumulation algorithms. In all experiments, we used the MERIT Hydro dataset\u00a0(Yamazaki et al., 2019) for the African continent. This dataset has a 3 arc-second resolution, which corresponds to almost 90\u00a0m resolution at the equator. It contains 87\u00a0600\u00a0\n\u00d7\n\u00a084\u00a0000 raster cells and represents a realistic high resolution dataset that can be used in global and continental scale modelling studies. All experiments were performed on one or more equivalent cluster nodes (Table\u00a02). Even though the latency of simulation models is a combination of time spent on computing output values and on I\/O, in our experiments we only considered the time spent on the compute part.\n\n\n\n\n4.1\nAlgorithm\nThe algorithms described in Section\u00a03 asynchronously spawn various kinds of tasks with dependencies between them. To gain insights into when these tasks get scheduled at runtime, we generated a trace for a single run of two flow accumulation operations, using the same script as used in the composability experiment (Section\u00a04.3). We performed the experiment on the same dataset as used in the weak scaling experiment over CPU cores of accu_threshold\n (Table\u00a03) when using 6 CPU cores. This array has 30\u00a0000\u00a0\n\u00d7\n\u00a030\u00a0000 cells and contains relatively few no-data cells. The results of this experiment are given and analysed in Section\u00a05.1.\n\n\n4.2\nPerformance and scalability\nTo put the results of the scalability experiments into perspective, we compared the performance of our new operations with the performance of similar operations from the PCRaster environmental modelling framework (Karssenberg et al., 2010). We compared the latencies of a single accu and a single accu_threshold call for the southern half of the African continent (56\u00a0059\u00a0\n\u00d7\n\u00a044\u00a0956 raster cells). The experiments were performed on a single CPU core. All variables (inflow, threshold, outflow, and residue) were represented by arrays containing 32\u00a0bit floating point elements.\nWe performed scalability experiments, on individual calls to a flow accumulation operation, and on a case-study model in which a flow accumulation operation was combined with several local operations, with data dependencies between them. The relative fraction of local operations versus flow accumulation operations used in the case-study model is comparable to existing hydrological models in which flow accumulation is used, like the PyCatch catchment model (Lana-Renault and Karssenberg, 2013) and the PCR-GLOBWB global water balance model (Sutanudjaja et al., 2017). In the case-study model a call to accu_threshold is surrounded by 57 local operations. A feedback variable is used to add a data dependency between operations from consecutive time steps.\nTo characterize the ability of each computation to use additional workers to perform work faster, we calculated the relative strong scaling efficiencies (\n\n\nRSE\n\n\ns\nt\nr\no\nn\ng\n\n\n). These are calculated by dividing the latency \n\n\nT\n\n\nS\n,\n1\n\n\n on a single worker by the latency \n\n\nT\n\n\nS\n,\nP\n\n\n on \nP\n workers, multiplied by \nP\n, while the problem size is kept constant (Eq.\u00a0(1)). To characterize the ability of each model to use additional workers to perform more work, we calculated the relative weak scaling efficiencies (\n\n\nRSE\n\n\nw\ne\na\nk\n\n\n). These are calculated by dividing the latency \n\n\nT\n\n\nW\n,\n1\n\n\n on a single worker by the latency \n\n\nT\n\n\nW\n,\nP\n\n\n on \nP\n workers, while the problem size scales according to the number of workers (Eq.\u00a0(2)). \n\n\n(1)\n\n\n\n\nRSE\n\n\ns\nt\nr\no\nn\ng\n\n\n=\n\n\n\n\nT\n\n\nS\n,\n1\n\n\n\n\nP\n\u00d7\n\n\nT\n\n\nS\n,\nP\n\n\n\n\n\u00d7\n100\n%\n\n\n\n\n(2)\n\n\n\n\nRSE\n\n\nw\ne\na\nk\n\n\n=\n\n\n\n\nT\n\n\nW\n,\n1\n\n\n\n\n\n\nT\n\n\nW\n,\nP\n\n\n\n\n\u00d7\n100\n%\n\n\n\n\n\n\nTo be able to use the differences between kinds of workers in the interpretation of the results of the scalability results, we performed the experiments over three kinds of workers: (1) the 6 CPU cores within a single NUMA node, (2) the 8 NUMA nodes within a single cluster node, and (3) 4 cluster nodes within a cluster partition. We used subsets of the MERIT Africa dataset and a resampled version thereof in the scalability experiments (Table\u00a03, Appendix\u00a0B). Since the size of the tasks depends on the size of the array partitions, and not every task size results in good performance (Section\u00a02), before performing the scalability experiments we first determined good partition sizes to use Appendix\u00a0C.\n\n\n\n\n\n4.3\nComposability\nIn order to characterize the composability of the flow accumulation operations, we used a model containing two calls to a flow accumulation operation (Listing\u00a01). In an actual model, the first one could simulate water transport, while the second one uses the outflow result for simulating the transport of sediment. The reason we used two calls to the same flow accumulation operation is that we relate the differences in latencies between model runs to the total latency of the model runs. Using two operations with very different latencies makes the results more difficult to interpret. We compared the latencies of executing the model with and without a synchronization point between the operations. The synchronization point, represented by a call to wait_all in Listing\u00a01, prevents the execution of the second operation until all output partitions of the first operation are ready. Without a synchronization point, the second operation is executed as soon as the first one has finished attaching continuations to its input partitions. The hypothesis is that, in case of load imbalance, composable operations result in lower model latencies by preventing workers from being drained of work. We performed the experiments on the same dataset as used in the weak scaling experiment over NUMA nodes of accu_threshold\n (Table\u00a03) when using 8 NUMA nodes. This array has 85\u00a0000\u00a0\n\u00d7\n\u00a085\u00a0000 cells and contains relatively few no-data cells. We executed each model variant 10 times and selected the smallest latencies. \n\n\n\n\n\n\n\n\n5\nResults\n\n5.1\nAlgorithm\nThe trace shows which flow accumulation tasks are executing over time (Fig.\u00a03(a)). All the time different kinds of tasks are executing in parallel. The kind of task spending the most time on the CPU cores changes over time. The sequence corresponds with the steps performed per partition by our algorithms: NIDP, intra-partition, inter-partition (Section\u00a03). Concurrent tasks, operating on different partitions, get scheduled in parallel.\n\n\n\n\n\n5.2\nPerformance and scalability\nCalculating a result for the southern half of the African continent with our new accu operation took 1.1\u00a0min while PCRaster\u2019s version took 3.2\u00a0min. Our new accu_threshold operation took 1.4\u00a0min while PCRaster\u2019s version took 3.3\u00a0min.\nBoth the strong and weak scalability of accu_threshold over the CPU cores within a NUMA node are higher than 80% (Table\u00a04). When scaling over the NUMA nodes within a cluster node, the algorithm has more trouble of using additional workers effectively. Also, the pattern of efficiencies over the number of workers becomes irregular (Fig.\u00a04). Additional experiments revealed that the procedure for distributing array partitions over processes contributes to this irregular pattern Appendix\u00a0D. The speed-up when using 8 NUMA nodes compared to using 1 is about 5.5. In case of the case-study model, strong scaling efficiencies are lower than weak scaling efficiencies. Given the latencies of the model when using a whole cluster node, the scaling efficiencies are high\u2014around 80%. Additional cluster nodes can be used effectively. In case of the strong scalability experiment for the case-study model, we performed an additional experiment, to determine at how many cluster nodes scalability stops and what the maximum associated speed-up is. At 8 cluster nodes, the speed-up is almost 4 and does not increase anymore Appendix\u00a0E.\n\n\n\n\n\n\n5.3\nComposability\nRunning the model from Listing\u00a01 without synchronization takes less time than with synchronization (22s versus 25s). The difference is relatively small, but in simulation models in which many operations are used these small performance gains may become relevant. Also, even a small load imbalance can cause workers to be drained of useful work to do, decreasing the scalability.\nThe traces in Fig.\u00a03 illustrate a result of a similar experiment, on a smaller dataset run on the 6 cores within a single NUMA node (Section\u00a04.1). In case of no synchronization point between the operations, even though there is a data dependency between the two operations, tasks from the second call execute while those from the first operation are still executing as well. This is especially apparent for the tasks calculating NIDP values. Since these only depend on the flow direction raster passed in, tasks created by both calls to accu_threshold can be scheduled for execution immediately. In case of a synchronization point between the operations, this does not happen. Only once all tasks from the first call to accu_threshold are finished, can tasks from the second call be scheduled for execution. This results in a larger fraction of time that workers are drained of useful work to do, identified by the arrows in Fig.\u00a03, which results in longer model latencies.\n\n\n\n6\nDiscussion\nOur new algorithms support the use of various criteria to determine how much of the total amount of material entering each cell remains in that cell as residue and how much flows towards the downstream cell. Compared to an existing implementation we compared the performance with, the single core performance of our algorithms is better. Given the scaling efficiencies of our new flow accumulation algorithms and the case-study model, we conclude that in case a modeller needs to decrease the latency of a model, or to use a model on a larger dataset, additional hardware can be used effectively. The use of AMT in the implementation of the algorithms supported the requirement that modelling operations should be composable. We showed that concurrent tasks from consecutive flow accumulation operations were scheduled to run in parallel. This led to an improvement of the overall latency, which is beneficial for the scalability of whole models.\n\n6.1\nPerformance and scalability\nWe conclude that the single CPU core performance of the LUE flow accumulation algorithms is good, albeit details in the functionality of different implementations might be different. The accu and accu_threshold algorithms are more than twice as fast as the same operations in PCRaster.\nThe variation in the scaling efficiencies when scaling accu_threshold over NUMA nodes suggests that each time workers are added something changes in the way the total amount of work is performed. Additional experiments Appendix\u00a0D showed that this is not related to differences in the flow direction field used in the experiments, but is \u2013 at least partly \u2013 related to how array partitions are distributed over processes. Using a different procedure for this resulted in a different pattern of scaling efficiencies over NUMA nodes. When using all 8 NUMA nodes in a cluster node, in case of the strong scalability experiment, using the Hilbert curve clearly outperformed the linear mapping procedure, both in terms of the scaling efficiency and the absolute performance. In case of the larger problem solved by the weak scalability experiment, the relative scaling efficiency and the absolute performance are similar, but in favour of the linear mapping procedure.\nWe did not see a similar variability in scaling efficiencies when scaling \n\n\n over CPU cores. This is likely related to the differences in latencies between the NUMA nodes. Different combinations of NUMA nodes exchange data at different speeds, depending on their locations in the cluster nodes. In case of using the Hilbert curve procedure for mapping partitions to processes sometimes increases the performance and sometimes decreases the performance relative to what can be expected given the performance on a single NUMA node. Using the linear mapping procedure, the performance varies less per set of NUMA nodes.\nIn the scalability experiments of the case-study model, any irregular variability in performance over NUMA nodes is likely to be hidden by the much larger number of tasks that are performed. As long as there is enough useful work to do, the HPX runtime hides latencies involved in communication between processes.\n\n\n6.2\nComposability\nThe lack of unnecessary synchronization points within our flow accumulation algorithm results in a relatively large set of tasks that are ready to execute. This limits the negative effect of load imbalance on the overall latency, even when executing a single call to a flow accumulation operation. While a task managing a partition containing a downstream part of a large scale stream is waiting for material to accumulate, there are likely other tasks that can do something useful.\n\n\n6.3\nFuture work\nOur results show that it is useful to apply the AMT approach to flow accumulation operations. Given this and our experiences with other operations (de\u00a0Jong et al., 2021), this suggests that it is useful to apply the approach to other modelling operations as well. This can potentially result in a set with which model developers can build a wide range of models that perform and scale well. To increase knowledge and experience when moving in this direction, in our view several aspects deserve attention. Transporting material using flow accumulation operations assumes that the duration of the simulated time step is longer than the material requires to reach the simulated area\u2019s outflow point. Other useful operations exist that do not require this and model flow routing in greater detail. Examples of these are the kinematic wave, diffusion wave and dynamic wave operations (Te\u00a0Chow et al., 1988). It is unclear how to express these operations using the AMT approach. Also, since these operations require more computations, the temporal load imbalance resulting from them will have a larger impact on the scalability than in the case of flow accumulation operations. To work around this, a procedure may be required to redistribute partitions over processes, based on load imbalance detected at runtime. Additionally, simulation models often require a lot of I\/O to read and write model state to datasets. Traditional serial I\/O prevents the scalability of models. Using a parallel file system and parallel I\/O allows the I\/O to be scalable, over I\/O nodes. But it is unclear how to best integrate parallel I\/O with the AMT approach.\nFinally, there are at least two opportunities for further improving the performance of the existing modelling operations. First, as we showed, the procedure for assigning partitions to processes is of influence on the performance and scalability of operations. Which procedure works best may depend on characteristics of the hardware, like the actual differences in latencies between NUMA nodes. Additionally, it may depend on the input data. In the case of flow accumulation operations, grouping partitions depending on their membership of a hydrological catchment may be useful to improve its performance. A second opportunity to improve the performance of operations is to integrate the use of GPGPU devices often available to the model developer. It remains to be seen how to integrate them in a modelling framework generating a different set of tasks for each model, and to what extend this increases the performance of models.\n\n\n\nCRediT authorship contribution statement\n\nKor de Jong: Conceptualization, Designed and implemented the framework, Writing - original draft. Debabrata Panja: Conceptualization, Providing inputs for writing the manuscript. Derek Karssenberg: Conceptualization, Providing inputs for writing the manuscript. Marc van Kreveld: Conceptualization, Providing inputs for writing the manuscript.\n\n","67":"","68":"\n\n1\nIntroduction\nSnow and ice cover a large portion of our planet and they are highly vulnerable to climate changes. To monitor the spatial and temporal extensions of these surfaces it is necessary to use both field and remote sensing data. The snowpack, defined as a layer of ice crystals of diverse sizes and shapes (Rees, 2006), and the ice surface can be recognized in the remotely sensed images considering the peculiarities of their spectral behaviors. While light-absorbing impurities (lithogenic minerals dust, algae, soot) impact more on the visible wavelengths range (Warren, 2019), the snow properties in the short-wave infrared domain (1400\u20132500\u00a0nm) are dominantly affected by the size and shape of the ice crystals (Domin\u00e9 et al., 2006). Compact ice surfaces have a spectral behavior like the snow surface in the visible range and they absorb almost completely the incident radiation in the short-wave infrared ranges (Rees, 2006). The optical behavior of snow and ice is therefore a key component of the knowledge required for calibrating and validating satellite data. The collection of ground-based observations, provided by portable spectroradiometers, is an ideal data source, which is extremely interesting if those hyperspectral observations were obtained during field campaigns located in remote regions. There is unfortunately still a gap between observations and modeling of the snow cover and the challenge consists in introducing the microphysical properties of the different snow layers, with particular attention to the snow grain habit mixture (Dang et al., 2016; He et al., 2018; Saito et al., 2019).\nA set of this kind of data, collected in Antarctica, is partially included in the spectral library (SISpec) aimed at supplying the interpretation of Landsat imagery (Casacchia et al., 2002). This library was adequately coupled to ancillary information and the dataset was distributed in different ways, both as an archive and browsed through a website (Ghergo et al., 2000) but to increase the value of such data, it is now necessary to apply the best practices to optimize interoperability, and reusability of the shared data. Starting from the goal to figure out a better solution to share the SISpec data, we did a state-of-the-art analysis of the metadata of some spectral libraries to define a metadata set that would not only increase the interoperability of SISpec but also define metadata that specifically describes snow.\nThe proliferation of data characterizes our age, but we risk what is described in the phrase \u201cWe are drowning in data but starved for information\u201d (Brown, 2014). This has become a common saying in this time of \u201cbig data\u201d, the vast amount of scientific data available has increased the need for rules and principles for data management and organization. Many scientific data are organized as a sort of \u201chidden\u201d archive whose discovery and recovery is a kind of scientific treasure hunt. The effort that is being aimed at in these years is to reach an efficient discovery of data and reuse of research results. Moreover, the scientific community has decided upon principles to grant \u201copen access\u201d in activities as the European Research Council (ERC) guidelines, the principles of findability, accessibility, interoperability, and reusability (FAIR) data, the Group on Earth Observations (GEO) Data Sharing Principles. Some data-sharing initiatives focused on field spectroscopy have lately appeared in the remote sensing community such as NASA's EOSDIS (Earth Science Data and Information System), the LTER (Long Term Ecological Research) network, the Australian Terrestrial Ecosystem Research Network (TERN), SpecNet (Rasaiah, 2015). All these experiences have in common that to make it possible to share data in whatever form it is produced and organized, there is a need for rules to be followed (Wilkinson et al., 2016). The value of shared data increases if datasets are findable or discoverable, enabling the attribute-based search through a detailed description of both data and metadata. Particular attention will be, therefore, necessary to prepare data and metadata that are accessible, and retrievable in a variety of formats suitable for humans and machines alike. A description of metadata that is interoperable must follow the guidelines of the communities that use it and follow a well-defined vocabulary. Finally, it must be taken into account also the reusability when the definition of essential, recommended, and optional metadata should be machine processable and checkable, the use should be straightforward, and data should be referenced to support data sharing and recognition of data value itself. All these requirements refer to the FAIR and GEO principles for the management of scientific data, and it is critical to consider these principles valid for both humans and machines.\nThe collection of hyperspectral measurements, firstly based on acquisitions performed under laboratory conditions in the 60s, and including later in-situ measurements, is defined as a spectral library (Clark et al., 1990). Spectral libraries (SL) include additional ancillary information (such as chemical compositions, measurement conditions, etc.) to support the identification of many different materials. The first spectral libraries, like those provided by USGS (Clark et al., 1993) and by the NASA JPL (Jet Propulsion Laboratory) (Grove et al., 1992), were dedicated to mineral and rock as an extension of the mineralogical and petrographic analysis and only later were they used to interpret the multi and hyperspectral images. In the last 20 years, spectroscopic techniques have acquired a growing relevance to study the planet's different surfaces, especially because spectroscopic measurements are rapid, precise, and inexpensive (Rasaiah et al., 2014; Viscarra Rossel et al., 2016) and the need for ground-truth observations for satellite data calibration and validation became a major requirement (Bojinski et al., 2003). In the scientific literature, several SL on minerals, soils, rocks, vegetation, and man-made materials are well known. In those SL spectral signatures are collected in different wavelengths that usually cover the wavelength intervals specific to passive optical images (Viscarra et al., 2016, Nidamanuri and Ramiya, 2014, Rasaiah et al., 2012, 2014, Jim\u00e9nez and Delgado, 2015, Fang et al., 2007, Herold et al., 2004). SLs were in some cases updated as data services where collections can be browsed online (Meerdink et al., 2019; Kokaly et al., 2017) and the spectra directly imported into software packages (QGIS, ENVI) using proprietary or text file formats.\nRecently, in addition to the increasing number of datasets, the libraries have evolved to novel paradigms such as Spectral Archives and finally to Spectral Information Systems (SIS). The SIS paradigm, applied for the Swiss Spectral Input\/Output (SPECCHIO) (Hueni et al., 2020) and EcoSIS (Wagner et al., 2019) collections, is based on the development of complex systems aimed at managing large datasets and sharing them through application programming interfaces (API) or web portals. While at first spectral signatures were collected in SLs archives used by individual researchers in their workplaces, the need to share information with a larger audience and advances in technology have led to a greater need for sharing.\nInformation contained in spectral libraries to be shareable and interoperable must be organized. Key components for describing data are metadata since, as the term implies, data about data, are \u201cstructured data about everything that can be named\u201d (DCMI, 2021). The metadata profile represents, in this case, the digital content using a set of predefined attributes. This information is useful for the description of the investigated materials and of the database that contains them (Gilliland, 2016). Moreover, metadata is common in information systems and occurs in many forms and is essential to the systems functionality, describes the content, enabling users to find what they are looking for, record important information about data, and helps share them. (Riley, 2017). Well-made metadata schemas should be designed according to international rules and standards such as the ones from the International Organization for Standardization (ISO), the Dublin Core Metadata Initiative (DCMI), or the Infrastructure for spatial information in European Community (INSPIRE). The INSPIRE directive 2002\/2\/EC imposes strict rules for European countries regarding drafting and sharing of metadata, to enable interoperability across the European Union and beyond. The ISO 19115 standard supports (ISO 2014, 2019) these practices describing structured relations between included elements, and additional components mentioned in other standards of the 19100 series, through the adoption of the Unified Modeling Language (UML) and the eXtensible Markup Language (XML) useful for the implementation of metadata that support the machine-to-machine exchange. The ISO 19115 identifies the minimum metadata set (core Metadata) required for describing datasets and it provides, associated with additional more specific references, guidelines for completing the description of attributes concerning: the base dataset metadata; the acquisition description; and, eventually, the characterization of the surface. While the first base component is already standardized following the ISO and INSPIRE guidelines, the acquisition section is only partially described by the ISO (2019) The information about the observing methodology requires the collection of many details that are domain-specific and, in our case, the experimental setup must consider a scheme provided in literature by different sources (Rasaiah et al., 2014; Jim\u00e9nez et al., 2014; Hueni et al., 2011). Although the available schemes are an ideal background for describing, with large flexibility, most of the possible experimental setups, all of them are unfortunately user-oriented not aligned to the INSPIRE directives aimed at discovering datasets. Similarly, a snow extension is not already described in the literature for the discovery of snow-related datasets, but a good backbone is represented also in this case by the Canadian Avalanche Association Markup Language (CAAML), described by (Haegeli et al., 2010). This standard was born for the electronic representation of information pertinent to avalanche safety operations, but it is also a tool for sharing data with the community. From this perspective, the alignment of this scheme, and consequently of the International Classification for Seasonal Snow on the Ground Fierz et al. (2009), to the ISO (2014; 2019) and INSPIRE guidelines (INSPIRE, 2007) is one of the gaps that this work aims to fill.\nThe use of standard attribute naming conventions is an additional point that must be addressed. Looking at the available SLs or SISs, none of them follows a standardized naming convention such as the Climate and Forecast (CF) convention (Eaton et al., 2020) or the Attribute Convention for Data Discovery (ACDD) guideline (ESIP, 2020). These resources contribute to a reliable description of what each variable means, and of the spatial and temporal properties of the data. Several examples of discrepancies between attribute descriptions are possible considering core metadata such as the \u201cinstitution\u201d, \u201cid\u201d and other basic attributes. This mismatch, considering that we are referring to the machine-to-machine interaction, is a major issue if we include the acquisition information (\u201cinstrument\u201d, \u201csensor\u201d, etc.).\nFinally, the selection of the most appropriate file format is a major discussion element since, as in our case (SISpec) a dataset including observations of the snow surface using field spectroscopy represents a complex object difficult to be handled using unstructured file formats (such as an ASCII file). Considering that these measurements are a geospatial data feature and that a self-described data format supports a higher level of interoperability, a reasonable choice seems to be the data model based on the Network Common Data Form (NetCDF) file format (Rew et al., 1997). The NetCDF is a set of machine-independent software libraries and data formats that provide support for the creation, access, and sharing of array-oriented scientific data. This data format is an Open Geospatial Consortium (OGC) standard maintained by the UNIDATA community and is already recognized by several agencies and institutions (Domenico, 2011).\nUnfortunately, the approach \u201cone-size-fits-all\u201d is unhelpful and restrictive. Having this in mind, we decided to proceed systematically to approximate the best possible description of a Snow and Ice Spectral (SISpec) scheme starting from the most common metadata standards to describe a specific library of spectral signatures of snow. To reach the goal we created a metadata profile compliant with current ISO standards and INSPIRE directive, starting from snow descriptions collected in the field to support radiometric measurements. The major goal of the proposed scheme is to approach the snow cover as a complex mixture of different crystal shapes and dimensions, using a harmonized classification and a standardized way to describe the microphysical behavior of the observed snow cover.\nThis study presents a methodology based on existing hyperspectral measurements of different snow surface types and on available interoperability standard resources (data format and guidelines). The proposed approach covers the survey about attributes required by interoperability standards, the definition of encoding specifications, and consequently the description of a specific metadata profile. The novelty is based on the definition of a snow-related extension aimed at obtaining fully-described metadata about the base data information, the instrumental setup and above of all about the surface snow description. The contribution of such a profile is not only investigated in terms of interoperability between spectral libraries but also as a trigger for the ingestion of snow hyperspectral observations into spectral information systems.\n\n\n2\nMethodology\nThe design of a metadata profile requires a test dataset, in this case, based on field hyperspectral measurements, which supports the definition of the necessary metadata components.\n\n2.1\nHyperspectral field measurements\nWe considered the first version of the SISpec library (Ghergo et al., 2000) where spectroradiometric measurements of snow surface were acquired on the field as well as some snow microphysical properties (snow grains shape and size). This dataset included hyperspectral measurements of snow and ice targets in the spectral range between 350 and 2500\u00a0nm. The measurements were collected in Antarctica (Casacchia et al., 2002) with the portable spectroradiometer, Fieldspec FR (ASD Inc. Boulder, CO, USA) as absolute reflectance, i.e., as the ratio between the radiation reflected from the surface and the radiation reflected from a Spectralon reference panel.\nThe measurements were performed in smooth and open areas, far from the mountainous reliefs, sufficiently wide (about 100\u00a0\u00d7\u00a0100\u00a0m) to be easily identified on satellite images with a spatial resolution of 30\u00a0m per pixel. Particular attention was paid to selecting areas with homogeneous microphysical characteristics of the snowpack. For each measurement site, the distinctive characteristics of the snowpack were observed and recorded, such as the shape and size of the snow grains. The density, hardness, and temperature of the surface layer of the snowpack were also measured. Local weather conditions were also noted during teach measurement session. The adopted standard for the description of the characteristics of the snow cover at the beginning of the data collection was from Colbeck et al. (1990) and then it was updated to the classification from Fierz et al. (2009).\n\n\n2.2\nThe metadata profile\nThe creation of a specific metadata profile for snow and ice was based on two core components: the metadata and the encoding technical specification. The first one, the metadata technical specification, where the metadata model is described using UML diagrams and tables. The metadata sections and elements that are part of the profile are listed (such as title, creator name, place, and date of creation, used to collect information about a resource), including both the ones already existing from international metadata standards and the newly introduced ones. The second one, the encoding technical specification, where technical details are present on how to encode in a specific machine format (e.g., XML, NetCDF, JSON, etc.) metadata documents that are compliant with the metadata profile. The specification may also include schemas, templates, or online tools to help assure compliance with the metadata profile.\nThe specific metadata set to describe the content of the SISpec library was based firstly on general-purpose standards: the Dublin Core for the general resources (Neiswender and Montgomery, 2010); the ISO standard for the geospatial information (ISO, 2014); and the acquisition standard (ISO, 2019), which is the extensions focused on the acquisition and the processing for the general description of the considered observations.\nThe appropriate suite of metadata schemas was carefully chosen, to best describe and provide access to databases. To complete the SISpec metadata profile, we chose to adopt the ISO 191xx series of metadata standards (ISO 2014, 2016, 2019). The first two ISO standards define a wide variety of metadata elements for describing geographic information. Part 1 contains fundamental elements (such as information about the metadata itself, citation, spatial-temporal extent, lineage, reference system, data quality), while part 2 focuses on elements for describing acquisitions and processing (such as environmental conditions, instrument, objective, operation, plan, platform, requirements). All the elements described in the two documents have been used as part of the SISpec profile. These elements represent the ISO 19115 comprehensive metadata element set, comprising both the ISO 19115 minimum mandatory components (with mandatory obligations) and all optional elements (in some cases mandating obligations). Additional requirements for the SISpec profile come from the European directive INSPIRE, as the SISpec profile aims to be fully compliant with it. Technical guidelines for the implementation of the INSPIRE required metadata for datasets based on ISO (INSPIRE, 2007) has been reviewed to include in the profile all the needed requirements (e.g., include with mandatory obligation selected optional ISO 19115 elements, or add domain constraints). Finally, ISO (2016) has been considered to realize the XML encoding of documents compliant with the SISpec metadata profile.\nFinally, we developed a snow-related extension where both regular standards and existing resources were not enough to fulfil our needs and describe the specific content of the SISpec profile. This component was defined following the ISO rules contained in the ISO 19115 Annexes, especially the Annex F \u201cMetadata extension methodology\u201d. The backbone of this extension was the classification for seasonal snow (Fierz et al., 2009), which is already represented for data exchange purposes by the CAAML (Haegeli et al., 2010).\nOnce we selected the standards for the metadata and defined the structure, we dedicated specific effort to drafting attribute naming conventions. This task is strictly associated with the use of the NetCDF data format since it is a self-described data model that requires standard attribute names for having complete interoperability. We referred our attention to the Climate and Forecast convention and the Attribute Convention for Dataset Discovery.\n\n\n\n3\nResults\nWe used the SISpec dataset as a testbed for the creation of snow-specific descriptive metadata. We are working to make available the following products as the result of the standardization process:\n\n\u25cf\nThe SISpec metadata profile technical specification, where the SISpec metadata model is described with UML diagrams and tables. The metadata sections and elements that are part of the profile are listed, including both the ones from ISO 19115 and the newly introduced ones (supplementary materials: appendix A-MC1)\n\n\n\u25cf\nThe SISpec XML encoding technical specification, where technical details are present on how to draft XML documents that are compliant with the SISpec metadata profile and can be validated according to the SISpec XML schema (based on ISO 19115 part 3 and community extensions); and the SISpec Schematron (supplementary materials: appendix A-MC2).\n\n\n\u25cf\nThe SISpec NetCDF (Network Common Data Form) encoding technical specification, where technical details are available on how to draft a NetCDF file that is compliant with the SISpec metadata profile, being NetCDF a standard for sharing data. This file is presented in Appendix A-MC3: supplementary materials: appendix A-MC3.\n\n\n\n\n3.1\nDescription of the snow metadata profile\nThe SISpec metadata profile was created considering different components (Fig. 1\n), which include: base, acquisition, and snow-information components.\nThe \u201cmandatory\u201d Base metadata elements are attributes mandatory or highly recommended by ISO (2014) to be included in profiles, such as the dataset title, the reference date, the party responsible for the data and metadata, the abstract about its content and so on. These mandatory base attributes are in total 40 elements and 36 of them are mandatory for INSPIRE requirements. Finally, the \u201ccomprehensive\u201d Base metadata from ISO (2014) are all the optional elements, 24 in total, that enable users to describe their datasets with full details. The ACDD convention recommends 19 additional attributes (4 are strongly recommended) useful for the data discovery. The \u201cacquisition\u201d component defined by the ISO (2019) completes the global attribute description of the considered dataset with a set of information about the instrument setup and the campaign geometric design. This component includes 13 elements, and 2 attributes are mandatory. The novel component is the snow information extension, which includes additional information about the experimental conditions for measuring the spectral reflectance, the surface description of the target and finally, the variables associated with the different available measurements and the microphysical conditions of the observed snow surface. While about 42 features are associated with the requirements defined by the different field spectroscopy communities (Hueni et al., 2020; Rasaiah et al., 2014; Jim\u00e9nez et al., 2014), the observed variables needed for describing the snow optical behavior and microphysics are defined by the snow survey community and by the CF convention (23 variables in this case).\nThe final scheme of the SISpec metadata profile will be composed of three different elements: the metadata base, the acquisition information, and the snow information (Fig. 2\n).\nWhile the metadata base and the acquisition information are standardized information, mostly about the location, the geographical and time domains, and the instrumental setup, the SISpec information represents the novel component to be described in detail. Referring to the novel extended elements, we created an additional metadata component aimed at including the specific aspects of the studied domain: the snow. We considered 13 variables (5 are mandatory) that can include the specific properties of snow and snow surfaces. The characteristics of the snow were described considering the paper of Fierz et al. (2009). Moreover, we considered in our profile 10 variables (8 are mandatory) derived from the CF convention (Eaton et al., 2020). We considered for the whole profile the use of standardized name alignment that was developed following the Attribute Convention for Data Discovery (ESIP, 2020).\nThe section dedicated to the snow information (Fig. 3\n) holds four different elements: i) the surface description, to describe the surface object of the measurements using one string field; ii) the graphic overviews, providing pictorial information (photos) about the target location; iii) the snow conditions during measurements and the specific observations; iv) the observed composition in terms of snow grain size.\nThe snow grains description was defined by considering the Grain code list (Fig. 4\n) and identifying the type and relative composition of the snow grains during the measurements. Each observed surface was described, following the International Association of Cryospheric Sciences (IACS) classification (Fierz et al., 2009), considering: the primary classification, which discriminates the different morphological characteristic of the snow crystals (e.g. precipitation particles, machine-made snow, faceted crystals, etc.); the secondary classification, which specifies the subclasses associated with the description of the physical process producing the particles (e.g. precipitation particles-stellar dendrites, faceted crystals-solid faced particle); the percentage, indicating the relative abundance of the grain concerning the total; the size of the grain, quantified as a measure. The metadata profile allows the description of main grain types (more representative) for each observed surface.\nThe section focused on the snow condition includes different surface descriptors as well as eventually measured parameters. The descriptors are hardness and roughness that are listed in Fig. 5\n. The quantitative parameters (thickness, density, roughness length and height, temperature, and humidity) require in addition to the measured value, also the description of the used methodology. Moreover, the metadata profile is designed to accept additional conditions, which can be nevertheless described, using the \u201cother Conditions\u201d extension point, present to increase flexibility and usability to the metadata model.\nFinally, the acquisition of hyperspectral measurements is included in the components related to the spectral information. The selected parameter is the reflectance value associated with each wavelength described in the Instrument description present in the Acquisition Information. Referring to the general metadata component, it is important to state that particular attention was paid to the terminological references for keyword metadatation; specifically, it was chosen to use the EU thesaurus GEMET and the Snowterm thesaurus on snow and ice (Plini et al., 2009).\n\n\n3.2\nApplication of the novel metadata profile to the field data collection\nThe impact of the novel metadata description is essentially focused on having hyperspectral measurements, described in terms of experimental setup, associated with the observations about the snow microphysics. The available hyperspectral measurements of the surface snow are generally tagged by labels or comments related to the size distribution neglecting the possibility to consider the crystal shape and the complexity of microphysical mixtures. The contribution of the IACS classification (Fierz et al., 2009) highlights the need of combining different crystal shape and size for an appropriate identification of different snow types. Having a rigorous identification of different crystal shapes and size distribution, it is possible to define the processes occurring on the observed snow layer. This information is critical for different disciplines, and it is a major request from communities involved in snow observations and modeling (He et al., 2018; Dang et al., 2016).\n\nFig. 6\n shows some hyperspectral measurements selected in the presented dataset, where the observations highlight large differences between surface types classified in ice, kinetic forms, and equilibrium forms. The difference could be smaller between different equilibrium forms, as in this example, and the IACS classification supports the discrimination between equilibrium growth forms composed both by rounded grains (RG in Fig. 4) with small and large rounded particles, RGsr and RGlr respectively. One of the two showed snow observations is therefore made up of both crystal shapes, where RGsr is the major component and RGlr is the secondary one. The flexibility of the new profile consists also in considering multiple crystal shapes and sizes in the same observations. This feature offers the possibility to describe mixed compositions instead of pure single snow types that rarely occurs in nature, and it enhance to have a detailed description of the observed snow layer.\n\n\n\n4\nDiscussion\nThe need for a specific snow-related metadata profile comes out from a review about the availability of such conventions in literature. Considering the availability of spectral libraries (SL), spectral archives (SA) and recently spectral information systems (SIS) focused on different domains, the need for a metadata profile useful for interoperating between different systems is a major requirement. The availability of a snow-related metadata extension strengthens, moreover, the opportunity to connect domain-specific collections (SL or SA) to general-purpose systems (SL, SA or SIS). The evolution of the system paradigm to SIS invokes furthermore the need for standard conventions (ISO and INSPIRE) aimed at increasing the machine-to-machine interaction. The available spectral collections can be grouped in general-purpose and domain specific. The first group includes the SPECLIB SL (Kokaly et al., 2017), the ECOsystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) SL (Meerdink et al., 2019), the SPECCHIO SIS (Hueni et al., 2009) and the LUCAS database with its SL (Orgiazzi et al., 2018). The second group includes the INTA SL (Jim\u00e9nez et al., 2014), and the Global vis-NIR SL (Viscarra et al., 2016). All these datasets are designed for managing observations obtained by laboratory and field instruments. Some of them, like ECOSTRESS and SPECLIB, are characterized by text files, where single measurements (spectra) are coupled to a limited amount (about 20) and not standardized metadata. On the other hand, measurements available in SPECCHIO and INTA are described with a more detailed and standardized metadata profile featuring more than 50 attributes. The most important difference between these groups consists in the use of standardized attributes, which supports the machine-to-machine dialogue, and the availability of aggregated attributes, where information is solely oriented to human interaction. Looking at snow observations, ECOSTRESS, SPECLIB, INTA and SPECCHIO provide few spectra related to the snow matrix, but the description is very coarse and difficult to be considered in specific studies. The presence of such observations in those spectral libraries highlights the importance of having such snow measurements and stresses at the same time the need for more details for describing the snow surface by the International classification defined by Fierz et al. (2009). For example, the ECOSTRESS SL classifies snow observations in the water category, as coarse granular snow, medium granular snow, fine snow, or frost. This size-oriented classification is a limiting factor for snow-related studies since complex mixtures of shape and dimensions are possible. The profile extension supports the description of multiple crystal types, up to three classes are frequent in nature, where the size is coupled to the shape as well as to genesis of such crystals, with more than 30 classes. This description is not only limited to the snow crystallography, but it also opened to harmonise additional microphysical properties, such as hardness and roughness, observed by using international standards. The need for a snow-related extension is, therefore, more emphasized by looking at spectral-domain specific libraries different from snow, where the communities focused on soil and vegetation are more advanced in defining a standardized and detailed metadata scheme. From this perspective, domain-related extensions are important, and the proposed approaches represent a significant guideline for preparing a snow-specific scheme. The Global vis-NIR SL and the LUCAS SL provide, in this case, soil-related extensions composed of attributes (8 and more than 30, respectively) specific for soil chemistry and physics that are not applicable for snow studies. Defining three different components in the metadata scheme (Base, Acquisition and Domain), the considered spectral libraries show significant differences (Table 1\n).\nWhile the general-purpose and human-oriented libraries (SPECLIB and ECOSTRESS) provides exclusively the Base component, the machine-oriented collections (SPECCHIO, INTA, LUCAS, the Global vis-NIR SL) include both the Base and the Acquisition components and they are already oriented to a machine-to-machine interaction.\nThe connection between the different spectral libraries represents an interesting perspective since during the last decades smaller databases collapsed into broader services useful for the international communities. While this process occurred in general-purposes databases for collapsing single-institution libraries (Meerdink et al., 2019), domain-specific case studies described how a soil-related metadata scheme is required for harmonizing measurements at a global scale (Viscarra et al., 2016). The example of the collapse process is what happened for the ECOSTRESS SL that represents the union between SLs prepared by the NASA's Jet Propulsion Laboratory (Grove et al., 1992), the USGS (Clark et al., 2003), the Johns Hopkins University previously included in the ASTER database (Baldridge et al., 2009). From this perspective, the potential interaction between the proposed SISPEC metadata profile and the already available spectral libraries is an additional issue that must be analyzed. Two interaction types must be considered: ingestion and harvesting. The first type refers to the ingestion of the SISPEC metadata scheme into the spectral data available in other databases. The first direction considers the possibility to provide SISPEC-described observations to other databases. Although a limited number of snow-related spectra are included in ECOSTRESS, SPECLIB and SPECCHIO, it is critical for SISPEC to interact with those state-of-the-art spectral libraries. Analyzing the different metadata schemes, it is possible to assure a complete overlap between SISPEC and the considered databases. While the conversion is direct for SISPEC-to-SPECCHIO, the SISPEC-to-ECOSTRESS and SISPEC-to-SPECLIB conversions require the aggregation between different attributes included in the SISPEC scheme. The opposite, to-SISPEC, process is unfortunately difficult in terms of conversions not only due to the lack of snow information but also due to the lack of some mandatory SISpec attributes. The required step for the future machine-to-machine ingestion of SISpec data into state-of-the-art spectral information system will be based on preparing specific conversion tools aimed at preparing the data and attributes in the right format. The availability of a larger number of attributes in SISpec will provide the possibility to be aligned with other required profiles as well as the possibility to aggregate attributes in specific metadata requirements.\n\n\n5\nConclusions\nScience needs data, but it is increasingly difficult to share and search them with accuracy and precision now that the increased availability of interconnected sensors and improved storage systems poses the challenges of \u201cBig Data\u201d.\nConsidering the FAIR rules and metadata standards, the spectral data and ancillary information contained in SISpec have been reconsidered and made compatible with the principles illustrated above. The result is the conversion of ancillary information into metadata. The new setup of the library will provide the polar area monitoring community with an effective tool.\nWe propose a metadata scheme, which coupled to a NetCDF data model, represents the solution for having a formal and shared standardization aimed at producing well documented and sound metadata for hyperspectral measurements. From this perspective, the availability of metadata for optical, spectrally resolved, field data is an important source of information useful for detecting remotely surface characteristics when field data cannot be collected. Reflectance spectral libraries can help to perform unsupervised hyperspectral image analysis to detect and map surface material. It is well known by now, that there is no \u201cone size fits all\u201d metadata schema and not even a standard for a controlled vocabulary. You must then choose case-by-case finding the most appropriate cataloguing standards to best describe and provide access to your resources. The creation of consistent, standards-based, continuously updated metadata can enable the researchers to publish information about their data and activities in a timely and efficient way and to disseminate this information more widely through specific protocols and data formats (e.g., NetCDF).\n\n\nCode availability section\nThis paper does not include the development of specific script\/software components. The presented dataset is openly available on a stable public repository (https:\/\/zenodo.org\/record\/4812454#.YOQee-gzahP), where technical encoding specifications and metadata schemas are openly accessible.\n\n\nAuthorship contribution statement\nSdF, RSalz, RSalv contributed to the conception, design, analysis, and implementation of the manuscript. RSalv contributed to the acquisition in the field of data measurements; RSalz encoded the NetCDF datasets, and EB encoded the XML metadata scheme. SdF and RSalz shared the joint co-authorship and lead in writing the manuscript. All authors contributed to improving the quality of figures and refining the text in the document during the revision. All authors gave their final approval of the manuscript.\n\n","69":"","70":"","71":"","72":"","73":"","74":"","75":"","76":"\n\n1\nIntroduction\nZircon grain shape has been proposed as a useful complement to traditional isotopic provenance tools, in an effort to better interpret detrital mineral origins and transport histories (Kirkland et al., 2020; Makuluni et al., 2019; Markwitz et al., 2017; Markwitz and Kirkland, 2018; Shaanan and Rosenbaum, 2018; Zeh and Cabral, 2021). The identification of sediment sources has been aided by the similarities between source and detrital zircon grain shape parameters (Makuluni et al., 2019; Markwitz and Kirkland, 2018; Yue et al., 2019). Zircon grain shape has also been used to infer sedimentary recycling and sediment transport mechanisms (Barham et al., 2016; Chae et al., 2021; Kirkland et al., 2020; Shaanan and Rosenbaum, 2018; Zeh and Cabral, 2021; Zoleikhaei et al., 2016). Despite modification during sedimentary transport, zircon grain shape parameters may be partially retained from the ultimate crystalline source to the depositional sink, with some shape parameters (e.g. minor axis) having better preservation potential than others (Makuluni et al., 2019; Markwitz and Kirkland, 2018). Zircon grain shape thus has potential to provide additional information with which to identify source regions of detrital zircon grains. This is of particular value when potential sources have non-unique U-Pb crystallization ages (Makuluni et al., 2019). Despite this, zircon grain shape is currently under-investigated, as it can be both laborious and time intensive to accurately capture grain shape information from images using currently available tools.\nMagmatic zircon shape is likely influenced by the chemical and physical conditions in the primary melt at the time of crystallization (Benisek and Finger, 1993; Pupin, 1980; Vavra, 1993). Grain shape properties will be modified to varying degrees via abrasion and dissolution during sediment transport and intermediate storage (Balan et al., 2001; Morton and Hallsworth, 1999). Post primary crystallization (re)growth processes may also serve to modify original grain morphologies through recrystallization and subsequent igneous or metamorphic overgrowths (Corfu et al., 2003). Nonetheless, a link through zircon shape between primary source rock and subsequent denuded product reflects a legacy of the magmatic controls on the original zircon crystal growth. Simple shape metrics such as minor axis, major axis, area, perimeter, and derivative parameters such as aspect ratio, roundness, and circularity, have been used for zircon grain shape characterisation with promising results (Barham et al., 2016; Chae et al., 2021; Makuluni et al., 2019; Markwitz and Kirkland, 2018; Shaanan and Rosenbaum, 2018; Yue et al., 2019; Zeh and Cabral, 2021). However, there is limited clear understanding of the quantitative relationships between zircon crystal shape, source rock, and sedimentary transport distance and mechanisms (Zeh and Cabral, 2021).\nZircon shape analysis can be accommodated directly into the routine workflow for U-Pb isotopic analysis with theoretically minimal effort. Sample zircon grains are typically cast into epoxy resin, which provides a firm substrate in instruments that require high vacuum conditions for analysis. Mounting facilitates polishing to half grain thickness to expose internal growth morphologies for imaging and isotopic analysis. Imaging is usually completed prior to U-Pb isotopic analysis using light microscopy or scanning electron microscopy (SEM) under various detectors. Zircon grain shape measurements may therefore be extracted from the images of polished mounts required as part of standard geochronology processes.\nWith the advent of modern computing, an increasing number of tools have been developed towards automating shape analysis in various disciplines, increasing measurement accuracy, objectivity, and throughput. Digital particle segmentation and measurement are frequently carried out using software such as ImageJ (Abr\u00e0moff et al., 2004; Schneider et al., 2012). There is a significant improvement in speed and accuracy afforded by image analysis software over manual measurement; however, these applications are most frequently designed with the biosciences in mind (Schneider et al., 2012; Tunwal et al., 2020). Limited development has been geared towards the geosciences (Tunwal et al., 2020) with, to our best knowledge, none customised for zircon shape analysis.\nThe extraction of shape parameters from digital imagery presents challenges. Shape parameters may be extracted only after manipulation to convert a colour image into a black and white image that clearly distinguishes foreground from background, a process termed binarisation. Once a binary image is obtained, the user must further separate clustered particles for measurement. A persistent challenge in zircon grain imaging is the poor separation of particles with shared boundaries, as in the case of touching and overlapping grains. This challenge may be addressed prior to imaging, by taking care to minimise touching grains when preparing loose particles for imaging (Campa\u00f1a et al., 2016; Shoji et al., 2018; Tunwal et al., 2020). Methods of pre-image grain separation may have limited success and at times may not be possible. Grain separation thus must be dealt with in the image analysis phase by removing particles with shared boundaries (e.g. Campa\u00f1a et al., 2016; Cox and Budhu, 2008; Shoji et al., 2018), applying separation algorithms to segment grains (e.g. Maitre et al., 2019; Markwitz and Kirkland, 2018), or by manual editing or tracing of grain boundaries (Tunwal et al., 2020). Automated separation routines may have limited success on zircon grains. The workflow for extracting zircon grain boundary information thus has significant scope for improvement in speed and accuracy.\nExisting image segmentation applications may present users with a steep learning curve due to: (i) the large number of image pre-processing and separation procedures available, (ii) the use of proprietary software, (iii) the requirement of coding knowledge to automate a suitable image segmentation routine, (iv) the need to manually adjust segmentation algorithm parameters per image, or (v) the need to export images to additional imaging software for further editing. Here we present AnalyZr, a Python application for zircon grain shape analysis, designed to facilitate investigation into the use of zircon shape as a tool for provenance and magmatic system analysis. AnalyZr is free and open-source, and benefits from a simplified interface tailored to zircon shape measurement. It allows the user to simultaneously reference reflected and transmitted light images to extract zircon shape parameters; provides the user with an automated binarisation routine customised for these image types that requires no knowledge of coding; leverages an updated segmentation algorithm to aid the separation of touching zircon grains; and allows for manual image adjustment if required. The software also provides the user with the ability to capture additional information associated with isotopically analysed grains. For example, analytical spot ID may be captured as it is useful for linking shape measurements to tabulated geochronological and geochemical data. Internal grain texture of isotopically analysed grains may be categorised via pre-defined text in a drop-down menu (e.g. homogeneous cathodoluminescence response or oscillatory zonation, etc.) or in a free-form comment.\n\n\n2\nThe application\nAnalyZr is designed to measure shape parameters from reflected and transmitted light images of zircon grains in polished epoxy mounts, for example those prepared for laser ablation-inductively coupled plasma mass spectrometry (LA-ICPMS) or secondary ionization mass spectrometry (SIMS). To achieve image segmentation, the application executes automated image binarisation, and automated separation of touching grains using a custom separation algorithm. Additionally, it enables manual adjustment of automatically defined grain boundaries and grain separations. The segmented binary image may be saved in .png format. The results of shape measurement may be output to a database table or written to a .csv file. An overview of the application workflow is provided in Fig. 1\n.\nAnalyZr has been applied to 3-channel images up to approximately 600\u00a0\u00d7\u00a0800 pixels in dimension and 2 megabytes in file size. Automated image binarisation and segmentation typically require a few seconds, to less than a minute in the case of large, complex images. Segmentation processing time is fastest for images with no touching grains and will slow as image size, number of grains in the image, and number of touching grains, increase. Manual edits to images will add to the processing time, proportional to the degree of detail required by the user.\n\n2.1\nData capture\nData capture is the first stage of the shape analysis pipeline. A folder of .png images is loaded for display via the Data Capture menu. While loading the images, the system will generate a .json file for each, if they do not yet exist. The user can step through the images sequentially, capturing information such as scale, analytical spot locations, and the positions of unwanted objects. This information is saved to the .json file. Each image file should be prefixed with the sample ID, which must be separated from the remainder of the file name by an underscore. As several images may belong to a single sample, prefixing with a sample ID allows the system to identify all images of a sample, and thus track spot ID's and scale measurements across all relevant images.\nZircon mounts are typically imaged prior to isotopic analysis to facilitate target selection. The location of analytical spots, where isotopic or chemical information was obtained, are frequently marked on the images by the user or recorded by the ablation system. The associated analytical data tends to reside in a dataset alongside the sample ID and analytical spot IDs. When processing images for shape analysis, it is advantageous to associate a zircon grain with its analytical spots, so that the grain's shape parameters may be linked with the corresponding isotopic analysis e.g. age and chemical or isotopic composition. The application thus allows users to capture the location and ID of analytical spots that have been marked on the images. Analytical spot capture is initialised via the Data Capture\\Digitise Spot command. The user can select a spot location by mouse-click and capture the associated spot ID via a pop-up dialogue box. The system tracks spot ID's per sample across all images that share a sample ID prefix and will not allow the capture of duplicate ID's for a sample. The spot ID is output alongside a grain's shape measurements when measurement is performed. The user may thus use Spot ID as a key field to link grain shape measurements to isotopic data in their preferred data manipulation software. While capturing spot ID, the user is also able to select a cathodoluminescence texture from a drop-down list, and capture a free-form text comment, both of which are associated with the analytical spot.\nIt may also be necessary to identify objects for removal, such as unwanted mineral phases, fragmented grains, bubbles, hairs, or other imperfections in the sample mount. Hence, the application has the functionality to mark objects for removal using the Data Capture\\Mark for Deletion command. When shapes are measured, objects marked for deletion are removed prior to the shape measurement.\nAn image scale is required to convert measurements into meaningful units. If a scale bar is present on the image, a user may specify the length of a linear scale in meaningful units using Data Capture\\Capture Scale\n. Historical images may not contain a scale bar. In this case the image magnification may be approximated, for example by referencing the known or approximate size of the analytical spot drawn or captured on the image. When no scale bar is present, spot sizes may be captured using the Data Capture\\Capture Spot Size command, which allows the user to draw a bounding box around the spot. The system tracks all spot sizes captured across all images for a given sample. From these, an average diameter is used to derive the image scale when shape measurement is performed. Spots are currently taken to be 30\u00a0\u03bcm in diameter as a working average when no more precise scale is available.\n\n\n2.2\nImage segmentation\nImage segmentation is the second stage of the shape analysis pipeline. Reflected and\/or transmitted light images are converted into a black and white (binary) image of zircon grains, from which shape measurements may be derived. Image segmentation may be accessed via the Image Segmentation Toolbox beneath the Segment Images menu.\nUnder transmitted light, zircon grains typically display dark boundaries and a mottled interior that is dark relative to the background. Transmitted light images are thus expected to have a light background with dark objects in the foreground. When mounted in resin, zircon grains may be partially obscured by resin; however, under transmitted light the extent of the grain beneath the resin is visible. Transmitted light images thus offer the benefit of capturing grain extents with greater accuracy. In transmitted light the contrast variations within the grains have the disadvantage of causing unwanted internal contours (Fig. 2\n).\nZircon grains are bright under reflected light and typically appear white. As such, the application expects light coloured objects on a darker background. Reflected light images are advantageous for capturing the internal area of the zircon but are frequently only sensitive to that portion of the zircon grain that is visible above resin. Hence, reflected light images may under-represent the extent of grain boundaries when zircon grains are mounted in epoxy resin (Fig. 2). Images with a similar light-dark profile, such as those derived from back-scattered electron SEM, will behave similarly to a reflected light image and may be segmented using the reflected light binarisation routine.\nShape characterisation is improved through combining both reflected and transmitted light images of the same sample field of view (Fig. 2). Both images are expected to have the same dimensions in pixels. On occasion, both images may not be available, and only one image type may be present. The user thus has the option to binarise either one or both of the input image types.\nBinarisation is achieved through the Binarise command of the Image Segmentation Toolbox. Images are converted to greyscale, smoothed twice with a bilateral filter, and treated with an Otsu threshold using tools from the OpenCV package for Python (Bradski, 2000). Reflected light images undergo a hole-filling procedure achieved with the SciPy package (Virtanen et al., 2020) and transmitted light images are inverted in colour. If both image types are available, the outputs are combined. Object boundaries are derived from the resultant binary image. Boundaries are discarded if they contain less than three points or have an area of less than fifty pixels.\nBinarisation is affected by variable image lighting, image quality, and thickness of zircon grains in a mount which is a function of the polishing depth. Consequently, the user may wish to manually delete, edit, or re-digitise boundaries around zircon grains to produce a final segmented image that best represents true grain boundaries. This digitisation functionality is accessed via the Grain Boundary Capture button of the Image Segmentation Toolbox. During this process, boundaries are overlain on the original images for visual referencing. The user can toggle the visual display between the reflected and transmitted light images, when both images are available.\nOnce a user is satisfied with the grain boundaries, the grain boundary separation algorithm can be used to separate touching grains. The algorithm aims to identify points of contact on touching grains, group them into logical pairs, and segment the image between pairs. These segmentation lines are presented to the user who may interactively delete unwanted segmentation lines, and manually digitise additional segmentation lines where needed, before saving the segmentation results. Automated segmentation is achieved through the Separate Grains command of the Image Segmentation Toolbox.\nThe user may delete, edit, and separate grain boundaries repeatedly. Each edit may be saved to the binary image using the Save Changes button of the Image Segmentation Toolbox. The binary image may be output in .png format using the Save Mask button. The file paths of the input reflected and transmitted light images and output binary image are recorded in the .json file. This allows the user to return later, load a pre-existing binary image with a .json file, and have the system automatically load the associated reflected and transmitted light images for visual referencing.\n\n\n2.3\nMeasure Shapes\nOnce objects are separated, and an image scale has been captured, the shape parameters of each object in the binary image may be automatically calculated. Shape measurement is executed via the Measure Shapes button of the Image Segmentation Toolbox.\nThe binary image is cleared of unwanted objects prior to shape measurement. Objects in contact with the image border are removed as these particles do not have their full shape exposed for measurement. The image is cleared of objects that have an area less than one sixth of the largest object in the image, as these frequently represent fragments of grains, or image noise. This generalisation is advantageous for rapid analysis; however, desirable zircon grains may be lost in the process. All objects marked as unwanted during the data capture phase are cleared.\nThe remaining objects receive a unique ID and undergo shape measurement. Thirteen shape parameters are calculated (Table 1\n). Area, equivalent diameter, perimeter, minor axis, major axis, solidity, and convex area are calculated using Python's Scikit-Image package (van der Walt et al., 2014). Form factor, roundness, compactness and aspect ratio are calculated in accordance with ImageJ's Shape Descriptor1u plugin (Chinga, 2005). Minimum and maximum Feret diameters are calculated using a custom algorithm. Spot areas and scale bars captured in the data capture phase are used to calibrate the images and return meaningful measurements of the zircon grains. Analytical spots are matched to the boundaries they are located in using the Shapely package for Python (Gillies et al., 2007). An output table is generated that lists each grain by its unique ID, with associated spot ID's, and the results of the shape measurement. Shape measurements may then be transferred to a Microsoft Access database or written to a .csv file using the Push to DB or Save to CSV buttons, respectively. The user is able to batch-process a folder of binary images and save results to a .mdb or .csv file using the Process Folder button.\n\n\n\n3\nBoundary separation algorithm\nIt is not always possible to separate objects prior to imaging, for example when bulk mounting zircon grains to minimise picking bias (Dr\u00f6llner et al., 2021; Sl\u00e1ma and Ko\u0161ler, 2012), or when working with historical image datasets. Separation algorithms may be used to accomplish separation during image post-processing.\nSeveral segmentation methodologies are common to software like ImageJ and computer vision modules such as Python's Scikit-Image and OpenCV. Examples include pixel erosion, image watershedding, and partial differential equation based methods such as Level Sets (Bradski, 2000; Ferreira and Rasband, 2012; van der Walt et al., 2014). However, their effectiveness is limited in separating zircon grains. Pixel erosion algorithms may potentially alter the shape of the grains, as shown in Fig. 3\n. Binary image watershed algorithms perform best on smooth convex objects (Ferreira and Rasband, 2012) but may over-segment particles (Fig. 3; OpenCV (2016)), while marker-based watershed algorithms require the user to manually mark each zircon grain to be separated (OpenCV, 2016). Partial differential equation based algorithms may fail to distinguish foreground from background due to the light-dark mottling of zircon grains under transmitted light (Fig. 3). In the absence of an existing algorithm tailored to zircon grain separation, an alternative separation methodology is now proposed. A segmentation algorithm tailored to zircon grain separation brings increased speed and objectivity to the zircon image segmentation procedure. It furthermore allows the full image segmentation routine to be handled within one application, thus removing the need for post-processing in additional software.\n\n3.1\nBoundary extraction\nAnalyZr uses OpenCV's findContours method to extract all contours from a binary image. Contours correspond to the boundaries of zircon grains. External contours are referred to as parent contours. Internal contours are termed child contours and lie nested within a parent contour (Fig. 4\n). Contours are grouped according to their nesting, with the groups of nested contours here referred to as families.\nWhen zircon grains touch, an inflection point with pronounced concavity is created in the boundary contour. Therefore, to segment the image, it is necessary to identify the inflection points with high concavity. However, identification may be confounded by boundary irregularities resulting from xenomorphic grains, fractured grains, or oxide adhesions to grain surfaces. These irregularities can create boundary inflections that are falsely identified as potential points of contact between grains. Spurious inflections are removed by simplifying the contours using elliptical Fourier descriptors (Fig. 5\n), an approach proposed by Mebatsion and Paliwal (2011, 2012).\nThe boundary of a 2D discrete object forms a closed contour, which may be defined in terms of elliptical Fourier descriptors (Kuhl and Giardina, 1982). Equations (1) and (2) represent the discrete elliptical Fourier approximation of an object boundary (Kuhl and Giardina, 1982). Here \n\nN\n\n is the number of Fourier harmonics, \n\nK\n\n is the total number of points along the contour, \n\nt\n\n is a unit pixel step along the contour such that \n\n\nt\n\np\n\u2212\n1\n\n\n<\nt\n<\n\nt\np\n\n\n for \n\n1\n\u2264\np\n\u2264\nK\n\n, \n\n\nA\n0\n\n\n and \n\n\nC\n0\n\n\n are coefficients of the zeroth harmonic, \n\na\n\n, \n\nb\n\n, \n\nc\n\n and \n\nd\n\n are the four Fourier descriptors of a given harmonic.\n\n(1)\n\n\n\nx\nN\n\n\n(\nt\n)\n\n=\n\n\nA\n0\n\n+\n\n\n\n\u2211\n\n\nn\n=\n1\n\nN\n\n\na\nn\n\n\ncos\n\n(\n\n\n2\nn\n\u03c0\nt\n\nT\n\n)\n\n+\n\n\nb\nn\n\ns\ni\nn\n\n(\n\n\n2\nn\n\u03c0\nt\n\nT\n\n)\n\n\n\n\n\n\n\n(2)\n\n\n\ny\nN\n\n\n(\nt\n)\n\n=\n\n\nC\n0\n\n+\n\n\n\n\u2211\n\n\nn\n=\n1\n\nN\n\n\nc\nn\n\n\ncos\n\n(\n\n\n2\nn\n\u03c0\nt\n\nT\n\n)\n\n+\n\n\nd\nn\n\ns\ni\nn\n\n(\n\n\n2\nn\n\u03c0\nt\n\nT\n\n)\n\n\n\n\n\n\nIf the total length of the closed contour is \n\nT\n\n, and the distance from the contour start to point \n\np\n\n is \n\n\nt\np\n\n\n, then \n\na\n\n, \n\nb\n\n, \n\nc\n\n and \n\nd\n\n are defined as in equation (3)\u2013(6) (Kuhl and Giardina, 1982):\n\n(3)\n\n\n\na\nn\n\n=\n\n\nT\n\n2\n\nn\n2\n\n\n\u03c0\n2\n\n\n\n\n\n\u2211\n\n\np\n=\n1\n\nK\n\n\n\n\u0394\n\nx\np\n\n\n\n\u0394\n\nt\np\n\n\n\n\n(\n\nc\no\ns\n\n(\n\n\n2\nn\n\u03c0\n\nt\np\n\n\nT\n\n)\n\n\u2212\nc\no\ns\n\n(\n\n\n2\nn\n\u03c0\n\nt\n\np\n\u2212\n1\n\n\n\nT\n\n)\n\n\n)\n\n\n\n\n\n\n\n(4)\n\n\n\nb\nn\n\n=\n\n\nT\n\n2\n\nn\n2\n\n\n\u03c0\n2\n\n\n\n\n\n\u2211\n\n\np\n=\n1\n\nK\n\n\n\n\u0394\n\nx\np\n\n\n\n\u0394\n\nt\np\n\n\n\n\n(\n\ns\ni\nn\n\n(\n\n\n2\nn\n\u03c0\n\nt\np\n\n\nT\n\n)\n\n\u2212\ns\ni\nn\n\n(\n\n\n2\nn\n\u03c0\n\nt\n\np\n\u2212\n1\n\n\n\nT\n\n)\n\n\n)\n\n\n\n\n\n\n\n(5)\n\n\n\nc\nn\n\n=\n\n\nT\n\n2\n\nn\n2\n\n\n\u03c0\n2\n\n\n\n\n\n\u2211\n\n\np\n=\n1\n\nK\n\n\n\n\u0394\n\ny\np\n\n\n\n\u0394\n\nt\np\n\n\n\n\n(\n\nc\no\ns\n\n(\n\n\n2\nn\n\u03c0\n\nt\np\n\n\nT\n\n)\n\n\u2212\nc\no\ns\n\n(\n\n\n2\nn\n\u03c0\n\nt\n\np\n\u2212\n1\n\n\n\nT\n\n)\n\n\n)\n\n\n\n\n\n\n\n(6)\n\n\n\nd\nn\n\n=\n\n\nT\n\n2\n\nn\n2\n\n\n\u03c0\n2\n\n\n\n\n\n\u2211\n\n\np\n=\n1\n\nK\n\n\n\n\u0394\n\ny\np\n\n\n\n\u0394\n\nt\np\n\n\n\n\n(\n\ns\ni\nn\n\n(\n\n\n2\nn\n\u03c0\n\nt\np\n\n\nT\n\n)\n\n\u2212\ns\ni\nn\n\n(\n\n\n2\nn\n\u03c0\n\nt\n\np\n\u2212\n1\n\n\n\nT\n\n)\n\n\n)\n\n\n\n\n\n\nThe Fourier descriptors of each contour are calculated using the PyEFD Python package (Blidh, 2016). Simplified contours are constructed using a reduced number of Fourier harmonics. The number of harmonics used is dependent on the length of the contour, as longer contours represent more complex shapes than shorter contours. For example, the longest contours in an image are associated with compound grain boundaries such as parent contours. Parent contours have more complex shapes than the shorter child contours internal to the parent contour. The number of harmonics is simplified to 20% of the number of points comprising the original contour. This reduces minor boundary irregularities and thus the number of high concavity points unrelated to boundary intersections.\n\n\n3.2\nIdentifying grain intersections on boundaries\nBoundary irregularities are identified as concavity maxima, the greatest of which correspond to the pronounced boundary inflections characteristic of two grains in contact with one another. For each point along a simplified contour, the angle between the adjoining line segments reflects the concavity at the point. For each, the supplementary angle is calculated, the size of which increases as the concave angle becomes more acute. Concavity maxima are those points whose supplementary angles are larger than calculated for those points directly adjacent (Fig. 6\n).\nOn a parent contour, positive angles that are greater than 90% of all maxima along the parent contour are identified as potential points of contact (Fig. 6). These points are referred to as nodes. Child contours capture the spaces in the resin mount framed by touching grains and frequently exhibit 3\u20134 nodes. As child contours are smaller than parent contours, they comprise fewer points, and an 80% threshold is required to extract sufficient nodes. The values of 90% and 80% were chosen empirically. The concavity maxima can be visualised with a concavity-distance plot (Fig. 7\n).\nA line of contact between two touching grains is described by a pair of nodes, between which a break line is constructed. The two grains will be separated along the break line. Node pairs may exist within a parent contour, between parent and child, or between adjacent child contours (Fig. 8\n).\nNodes are paired based on a shortest distance preference, and only between contours of the same family. Nodes from the same child contour may not be paired. Two nodes may not be paired if the break line between them extends outside of the parent contour, extends inside of a child contour, or extends across an existing break line. Additionally, nodes may not be paired if the angle between the break line and the bisector of each node's internal angle is greater than 80\u00b0. The angle of 80\u00b0 is empirically chosen. Finally, parent break lines are filtered to ensure that the contours generated when the parent contour is divided by the break line, are sensible in the context of zircon grains. Specifically, we calculate a length-to-volume ratio, described by equation (7). If the length-to-volume ratio is above the emperically chosen value of 0.8, the break line is discarded. In equation (7), \n\nT\n\n is the length-to-volume ratio, \n\nB\nL\n\n is the length of the break line, \n\nA\n1\n\n and \n\nA\n2\n\n are the areas of contour 1 and contour 2, created when a parent contour is split by the break line.\n\n(7)\n\n\nT\n=\n\nBL\n\nminimum\n\n(\n\n\nA\n1\n\n\n,\n\n\nA\n2\n\n\n)\n\n\n\n\n\n\n\n\nOnce two nodes are paired, they are removed from the pool of available nodes (Mebatsion and Paliwal, 2011). Any remaining unpaired nodes are discarded.\n\n\n\n4\nCase study 1: crystalline rock\nThe functionality of the application was tested on three samples of igneous zircon sourced from compositionally distinct crystalline basement units in Western Australia. These basement units would reasonably be assumed to show distinct zircon crystal shapes given their different chemistry and age (Fig. 9\n; Figure A1). Sample 178164 is a leucogabbro from the Bullock Hide Intrusion, Sholl Terrane, West Pilbara Superterrane (Wingate and Hickman, 2009), which is chemically defined as a diorite. This sample yields a magmatic crystallization age of 3122\u00a0\u00b1\u00a03\u00a0Ma. Sample 189937 is a monzogranite from the Lewis Granite, Granites-Tanami Orogen, which yielded a magmatic crystallization age of 1788\u00a0\u00b1\u00a08\u00a0Ma (Lu et al., 2018). Sample 194763 is a gabbro from the Warakurna Supersuite, west Musgrave Province, with a magmatic crystallization age of 1070\u00a0\u00b1\u00a07\u00a0Ma (Kirkland et al., 2011). Sample images were sourced from the Geological Survey of Western Australia\n1\n\n\n1\nFurther sample details can be accessed from http:\/\/www.dmp.wa.gov.au\/geochron\/.\n.\nImages were segmented and the grain shape parameters measured using AnalyZr (Figure A2). Shape measurements are provided in the Supplementary data. The three samples show statistically dissimilar populations for the thirteen shape parameters measured, as determined by an independent samples Kolmogorov-Smirnov (KS) test (Table A1). The KS test evaluates the null hypothesis that the distributions are drawn from the same parent population. Examples of sample populations and KS test results are provided for roundness, major axis, and minor axis parameters (Fig. 10\n, Table 2\n).\nDiffering zircon shapes may reflect different magma age (Chae et al., 2021; Makuluni et al., 2019; Markwitz and Kirkland, 2018), magma chemistry (Benisek and Finger, 1993; Pupin, 1980; Vavra, 1993), zircon growth environment (Corfu et al., 2003), and\/or laboratory processing (Chew et al., 2020; Dr\u00f6llner et al., 2021; Sl\u00e1ma; Ko\u0161ler, 2012; Zutterkirch et al., 2021). The potential influence of laboratory processing is, in this case, minimised as all three samples were processed via the same workflow by the Geological Survey of Western Australia following standardised procedures (Nelson, 1997, 1999).\n\n\n5\nCase study 2: detrital zircons\nTo demonstrate that geologically meaningful data can be recovered from grain shape analysis of detrital zircon grains, GSWA sample 195464 (Wingate et al., 2021) was analysed by AnalyZr. The sample was collected from a petroleum well at a depth interval of 1917.42\u20131916.55\u00a0m, and represents the Acacia Sandstone Member of the Willara Formation, Canning Basin, Western Australia (Fig. 11\n). The age spectrum of sample 195464 indicates at least five discrete age components (Fig. 11), thus the sample has potential for variable basement or reworked detrital zircon sources.\nThe sample was binarised, segmented, and measured using AnalyZr. Analytical grain spot IDs were captured during this process and used to link grains with their isotopic age. Single grain concordia ages were calculated using Isoplotr (Vermeesch, 2018), and the dataset was divided into five age groupings that correspond to modes in the age histogram: i) 900-700Ma, ii) 1300-1000Ma, iii) 1900-1700Ma, iv) 2100-1900Ma, and v) 2700-2500Ma.\nAnalyZr was able to extract geologically meaningful patterns between age and zircon grain shape parameters. For the detrital sample in question, measurements of length such as major axis length, as well as size such as area, are negatively correlated with age. In contrast, roundness is positively correlated with age. Measurements of width, such as minor axis length, show no significant correlation to age. The variance decreases with increasing age for all shape parameters. These observations are consistent with those of Markwitz and Kirkland (2018) who found that minor axis length was least affected by transport and suggested that zircon grains are most likely to break perpendicular to their major axis during sedimentary transport. Increased rounding and shortening of the zircon grain long axis with age may reflect the increased sedimentary transport and\/or recycling of older grains (Zoleikhaei et al., 2016). Shape measurements are provided in the Supplementary data, with shape-age trends summarised in Fig. 12\n.\n\n\n6\nConclusion\nWe present AnalyZr, a Python application for image segmentation of transmitted and reflected light images of zircon grains in resin mounts. This application was developed to address a need for specialised, automated image segmentation tools for the geosciences, and specifically to leverage grain shape as a tool for provenance and magmatic system analysis. Potential applications of AnalyZr include studies of provenance analysis where potential sources have non-unique U-Pb ages; the relationship between crystalline rock chemistry and final zircon grain shape, and the effect of sedimentary processes on zircon grain shape. AnalyZr thus has the potential to assist the geological community in establishing robust and quantitative models of the response of zircon grain shape to rock chemistry, sedimentary transport mechanisms and transport distance.\nAnalyZr leverages an updated object separation algorithm that uses elliptical Fourier descriptors in conjunction with concavity, to improve the separation of touching zircon grains in images. This algorithm brings increased speed and objectivity to the zircon image segmentation routine. Automated grain boundary extraction and segmentation may be manually adjusted by users, which allows the full image segmentation procedure to be handled within a single application, effectively removing the need to post-process binary images in additional image manipulation software.\nThe application and its algorithms are built on free, open-source platforms to increase accessibility and encourage the further development of automated tools for the geoscientific community. Outputs of the application include a black and white binary image and a tabulation of 13 shape parameters derived therefrom. Shape measurements may be output to a .csv or .mdb file.\nAnalyZr was applied to three samples of igneous zircon grains from lithologically and compositionally distinct crystalline basement rocks from across Western Australia; namely, granite, diorite and gabbro. The software processed images and measured grain shapes with sufficient accuracy to resolve statistically significant zircon grain shape differences between the different igneous rock types. When applied to a detrital sample, the application was able to resolve a trend towards smaller and rounder zircon grains with increased grain age. In addition, measurements of minor axis length show no trend with age, which may reflect a resistance to change during sedimentary transport.\n\n\nComputer code availability\nAnalyZr is developed as a Python 3.7 application and is available under a MIT license. The application may be downloaded from https:\/\/github.com\/TarynScharf\/AnalyZr. AnalyZr is designed for Windows 10 and Ubuntu. For enquiries contact t.scharf@postgrad.curtin.edu.au.\n\n\nAuthorship contribution statement\nTaryn Scharf: Data curation, formal analysis, investigation, methodology, software, validation, visualization, writing \u2013 original draft, writing \u2013 review and edit. Chris L. Kirkland: Conceptualization, funding acquisition, resources, supervision, writing \u2013 review and edit. Matthew L. Daggitt: Software, methodology, writing \u2013 review and edit. Milo Barham: Funding acquisition, supervision, writing \u2013 review and edit. Vladimir Puzyrev: Methodology, supervision, writing \u2013 review and edit.\n\n","77":"\n\n1\nIntroduction\nMorphometric characteristics such as the bankfull width (B), bed slope (S), centreline radius of curvature (R), bankfull depth (H\n\nB\n) and related non-dimensional parameters such as the ratio of width to centreline radius of curvature (B\/R), channel aspect ratio (B\/H\n\nB\n) or sinuosity (SI), play a key role in the classification of river systems (da Silva and Yalin, 2017; Leopold and Wolman, 1957; Schumm, 1981). These parameters also serve as input parameters or boundary conditions for hydrodynamic models of meandering rivers (Blanckaert and De Vriend, 2010; Ikeda et al., 1981; Ottevanger et al., 2013; Parker et al., 2011) and for meander planform evolution models (Abad and Garcia, 2006). However, the largest meandering systems on Earth are subaquatic channels and canyons (Harris and Whiteway, 2011). These are carved by gravity-driven density currents, called turbidity currents, and form deeply incised V-shaped canyons with terraces on the seabed with generally a transition to U-shaped channels with flanking levees (Covault et al., 2014). Canyons and channels can also form outside Earth, as observed on Mars (Alemanno et al., 2018; Baker, 2001; Masursky et al., 1977) and Titan (Burr et al., 2013; Lorenz et al., 2008).\nThe similar visual appearance of subaerial, subaquatic and extra-terrestrial systems, illustrated in Fig. 1\n, has inspired analogies between them (Imran et al., 1999; Konsoer et al., 2013; Straub et al., 2007). The same subaerial morphometric characteristics are used in subaquatic systems to understand their formation and dynamics (Covault et al., 2014; Gales et al., 2019; Heijnen et al., 2020; Konsoer et al., 2013; Pettinga et al., 2018), and in some cases applied to extra-terrestrial channels to investigate their origins (Williams and Phillips, 2001). Due to this broad relevance of morphometric characteristics, it is important to have an efficient and easy-to-use tool for determining these characteristics.\nTo date, specialised tools exist for individual morphometric characteristics. However, most are only applicable to rivers, and no universal procedure for extracting morphometric characteristic of subaerial, subaquatic and extra-terrestrial systems is available. There are several methods like RivWidth (Pavelsky and Smith, 2008), ChanGeom (Fisher et al., 2013), RivMAP (Schwenk et al., 2017), RivaMap (Isikdogan et al., 2017) or Orinoco (Marshak et al., 2020) for processing remotely sensed imagery to derive a pixel-based centreline and determine the width of a river. The package cmgo (Golly and Turowski, 2017) uses discrete bank points to determine the centreline and associated width without the need for pixel data. Clerici and Perego (2016) go one step further and additionally enable the determination of sinuosity. There are many other methods that deal with the planform description of rivers, but they are often extensions of closed-source GIS programmes, which limits access and changes to the code (Golly and Turowski, 2017). Merwade et al. (2005) work with bathymetry data, which makes it possible to determine the thalweg and provides channel geometry for modelling tasks, but does not output any other morphometric characteristic.\nIn subaquatic systems where acoustically-based bathymetric digital elevation models (DEMs) are available instead of imagery, morphometric characteristics have mostly been determined manually (Babonneau et al., 2002; Heijnen et al., 2020; Pirmez and Flood, 1995). Shumaker et al. (2018) developed a Matlab script that determines the bankfull width (B), the bankfull depth (H\n\nB\n), cross-sectional area (A) and channel aspect ratio (B\/H\n\nB\n), from a manually predefined thalweg. This script is not publicly available. In addition, when describing the condition in underwater bends, usually only a manual estimated constant radius of curvature (R) is used without taking the continuous change in curvature into account (Sumner et al., 2014; Wei et al., 2013). In extra-terrestrial systems, topographic DEMs are created using optical or radar measurement methods (Burr et al., 2013; Jaumann et al., 2007; McEwen et al., 2007; Smith et al., 2001), which are often evaluated manually in terms of morphometric characteristics (Williams and Phillips, 2001).\nThe main objective of this paper is to present an efficient and quasi-automatized Matlab script that extracts a detailed set of morphometric characteristics of subaerial, subaquatic and extra-terrestrial rivers, channels and canyons as shown in Fig. 2\n. Only a definition of the bounding levees or banks is required as input variables, which do not have to fulfil any specific requirements. The script determines the centreline, even for very irregular levee or bank lines and sharp bends, and further provides the gradually evolving radius of curvature (R), sinuosity (SI) and associated bankfull width (B). If a DEM of bathymetry or topography is available, the script provides additional morphometric characteristics, such as thalweg position, bed slope (S), bankfull depth (H\n\nB\n), cross-sectional area (A), channel aspect ratio (B\/H\n\nB\n) and levee slope (\u03b1). The presented script therefore provides an efficient tool for analysing the ever-growing database of planimetric and DEM data on subaerial, subaquatic, and extra-terrestrial rivers, channels and canyons (Fisher et al., 2013; Jaumann et al., 2007; Lorenz et al., 2008; McEwen et al., 2007; NOAA National Centers for Environmental Information, 2004).\n\n\n2\nMethodology\nA flowchart of the individual steps in the procedure is given in Fig. 3\n. In a first step, the script determines morphometric characteristics related to the planimetry. The only input data required are a definition of the levee or bank crests (in MAT, CSV or XLS format) as indicated in Figs. 1 and 2. Based on this, a centreline is defined as the curved line in the middle between both levees or banks. In spite of the seemingly simple nature of this intuitive definition, its transcription in a robust mathematical formulation is surprisingly difficult. The adopted approach uses midpoints from the independent defined levee or bank crest points, which leads to a continuous analytical description of a splined centreline. By a specific sorting out of the midpoints, which is described in detail in section 3.2, this method leads to a robust definition of the centreline, especially in irregular geometries, with no need for defining opposite points as in Legleiter and Kyriakidis (2006). The use of Voronoi polygons would also not require opposite points but has difficulties in abrupt planform variations and therefore requires filtering (Golly and Turowski, 2017). Furthermore, the use of Voronoi polygons does not lead to a continuous description of the centreline, which complicates further calculations such as the estimation of the curvature. Approaches to centreline computation using grid points (Fisher et al., 2013; Isikdogan et al., 2017; Marshak et al., 2020; Pavelsky and Smith, 2008; Schwenk et al., 2017), usually from remotely sensed imagery, also do not need the definition of opposite points, but must perform computations at each grid point. The method reported here does not need any grid information up to this point (no remotely sensed imagery or DEM needed) and is therefore more universally applicable to a wider range of data. Moreover, it is computationally more efficient than grid-point-based methods.\nFrom the definition of the centreline, additional morphometric characteristics are estimated, e.g. the bankfull width (B), radius of curvature (R), bend apices where by definition 1\/R attains it maximum magnitude, and crossovers where by definition 1\/R\u00a0=\u00a00, and the sinuosity (SI). Note that the centreline curvature is systematically represented by 1\/R to avoid infinite values at cross-overs. In this paper, a channel-centred curvilinear reference system (s, n, z) is adopted. The s-coordinate corresponds to the distance downstream along the centreline and the n-coordinate corresponds to the perpendicular distance to the centreline with the n-axis positive towards the left crest (Legleiter and Kyriakidis, 2006).\nIf a DEM of bathymetry or topography is available, it is converted in the same channel-centred curvilinear reference system, thus allowing for a straightforward analysis of additional morphologic characteristics in cross-sectional planes, such as bankfull depth (H\n\nB\n), cross-sectional area (A), channel aspect ratio (B\/H\n\nB\n), levee slope (\u03b1) and thalweg position. These are illustrated in Fig. 2.\n\n\n3\nMatlab script description\n\n3.1\nDefinition of the coordinates of the crests of the bounding levees or banks\nThe levee or bank crests are represented by an array of discrete points that are defined by their coordinates in an arbitrary orthogonal Cartesian reference system. A spacing between points on the order of half the channel width is recommended. How these points are defined depends on the available data source and the research question to be answered. For example, remotely sensed imagery is often available for rivers, where the boundary of the water surface can be determined with the help of software (Ziou and Tabbone, 1998). If these points are used as bounding bank crests, this leads to the river width (B\n\nRi\n), as illustrated in Fig. 1c. In geomorphology, the bankfull geometry is often of interest. It is delimited in channels by the highest point of the external levee or bank crests (Konsoer et al., 2013; Shumaker et al., 2018) and in canyons by the first distinctive terrace of the active channel as illustrated in Fig. 2. How exactly the levee or bank crests are obtained is not the subject of this paper, they are only treated as input data for the script.\n\n\n3.2\nDefinition of centreline (01_Definition.m)\nThe first computational module of the Matlab script defines a centreline as a basis for the further extraction of the principal planform morphometric characteristics (radius of curvature (R), sinuosity (SI) and associated bankfull width (B)). Both levee or bank crests are independently defined by means of an array of discrete points, as described before (Section 3.1). The array of points is used to create a continuous analytical description of both crests by using smoothing splines. Smoothing splines provide smooth, interpolated curves which lie within a given tolerance of the given data points (G\u00fcneralp and Rhoads, 2008; Reinsch, 1967). These smoothing splines allow analytical descriptions of derivatives, which is essential for accurately estimating the local radius of curvature of the levees\/banks and centreline (G\u00fcneralp and Rhoads, 2008). These smoothing splines also allow for resampling both levees on a new s-grid. The step size \u0394s can be chosen by the user and is independent of the original spacing of the levee or bank coordinates points. A spacing of about half the channel width (B\/2) is recommended. For all resampled points on the new s-grid, a nearest-neighbour approach is adopted to identify the distance to the opposite levee or bank crest.\nIn mildly or moderately curved bends and configurations with gradually-varying planform, the centreline would easily be found at the mid-distance to the opposite levee or bank. However, in sharply-curved bends or configurations with abrupt planform variations this procedure often fails. Fig. 4\na illustrates how erroneous points are identified on the outer crest when searching for nearest neighbours of points on the inner crest. The procedure is robust, however, when starting from a point on the outer crest and searching for the nearest neighbour on the inner crest, as also illustrated in Fig. 4a. The analytical description of both levees or banks with smoothing splines allows computing their curvature and identifying if they are locally at the inside or outside. For example, the outer levee or bank is situated at the right side in a left-turning bend and at the left side in a right turning bend (Fig. 4). Left turning bends are characterized by a radius of curvature R\u00a0>\u00a00 and right-turning ones by R\u00a0<\u00a00 in the adopted (s, n, z) reference system.\nIn the next step, all centreline points originating from the outer crest are reordered based on a nearest-neighbour approach. Finally, based on these reordered centreline points, a continuous analytical representation of the centreline is created with a smoothing spline.\nThis continuous analytical description of the centreline is now resampled in \u0394s-steps to obtain the final channel-centred curvilinear (s, n, z) reference system with cross-sections perpendicular to the centreline as defined in Fig. 2. Opposite points on both levee or bank crests are found as the intersection of the cross-section with the spline representations of both crests. This provides the bankfull channel width (B) in each cross-section. In very sharp or irregular bends, the line perpendicular to the centreline may not always intersect with both opposite crests, as illustrated in Fig. 4b. In these cases, which are automatically detected, a corrected cross-sectional orientation is defined by interpolation as also illustrated in Fig. 4b. Thus, a smoothly varying orientation of the cross-sections is achieved. Shumaker et al. (2018) encountered a similar problem in the definition of cross-sections, which they based on the manual identification of the thalweg. They ignore the problematic cross-sections, leading to a loss of important information in sharp or irregular bends.\nThe script then displays a set of figures for the purpose of quality checking, and it provides the principal morphometric planform characteristics. At this stage, the main morphometric planform characteristics are computed from the final spline representation of the centreline. These include 1\/R, bankfull width (B), the position of the cross-overs and the apices, and the sinuosity (SI). The calculation of SI follows the procedure of Clerici and Perego (2016). Here, the centreline is segmented in fixed lengths and then divided by the straight-line-distance between its endpoints. The centreline tract is progressively shifted downstream by \u0394s along the entire centreline and the computed sinuosity value is assigned to the midpoint of the tract. The choice of segment size for the sinuosity calculation has a major influence on the results. Therefore, sinuosity calculations are carried out for different segment sizes. An estimation of SI that is independent of the scale of the system or of the user-defined fixed segment size, called Sinuosity0, is obtained by taking segments that coincide with individual bends as limited by successive cross-overs. Another estimate of SI that is independent of the scale of the system is the variant called Sinousity1, where a multiple of the characteristic width of the whole system is used as segment size. Here the median width (B\n\nM\n) is adopted as characteristic width and a segment length of 20 B\n\nM\n is defined as the default value in the script, which typically corresponds to a wavelength in meandering rivers (Ferguson, 1975). Other segment lengths expressed as multiples of B\n\nM\n can be user-defined. The planform morphometric characteristics are displayed in a figure, as illustrated further in the paper in the application example (Fig. 7). The spline descriptions of the centreline, bankfull width (B) and crests are saved for transfer to the next computational module of the script.\n\n\n3.3\nDEM conversion (02_DEM_Extraction_xyz.m)\nIf a DEM of bathymetry or topography is available, it is converted into Matlab matrices for the final computational module. The DEM data must be input in GeoTIFF or XYZ format in the same Cartesian reference system as the crest input data. Since the DEM data are the most memory-intensive data sets, the conversion is outsourced to a separate file so that it only has to be executed once and can remain unchanged when changes are made in other computational modules. After successful conversion, the DEM data set is visualized and saved for further processing in the next computational module.\n\n\n3.4\nMorphologic characteristics (03_Analysis.m)\nFirst, DEM data are resampled on the centreline fitted (s, n, z) reference system with the integrated Matlab-interpolation for 2-D gridded data. In this interpolation procedure, the grid size in s and n direction can be defined by the user. It is recommended that the user takes the same s grid as in the first computational module of the script. For the grid size in the n direction, it is advisable to retain a value close to the resolution of the DEM. In case the s resolution is changed from the first computational module, R, B, SI, and the location of bend apices and cross-overs are re-calculated following the same method described in Section 3.2. For the analysis, the cross-sections are extended beyond the levee or bank crests by a user-defined additional corridor width (W\n\nC\n) as illustrated in Fig. 2.\nFrom the DEM data in this channel-centred curvilinear (s, n, z) reference system, it is straightforward to determine some important morphometric characteristics for every cross-section. The cross-sectional area (A) is calculated by integrating the area below the line connecting both levee or bank crests, as defined in Fig. 2. The wetted perimeter is obtained as the length along the bed between both crests. The thalweg is defined by the minimum bed elevation in the cross-section between the levees or bank crests. Following the conventional definition of \u201cbankfull flow\u201d (Konsoer et al., 2013), the bankfull channel depth (H\n\nB\n) of the active channel is defined as the vertical distance between the thalweg and the average elevation of both crests, as shown in Fig. 2. The surrounding elevation is defined as the highest DEM elevation within the corridor width (W\n\nC\n). It is of interest, especially in canyons, where the levee or bank crests only represent the active channel and the canyon depth (H\n\nC\n) is defined as the vertical distance between the surrounding elevation and the thalweg. The thalweg is used to calculate the longitudinal bottom profile, along which the thalweg slope (S) is derived. For the estimation of the levee or bank slopes in the active channel (\u03b1), the levee or bank is first delimited by eliminating the central bottom part of the cross-section, identified by a transverse slope smaller than 0.05. In order to eliminate the effect of irregularities in the DEM data, \u03b1 is then obtained by the 75% quantile value of the transverse slope.\nThe most relevant morphometric characteristics are displayed in a series of figures described in detail in the included manual. The main variables and characteristics are also saved in a.MAT file for possible further processing going beyond this script.\n\n\n\n4\nRobustness and user-defined options\nThe definition of the bounding levee or bank crest is user-defined and therefore subjective. In the description and example in this manuscript, the bankfull flow approach was chosen to define the levee crests, as has been described before (Konsoer et al., 2013; Shumaker et al., 2018). The crests of the bankfull active channel should closely follow the course of the thalweg. The visual estimation of the position of the crests from DEMs relies on expert judgement. In case only remotely-sensed imagery is available, the position of the transverse edges of the water surface can be used as an approximation (Legleiter and Kyriakidis, 2006; Ziou and Tabbone, 1998). Obviously, these vary with discharge and the flow level.\nSmoothing splines (Reinsch, 1967) are used throughout the script, in particular for determining the centreline and for compensating for irregular and noisy input data. In contrast to the use of splines which can lead to more noise (G\u00fcneralp and Rhoads, 2008), smoothing splines represent, by definition, a user-defined optimal compromise between smoothness and closeness to data points. The default parameters in the script have proven robust in a wide variety of configurations, but may have to be optimised for other specific configurations.\nThe most important user-defined variable is the step size \u0394s for interpolation and splining of the crest lines and the centreline (s-coordinate for the channel-centred curvilinear (s, n, z) reference system), which also defines the spacing between cross-sections. G\u00fcneralp and Rhoads (2008) and Legleiter and Kyriakidis (2006) recommended \u0394s\u00a0\u2248\u00a0B. Their analysis mainly concerned meandering river bends with gradual curvature variations. The choice \u0394s\u00a0\u2248\u00a0B is appropriate for gradually varying curvatures, but when applying the here-proposed script to bends with abrupt curvature variations as shown in Fig. 4, \u0394s\u00a0\u2248\u00a0B leads to insufficient spatial resolution. Therefore, a value of \u0394s\u00a0\u2248\u00a0B\/2 is recommended as an optimal value that provides good levels of information even with abrupt changes, but still does not lead to problems with overlapping as with even smaller \u0394s. If \u0394s is too large, local planimetric variations may be overlooked. This is shown in a sensitivity analysis using the application example in the supplementary material. For large-scale applications with no interest in local features, a larger value can of course be useful, depending on the specific research question.\nFor \u0394n (grid size in n-direction) in the third computational module, it is advisable to retain a value close to the resolution of the DEM. A smaller value will not increase the accuracy and a larger value will cause a loss of DEM information. In order to reduce the size of the DEM matrix, the corridor width (W\n\nC\n) outside the crests must be chosen as small as possible, but for canyons at least large enough so that the whole canyon outside the active channel is covered (Fig. 2).\nThe evolutions of the morphometric characteristics (B, H\n\nB\n, H\n\nC\n, A, R, \u03b1, SI) along the entire system are typically characterised by a relatively high degree of scatter. This scatter is reduced with smoothing splines. The default parameters of the splines relate to the system size and have proven robust in a wide variety of configurations. Obviously, the spline parameters can be further optimised by the user.\nThe longitudinal bottom profile will be influenced by features with different spatial scales; knickpoints represent local steep steps in channel gradient that migrate upstream (Crosby and Whipple, 2006; Guiastrennec-Faugas et al., 2021; Heijnen et al., 2020; Howard et al., 1994); bedforms represent small-amplitude features that are defined over relatively short distances (Kostic et al., 2010; Normandeau et al., 2020); and the longitudinal bottom slope is defined over longer reaches. The third computational module includes by default three levels of smoothing of the longitudinal bottom profile. The level with the lowest level of smoothing identifies bedforms, an average level of smoothing identifies knickpoints, and the highest level of smoothing is applied to estimate the longitudinal bottom slope (S). These three different levels of smoothing are assumed to be case-dependent and have to be optimised by the user.\n\n\n5\nApplication example: Lake Constance\nTo demonstrate the utility of the presented script, it is applied to the 16\u00a0km long canyon-channel system shaped at the bottom of Lake Constance (Germany\/Austria\/Switzerland) by the inflow of the Alter-Rhein River. Fig. 1 illustrates this canyon-channel system based on bathymetric DEM from a 2013 survey (Wessels et al., 2015). The input bathymetry data are available with a horizontal resolution of 3\u00a0m (Internationale Gew\u00e4sserschutzkommission f\u00fcr den Bodensee IGKB, 2015). It is provided in the reference system of \u201cEPSG:5677: DHDN\/3-degree Gauss-Kruger zone 3 (E-N)2\u201d and the coordinates of the levee crests have been manually defined in the same reference system, as illustrated in Fig. 5\n. In the following, only the most important result figures of the script are presented in a raw form without post-processing. The complete figures are given in the supplementary manual or in the provided example files.\nA spacing of \u0394s of 50\u00a0m was chosen in the first computational module of the script, which corresponds to about half the width of the channel and leads to good definitions of the centreline and the cross-sections, as shown in Fig. 6\n\n.\nThe final Fig. 7 of the first computational module shows the principal planform morphologic characteristics.\nFor the morphometric analysis based on the bathymetric DEM, \u0394s was left unchanged at 50\u00a0m. For \u0394n, 3\u00a0m was chosen in reference to the resolution of the bathymetric data and 300\u00a0m was selected as the corridor width (W\n\nC\n). The water surface in lake Constance is kept constant at 395\u00a0m asl. For convenience, the z-coordinate is given as distance below the water surface. The Matlab script provides the principal morphometric characteristics in the form of Fig. 8\n and Fig. 9\n. Fig. 8 provides a planform overview of the entire Lake Constance subaquatic system including the levee crests, centreline and distance along the centreline, thalweg, bend apices and cross-overs. Fig. 9 summarises the most significant morphometric characteristics: longitudinal profiles of the crest levels and thalweg, longitudinal slopes, bankfull depth (H\n\nB\n), canyon depth (H\n\nC\n), bankfull depth (B), cross-sectional area (A), channel aspect ratio (B\/H\n\nB\n), sinuosity (SI), width-to-curvature ratio (B\/R) and levee slope (\u03b1).\nAlthough the data analysis is not the purpose of the present paper, some basic interpretations are given for the purpose of illustrating the added value of the figures. Distance hereafter are given in km from the river-lake confluence. At km 4, the Lake Constance canyon-channel system undergoes significant change and transitions from a canyon to a channel (Fig. 9a,c). Downstream of this point, different patterns of longitudinal slope (Fig. 9b) and sinuosity (Fig. 9g) develop. Between km 4 and 12, the meandering section of the channel is characterised by a good correlation between the radius of curvature (Fig. 9h) and the levee slopes (Fig. 9i). Downstream of km 12, the degree of meandering is smaller, as indicated by the sinuosity (Fig. 9g), radius of curvature (Fig. 9h) and levee slope (Fig. 9i). A knickpoint at km 12 is recognisable from the longitudinal profile (Fig. 9a) and longitudinal slope (Fig. 9b).\nThe results shown are the outcome of a user-friendly and computationally efficient script. For a system of the size of Lake Constance and an unexperienced user, it takes about 2\u00a0h from the creation of the input files to the final figures. It takes about 40\u00a0min for the manual definition of the levees, about 20\u00a0min for the preparation and conversion of the input files, about 30\u00a0min for the first computational module of the script to find the centreline, and about 30\u00a0min for the remaining computational modules. This is also thanks to the programming, which is intentionally kept as simple as possible in \u201ccopy and paste\u201d style, so that even non-expert Matlab users can use the script and make individual adjustments or improvements.\n\n\n6\nDiscussion\nThe application range of the Matlab script is limited to single-thread planforms. Nevertheless, it still has a broad application range in subaerial, subaquatic and extra-terrestrial systems. In all three settings, the amount of high-resolution DEM data is rapidly growing. For subaquatic channels and canyons, increasing numbers of systems are being surveyed and studied (Talling et al., 2013), and in some cases even repeated surveys are available (Guiastrennec-Faugas et al., 2021; Heijnen et al., 2020; Pope et al., 2022; Vendettuoli et al., 2019). For subaerial rivers, an abundant database of freely available remotely sensed imagery is already available (Fisher et al., 2013), and the database of bathymetric DEM data from multi-frequency LIDAR is steadily growing (Lague and Feldmann, 2020; Mandlburger et al., 2015). For extra-terrestrial channels and canyons, there have been missions, especially to Mars and Titan, in the last decades that have carried out increasingly detailed surveys (Alemanno et al., 2018; Jaumann et al., 2007; Lorenz et al., 2008), and more celestial bodies will follow in the future. For this ever-growing amount of bathymetric and topographic DEM data, the Matlab script provides an efficient way of determining the morphometric characteristics as a basis for further analysis and applications in research and engineering.\nA first research application concerns an extensive comparison of different subaquatic systems. Due to the sparsity of available data and the manual character of tools for morphometric analysis, previous comparisons were limited in scope (Konsoer et al., 2013; Shumaker et al., 2018). Such an extensive comparative analysis will allow identifying similarities and differences between systems, and give indications on the formative geomorphologic processes, such as the role played by turbidity currents or extreme events.\nA second research application concerns the analysis of similarities and differences between subaquatic and extra-terrestrial systems at the one hand, and the more intensively investigated subaerial systems at the other. This analysis will indicate to what extent theories for subaerial systems, such as the regime theory that relates characteristics of water and sediment fluxes to morphometric characteristics (da Silva and Yalin, 2017; Konsoer et al., 2013), can be applied to or extended for subaquatic and extra-terrestrial systems.\nA third research application concerns the identification of specific morphological features which are known to play an important role in the geomorphologic system development. The script can, for example, identify knickpoints, which were shown by Heijnen et al. (2020) and Guiastrennec-Faugas et al. (2021) to control subaquatic channel evolution, and it can identify landslides, which were shown by Pope et al. (2022) to have a damming effect in subaquatic canyons.\nA fourth research and engineering application concerns the generation of the morphometric input data from numerical models. The output of the script can, for example, directly be used as input data for the river meander models of (Blanckaert and De Vriend, 2010; Ikeda et al., 1981; Ottevanger et al., 2013; Parker et al., 2011).\nA fifth research application concerns the meta-analysis of subaerial, subaquatic and extra-terrestrial systems. At present, a script is being developed for the automatic determination of the bounding levee or bank crests based on DEMs. Once this script is available, it will allow a fully-automatic analysis, which is appropriate for a meta-data approach.\n\n\n7\nConclusions\nThis paper presents a new Matlab script for the determination of morphometric characteristics in subaerial, subaquatic and extra-terrestrial rivers, channels and canyons. From pre-defined levee or bank crests, the script determines a centreline. As an improvement over previous methods, the script is also robust for very sharp bends of irregular planform with abrupt curvature changes. This script provides planform morphometric characteristics such as bankfull width (B), centreline radius of curvature (R) and sinuosity (SI), and determines the orientation of cross-sections, and the location of bend apices and cross-overs. If bathymetric or topographic digital elevation data are available, the script provides additional morphometric characteristics such as thalweg, slope (S), bankfull depth (H\n\nB\n), cross-sectional area (A), channel aspect ratio (B\/H\n\nB\n) and levee slope (\u03b1).\nThe application example of Lake Constance demonstrates that the script is user-friendly and computationally efficient. For this 16\u00a0km long canyon and channel system, an unexperienced user will need about 2\u00a0h to set-up and run the script and acquire all the relevant morphometric characteristics. The Matlab script is robust with the implemented default parameters, but user-defined options allow for flexibility and further optimisation.\nFor the ever-growing amount of bathymetric and topographic DEM data, the Matlab script provides an efficient way of determining the morphometric characteristics as a basis for further use in a broad variety of applications in research and engineering.\n\n\nAuthorship contribution statement\nMartin Hasenh\u00fcndl: Wrote the manuscript, improved and adapted script and methodology.\nKoen Blanckaert: Supervised this work, started the script and methodology, helped in the written part.\n\n\nCode availability section\nThe script (developed by Martin Hasenh\u00fcndl and Koen Blanckaert, Institute of Hydraulic Engineering and Water Resources Management, TU Wien, Karlsplatz 13, 1040 Vienna, Austria, e-mails: martin.hasenhuendl@tuwien.ac.at, koen.blanckaert@tuwien.ac.at), is freely available (CC BY-SA 4.0) with 172\u00a0KB in size at https:\/\/gitlab.tuwien.ac.at\/martin.hasenhuendl\/matlab-script-for-the-morphometric-analysis (including user manual, script files with self-explanatory comments and the application example).\nThe raw bathymetric DEM data for the application example of Lake Constance is also freely available at https:\/\/doi.pangaea.de\/10.1594\/PANGAEA.855987.\nThe script requires the basic module of Matlab (http:\/\/www.mathworks.com\/products\/matlab\/) with the following toolboxes: \u2018Signal Processing Toolbox\u2019, \u2018Mapping Toolbox\u2019, \u2018Curve Fitting Toolbox\u2019 and \u2018Statistics and Machine Learning Toolbox\u2019. There are no special system requirements, an ordinary PC or notebook suits well. The stability of the Matlab calculation depends mainly on the size of the DEM data (available resolution). The script is tested for Matlab versions 2018a and higher under 32\/64-bit Windows systems.\n\n","78":"\n\n1\nIntroduction\nArtificial intelligence (AI) has received increasing attention in geosciences in the past decade (Gil et al., 2019). In particular, for data-intensive geosciences there has been a significant growth of machine learning (ML) and deep learning (DL) applications in recent years (Lary et al., 2016; Bergen et al., 2019; Karpatne et al., 2018; Reichstein et al., 2019). Besides ML and DL, knowledge engineering, logic, and reasoning are also essential topics in AI (Russell and Norvig, 2021), among which the knowledge graph (KG) rises as a unique subject. A KG is a graphical representation of structured knowledge from the real world, in which the nodes represent entities of interest and the edges represent relationships between those entities (Sheth et al., 2019b; Hogan et al., 2020). In a data life cycle (Wing, 2019), such as the data-intensive geoscience research (Gil et al., 2019), the associated works of KG connect the upstream work of knowledge engineering and representation, the midstream work of data curation and integration, and the downstream work of data analysis and result communication. For instance, the OneGeology-Europe project (Laxton, 2017) illustrated intelligent applications of KGs in geologic map integration and service. About 20 European countries participated in the project to share national geologic map services, but many of them were originally recorded in their national official languages. The project has built multi-lingual vocabularies to mediate across those map services. On the data portal of OneGeology-Europe, a user can write a query with English labels of rock age or type, then the functions based on the vocabularies can translate the query into different languages and send them to the corresponding services. The records returned from multiple services are organized in a consistent form just like they are returned from a single European geologic map service.\nAs a reflection, earlier publications in geoinformatics and geomathematics have addressed the importance of machine-readable knowledge models in the cyberinfrastructure (e.g., Loudon, 2000, 2009) and the flexible application of data-driven and knowledge-driven approaches in data analysis (e.g., Bonham-Carter, 1994; Carranza, 2009). Very recently, Gutierrez and Sequeda (2021) reviewed the interweaving of data and knowledge since the advent of modern computing in the 1950s, to reveal the historical roots of the KGs in nowadays. They suggested that both statistical and logical methods contribute to the convergent work of data science, and the next-generation scientists should be aware of the KG developments in addition to the overwhelming ML and DL studies. However, comparing with the many recent review papers on ML and DL in geosciences, there is a shortage of summary and review of KGs in geosciences. Although there has been some progress in geoscience KG construction and application in the past decades, such as the work on geospatial semantics (Compton et al., 2012; Janowicz et al., 2012; Tandy et al., 2017), the entrance barrier to KG still seems high to many geoscientists, especially newcomers.\nThe history of KG can be traced back to ancient people's idea of representing knowledge in a diagrammatic form (Gutierrez and Sequeda, 2021). The Google Knowledge Graph released in 2012, together with similar ideas at Microsoft, Facebook, eBay, and IBM, significantly increased the visibility of KG as an AI approach to researchers and the public (Noy et al., 2019b). Yet, for KG practitioners in geosciences, it is necessary to realize that KG is rooted in several areas in computer science. At the 2019 U.S. Semantic Technologies Symposium (Durham, NC), there was an active discussion on the statement that \u201cIn the 1990s, we talked about vocabularies; in the 2000s, we talked about ontologies; and in the 2010s, we began to talk about knowledge graphs.\u201d There have been several initiatives on building vocabularies, ontologies and KGs in geosciences and applying them to improve the data life cycle in geosciences. The Commission for Geoinformation within the International Union of Geological Sciences (IUGS-CGI) is a facilitator of standardized geoscience vocabularies and schemas for geologic data (Asch and Jackson, 2006). Part of the IUGS-CGI outputs were adapted in the OneGeology, OneGeology-Europe and the INSPIRE programs to harmonize geologic data from distributed sources (Laxton, 2017). Federal agencies in U.S. such as USGS and NASA have also invested efforts on KGs for geoscience data management and analysis (e.g., Zhang et al., 2016; USGS NCGMP, 2020). The EarthCube, an NSF initiated program, has led to many recent progresses on geoscience vocabularies, ontologies and KGs (e.g., Richard et al., 2014; Gupta et al., 2015; Zhou et al., 2020). Two recent reports released by the World Wide Web Consortium (W3C) summarized the best practices for publishing data on the Web: one focused on the open data in its broad sense (Loscio et al., 2017) and the other specifically on spatial data (Tandy et al., 2017). Those best practices show a clear trend that KGs will take an essential role for better data services on the Web. It is also encouraging to see that a few examples from geosciences were included in the two reports.\nGeoscience KG is an interdisciplinary subject. Despite those above-mentioned progresses of KG in geosciences, the gap between geoscience and computer science still makes it hard for many real-world practitioners to see a roadmap to incorporate KGs into data-intensive geoscience research. Semantic technologies (Berners-Lee et al., 2001; Bizer et al., 2011) are a key topic of KG in existing studies. Narock and Wimmer (2017) conducted a bibliometric analysis of semantic technologies with literature from the American Geophysical Union (AGU) Fall Meetings (i.e., a representative geoscience conference) and the International Semantic Web Conference (ISWC) series (i.e., a representative computer science conference). Their results show that the overlap between AGU and ISWC is minimal. While computer scientists focus more on the precision of their algorithms and the efficiency in big data processing, geoscientists and geoinformaticians focus on the actual improvement enabled by semantic technologies in their geoscience work (cf. Hogan 2020; Hitzler 2021). Comparing with the KG construction and application in biology and biomedical studies (e.g., Ashburner et al., 2000; Gene Ontology Consortium, 2019; Nicholson and Greene, 2020), most existing geoscience KGs focus on lightweight semantics, and their applications are limited to data harmonization and integration. Computer scientists can see the potential of deeper applications of KGs in geosciences, but geoscientists would like to see a list of KG technologies that can guide them from simple to sophisticated applications (4D Initiative, 2018; Gil et al., 2019; NASEM, 2020; Wang et al., 2021).\nThe purpose of this paper is to review the existing work of KGs in geosciences, summarize the best practices, and discuss the trends of KG construction and application. The remainder of the paper is organized as follows. Section 2 summarizes the concepts associated with KG and ways to construct a KG in geosciences. Section 3 focuses the progress of KG applications in geoscience data collection, curation, and service. Section 4 summarizes KG applications in geoscience data analysis, including topics of data mining processes, social media and literature data, image analysis, vector data, and integrated applications. Section 5 discusses the trends in the near future. Finally, Section 6 concludes the paper. We hope this review will be beneficial to many geoscientists who would like to deploy KGs in their data-intensive studies.\n\n\n2\nKnowledge graph construction: associated concepts and approaches\nA KG, in its broad sense, can be envisioned as a group of nodes connected by edges, where the nodes represent entities in the real world and edges for the relationships between those entities. This is a good way to lower the barrier of entrance for geoscientists to work on KG. However, it is important to note that a graphic conceptual map is just the beginning stage. A more functional part of KG is the logical assertations we can add to the nodes and edges and the capability of reasoning and inference enabled by them.\n\n2.1\nA spectrum of knowledge graphs\nAs introduced in Hogan et al. (2020), Abu-Salih (2021), and Gutierrez and Sequeda (2021), the work on KGs in AI has close relationship to scientific advancements in Semantic Web, databases, knowledge engineering, natural language processing, and ML. In the past decades, the approach of an ontology spectrum (Welty, 2002; McGuinness, 2003; Obrst, 2003; Uschold and Gruninger, 2004) has established a roadmap for many researchers to build vocabularies, schemas, and ontologies to meet the needs of various applications. Intuitively, we can adapt that approach to establish a KG spectrum (Fig. 1\n) to guide KG construction in geosciences.\nFor all the KG types in Fig. 1, there are existing examples in geosciences. Here we will give an inter-comparison about the characteristics of those types by using those real-world examples. Catalog and glossary are often seen at the end of a book. They are normally an alphanumerical list of keywords for the content of the book. In some glossaries, each keyword is appended by all the page numbers where the keyword appears, which offer readers a quick overview about the major subjects of a book. Some glossaries are also published independently, such as the Glossary of Geology (Neuendorf et al., 2011). Taxonomy is the classification of concepts, which often shows a supergroup-subgroup structure. For example, paleobiologists use the taxonomy of domain, kingdom, phylum, class, order, family, genus, and species in the classification of life. In the geologic time scale, there is a hierarchal structure of eon, era, period, epoch and age. The periodic table arranges chemical elements by their atomic number and electron configuration, and it demonstrates the periodic trends in the rows and columns of the table. Thesaurus, sometimes called controlled vocabulary, is like a mixture of glossary and taxonomy, in which the terminology is organized within a hierarchy. The Glossary of Geology (Neuendorf et al., 2011), although organized in an alphabetical structure, shows such taxonomical information in the annotation of some terms. There are more typical examples of geoscience thesaurus (e.g., AQSIQ, 1988; Rassam et al., 1988; Gravesteijn et al., 1995; CCOP and CIFEG, 2006), and an interesting pattern of them is the inclusion of multilingual labels. Recently, many thesauri (e.g., Caracciolo et al., 2013; Stevens, 2019) were also encoded with semantic technologies, such as the Simple Knowledge Organization System (SKOS) (Miles and Bechhofer, 2009).\nConceptual schemas, also called conceptual models, are often seen in the design of data structures for relational databases. Sometimes there will be formal relationship of superclass-subclass for two entities in a schema, where a subclass inherits all the properties of the superclass. The Unified Modeling Language (UML) is widely used in the design of conceptual schemas. A good example is the conceptual model for the geologic maps in North America (NADM Steering Committee, 2004). There were also conceptual schemas designed for data exchange on the Internet, such as GeoSciML (Sen and Duffy, 2005). The INSPIRE program, a pan-European spatial data infrastructure, is developing data and metadata schemas for 34 subjects in Earth and environmental sciences, with the full implementation aimed by 2021 (Bartha and Kocsis, 2011). Ontology with formal logical assertions is the last type on the KG spectrum (Fig. 1). Each ontology is the formal specification of a shared conceptualization of a domain (Gruber, 1995). Semantic technologies such as Resource Description Framework (RDF) (Klyne and Carroll, 2004) and Web Ontology Language (OWL) (McGuinness and van Harmelen, 2004) are widely used to add logical assertations on classes and properties in an ontology, such as disjoint classes, equivalent classes, transitive properties, and more. A well-known ontology in Earth and environmental sciences is SWEET (Raskin and Pan, 2005). There are also ontologies built for themed geoscience subjects, such as geologic time (Cox and Richard, 2015), hydrology (Brodaric et al., 2019), hydrogeology (Tripathi and Babaie, 2008), structural geology (Babaie et al., 2006), fractures (Zhong et al., 2009), and sensor networks (Compton et al., 2012), just to name a few.\nAs reflected by the spectrum in Fig. 1, A KG in the real-world geoscience applications is often seen as a mixture of TBox and ABox. The former is the classes and properties representing a domain (cf. logical assertion statements at the right part of Fig. 1), and the latter is the instances of those classes (cf. terminology statements at the left part of Fig. 1). To which level should we detail the semantics of a KG is decided by the needs of research activities.\n\n\n2.2\nHow to build knowledge graphs\nKG construction is an iterative engineering process where many methods and tools can be applied (Fox and McGuinness, 2008). The existing approaches can be grouped in two clusters: top-down and bottom-up. The top-down approach stems from the modeling process in database construction (Fig. 2\n). First, a subject domain and a list of research needs are identified. Second, a conceptual model will be designed to collect the entities of interest, their inter-relationships, and the categories. A useful tool for conceptual modeling is the CmapTools (Cmap, 2021). Third, the logical and physical models will add logical representation and assertions to the collected entities and relationships. Fourth, the technical development and implementation need to consider the coding language to use (e.g., RDF and OWL), the serialization formats (e.g., RDF\/XML, Turtle, and JSON-LD), and the KG development platforms such as Prot\u00e9g\u00e9 (Tudorache et al., 2008) and DOGMA (Spyns et al., 2008). The last step is to deploy the KG as a service to allow the community reuse and provide feedback. In general, this is a process to transform the knowledge in the domain experts\u2019 brain to a machine-readable representation. Many existing geoscience KGs were constructed through this approach, such as the schema for mineral classification (Garvie, 1995), the SWEET ontology (Raskin and Pan, 2005), the GeoCore ontology (Garcia et al., 2020), and the other examples mentioned in Section 2.1. Recently, the Deep-time Digital Earth (DDE) Big Science Program of the International Union of Geological Sciences built its own platform for building and serving KGs (Shi et al., 2020; Wang et al., 2021). KG practitioners can also refer to summaries and reviews of KG development tools (e.g., Corcho et al., 2003; Slimani, 2015; W3C, 2015) to find a good match to their work.\nThe bottom-up approach of KG construction is based on crowed-sources data, such as social media and the literature legacy. Earlier discussions include mining Web content to build knowledge bases (Craven et al., 2000) and use an observation-driven approach in geo-ontology engineering (Janowicz, 2012). The thriving social media and open access to published literature further extend the scope of data sources to be used in KG construction. The number of publications following this bottom-up approach has increased significantly in recent years. For example, Gao et al. (2017) used Hadoop to process geotagged data in Flickr and successfully built gazetteers in geography. Zhu et al. (2017), Wang et al. (2018b) and Fan et al. (2020) used natural language processing (NLP) and text mining to process geoscience literature (reports, books, and journal papers, etc.) and then use the results to guide the process of KG construction. Although the bottom-up approach is able to process a large number of datasets and quickly build a big KG, a remaining challenge is the precise logical representation and assertations for the entities and relationships in the resulting KG. Very often, they still need to be specified by the domain experts and knowledge engineers, where existing KGs can be reused.\n\n\n2.3\nBest practices in knowledge graph construction\nResearchers have summarized workflows and recommendations for KG construction, and some of them are based on examples from geosciences (Fox and McGuinness, 2008; Kendall and McGuinness, 2019). They highlighted a use case-driven iterative approach to leverage existing resources and improve the usability of the resulting KG. Fig. 3\n put together those recommendations together with the approaches discussed in Sections 2.1 and 2.2 to present a suggested workflow for building and applying KGs in geosciences. Each use case has a specific topic relevant to the domain, such as discovering datasets with one or a few keywords, recommending algorithms to analyze a certain type of data, and finding researchers who share the same research interests. Domain experts (e.g., geoscientists) will work together with knowledge engineers to analyze each use case to get a draft list of entities, relationships, categories, and structures. If necessary, the bottom-up approach can also be used to augment the list. Based on the first one or two use cases, a KG prototype can be established and tested. Then more use cases will be analyzed in an iterative process to enrich the KG. In this process, some ontology design patterns (Gangemi, 2005; Gangemi and Presutti, 2009; Blomqvist et al., 2016) can be reused and adapted from community standards (e.g., the mineral classification chart, the nomenclature of petrology, and the geologic time scale) as well as existing ontologies and vocabularies (e.g., the SWEET ontology). Ontology design patterns are distinctive and repetitive invariants across the various models, data and processes of a domain. Reusing them will improve the interoperability and usability of the resulting KG.\nThere is a 3C (Correct, Consistent, and Complete) guideline (Asch and Jackson, 2006) to determine an appropriate termination point for the use case analyses. The practitioners need to verify that the entities and relationships collected in the KG are correctly defined and annotated, and they are organized in a consistent structure. Moreover, the established entity and relationship lists and the logical assertations are complete enough to address the subject areas and research questions proposed in the beginning of the whole work. Once a relatively stable version of the KG is generated, a service can be set up for it, either through an individual server or a community portal (right part of Fig. 3). As workflow platforms such as Jupyter (2021) and RMarkdown (RStudio, 2021) are increasingly used by geoscientists in nowadays for data-driven discoveries, for the KG service it is a good practice to develop a Python or R package as the interface to access the KG server. Then users can apply the KG from workflow platforms together with many other data and model resources in the open science world. They can also provide feedback to the KG developers. As the FAIR (findable, accessible, interoperable, and reusable) data principles (Wilkinson et al., 2016) are widely accepted in the open data endeavors of various disciplines, there were also discussions on how to build FAIR KGs. For example, Cox et al. (2020) proposed \u201cTen Simple Rules\u201d towards FAIR vocabularies: 1) Verify the license for repurposing a legacy vocabulary; 2) Determine the governance model and custodian for the legacy vocabulary; 3) Check minimal term definition completeness; 4) Select a domain and service for the Web identifiers; 5) Design a pattern for the identifier scheme; 6) Reuse semantic standards for the vocabulary to increase its interoperability; 7) Add rich metadata to increase reusability; 8) Register the vocabulary to increase findability; 9) Make the Web identifiers resolvable to increase accessibility; and 10) Implement a mechanism for maintaining the FAIR vocabulary.\n\n\n\n3\nKnowledge graphs in geoscience data collection, curation and service\nGeoscientists have realized the importance of using machine-readable standards in data collection and management since the 1950s when they began to use digital computers. Many publications have discussed topics associated with KG, such as consensus on data models (Dillon, 1964; Hubaux, 1970, 1972, 1973), semantic symbols and nets (Dixon, 1970; Garvie, 1995), controlled vocabularies (Rassam and Gravesteijn, 1982; Shimomura, 1989), rules for spatial data manipulation (Buttenfeld and McMaster, 1991; Chung and Fabbri, 1993), and more. Now, in the era of the Internet and Web, KG still takes an essential role in geoscience data management, and there are new progresses on applying KGs for open and FAIR data.\n\n3.1\nKnowledge graphs and FAIR data\nWhile almost all geoscientists are using computers in their work, many people are spending about 80% of their time on data preparation before analysis (i.e., the 80\/20 rule) (Press, 2016; Mons, 2018; Fox, 2019).\nThe FAIR data principles (Wilkinson et al., 2016) emphasize the machine-readability and machine-actionability of data, i.e., improving the capacity of computer systems to find, access, interoperate, and reuse data. In that way, the manual intervention and operation from human scientists will be reduced to the minimum and, thus, to mitigate or even reverse the 80\/20 rule. The FAIR principles have been well received by researchers in various disciplines in the past five years. In particular, the geoscience communities have not only showed the support but also analyzed the challenges and drafted action items towards FAIR data in geosciences (Stall et al., 2018, 2019). Here we would like to address the close relationship between the FAIR principles and the theories and technologies of KG (Table 1\n). The findability and accessibility rely on the cyberinfrastructure for persistent and stable identifiers and the protocols and interfaces to resolve those identifiers and retrieve the metadata associated with them. Most of the principles under those two themes have light to medium relevance to KG. In comparison, most items under interoperability and reusability can be directly supported by KGs (Mons, 2018; Guizzardi, 2020). The FAIR principles can also be compared to the Five-Star Open Data scheme proposed by Berners-Lee (2009). Hasnain and Rebholz-Schuhmann (2018) conducted a detailed mapping between the FAIR principles and the Five-Star scheme, and showed that they share topics on identifiers, metadata, vocabularies and community standards.\nAlthough the FAIR principles were recently proposed, there have been many earlier efforts working on various items covered in the principles, and some of them highlighted the use of KGs. For example, in the Virtual Solar-Terrestrial Observatory (Fox et al., 2009), a set of OWL-based ontologies were developed to represent the concepts, relationships and attributes in the fields of solar physics, space physics and solar-terrestrial physics. The ontologies were then used to reconcile distributed and heterogeneous datasets and present them to the end users in an organized form. In the EarthCube Geolink project (Krisnadhi et al., 2015; Cheatham et al., 2018), the method of ontology design patterns (Gangemi, 2005) was used to develop a modular ontology to support data integration from seven geoscience data repositories. The Google Dataset Search was released in 2018. It is based on Schema.org, which provides metadata schemas to markup datasets shared on the Web (Noy et al., 2019a). Numerous geoscience datasets can already be discovered on the Google Dataset Search. Researchers in the EarthCube GeoCODES project have been conducting more case studies to adapt and extend Schema.org, with the aim to build best practices to enable cross-domain discovery and access to geoscience data and research tools (Shepherd et al., 2019). Another interesting work is using ontologies to represent the FAIR principles and evaluate the FAIRness of open data. Examples can be seen in Alowairdhi and Ma (2019) and Brewster et al. (2020).\nA comparison can be made between the approaches of Google Dataset Search and the Linked Open Data. Although they both have strong relationships with the semantic technologies, the focus of Google Dataset Search and Schema.org is on the metadata. Accordingly, when a data repository incorporates Schema.org in its structure, the technical development is mostly on the metadata schemas. Although domain-specific vocabularies might also be built to facilitate data annotation and discovery, the data repository can retain its original data structure and data format rather than be transformed into RDF. The Linked Open Data has also been a big success (Auer et al., 2014) on several aspects: 1) extraction, creation and enrichment of structured RDF data, 2) interlinking and fusion of RDF data from different sources, 3) management of RDF data to a large scale, and 4) exploration and visualization of Linked Data. It is clear that a big effort of Linked Open Data is the creation and curation of data in RDF format. Accordingly, specific KGs are needed to underpin the RDF data and the work is more extensive than the work focused on metadata. This perhaps is a partial reason that very few geoscience repositories have fully deployed the Linked Open Data approach in their technical development. Nevertheless, Linked Open Data has initiated many discussions on how to improve the visibility and accessibility of data on the Internet and Web. Many established methods in Linked Open Data, such as enrichment and interlinking of RDF data, can also be adapted in the deployment of Schema.org metadata in geoscience data repositories, to help pursue the goal of FAIR data.\n\n\n3.2\nKnowledge as a service in open data and open science\nWhen the KGs of a domain are established, one way to continue their maintenance and populate their application is to build a service for them on the Internet and Web. For example, in the field of biology and biomedical studies, the BioPortal provides Web services to various ontologies, which can be used to drive data integration, information retrieval, data annotation, natural language processing, and decision making (Noy et al., 2009; Whetzel et al., 2011). The Web-based concept browsing and graph visualization allow users quickly see the landscape of a subject domain of interest, while the logical assertions and rules in the KGs can be used in the data integration and analysis processes. Geospatial semantics is another domain where significance progress has been made on KG development and service in the past decades (Frank, 2001; Kuhn, 2001; Lutz and Klien, 2006; Janowicz et al., 2012). Besides the increasing number of books and journal articles, geospatial semantics has also been a long-lasting theme in many scientific communities and their conferences, such as the American Association of Geographers, the International Society for Photogrammetry and Remote Sensing, the International Cartographic Association, and the Conference on Spatial information Theory, just to name a few. Relevant committees and\/or working groups have also been established in big computer science communities such as those in the Institute of Electrical and Electronics Engineers (IEEE) and the Association for Computing Machinery (ACM). Several KG outputs were formally released by W3C and\/or the Open Geospatial Consortium (OGC), such as GeoSPARQL (Battle and Kolas, 2011) and the Semantic Senor Network ontology (Compton et al., 2012). Many of the established technologies in geospatial semantics have been used in geoscience for data and knowledge service. For instance, in the W3C Working Group Note \u201cSpatial Data on the Web Best Practices\u201d (i.e., Tandy et al., 2017), examples from several geoscience disciplines were introduced.\nThe geoscience communities have also taken initiatives to build similar services. For instance, NASA is leading the maintenance and service of the SWEET ontology (Raskin and Pan, 2005) and the GCMD keywords (Stevens, 2019). The former is a foundational ontology that covers more than 200 subject areas and over 6,000 concepts in Earth and environmental sciences. The latter is a hierarchical set of controlled vocabularies covering 14 categories of keywords in Earth science, and it has been used in NASA's Earth Observing System Data and Information System (EOSDIS). USGS has been developing and maintaining thesauri in the past two decades with semantic technologies. The current USGS thesaurus service (USGS, 2021b) hosts a long list of controlled vocabularies that provide category terms for data and information products of USGS. IUGS-CGI has also built a website to host the services of the geoscience schemas and vocabularies built by its international working groups (IUGS-CGI, 2021). Researchers have also discussed methods for building service structures of geoscience KGs and best practices (Cox and Richard, 2015; Zhao et al., 2019; Cox et al., 2020; Ma et al., 2020). Very recently, the Semantic Technologies Committee of the Federation of Earth Science Information Partners (ESIP) has established a community ontology repository (COR) (ESIP, 2021) to host KGs from the geoscience communities, coordinate collaboration, and promote best practices.\nA recent topic of high interest among the geoinformatics community is Knowledge as a Service (KaaS). Besides the service capabilities mentioned in the above paragraph, another key advantage of KaaS is to provide context information for data and data science processes. A key work in the Semantic Web community, the Provenance Ontology (PROV-O) (Lebo et al., 2013), has been widely applied in the past years to enable the documentation of context information. Provenance literally means the origin of something. In data science it means to chain up scientific results and findings with the various data, methods, platforms, instruments, people, organizations involved in research (Groth et al., 2012). For example, in the Global Change Information System (GCIS) of the U.S. Global Change Research Program, a PROV-O-based GCIS ontology was built to capture the provenance of global change research. The collected information was published on the GCIS portal (Tilmes et al., 2013; Ma et al., 2014b). In the work on Essential Climate Variables in Europe, approaches similar to GCIS have also been taken to enable traceability of scientific results (Zeng et al., 2019). The granularity of provenance can go even deeper to steps in algorithms and data analytics workflows. For instance, The METACLIP R package developed by Bedia et al. (2019) was able to capture the detailed steps in an R workflow (e.g., raw data input, derived data, packages import, functions, and variables, etc.) that leads to a resulting image. In the work of Stasch et al. (2014), KGs were used to suggest appropriate steps in spatial statistics for certain structures and patterns in the input data. An increasingly discussed topic in computer science of nowadays is explainable AI (Hagras, 2018; Lundberg et al., 2020). Provenance, semantic technologies, and KGs will make solid contributions to that field of work (cf. Goebel et al., 2018; Palmonari and Minervini, 2020; Kale et al., 2022).\n\n\n3.3\nBest practices of applying knowledge graphs for data curation in the data ecosystem\nResearchers have argued that the power of machine learning and big data processing does not mean we can simply dump all the digital records without any structure and order and rely on machine to find patterns out of the chaos \u2013 If the data is the train, then semantics will be the rail (Janowicz et al., 2015). An essential goal of the Web is to promote interconnection, interaction, and intercreation among different people, resources, and facilities (Berners-Lee and Fischetti, 2000). Now, the open data and open science activities have created a data ecosystem on the Internet and Web (Berman, 2008; Wing, 2019). This is a socio-technical system of many interacting factors. The technical part covers many topics relevant to data collection, curation, distribution, analysis, and communication. The social part covers topics of data privacy, license, ethics in data access and reuse, citation guidelines, feedback from data consumers, trustworthiness, informed decision making, and more. Appropriate handling of those issues will help establish a virtuous cycle in the data ecosystem to facilitate data-driven science.\nThe W3C community have summarized a list of best practices about the publication and application of data on the Web and their benefits to the data ecosystem (Loscio et al., 2017). Table 2\n puts the list together with the FAIR data principles and shows the relevance of each best practice to KGs. As reflected in the table, those items have strong relevance to KGs: metadata and annotation, provenance of data source and origin, standards and vocabularies, and data structure and formats. For data on the Web, vocabularies, models and ontologies enabled by semantic technologies will be a big advantage to increase machine accessibility and readability. We currently mark a light relevance between KGs and data identifiers. However, there are many interacting factors in the data ecosystem, such as platforms and instruments, people, organizations, research programs, models and algorithms, software packages and functions, workflows and model-runs, with others. If we want to offer formal definition for the categories and properties of those factors and then assign unique identifiers for all of them, then KGs will also take a fundamental role in that work.\n\n\n\n4\nKnowledge graphs in geoscience data analysis\nA good way to envision the role of KG in geoscience data management and analysis is to put it in the context of the data-information-knowledge-wisdom (DIKW) model (Fig. 4\n). Conventionally, people think DIKW is a one-direction process, and the steps of knowledge and wisdom rely more on human experience and decision-making. KGs will complement the DIKW process by encoding human knowledge in machine-readable formats, which can be applied to aid data management and analysis. Section 4 has given a summary of KGs in geoscience data management. This section will focus on KGs in geoscience data analysis. In geoinformatics and geomathematics, researchers have discussed the studies of embedding qualitative AI methods in quantitative data analysis models since decades ago (e.g., Bugaets et al., 1991; Dimitrakopoulos, 1993). Now, the big geoscience data such as literature and crowd-sourced records, remote sensing images, and accumulated digital maps pose both challenges and opportunities for the application of KGs in data analysis.\n\n4.1\nKnowledge graphs and literature and crowd-sourced data analysis\nTextural records are a unique type of big data in geosciences, and they are widely distributed in published literature and the crowd-sourcing data platforms. KGs such as community-level dictionaries and ontologies have been used to aid NLP and text mining in geoscience literature analysis. Typical use cases include: 1) To summarize and visualize the key information of a document in a graph; 2) Inter-comparison of themes and writing patterns of chapters\/sections in a long document; 3) Domain-specific gazetteer or corpus construction; and 4) KG augmentation and iterative usage in text mining. Wang et al. (2018b) used community-level standards, including geological dictionaries and terminology classification schemes (AQSIQ, 1988) to build a large corpus, then used it to train word segmentation rules and applied them together for processing geologic reports. The results included word frequency diagrams, word clouds, bigrams showing clusters of key content-words, and chord graphs showing inter-relationships between content words. The results can uncover the key subjects and structure of a document and show the potential of KG augmentation based on multi-document analysis. In Qiu et al. (2020a), spatial and temporal gazetteers were built to support the process of information extraction for literature. The spatial gazetteer included place names and spatial relationships well known in geosciences, and the temporal gazetteers included both geologic time scale and the general temporal expressions in the Gregorian calendar form. In Qiu et al. (2020b), a geoscience dictionary matching step was used to guide the bidirectional long short-term memory (LSTM) neural network in text classification.\nIn the field of geoscience literature mining, the work of GeoDeepDive (Zhang et al., 2013; Peters et al., 2017b) is worth a special note. GeoDeepDive is a machine learning package and digital library for discovering data and knowledge from published literature. Many publishers in the field of geosciences, such as Elsevier, Wiley, Taylor & Francis, USGS, the Society for Sedimentary Geology, the Geological Society of America, Canadian Science Publishing, and PubMed have signed agreements to set up full-text access to GeoDeepDive. By March 2021, GeoDeepDive has preprocessed more than 13.4 million documents, and set up interfaces and guidelines to allow other researchers to use the data. Peters et al. (2014) have successfully used GeoDeepDive to extract fossil records and enhance the Paleobiology Database, which in turn has benefited several recent data-driven studies (e.g., Peters et al., 2017a; Muscente et al., 2018). The workflow of GeoDeepDive (Peters et al., 2017b) shows that a good way to rescue dark data from literature is by ingesting a structured vocabulary with specific scientific foci. Then the terms in the vocabulary can be indexed against the preprocessed literature in GeoDeepDive to create a subset of documents for data extraction.\nAnother type of textual data is collected through the crowd-sourcing mode, such as social media platforms, news reports, and citizen science Web portals. They have been increasingly used in hazard mitigation, public health surveillance in space and time, and other themed geoscience studies. A review of social media data analysis (Ravi and Ravi, 2015) shows that lexica are functional in opinion mining and sentiment analysis. In the context of that paper, a lexicon is a controlled vocabulary of sentiment words with respective sentiment polarity and strength value. Lexica can be used together with ontologies to enable reasoning and inference tasks. A similar technical approach was seen in Wang and Stewart (2015), but on a different scientific topic: hazard information extraction from news reports. In their work, ontologies were used together with natural language gazetteers to improve the quality of hazard event extraction from online news reports. Then, the spatiotemporal patterns (i.e., occurrence and evaluation) of those events were analyzed. In Jayawardhana and Gorsevski (2019), ontologies were used for similarity computation, with the aim to tackle the heterogeneous labels in Tweets and maximize the detection of influenza. Another interesting example of crowd-sourcing data and KG construction and application is Mindat (2021). It is a leading web portal on minerals and their localities, deposits and mines worldwide. By March 2021, Mindat has more than 55,000 users and about 6,000 of them have contributor rights. Many Mindat data such as alternative names of mineral species and literal records of localities depend on users with local expertise of a certain region to cleanse and reconcile the records. In the meantime, the Mindat team has applied community standards such as nomenclatures in mineralogy and petrology, taxonomy in paleobiology, and terminology in geologic time, and has set up mappings between community standards and the alternative names. Mindat has underpinned many data-driven geoscience studies in recent years (Hazen et al., 2019).\n\n\n4.2\nKnowledge graphs and geographic object-based image analysis\nThe Geographic Object-based Image Analysis (GEOBIA) is a new paradigm for remote sensing image analysis in addition to the conventional \u201cper-pixel paradigm\u201d (Blaschke et al., 2014). Here the image-objects are meaningful entities or scene components that are distinguishable in an image, such as a house, a tree, or a vehicle (Blaschke, 2010). Ontologies and semantics are key components in the workflow of GEOBIA as they provide a machine-readable representation of objects in the real world (Fig. 5\n). Blaschke et al. (2014) addressed that there are no one-fit-all ontology solutions even for the same types of objects in GEOBIA. As reflected in Fig. 5, the GEOBIA workflow is normally an iterative process. For the domain of the image-objects, ontologies will be constructed to capture the knowledge of domain experts and will be used together with a rule set in image analysis. The initially generated image-objects will be classified and enhanced iteratively by applying the ontology and the rule set. In this process, the ontologies can also be extended or updated. Although the focus of Fig. 5 is image analysis, the iterative workflow in it can be compared to Fig. 3. Another thought is that the KG engineering workflow in Fig. 3 can be used to extend the ontology engineering step in GEOBIA.\nGEOBIA, the \u201cper-object paradigm\u201d, and the methodology of incorporating ontologies and semantics in image analysis have received significantly increasing attention in the past two decades (Liu et al., 2007; Arvor et al., 2013, 2019; Blaschke et al., 2014; Gu et al., 2017). There have been successful applications of this new paradigm of remote sensing image analysis in many geoscience domains. In Dr\u0103gu\u0163 and Blaschke (2006), a list of nine classes were built to represent landform elements based on the surface shape and the altitudinal position of objects. The classes were defined using flexible fuzzy membership functions and were successfully used for automated classification of landform elements in two case studies. To detect and classify off-shore oil slicks, Akar et al. (2011) applied object-based classification with fuzzy membership functions derived from the features of categorized scenes in the ENVISAT Advanced Synthetic Aperture Radar (ASAR) imagery. The parameters of the detection algorithms were tuned for each category to improve the quality of results. In de Bertrand de Beuvron et al. (2013), an ontology was built to represent urban objects and the spatial relationships between them, which came to be a powerful support for object-based image analysis in urban environment studies. Kohli et al. (2012, 2013) built ontologies of slums by using indicators related to the morphology of the built environment, and successfully used them for slum identification from high-resolution imagery (i.e., GeoEye-1). In Belgiu et al. (2014), an ontology was created to represent three classes of building types, and then used in an GEOBIA process to identify buildings extracted from airborne laser scanning data. The Random Forest classifier was applied to select the relevant features for predicting the classes of interest. An interesting finding of their work is using the Random Forest classifier to predict the explanatory power of the input variables (i.e., Variable Importance), which was addressed again in a review article later (Belgiu and Dr\u0103gu\u0163, 2016). From our point of view, the Variable Importance can also be used to augment ontology engineering in the iterative GEOBIA process (cf. Janowicz, 2012).\n\n\n4.3\nKnowledge graphs and digital map analysis\nIf remote sensing images are the big raster data, then the digital maps and associated databases are the big vector data. In the domain of cartography and GIScience, the incorporation of semantics and KGs to spatial data service and analysis has been an active research topic for decades (L\u00fcscher et al., 2009; Janowicz et al., 2010; Li et al., 2014; Gould and Mackaness, 2016). Many of them have been mingling with the standards and building blocks established by OGC, W3C, and other communities. Yue et al. (2007, 2011) have done extensive work to establish online spatial data processing service chains by integrating semantic technologies and spatial data services. Stasch et al. (2014) incorporated KGs to estimate the correspondence between data sets and analysis functions, and they developed a prototype of meaningful spatial statistics. Scheider et al. (2017) examined the role of semantic technology in data-driven analysis and workflow platforms and proposed eight challenging questions for future work. Very recently, Geographic Question Answering (GeoQA) became a new topic of interest in GIScience. Mai et al. (2021) gave a comprehensive review of that domain, including the role of KG. Scheider et al. (2021) also reviewed the same subject, but with a standpoint in computation and automation of workflows. Now, the FAIR data principles (Wilkinson et al., 2016) and the Five-Star Open Data scheme (Berners-Lee, 2009) are driving spatial data to be made open in more structured and interoperable forms. OGC and W3C are also working on more powerful fundamental KGs for spatial data. For example, the GeoSPARQL (Battle and Kolas, 2011) has incorporated spatial topology and the Time Ontology (Cox and Little, 2020) has included temporal topology. Those endeavors together have laid the foundation for more innovative approaches of online spatial data analysis (Varanka and Usery, 2018).\nGeologic mapping is a fundamental work in geosciences and has seen many studies on developing and implementing KGs. When GIS software was first introduced to the work of field geologic mapping in the early 2000s, geoscientists already began to use ontologies to maintain consistent data structure and facilitate interoperability between databases (e.g., Brodaric, 2004; De Donatis and Bruciatelli, 2006). As the digital geologic maps were increasingly shared online, researchers also began to implement ontologies to mediate multi-source geologic map services, such as those produced at different states in US (Lin and Lud\u00e4scher, 2003). In the OneGeology map data portal (Jackson, 2007), a common geologic data schema GeoSciML (Sen and Duffy, 2005) was used to mediate distributed map services from more than one hundred countries across the world. In OneGeology-Europe (Laxton, 2017), multilingual vocabularies were developed for rock age and type, and were used to support federated data queries sent to map services in different languages. With the multilingual vocabularies, functions were developed to match the query keywords with the map services in their original languages. Although there are multiple map service providers across the European countries, the front end of OneGeology-Europe is built like an integrated data portal with harmonized map services, which is a great advantage for end users. Using the open geologic map services, researchers were able to incorporate data visualization techniques and other open data and knowledge resources to build themed data analysis functions (e.g., Ma et al., 2012; Ma, 2017; Wang et al., 2018a). Similar to the active discussion in cartography and GIScience, KGs in geologic map service and analysis will be a long-lasting research topic (cf. Mantovani et al., 2020).\n\n\n4.4\nIntegrated application of knowledge graphs and machine learning\nComparing with KG construction and KGs for geoscience data curation, the application of KGs in geoscience data analysis is still in the early stage, and it is hard to list the best practices. However, we can summarize some integrated applications of the above-mentioned technologies. A common question from many geoscientists is how KGs and KG-enabled capabilities could be used to drive new discoveries in geoscience, either on scientific or engineering topics. In particular, geoscientists would like to see platforms and applications that are able to lower the access requirements of semantic and AI technologies to them, such as the Google Dataset Search engine (Noy et al., 2019a) and the Question Answering systems (H\u00f6ffner et al., 2017). The highlights of a few recent examples from both industry and academia are summarized below.\nThe interweaving between KGs and machine learning has generated successful applications in the industry. Marr (2019) listed several latest works at Google, Oracle, Facebook, Netflix, Siemens, and described the trends of integrating KGs and machine learning in the field of financial services. For the field of oil and gas exploration, there has been solid progress of using KGs to boost big data processing and aid decision making (Kimbleton and Matson, 2018; Sumbal et al., 2017). Specific examples can be seen in the capabilities enabled by IBM. In Guichet et al. (2019), the IBM Watson was used to identify documents relevant to source rock characterization in petroleum exploration. Two types of machine learning algorithms were tested. The first was trained to identify images and charts in literature, and the second was trained to understand the semantic framework of textual records related to source rocks. The two algorithms were applied to extract information from many documents and save the result in a database. Finally, a user interface was built to translate natural language questions into computer queries to the database. The work showed promising performance in finding the most relevant documents. In another work (Bekas and Staar, 2019), a KG was built based on large amounts of geological, physical and geochemical data. Geoscientists then were able to use the KG to contextualize questions and retrieve relevant information. The work was useful in the identification and verification of alternative exploration scenarios, and it can help geoscientists to improve decision making.\nPutting those examples from industry together with the progress mentioned in above sections, we can see the application of KG in data analysis is often an iterative approach of dual benefits (cf. Ristoski and Paulheim, 2016). KGs can be used to improve data analysis workflows, and in turn KGs themselves can also be extended and enhanced when more patterns and information are discovered in data analysis. Recent work on mineral evolution resonates with this approach. Mineral evolution is the study of mineral diversity and distribution through the Earth's long history (Hazen, 2010). Abductive (i.e., exploratory), deductive (i.e., knowledge-driven), and inductive (i.e., data-driven) approaches (Fig. 6\n) have all been used in recent studies of this field (Hazen, 2014; Hazen et al., 2019). A typical example that demonstrates the dual benefits to both KG and data analysis is the natural kind clustering of mineral species. This is a subfield of mineral evolution with the aim to amplify the current mineral taxonomy. The present mineral classification system is based on idealized major element chemistry and crystal structure, which lacks consideration on time and cannot reflect planetary evolution or formational conditions (Hazen, 2019a,b; Cleland et al., 2021). Natural kind clustering relies on the many attributes of mineral samples to relate each sample to its paragenesis and thereby develop a scheme for classifying the origin of mineral samples when their context is unknown. Two recent studies of natural kind clustering have demonstrated impressive results. The first is classifying formational environments of pyrite based on geochemical information (Zhang et al., 2019), and the second is analyzing the presolar silicon carbide grains (Boujibar et al., 2020).\n\n\n\n5\nA vision for geoscience knowledge graphs in the near future\nWith data science thriving in geosciences, we anticipate more KGs will be built and implemented. Several recent review and survey articles (Noy et al., 2019b; Hogan et al., 2020; Abu-Salih, 2021; Gutierrez and Sequeda, 2021) have discussed the challenges that KG practitioners face, which are synthesized below:\n\n\u2022\nKG entity disambiguation and identification, and quality measure: Synonyms, homonyms, entity types are still active research topics, especially for KG construction from un-structured literature. To sustain KGs in the cyberinfrastructure, the unique, persistent and Web-resolvable identifier of each entity needs more coordination among different communities. A system of metrics is also needed to measure the quality and usability of KGs.\n\n\n\u2022\nSemantic enrichment and reasoning capability: KGs and data are increasingly bound together. A topic worth attention in KGs is the granularity of semantics in the definition and annotation of entities and relationships, as well as how it will address the needs of data curation. Another topic is the reasoning capability enabled by the logic assertions in KGs, which will be necessary to further leverage KG usage in data analysis.\n\n\n\u2022\nKG evolution and versioning: Our knowledge is evolving with the progress of scientific discoveries and new understanding of the world. Also, there will be new encoding languages for KGs as well as new KG management systems. Method and technologies are needed to organize KG evolution and versioning, and to provide KG as a stable service in the cyberinfrastructure.\n\n\n\u2022\nInterconnection among KGs and scaling up in big data applications: The works on KG construction and application are scaling up, and interconnection will be needed between high-level and domain-specific KGs, as well as between KGs of different domains and subjects. Multilingualism is another topic to be addressed when KGs are scaled up and used together with big data analysis.\n\n\n\u2022\nSecurity, privacy and ethics: Similar to the community recommendations and best practices in open data and open science, KGs will also need a system of licenses for sharing and reuse. Also needed are the regulations and guidelines for protecting privacy and sensitive information, and recommendations for ethical operation of KGs.\n\n\n\nSections 2 to 4 in this paper summarized the progress of KG construction and application in geosciences. By incorporating the best practices and exemplar studies from them, this section will discuss the trends of geoscience KG in the next decade and present a few suggestions for practitioners to address the challenges listed above.\n\n5.1\nKnowledge graph creation and curation in geosciences\nAn appropriate workflow for ontology engineering in geosciences in a mixture of the bottom-up and top-down approaches through a use case-driven, iterative process (Fig. 3). The bottom-up approach can benefit from the powerful NLP and text mining technologies and the large amounts of accumulated literature legacy and crowd-sourced data. The patterns discovered through big data analysis may reflect interesting rules that are outside the existing human expertise. The top-down approach can bring together researchers sharing the same research interests and leverage existing community standards and ontology patterns. Geoscientists\u2019 verification and control can improve the quality and precision of the outcomes from the bottom-up approaches. The adaptation of community standards and ontology patterns can reduce inconsistency and duplicated efforts in the resulting KGs. The use-case driven, iterative process has been proven efficient for facilitating the collaboration between geoscientists and data scientists, as well as increasing the usability of the resulting KGs. The 3C (Correct, Consistent, and Complete) guideline (Asch and Jackson, 2006) and the Ten Simple Rules (Cox et al., 2020) for KG construction were proposed by researchers in the field of geoinformatics, and they are applicable to many geoscience topics.\nGeoscience KG evolution and curation will need more attention. New entities and relationships can appear in a field of study as our understanding deepens. Also possible is the update and revision to existing definitions and descriptions, as well as the inter-mapping between KGs. Technical approaches are needed to tackle those different situations and take actions to update the KG at different levels, such as numeric and literal attributes, instance records, data properties, object properties, classes, and even the whole KG. The situation can be more complicated as KGs are increasingly bound with steps in the data life cycle (Ma et al., 2014a; BDIWG-NITRD, 2018), such as standardizing the structure of databases and terminology of records, annotating data products, providing precise results in data search and discovery, and enabling innovative operations in data analysis. The goal is that the updated KGs will benefit the data life cycle, but will that require extra work to update the data and the steps mentioned above? One possible way is to use persistent and resolvable Web identifiers for different types of records in a KG and archive detailed versioning history of any updates. When the content of that KG is used, the identifiers and version codes can be cited.\nCommunity of practice remains an effective way to facilitate the creation, evolution, and curation of geoscience KGs. W3C and OGC have had successful collaborations on large KGs relevant to geosciences, such as GeoSPARQL (Battle and Kolas, 2011) and the Semantic Sensor Network ontology (Compton et al., 2012). The Federation of Earth Science Information Partners (ESIP) has created a Community Ontology Repository (COR) (ESIP, 2021) to host many KGs from the geoscience community, such as the SWEET ontology (Raskin and Pan, 2005), the geologic time ontology and vocabularies (Cox and Richard, 2015), the GCMD keywords (Stevens, 2019), and many others. The ESIP Semantic Technologies Committee is also coordinating the revision of a few widely used KGs, such as the SWEET ontology (McGibbney, 2018). The IUGS-CGI is continuously leading the creation of geoscience schemas and vocabularies the coordination of their applications across the world (IUGS-CGI, 2021). The ESIP and IUGS-CGI efforts represent the essential nature of KGs: from the community, by the community, and for the community. Geoscientists in different disciplines have also begun to work with computer scientists to standardize the terminology, data structures, and data formats in their work. A representative example is the PaCTS 1.0 data standard in paleoclimatology, in which both the bottom-up and top-down approaches for KG engineering were applied (Khider et al., 2019). In the United States, the academia, industry, and government are jointly promoting a national Open Knowledge Network, with the aim to establish an open infrastructure that links cross-disciplinary KGs and underpins the cyberinfrastructure ecosystem (Guha and Moore, 2016; BDIWG-NITRD, 2018; Baru, 2018; Sheth et al., 2019b). In that endeavor, community of practice is recommended for increasing the interoperability and reusability of KGs.\n\n\n5.2\nIntelligent geosciences underpinned by knowledge graphs\nThe thriving AI and data science applications are moving geosciences into the \u201cintelligent\u201d stage (Merriam, 2004; Ma, 2018; Gil et al., 2019). As discussed by both computer scientists and geoscientists (Domingos, 2012; USGS, 2021a), data alone are not enough to drive the scientific discovery. Each data mining, predictive analytics, or machine learning process needs to embody some knowledge or assumptions besides the data that are given. The interaction of data and knowledge in the data science process can be explained with the abductive, deductive, and inductive approaches (Tukey, 1977; Ho, 1994; Hazen, 2014). For example, as illustrated in Fig. 6, if there is enough knowledge about the requested attributes of each class, then a deductive approach can be the best option to conduct logic inferences. If not, then the data-driven inductive approach can be applied. The abductive approach is another useful approach in the open data environment when a study is based on other people's data. It means to explore the characteristics of the data and generate assumptions or hypotheses for the scientific discovery. Ho (1994) summarized that abduction creates, deduction explicates, and induction verifies. Brodaric (2012) also discussed abduction, deduction, and induction as a virtuous cycle for KG creation and evolution in geosciences.\nGeoscience KGs need to enrich their embedded semantics to improve the capacity of reasoning, inference, and verification in a data science process. For example, the GeoSPARQL (Battle and Kolas, 2011) defines a vocabulary for representing spatial data on the Web. More importantly, it embeds the spatial topology in its design and can describe various relationships between spatial objects (e.g., points, lines, and polygons). Based on those, it is able to support both quantitative and qualitative query and spatial reasoning. Similarly, the Time Ontology (Cox and Little, 2020) embeds temporal topology in its design and can describe relationships between temporal objects (e.g., instants and intervals). They both have been used in many geoscience applications (Ma et al., 2020). For many other subjects in geosciences, such as rock types, mineral species, and fossil species, the detailed semantics are already included in conventional databases and can be transferred into KGs. Chen et al. (2020) summarized the existing methods of knowledge reasoning into three categories: rule-based reasoning, distributed representation-based reasoning and neural network-based reasoning. They also listed several applications that can be supported by knowledge reasoning, such as KG completion, question answering, and recommender systems. More specifically, Gil et al. (2019) summarized several geoscience research themes that can benefit from knowledge-rich intelligent systems, including model-driven sensing, thrusted information threads, theory-guided learning, and integrative workspaces.\nKGs will take active roles in machine learning processes to tackle the challenge of big data. Geosciences are facing a boost of machine learning and deep learning applications (Lary et al., 2016; Bergen et al., 2019; Karpatne et al., 2018; Reichstein et al., 2019), and there is a big potential for deploying KGs in those applications. Sheth et al. (2019a) discussed three types of knowledge-infused learning, shallow, semi-deep, and deep. The shallow infusion means using KGs to improve the semantics and conceptual processing of data. The semi-deep infusion means congruent integration of KGs in machine learning techniques, and deep infusion means combining the bottom-up statistical intelligence with the top-down symbolic intelligence for hybrid intelligent systems. Hogan et al. (2020) presented similar perspectives, and pointed out the integrated machine learning processes can also be a way to update, extend, and improve the KGs. A unique topic in those hybrid, integrated processes is using machine learning to analyze knowledge graphs and\/or data in graph forms, which has also been incorporated into the workflow of big data processing (e.g., Li and Chen, 2013; Nickel et al., 2015; Martinez-Rodriguez et al., 2020). The perspectives presented by Sheth et al. (2019a) and Hogan et al. (2020) as well as the recent discussion of AI approaches in GIScience (Li, 2020; Gahegan, 2020) all resonate with the above-mentioned integration of abductive, deductive, and inductive approaches. A few innovative examples of those knowledge-infused intelligent systems have already appeared in geosciences, such as mineral grains recognition (Maitre et al., 2019), rock classification (Ran et al., 2019), petrographic microfacies classification (de Lima et al., 2020), and map service theme classification (Wei et al., 2021). Such systems and applications will significantly increase in the coming years.\nKGs are also able to provide support to explainable AI (XAI), which recently has received a lot of attention. For opaque machine learning processes such as neural networks and genetic algorithms, KGs can help document the provenance of the workflow and improve the interpretability of results. A key feature of KGs is their capability of defining groups or clusters and their associated attributes, which can be leveraged to add a semantic layer to many machine learning algorithms (Lecue, 2020). For example, by explicating typical attributes of instances in a subgroup, KGs can explain the grouping process in a machine learning process and demonstrate the meaning of results (Ristoski and Paulheim, 2016). Geoscientists have used the W3C PROV-O ontology (Lebo et al., 2013) for documenting provenance of data and scientific workflows (e.g., Tilmes et al., 2013; Bedia et al., 2019). Those studies share common topics with XAI. With the wide use of workflow platforms such as Jupyter and RMarkdown in geosciences, there will be more studies of using KGs to improve XAI.\n\n\n\n6\nConcluding remarks\nData-intensive geosciences often rely on the collaboration of researchers from different disciplinary backgrounds, such as computer science, statistics, information science, and the various sub-disciplines in geosciences. KGs have been proved to be an efficient way to bridge the gap between those disciplines and facilitate communication and collaboration within a team. First, KGs can present a quick overview of the major entities, relationships, and structures of the scientific subjects in research. Second, there can be smart functions that chain up data, software, research topics, and researchers in the cyberinfrastructure underpinned by KGs, such as those in recommender systems. Third, KGs can be used into data analysis workflows to improve the quality and interoperability of results. Together with the open data environment, advanced data science methods, and innovative data visualization techniques, KGs will make solid contribution to data-intensive, multi-disciplinary geoscience studies.\nThis review paper shows that there is a lot of space and flexibility for the future work of KG creation and application in geosciences. In the field of Semantic Web, there is a famous slogan \u201cA little semantics goes a long way\u201d, which is also true for KGs in geosciences. Any KG-based updates to the data life cycle, such as metadata annotation, data discovery, data cleansing and integration, and KG-infused machine learning will benefit the data-intensive geosciences. Usually, researchers need to balance three factors relevant to a KG: expressivity, implementability, and maintainability (Ma and Fox, 2013). Expressivity is the granularity of semantics in a KG; implementability is the usability and usefulness of the KG in the real-world applications; and maintainability is the evolution and upgrading of the KG in a long-term perspective.\nA higher visibility of KGs in geosciences rely on the appearance of more innovative research results as well as the education of this topic among geoscience practitioners, especially students. The Living Textbook developed by geoscience researchers and educators (Augustijn et al., 2018; Lemmens et al., 2018) demonstrate several interesting features by using KGs. It deploys a concept map to visualize the key knowledge items and their relationships in a course, together with wiki-style text to show the details. Several interactive functions are made available for teachers and students. Teachers can create mind maps to customize the clusters and learning paths of subjects in a course. Students can explore the concept map of the whole course, follow the learning paths created by teachers, and make notes in the text. The Living Textbook not only creates a better learning experience of geosciences but also demonstrates the advantage of KGs to students.\nWe hope the concept descriptions, exemplar studies, best practices, and trend analyses presented in this paper will be of benefit to both geoscientists and computer scientists, especially those who are working on the creation and implementation of KGs in geosciences.\n\n\nComputer code availability\nNo software\/code was developed or used in this paper.\n\n","79":"","80":"","81":"","82":"","83":"","84":"","85":"","86":"\n\n1\nIntroduction\nThere have been few studies on how well ray tracing using highly simplified atmospheric modeling fares when compared to actual observations. Truly accurate atmospheric models for ray tracing must be based on actual radiosondes. But when predictions for the future are needed, models of the atmosphere must suffice. Although there has been some work in extrapolating radiosondes to nearby places at different heights and terrain (Cogan and Reen), they can't be used for predictions or easily applied. Finally, the effect of terrain on sunrise, which is often neglected, is actually more important than modeling the atmosphere in great detail, as we will show.\nTwo types of horizons are referred to throughout this paper, i.e., the \u201cvisible\u201d and the \u201castronomical\u201d horizon. The \u201cvisible horizon\u201d is the actual horizon an observer sees (Fig. 1\na). The observer's \u201castronomical horizon\u201d is defined to be the horizon he would see if all terrain around him could be flattened to sea level (Fig. 1\nb). These horizons need not be necessarily different, and the altitude of the sun in both cases is dependent on atmospheric refraction accumulated along the light ray's path from the top of the atmosphere until the observer (see Fig. 1b).\nIn our first paper on this subject, atmospheric refraction was calculated using the ray tracing method of M. Menat, using the slightly modified mid-latitude summer and winter atmospheres associated with LOWTRAN 6 (Kneizys). In that ray tracing method, the spherical geometry of the ray's path is approximated as straight lines by taking sufficiently small increments in elevation along the ray's trajectory and readjusting the coordinate system at each increment so as to be aligned with the Earth's surface. For this paper, we report on results using the more conceptually appealing, radially symmetric model developed by Siebren van der Werf who designated the program as REF2017\n2\nVan der Werf's original ray tracing program, REF2017.bas, can be downloaded at https:\/\/siebrenvanderwerf.nl\/index.php?cat=other\/.\n.\n2\n It has a built-in (simplified) 1976 U.S. Standard Atmosphere (shown in Fig. 7), modified to accommodate any temperature and pressure at its base (which will be referred to as \u201cground temperature\u201d and \u201cground pressure\u201d). REF2017 can also be easily modified for local temperature profiles, e.g., radiosondes, or custom models that vary in two dimensions, i.e., as a function of distance and height.\n3\nWhen we use a modified atmosphere we designate the program as the \u201cmodified REF2017\n\n\n3\n (Note., the Menat-type ray tracing method can also easily handle custom atmospheric models, unlike the earlier ray tracing methods of Auer and Standish and Hohenkerk and Sinclair that employed radial symmetry.) When accuracies of a few arcminutes suffice, the ground pressure can be left at the REF2017's default value of 1013.25\u00a0mb (see Fig. 5 in Van der Werf's paper), and humidity can be ignored (his Fig. 6). Furthermore, although REF2017 relies on a simplified atmosphere, it can be used to calculate refraction times that are accurate to within three arcminutes (\u00b115\u00a0s of sunrise time) for 75% of the year in Israel. For the other 25% of the year in Israel, REF2017's atmospheric model must be modified (see Section 3).\nAt the equator, the sun moves 0.25\u00b0 every minute, i.e., 1\u00b0 every 4\u00a0min.\n4\n\n\n4\nAt Israel's approximate latitude of +32\u00b0, it takes about 1\/cos(latitude) longer, or 18\u00a0s. Therefore, when converting from degrees of solar altitude to seconds of sunrise time we use a conversion factor of 60\/18\u00a0=\u00a03.33.\n To predict sunrise times accurate to within 15\u00a0s, one would need to be predict the sun's position to within 0.06\u00b0 or 3.6 arcminutes. This angular resolution corresponds approximately to being able to resolve 5\u00a0m in height at a distance of 5\u00a0km, i.e., tan(5\/5000). The digital terrain models we employ have a typical vertical accuracy of 5\u00a0m. Therefore, if the horizon is 5\u00a0km from the observer, the best angular resolution one could expect to achieve is 3.6 arcminutes, or \u00b115\u00a0s in sunrise time. Better vertical resolution is required for latitudes above the equator since the sun rises at an oblique angle, taking approximately 1\/cos(latitude) longer to rise. For example, using a terrain model accurate to 5\u00a0m for Edmonton, Alberta at +53.5\u00b0 latitude, would limit sunrise predictions to \u00b125\u00a0s. When better resolved terrain models are not available, this latitude-based limit to resolution must be dealt with when predicting sunrise time for different locations (e.g., by adding a varying time cushion to the sunrise time prediction, as we do).\nThis paper is organized as follows. We will first describe the ground temperature model we employed with REF2017 for modeling the refraction via ray tracing. We will then discuss the terrestrial refraction contribution to the atmospheric refraction; how it affects the visible horizon, and how it can be expressed by a simple expression that fits to a good approximation with the ray tracing calculations. We then derive polynomial expressions for the atmospheric refraction for various observer elevations and ground temperatures, as well as determining scaling relationships for the refraction as a function of temperature and pressure. We will then discuss how details of the visible horizon are calculated, and how it is used during the actual calculation of sunrise. Afterwards, we compare calculations of the atmospheric refraction at three locations in Israel to actual observations undertaken at those same locations over the course of several years. We then discuss how to refine REF2017's atmospheric model during the winter months. Finally, we will discuss the application of our model to the observations of sunrise in Edmonton Alberta by R. Sampson.\n\n\n2\nMethods and discussion\nFor all the results presented in this paper the ground temperatures for each observation's place and date were modeled using the 30 arcsecond temperature model of WorldClim version 2.1 (Fick and Hijmans). The observations used to verify our calculations were undertaken during the years 1961\u20132013. WorldClim 2.1 spans the years 1970\u20132020 and was deemed to have sufficient overlap to be adequate for our use. The temperature is close to the diurnal minimum at sunrise (e.g., see Betis). In fact, the use of the minimum diurnal ground temperature (rather than the average or maximum ground temperature) improved our model's predictive accuracy by at least 15\u00a0s for most portions of the year.\nIn cases of non-astronomical horizons, terrestrial refraction, \u03a9, modifies the apparent angular height of the topography and needs to be calculated before calculating the total atmospheric refraction, \u03b4. For this paper, we used Bomford's simple expression (equation (1)) for the terrestrial refraction. In equation (1), L is the actual path length of the curved ray path from the observer to the local horizon (see Fig. 1a), P and T are the atmospheric pressure (mb) and temperature (degrees Kelvin) at the observer respectively, \n\n\n\nd\nT\n\n\nd\nh\n\n\n\n is the temperature gradient (the negative of the lapse rate) at the observer, and f is a function, determined from fitting to REF2017 ray tracing, that is dependent on the path length, L, and the difference in elevation between the observer and the horizon, \u0394h. The lapse rate in REF2017's standard simplified atmosphere is the 1976 U.S. Standard Atmosphere's tropospheric value of 0.0065 degrees Kelvin\/m. Although this choice of the lapse rate is not physically correct for many, if not most, cases near sunrise, the actual lapse rate at the observation site is unknown and can't be easily extrapolated from distant radiosonde measurements; see Section 3. Collecting terms, the terrestrial refraction, \n\n\u03a9\n,\n\n is then,\n\n(1)\n\n\n\u03a9\n\n=\n\n8.15\n\n\nP\n\nT\n2\n\n\n\n\n(\n0\n.\n00342\n+\n\n\nd\nT\n\n\nd\nh\n\n\n)\n\n\n\u00d7\n\nf\n\n(\nL\n,\n\u0394\nh\n)\n\n\n\n(\na\nr\nc\ns\ne\nc\no\nn\nd\ns\n)\n\n\n\n\nwhere \n\n\n\nd\nT\n\n\nd\nh\n\n\n=\n\n- 0.0065 degrees K\/m. Assuming that f(L,\u0394h) has the form of an exponent, we write,\n\n(2)\n\n\n\u03a9\n\n=\n\n8.15\n\n\nP\n\n\nT\n\nm\ni\nn\n\n\na\nv\ng\n\n\n2\n\n\n\n(\n0\n.\n00342\n-\n0\n.\n0065\n)\n\n\u00d7\n\n\u0393\n\u03b3\n\n\n\n\nwhere \n\n\nT\n\nm\ni\nn\n\n\na\nv\ng\n\n\n\n is the minimum WorldClim temperature averaged over the entire year for a particular location (see Section 2 for further discussion), and \u0393 (units of meters) is the parabolic path length approximation to the curved path along the earth of a nearly horizontal ray without refraction (Lehn; Bruton),\n\n(3)\n\n\n\u0393\n=\n\n\n[\n\n\u0394\n\nx\n2\n\n\n+\n\n\n(\n\n\u0394\nh\n\n\u2212\n\n\n\n\u0394\n\nx\n2\n\n\n\n2\nR\n\n\n\n)\n\n\n2\n\n\n\n\n]\n\n\u00bd\n\n\n\n\n\n\n\n\u0394x is the flat-Earth distance between the observer and the mountain horizon, and \u0394h is the height difference between the observer and the horizon. R is the radius of the Earth. Since equation (2) is only an approximation of the terrestrial refraction, and not based on actual ray tracing, and since equation (3) is only an approximation to the length of the actual path, the validity of these equations as a substitute for actual ray tracing must be shown. The best fit to the exponent, \u03b3, if equations (2) and (3) are valid approximations, should be very close to 1.0 since \n\n\n\u0393\n\u03b3\n\n\nm\nu\ns\nt\n\nh\na\nv\ne\n\n dimensions of length (i.e., it substitutes for L in the Bomford expression). To determine the value of the exponent, we developed a search algorithm based on REF2017 to find the terrestrial refraction when the light rays just glance off the visible horizon at a specific height, h\n\nobstruction\n, and distance, L (see Fig. 1a for a depiction of these terms). To test different terrains, we calculated the horizon profile for two places located in Jerusalem (eastern horizons), one place east of Boulder, CO, (western horizon, i.e., facing the Rockies), and finally the eastern horizon of Mexico City (See Fig. 2\n for an example of one of the observation places in Jerusalem). The best fit to \u03b3 for all these places was,\n\n\n\n\u03b3\n=\n\n\n(\n\n\n\n\n0.9900\n,\n\ni\nf\n\n\u0394\nh\n\n>\n\n1000\n\nm\n,\n\n\n\n\n\n\n\n0.9965\n,\n\ni\nf\n\n\u0394\nh\n\n\u2264\n\n1000\n\nm\n.\n\n\n\n\n\n)\n\n\n\n\n\n\nand is very close to 1.0 as expected. Using the best fit to \u03b3 among all four places mentioned above produced a mean variance from the ray tracing results no larger than 0.13 arcminutes (approximately 0.5\u00a0s in sunrise time).\nThe first phase of calculating visible sunrise times requires determining the shape of the physical horizon. For places in Israel, the non-refracted shape of the physical horizon was determined using the 25-m Digital Terrain Model (DTM) of Israel and Western Jordan (Hall et al.). In the initial phase of calculating the terrestrial refraction modification of the non-refracted horizon, an approximation to the visible horizon is calculated from that DTM assuming an average value of the temperature. That is, some value of the temperature must be assumed when calculating the terrestrial refraction, \n\n\u03a9\n,\n\n using equation (2), since we don't know a priori at what azimuth the sun will rise over a portion of a hilly horizon for any day of the year. As a first guess, the yearly average minimum WorldClim temperature is used to determine the horizon's shape. (During the actual sunrise calculation, when we do know the azimuth and altitude of the sun, we substitute the previously calculated contribution to the terrestrial refraction with a better estimate of the temperature, i.e., the monthly minimum WorldClim temperature.) Fig. 6 shows these horizon calculations for the three observation sites in Israel and for Sampson's observation place in Edmonton, AB (see discussion in Section 4). The 1 arcsec. SRTM DTM was used for the latter.\nTo speed up actual calculations of sunrise, we ran REF2017 through a wide range of observer heights, horizon distances and heights, view angles, and ground temperatures. The results were then fitted to polynomial functions of sufficient degree to faithfully reproduce the results of the ray tracing. The fit coefficients, valid for observer heights from zero to 3\u00a0km (above sea level), are listed in Table I. They were used to calculate the total atmospheric refraction, \n\n\u03b4\n\n, as follows,\n\n(4)\n\n\n\u03b4\n\n(\nh\n,\n\n\u03b2\n)\n\n=\n\n\u2211\n\ni\n=\n0\n\n8\n\n\n\u03b2\ni\n\n\u00d7\n\n(\n\n\u2211\n\nj\n=\n0\n\n4\n\n\n\nh\nj\n\n\na\n\ni\nj\n\n\n\n)\n\n\n\n\nwhere \u03b2, is the view angle (apparent solar altitude), \n\nand\n\nh\n\n is the height of the observer. The analytical function can be rapidly calculated by a computer program for any future calculation. We tabulate the coefficients, \n\n\na\n\ni\nj\n\n\n\n,in Table 1.\nVan der Werf published useful scaling relationships for temperature, etc., following the method of the Pulkovo tables. The scaling relationship for atmospheric refraction; \n\n\u03b4\n\n, as a function of temperature (equation (6)), is used throughout our computer code. Since atmospheric refraction is also a function of the observer's height, h, and the view angle, \u03b2, the temperature exponent depends on the height and view angle. This dependence can be modeled by a polynomial expansion, A(h \n\n\n\u03b2\n\n), in these two variables,\n\n(5)\n\n\nA\n\n(\n\nh\n,\n\u03b2\n\n)\n\n\n=\n\n\u2212\n\n0.0001041\n\nh\n\n+\n\n\n\n\u2211\n\n\ni\n=\n0\n\n4\n\n\nd\ni\n\n\n\u03b2\ni\n\n\n\n\n\n\n(The values of \n\n\nd\ni\n\n\n are listed in Table II\n\n.) We denote the standard ground temperature and pressure as \n\n\nT\no\n\n\n\u00a0=\u00a0288.15 degrees K, and \n\n\nP\no\n\n\n\u00a0=\u00a01013.25\u00a0mb respectively, so the REF2017 scaling relationship adds the temperature and pressure dependence of \n\n\u03b4\n\n as follows.\n\n(6)\n\n\n\u03b4\n\n(\n\nh\n,\n\u03b2\n,\n\nT\n,\n\nP\n\n)\n\n\n=\n\n\n\n(\n\nP\n\nP\no\n\n\n)\n\n\u03ba\n\n\n\n(\n\n\nT\no\n\nT\n\n)\n\n\u03bb\n\n\n\u03b4\n\n(\nh\n,\n\u03b2\n,\n\nT\no\n\n,\n\nP\no\n\n)\n\n\n\n\nwhere \u03ba\u00a0=\u00a01.0856, and \n\n\u03bb\n\n \u2261 A(h,\u03b2)\u00a0=\u00a01.682 for a zenith angle of 90\u00b0, i.e., \u03b2\u00a0=\u00a00\u00b0, at sea level, i.e., h\u00a0=\u00a00, (Table II). (Van der Werf wrote that \n\n\u03bb\n\n was equal to 1.7081, but we used the value we determined from our own fitting; the difference is actually insignificant to the level of accuracy we required for our sunrise calculations). In fact, the entire height and view angle dependence of \n\n\u03bb\n\n only changes the sunrise times by a few seconds for ground temperatures in the range of validity of equation (6), i.e., 260 \u2264 To \u2264 320 oK, and 0\u00a0\u2264\u00a0h\u00a0\u2264\u00a03000\u00a0m, and can be definitely ignored for the sake of simplicity when only moderate accuracies are required.)\nThe initial step in our calculation of the visible sunrise requires determining the time of the astronomical sunrise, i.e., the sunrise seen from a height, h, while the horizon is at sea level. To calculate that sunrise, we iterate in declination, until the hour angle at the total zenith distance, z, is constant to the desired accuracy (refer to Fig. 1b for a pictorial reference of the symbols below).\n5\n\n\n5\nThe code that calculates the astronomical sunrise times is a part of the software package that we have posted on Github at: https:\/\/github.com\/chaimkeller\/software\/tree\/master\/StringRoutines. It is based on several sources: Kosower, M., the MatLab routines of Eren Ofek (https:\/\/webhome.weizmann.ac.il\/home\/eofek\/matlab\/) and code derived from the Home Planet Program of John Walker (https:\/\/www.fourmilab.ch\/homeplanet\/). The accuracy of the astronomical constants used in these programs is comparable to those of the Almanac, i.e., 6\u00a0s.\n The total zenith distance, z, can be written as the sum of the total altitude of the ray, \n\n\n\u03b2\no\n\n\n, i.e., the negative of what is commonly referred to as the \u201cdip\u201d, i.e., D(H) in the Explanatory Supplement to the Astronomical Almanac (ESSA) equation 9.331, which is the combination of the local view angle, as seen by the observer at height h, and the spherical coordinate angle \u03c6, and the contributions from atmospheric refraction. It is convenient to divide the refraction into two components along its path, \n\n\n\u03b4\n\n0\n\nt\no\n\nh\n\n\n\n\n(\n\nE\nS\nA\nA\n\nu\ns\ne\ns\n\nt\nh\ne\n\nl\ne\nt\nt\ne\nr\n\nR\n\n)\n\n,\n\na\nn\nd\n\n\n\u03b4\n\n0\n\nt\no\n\n\u221e\n\n\n\n(\nE\nS\nA\nA\n\nu\ns\ne\ns\n\nt\nh\ne\n\nl\ne\nt\nt\ne\nr\n\nA\n)\n.\n\n They are the contributions from atmospheric refraction along the ray's path from the observer's position at height h until the astronomical horizon, and the contribution to the refraction for the ray's path from the astronomical horizon to the edge of the atmosphere, respectively. Then z can be written as 90o\n\u2013 D \u2013 R\u00a0+\u00a0A, or more generally (and switching to radians) as,\n\n(7)\n\n\nz\n\n(\nh\n,\nT\n,\nP\n)\n\n=\n\n\u03c0\n\/\n2\n\n+\n\n\u03b2\no\n\n\n(\nh\n,\nT\n,\nP\n,\n\u03c6\n)\n\n+\n\n\u03b4\n\n0\n\nt\no\n\nh\n\n\n(\nh\n,\nT\n,\nP\n)\n+\n\n\u03b4\n\n0\n\nt\no\n\n\u221e\n\n\n\n(\n\nT\n,\nP\n\n)\n\n,\n\n\n\nwhere \n\n\n\u03b4\n\n0\n\nt\no\n\n\u221e\n\n\n\n is by definition, independent of the observer height, h (N.b., all heights in this paper are defined with respect to sea level.) Since the other contributions to z in equation (6) are functions of the observer's height, their height dependence was fitted to a polynomial series expansion in h, i.e.,\n\n(8)\n\n\nlog\n\n(\n\n\n\u03b4\n\n0\n\nt\no\n\nh\n\n\n\n(\nh\n)\n\n)\n\n=\n\n\u2211\n\ni\n=\n0\n\n5\n\n\n\na\ni\n\n\n\n[\n\nl\no\ng\n(\nh\n\n\u00d7\n\n0\n.\n001\n)\n\n]\n\ni\n\n,\n\n\n\n\n\n\n\n(9)\n\n\nl\no\ng\n(\n\n\u03b2\no\n\n(\nh\n,\nj\n)\n)\n=\n\n\u2211\n\ni\n=\n0\n\n4\n\n\n\nb\ni\n\n\n\n[\n\nl\no\ng\n(\nh\n\n\u00d7\n\n0\n.\n001\n)\n\n]\n\ni\n\n,\n\n\n\n\n\n\n(See Table III\n for the actual values). Actual REF2017 ray tracing verified that the scaling relationship for temperature can be successfully used to a high accuracy to decouple the temperature dependence from the height dependence. That is, equation (6) reproduces the actual ray tracing results for any temperature and height throughout the height range tested from 0 to 3000\u00a0m and from temperatures 260K\u2013320\u00a0K with a maximum error of 0.38 arcminutes (less than 2\u00a0s in time). and a mean error of 0.0034 arcminutes. The standard deviation was 0.058 arcminutes. (The accuracy of the polynomial fit will degrade beyond those ranges.) However, there is no simple scaling relationship that can completely decouple the dependence of the ray's local altitude, \n\n\n\u03b2\no\n\n\n, on the observer's height, h, and on the temperature, T. This is because it depends on the combination of the local change in refraction, minus the local change of the angle, \u03c6, that the ray subtends around the Earth's center (Young (2006)) (see Fig. 1b). However, for the same reason, its temperature dependence is small (since it is mostly dependent on the more or less constant geometry), and the variation of that temperature dependence due to height is even smaller, and thus can be ignored for our calculations. We therefore employ the same sort of scaling relations for all the terms in equation (7), as well as for the terrestrial refraction, \u03a9, (equation (2)), viz.,\n\n(10)\n\n\n\n\u03b4\n\n0\n\nt\no\n\nh\n\n\n(\nh\n,\nT\n)\n=\n\n\u03b4\n\n0\n\nt\no\n\nh\n\n\n(\nh\n)\n\u00d7\n\n\n(\n\nT\n\/\n\nT\no\n\n\n)\n\n\u03c4\n\n,\n\n\u03c4\n=\n2\n.\n18\n,\n\n\n\n\n\n\n(11)\n\n\n\n\u03b4\n\n0\n\nt\no\n\n\u221e\n\n\n\n(\nT\n)\n\n\n=\n\n9\n.\n567\n(\nm\nr\na\nd\n)\n\u00d7\n\n\n(\n\nT\n\/\n\nT\no\n\n\n)\n\n\u03c1\n\n,\n\n\u03c1\n=\n1.7\n,\n\n\n\n\n\n\n(12)\n\n\n\n\u03b2\no\n\n\n\n(\nh\n,\nT\n,\n\u03c6\n)\n\n=\n\n\u03b2\no\n\n\n(\nh\n,\n\u03c6\n)\n\n\u00d7\n\n\n(\n\nT\n\/\n\nT\no\n\n\n)\n\n\u03b6\n\n,\n\u03b6\n=\n0.2\n,\n\n\n\n\n\n\n(13)\n\n\n\u03a9\n\n(\nT\n)\n\n=\n\u03a9\n\n(\n\nT\n0\n\n)\n\n\u00d7\n\n\n(\n\nT\n\/\n\nT\no\n\n\n)\n\n\u03c1\n\n,\n\u03c1\n=\n1.7\n\n\n\nwhere \n\n\nT\no\n\n\n\u00a0=\u00a0288.15 degrees K.\nThe main purpose of this paper is to report our results and justify our methods, and not to step through the details of how our code actually calculates the visible sunrise over the physical horizon (For more details, please see our first paper, and the links in footnote 5.). It suffices to say that we first calculate the likely horizon profile using our model for terrestrial refraction, \n\n\u03a9\n\n. We then increment in hour angle, starting at the time of the astronomical sunrise (calculated at an earlier program step), until the apparent altitude, \u03b2, of the sun's upper limb matches the horizon's view angle at the azimuth of the sun. \u03b2 needs to be recalculated for each hour angle by iteration since it depends on itself, as well as on the total atmospheric refraction, \n\n\u03b4\n\n, at that hour angle. The visible sunrise occurs when the apparent solar altitude, \u03b2, is sufficiently large so that the upper limb of the sun skims the horizon.\n\n\n3\nResults\n\nFigs. 3\u20135\n\n\n\n show how well our calculations compare to observations of the visible sunrise at three different locations in Israel, taken by three different observers over a period of about thirty years using the WorldClim monthly minimum temperature. The place names and their elevations are described in the figure captions. The day numbers for all the observations have been renormalized to a single year, shown as the x-axis caption in the figures. In Fig. 3 we have also overplotted the results obtained from using the formula for refraction as a function of apparent solar altitude, equation 3.283\u20132, of the ESAA (It is also published in the Astronomical Almanac.). This formula, apparently based on empirical sources (see Wilson), is usually attributed to Sinclair (for small view angles, \n\n\n\u03b2\n\n<\n\n\n15\no\n\n\n) and to Bennett (1982)\n\n6\n\n\n6\nSee Reijs, V., http:\/\/www.archaeocosmology.org\/eng\/ComparingRayTracingwithFormula.htm, and.\n (for larger view angles \n\n\u03b2\n\n\u2265\n\n\n15\no\n\n)\n\n. It will be referred to from now on simply as SB.\n\n(14)\n\n\n\nR\n\nS\nB\n\n\n(\n\u03b2\n)\n=\n\u03b4\n(\n\u03b2\n,\nT\n,\nP\n)\n=\n\n(\n\n\n\n\n\n(\n\n\n0.28\n\nP\n\n\nT\n+\n273\n\n\n)\n\n\n\n\n\n0\n0\n\n.\n0167\n\n\nt\na\nn\n\n(\n\n\u03b2\n\n+\n\n7.31\n\/\n\n(\n\n\u03b2\n+\n4.4\n\n)\n\n\n)\n\n\n\n\n\u03b2\n\n\u2265\n\n\n15\no\n\n\n\n\n\n\n\n\n(\n\nP\n\nT\n+\n273\n\n\n)\n\n\n\n\n\n0\no\n\n.\n1594\n\n+\n\n0.0196\n\n\u03b2\n\n+\n\n0.00002\n\n\n\u03b2\n2\n\n\n\n1\n\n+\n\n0.505\n\n\u03b2\n\n+\n\n0.0845\n\n\n\u03b2\n2\n\n\n\n\n\u03b2\n\n<\n\n\n15\no\n\n\n\n\n\n)\n\n\n\n\n\n\nEquation (14) is the SB equivalent of equations (4) and (5). When evaluating the total atmospheric refraction using SB refraction, we must use the standard values for view angle and refraction from the horizon to infinity found in ESAA section 9.331, i.e.,\n\n(15)\n\n\nz\n(\nd\ne\nd\nr\ne\ne\ns\n)\n=\n\n90\n\u2218\n\n\u2212\n\n34\n\u2032\n\n\u2212\n\n2\n\u2032\n\n.\n12\n\nh\n\n\n\n\nwhere h is the observer's height in meters. (Equation (15) would replace formulas 8-12 when following the ESAA type analysis). However, since the terrestrial refraction is not developed by ESAA, we use the same expression for the terrestrial refraction of equations (1) and (2). The scaling law used for the SB formula is simply the P\/T term already built into the formula. However, for the terrestrial refraction, \u03a9, we have used the ratio (288.l5\/Tground(oK)) as the multiplicative scaling factor when calculating the sunrise using equation (14).\n7\n\n\n7\nFor details or how to perform the sunrise calculations using the Bennett formula, see our code in the Github repository at: https:\/\/github.com\/chaimkeller\/software\/tree\/master\/netzski6_c_2\/netzski6.\n\n\nAs seen in the figures, the sunrise can be predicted via REF2017 to an absolute mean difference of the observations from our calculations of about \u00b115\u00a0s, for only about 75% of the year, i.e., with the notable exceptions of the winter months from November to February. For these months, sunrise was typically observed 15\u00a0s earlier than the predicted time. However, the scatter in observed sunrise times around a new lower mean is still approximately \u00b115\u00a0s. Note that we present the observations as the difference of the calculated sunrise times for the visible horizon subtracted by the calculated times for the astronomical horizon. In this way, any error in the atmospheric model or in the terrain model, will show up in the difference. Although there is also error associated with the calculation of the astronomical sunrise background, it can't be decoupled from the total refraction contribution to the error without special observations of astronomical horizons. Other sources of errors and inaccuracies in determining the terrestrial refraction which we estimate to contribute 10% of the total error, and errors from weather related effects (which we will discuss below). See the captions of Figs. 3\u20135 for actual values of the accuracy obtained. Those values also serve as an estimate of the total error. For distant horizons, i.e., beyond 40\u00a0km, we can ignore the \u00b15\u00a0m vertical inaccuracy of the 25\u00a0m DTM of Israel (it is on the order of arctan(5\/40000) or 0.5 arcminutes, i.e., \u00b12\u00a0s in sunrise time). The analysis using the ESAA formulas with the SB expression for the refraction term, R, gives slightly less accurate results for most of the year, but only marginally less accurate as can be seen in Fig. 3 (see the caption for more details).\nWhy are our calculations less accurate for winter months? During that time of year low altitude inversion layers are quite common at sunrise, especially during the winter after long, cloudless, and windless nights (e.g., see Liljequist). These inversion layers can produce changes in refraction causing the sunlight to arrive at angles normally invisible to an observer, making the sunrise appear earlier. We have several indications that inversion layers are causing the discrepancy in the winter sunrise times. First of all, there doesn't appear to be any such effect in the observations for Bnei Brak (Fig. 5). Bnei Brak's eastern horizon is the Shomron Hills, about 1000\u00a0m high and only 40\u00a0km to the East, which makes the horizon view angles of about +1\u00b0 during the winter months (Fig. 6). Young (2004) (Fig. 9 in that reference) has shown that even for such small view angles, the effect of inversion layers on the atmospheric refraction is still minimal. On the other hand, the Jerusalem observation sites have view angles are close to zero for the entire year (and even less than zero when the sun rises in the southeast during the winter months, see Fig. 6). Secondly, subtracting 15\u00a0s from the calculated sunrise times for days that match simple criteria that promote inversion layers improved our results. These criteria are 1) long nights (Liljequist), and 2) near horizontal view angles (Young 2004). The actual criteria we used were: (1) nights longer than 13\u00a0h, and (2) the sun rose that day over terrain with a near horizontal view angle of 0.01\u00b0 or less. This simple fix worked for both Jerusalem observation sites even though the terrain defining their horizons (Fig. 6) is significantly different (See Figs. 3\u20134 for a comparison of our results with and without subtracting 15\u00a0s on days that meet those criteria.).\nAs mentioned in the introduction the best way to investigate the earlier than expected winter sunrises would be to calculate the refraction using actual radiosondes observations of the atmospheric temperature and pressure. Unfortunately, radiosondes are only available in Israel for the weather station at Beit Dagan,\n8\nThe radiosonde data we used are archived in the University of Wyoming Radiosonde database, http:\/\/weather.uwyo.edu\/upperair\/sounding.html. We used the available radiosonde observations closest to sunrise, as long as these showed the largest inversions. For the winter months, observations were at 06Z, about one and half hours after sunrise. For the summer, we used the 00Z radiosondes, about two and a half hours before sunrise. (N. b., inversions can often be stable for hours, see Haiken. N. et al.). We limited our use of the recorded radiosondes to only cloudless days with little or no wind, i.e., those with weather that promotes thermal inversions.\n\n\n8\n more than 40\u00a0km to the west and over 700\u00a0m lower in elevation. For the last factor alone, one would expect the near ground atmospheric layering for Jerusalem to be different, since it is a function of topography, even when there is no wind to distort the layers (Liljequist,Lieman and Alpert, Hashmonay and Cohen, Haikin, N. et al. For example, Tschudin recently published a comparison of calculated refraction values obtained by using radiosondes to the refraction values published by R. Sampson for Edmonton, Alberta. This particular comparison to is clearly much poorer than his comparisons for places where the observations and radiosondes were undertaken at similar altitudes and topography.) Furthermore, changes in topography and climate along the light path in the lowest part of the atmosphere needs to be taken into account since the index of refraction along the ray's path is dependent on such factors (McClusky). (Note, Van der Werf et al. were perhaps the first to use two dimensional modifications to the simplified standard atmosphere temperature layering in order to reproduce historical observations of the Novaya Zemyla effect.) The need for two dimensional modifications to the standard atmosphere for the eastern horizon of the Armon Hanatziv observation site seems obvious, since the elevation drops to the East by 1000\u00a0m within 20\u00a0km, and three Kl\u00f6ppen climate zones are juxtaposed within a few kilometers of one another in that direction (Goldreich, Y.). To avoid over-kill, the topography east of the Armon Hanatziv observation site for all azimuths was approximated by a smoothed polynomial based on the due-East topography (Fig. 12). We then renormalized the vertical axis of the radiosonde measured atmosphere along each step of the ray tracing, as though the atmosphere followed the contours of the ground. For example, if the light attains an altitude h along its path, and the ground elevation at that place along its path is hground, the temperature will be set to what was observed at Beit Dagan for the height h - hground. In order to deal with these custom atmospheres, we modified the REF2017 code to handle any sort of atmospheric layering by simply substituting observed temperatures and pressures for the ones normally used in the standard REF2017 code (i.e., the 1976 U.S. standard atmosphere). We first tested our modifications in just one dimension, i.e., the vertical direction, by comparing our results using six LOWTRAN 6 model atmospheres of Kneizys, F. X. et al., using two totally different ray tracing methods built into our program. One was based on M. Menat's original code, and the other based on the modified REF2017. Both the modified REF2017 and Menat's ray tracing methods produced the same refraction along the entire path of the light rays within 0.2\u00a0\u00b1\u00a01 arcseconds for all six LOWTRAN 6\u00a0atm. We then compared results from these two ray tracing methods using the radiosonde observed atmosphere that Nauenberg published for Oakland, CA (we reconstructed his data from his Figs. 1 and 4-6, his Table I, and his equation (10)). We first analyzed his data using the modified Menat's method. Our results were in almost exact agreement with Nauenberg's value for the accumulated refraction up to 600\u00a0m of elevation. However, the modified REF2017 code calculated an additional 1.7 arcminutes (0.5\u00a0mrad) of refraction up to that height, i.e., 4% larger than Nauenberg's results. By comparing total refraction as a function of height for our two calculation methods, it was seen that the modified REF2017 code accumulated (integrated) more refraction through regions of low altitude thermal inversions (since both calculated almost identical indices of refraction as a function of elevation). Van der Werf claimed that the integration method used by REF2017 ignores discontinuities in curvature, but implied that less abrupt changes in curvature are handled properly without any smoothing. On the other hand, Menat's method assumes that the light path can be approximated as a straight line for small steps in height, which may not be true in the case of large thermal gradients associated with strong inversions, leading to an underestimation of the refraction. We believe that it is for this reason that Menat's ray tracing method produced 16% less total atmospheric refraction than the modified REF2017 method did for the entire 25\u00a0km of radiosonde observation heights that Nauenberg published for Oakland, CA. (We suspect this may also be the case for Nauenberg's calculation of the refraction, which used Newcomb's numerical integration method; see Young 2004, for a discussion of that method. This discrepancy warrants a general review of how different numerical integration techniques handle inversions.)\n\nFig. 8\n\n\n plots the comparisons between the observed and calculated effect for refraction using the radiosonde-based modified atmosphere instead of the REF2017 standard atmosphere. The values plotted in Fig. 8 are (a) the difference between the observed sunrise times and the calculated sunrises using the standard REF2017\u00a0atm with WorldClim ground temperatures (blue circles), and (b) the difference between the refraction calculated using the standard REF2017\u00a0atm and that calculated using the radiosonde-based two-dimensionally customized atmospheres (red circles). We also plotted a repeat of (b), instead using the Beit Dagan radiosonde-based atmosphere at the Armon Hanatziv observation site without any modification (green dots). To be able to compare (b) to (a) we used our approximate conversion of the refraction difference to difference in minutes of sunrise by multiplying by 3.33 (see footnote 4). The view angle of the upper limb of the sun at sunrise over the visible horizon for each radiosonde date was used as the observer's view angle in the ray tracing as determined by our sunrise program. Our very simple model for topography-based atmospheric temperatures was considerably more successful in reproducing the anomaly seen in the observations. That is, earlier than expected sunrise times for the winter months than could be calculated without shifting the radiosondes vertically and horizontally (the former had a third of the absolute mean variance of the latter when compared with the observations for the winter months). The shifted atmospheres also reproduced the summer months sunrise times, already adequately calculated using the REF2017 standard atmosphere. The mean absolute difference between the two-dimensional atmosphere model and observations for all the points plotted in Fig. 8 was 9\u00a0s for the entire year. For the winter months it was 12\u00a0s. That even such a simple model could reproduce the discrepancy observed in winter month sunrise times gives us confidence that low altitude temperature inversions are behind this discrepancy. This result also supports dealing with this discrepancy in a simplified way, by subtracting a fixed number of seconds from the calculated sunrise times for those winter days promoting thermal inversions.\n\n\n4\nObservations in Edmonton, Alberta\nDuring the years 1990\u20131993 R. Sampson undertook observations of the sunrise and sunset in Edmonton, Alberta. Fig. 9 shows the expected sunrise times applying our methodology to Sampson's observations. Note that the observations are quite different than the predictions, the mean absolute variance being 41\u00a0s. However, investigation of the terrain revealed that the distance to the horizon (the distance from the observer to the place where the sun is seen to rise; d in Fig. 1a) was less than 15\u00a0km. The distance to the horizon is shown in Fig. 10\n. That figure shows that the closest distances are associated with the worst predictions, as one would expect for results dependent on inaccuracies in the height of the obstructions, and not dependent on errors in the refraction calculation. Although the terrain heights for Edmonton we used are based on the SRTM 1 arcsecond (approximately 30-m) digital terrain model with a claimed vertical accuracy of anywhere from 3 to 7\u00a0m, the vertical accuracy has been reported to be much less (Ueema E. et al.). Digital terrain accuracy is much more essential when the distance to the horizon is smaller. Since the view angle is approximately (for small angles) equal to the difference between the horizon and observer's heights, divided by the distance to the horizon, any inaccuracy in that difference will be reflected by unexpected sunrise times when comparing observations to calculations. This can happen, for example, if tree-tops define the visible horizon. Assuming a mean tree height of 15\u00a0m (pine trees in that vicinity can grow much higher), we recalculated the terrain profiles adding the tree-top height to all the terrain heights, and recalculated the expected sunrise times. The results are shown in Fig. 11\n\n. Note that the comparison to observations is much better, i.e., having an absolute mean variance of about 24\u00a0s (This roughly corresponds to an absolute mean variance of 15\u00a0s near the equator, as explained in Section 1), and a four times improvement over the methods used by Wilson. Although this doesn't prove that poorly defined terrain heights are the source of the discrepancy apparent in Fig. 9, it strongly suggests this is the case. A further indication of this is the way the discrepancy correlates with the obstruction's distance from the observer (plotted in Fig. 10). The discrepancy is worst for closest distances, and less for greater distances, and all discrepancies are resolved equally well by the simple addition of a constant height to the terrain ground heights. Notice that there is still a section of the data in Fig. 11 near days 50 and 300 that cannot be accounted for by our fix. Sampson suggested that atmospheric mirages may have caused some of the large discrepancies he observed, viz., the Novaya Zemlya effect, although he could not support those suggestions via ray tracing.\n9\n\n\n9\nPrivate communications.\n The winter observations at Armon Hanatziv lend some credence to that possibility.\n\n\n5\nConclusions\nWe have shown that simple models for atmospheric refraction and ground temperatures can be used to calculate sunrise times to an accuracy of \u00b115\u00a0s for three places in Israel, and to an accuracy \u00b124\u00a0s for Edmonton. An accurate model of the terrain is the most essential factor for achieving moderate accuracies. Accuracy may be improved at times of the year when temperature inversions are common by subtracting a fixed amount of time. Based on our results for these four locations, we surmise that our calculation method should maintain its accuracy for many other inhabited places in the world.\n\n","87":"","88":"","89":"\n\n1\nIntroduction\nThe Cassini\u2013Huygens mission was launched in 1997 to perform the first close-up study of Saturn and its moons. Cassini was a pioneering success, proving significant insight into previously unseen physical processes occurring on Saturn\u00a0(Spilker, 2019). With the help of the Cassini Plasma Spectrometer (CAPS)\u00a0(Young et al., 2004), Magnetometer (MAG) and Magnetospheric Imaging Instrument (MIMI) onboard, Cassini spent \n\n13\n\n years imaging Saturn\u2019s dynamic magnetosphere and the many electromagnetic phenomena influencing it.\nThe motivating question we consider here is: given the rich source of data collected by Cassini over \n\n13\n\n\u00a0years of observations, can we improve the quality of science collected on future missions? In particular, we seek to understand if changes in the spacecraft\u2019s immediate electromagnetic environment can be detected in situ in order to adaptively switch data-collection modes for scientific instruments (such as CAPS).\nAs a first step towards enabling responsive instrumentation, we investigate the feasibility of detecting electromagnetic environment transitions in CAPS ELS (Electron Spectrometer) data, specifically, the Saturn bow shock and magnetopause. CAPS ELS made multiple observations of Saturn\u2019s magnetic field boundaries, which have been manually annotated by domain experts\u00a0(Jackman et al., 2019). We assess four approaches to the problem of detecting these boundary transitions in CAPS ELS data that could be used to enable autonomous onboard instrument adaptation.\nPrevious approaches to plasma event detection have employed supervised machine learning methods, which require a labeled set of positive and negative examples of the phenomena to be detected\u00a0(Yeakel et al., 2018). The Earth-orbiting Time History of Events and Macroscale Interactions during Substorms (THEMIS) and Magnetospheric Multiscale Mission (MMS) missions have provided a rich source of data that has driven substantial interest in using such supervised methods to automatically classify the regions of Earth\u2019s magnetosphere (Argall et al., 2020; Olshevsky et al., 2021; Breuillard et al., 2020; Nguyen et al., 2019). Azari et al. (2018) developed an algorithm to detect interchange injection events in Saturn data observed by the Cassini Charge-Energy-Mass Spectrometer (CHEMS) using manually labeled data to train and evaluate. Apart from enabling adaptive spacecraft observations, the reliable detection of plasma events would allow for fine-grained modeling of the dynamics of planetary magnetospheres, as has been done for Earth (Codd et al., 2021; Lee et al., 2020).\nWe instead explored methods that operate in an unsupervised fashion, meaning that they are not trained on pre-existing examples of the events but instead look for statistical irregularities. This allows the system to make fewer assumptions about the exact nature of the transitions before they are observed. Unsupervised methods have been explored before in the planetary sciences for classifying solar wind observations (Heidrich-Meisner and Wimmer-Schweingruber, 2018; Bloch et al., 2020), a problem of a different nature than the current setting. The techniques we explore here are much more sophisticated because they need to detect instantaneous transitions rather than performing a post-hoc classification of an observation.\nTo the best of our knowledge, ours is the first attempt to automatically identify transitions between regions of different field and plasma characteristics in satellite plasma spectrometer data. The methods we evaluate in this work are \u201cchange point\u201d detection using Relative Unconstrained Least Squares Information Fitting (RuLSIF)\u00a0(Liu et al., 2013), state change detection using Hidden Markov Models (HMMs) (including a Bayesian Non-Parametric extension (Teh et al., 2006)), anomaly detection using Heuristically Ordered Time Series with Symbolic Aggregate Approximation (HOT SAX)\u00a0(Keogh et al., 2005), and anomaly detection using a novel extension of the Matrix Profile (Yeh et al., 2016), called the Multidimensional Matrix Profile.\nBecause the ultimate purpose of these methods is to enable the in situ detection of environmental changes onboard future spacecraft, runtime is a key consideration due to the scarce computational resources available for radiation-hardened spacecraft processors.\nThe key contributions of this work are:\n\n\n\n1.\nWe evaluate four different unsupervised approaches to identify magnetic field transitions in CAPS ELS data from the Cassini mission to Saturn, in terms of both detection performance and runtime.\n\n\n2.\nWe propose an extension to the Matrix Profile\u00a0(Yeh et al., 2016) for anomaly detection in multidimensional time series data.\n\n\n3.\nWe demonstrate that bow shock transitions from CAPS data can be detected best by the Multidimensional Matrix Profile and the non-Bayesian HMM.\n\n\n4.\nWe find that all four approaches struggle to identify magnetopause transitions from CAPS data. Across all methods, the significant differences between spacecraft orbits across years limits the generalizability of parameters optimized on a single year and suggests that online (dynamic) adaptation of parameters may be beneficial.\n\n\n\nWe expect that these findings are not limited to the Cassini mission, and they will be broadly applicable to future spacecraft missions such as Europa Clipper, described in Section\u00a06.2.\n\n\n2\nDescription of CAPS ELS data\nAs mentioned above, to evaluate these algorithms empirically, we used an existing set of observations collected by CAPS ELS\u00a0(Young et al., 2004), with known (manually annotated) magnetic field boundary crossings\u00a0(Jackman et al., 2019). CAPS ELS measured the electron counts across various energies from Cassini\u2019s arrival at Saturn in mid-2004 to the CAPS instrument\u2019s failure in mid-2012. The electron counts per sample time are split into \u201cbins\u201d corresponding to specific energy ranges.\n\nWe employed several pre-processing steps to prepare CAPS ELS data for analysis. Many of these pre-processing steps are necessitated by the lack of access to the original data in the onboard setting.\n\n2.1\nChoice of anode\nCAPS ELS has \n8\n different anodes that collect particles incident from different directions onto the instrument. Following the recommendations of the CAPS PDS User Guide\u00a0(Wilson et al., 2012), we used data from the least-obstructed anode \n5\n.\n\n\n2.2\nAddressing inconsistent dimensionality\nThe original CAPS ELS data consists of observations at \n\n63\n\n energy bins, but some observations were reduced to \n\n32\n\n bins prior to downlink to reduce bandwidth consumption. For consistent dimensionality, we excluded the lowest-energy bin entirely (due to residual spurious signals) and converted all remaining \n\n62\n\n-bin observations to \n\n31\n\n-bin observations by storing the average value for pairs of adjacent bins.\n\n\n2.3\nAddressing inconsistent sampling cadence and missing values\nCAPS ELS data also has an irregular sampling cadence because some observations adjacent in time were combined prior to downlink. For example, the elapsed time between samples for data collected during 2004 was either \n2\n seconds (\n\n45\n\n% of samples), \n4\n seconds (\n\n21\n\n%), or \n8\n seconds (\n\n33\n\n%). Further, around \n\n0\n.\n6\n\n% of individual bin observations are missing due to downlink gaps.\nTo standardize the cadence and address missing values, we performed a linear interpolation across the time dimension independently for each of the \n\n31\n\n bins to generate samples every \n2\n seconds (chosen as the most common and finest-grained cadence in the input data). Any negative counts obtained after interpolation were clipped to \n0\n. In our training set spanning \n\n180\n\n hours of observations from 2004, fewer than \n\n0\n.\n1\n\n% of the entries were clipped.\n\n\n2.4\nFiltering of noise\nWe investigated the use of a \n1\n-dimensional Gaussian kernel to smooth the time series by removing high-frequency components. Further, we also investigated the use of minimum, median, and maximum filters, with the aim of filtering out noise. The optimal width of the Gaussian kernel, the filter type and filter width were chosen independently for each algorithm via performance on a small subset of data: see Section\u00a05 for details. Given the goal of onboard use, we considered Gaussian kernels of small effective width, either \n6\n, \n\n12\n\n or \n\n30\n\n seconds. Finally, we employed a \n1\n-dimensional Anscombe\u2019s transform to each binned count \nc\n to convert the observations (count-based Poisson distribution) to a variable with an approximately Gaussian distribution, \n\nc\n\u2192\n\n\nc\n+\n\n\n3\n\n\n8\n\n\n\n\n\n.\n\n\n2.5\nAddressing label imprecision\nThe list of known crossing events from\u00a0Jackman et al. (2019) contains information about the crossing times only at the resolution of a minute, whereas the pre-processed data operated on by the algorithms has a resolution of \n2\n seconds, as described in Section\u00a02.3. We therefore allow a margin of \n1\n\u00a0minute for detections before the true crossing event in the evaluation procedure, described in detail in Section\u00a05.\nTo aid in reproducibility, we have compiled and released a dataset of CAPS ELS observations together with the labeled crossing events in a convenient format (Jackman et al., 2020)\n1\n\n\n1\n\nhttps:\/\/doi.org\/10.5281\/zenodo.3928593.\n. Further, we have published open-source code\n2\n\n\n2\n\nhttps:\/\/github.com\/JPLMLIA\/CAPS-ELS-Transition-Detection.\n that implements these pre-processing steps. See Section \u2018Computer Code Availability\u2019 and Section \u2018Data Availability\u2019 for details.\n\n\n\n3\nMagnetospheric boundary crossings\nSaturn\u2019s magnetospheric structure is complex, with multiple plasma sources that include the solar wind, Saturn\u2019s many moons, its rings, and its ionosphere\u00a0(Gombosi et al., 2009; Blanc et al., 2015). Prior to the Cassini mission, many of the processes contributing to the plasma dynamics within the magnetosphere were not completely understood. Among its many discoveries, the Cassini mission identified water-group ions from Enceladus\u2019 plumes as a major source of Saturn\u2019s plasma. However, many questions about the seasonality of the magnetosphere, the dynamics of the ionosphere, the nature of plasma loading, transport and loss remain open.\nTo continue our analogue study of existing data from the Cassini spacecraft, we employed a list of \n\n2772\n\n known crossing events experienced by Cassini near Saturn from 2004 to 2012 that were compiled by\u00a0Jackman et al. (2019). This list includes two kinds of boundaries: magnetopause (magnetosphere-magnetosheath) and bow shock (magnetosheath-solar wind). Both inbound and outbound (with respect to Saturn) crossings are included.\nWe omitted events that lack corresponding ELS data, yielding a final set of \n\n2399\n\n crossing events for evaluation.\n\nFig.\u00a01 shows examples of several boundaries, defined by the change in electron populations. The magnetosheath is characterized by high energy electrons, which are not present in the magnetosphere due to the shielding created by the magnetic field. In general, the bow shock boundaries are more prominent in CAPS ELS data, while the magnetopause boundaries are more diffuse.\nThe number of events of each type observed during Cassini\u2019s mission, broken down by year, is shown in Table\u00a01.\n\nThe number of magnetopause crossing events is nearly three times that of bow shock events, due to the limitations of Cassini\u2019s orbital trajectories. We note that there are often multiple crossings in a single flyby. This is because the location of the boundaries are not fixed. For example, the magnetopause location is determined by a delicate pressure balance between the upstream solar wind dynamic pressure and the planetary magnetic and plasma pressure. These values (and thus the boundary position) are in constant flux, and thus a spacecraft orbiting in the vicinity of the nominal boundary location may cross the boundary several times on a given flyby.\n\n\n4\nEvent detection methods\nWe evaluated four approaches to detecting magnetospheric boundary crossings in CAPS ELS data. These crossings can be viewed either as \u201cchange points\u201d, as state changes, or as anomalies with respect to the rest of the data set.\n\n4.1\nChange point detection using RuLSIF\nThe time at which the spacecraft transitions between the magnetospheric regions can be characterized as a \u201cchange point\u201d. The observations collected by the CAPS ELS instrument exhibit a significant change that is reflected in the distribution of electron energies before and after such crossings.\nTo detect a change point, we compare a set of \nw\n samples (\n\n\ns\n\n\n1\n\n\n) collected before the current time point \ni\n with a set of the same number of samples (\n\n\ns\n\n\n2\n\n\n) collected after \ni\n. Since we must wait to accumulate \nw\n samples in \n\n\ns\n\n\n2\n\n\n, the value \nw\n also constitutes the minimum delay before a crossing can be detected. We quantify the difference in the distribution of observations within each set to produce a change-point score for each time step. If the score is sufficiently high, a change (boundary crossing) is detected. Following\u00a0Liu et al. (2013), we incorporate temporal context within each sample by analyzing subsequences of length \nk\n rather than a single time step at a time. We refer to this procedure as \u201cpacking\u201d the samples. This increases the dimensionality of each sample by a factor of \nk\n.\n\nThe RuLSIF\u00a0(Liu et al., 2013) (Relative Unconstrained Least Squares Information Fitting) algorithm is designed to detect abrupt changes in time-series data. RuLSIF computes the difference (divergence) between two distributions, \np\n and \nq\n, that describe the packed samples in \n\n\ns\n\n\n1\n\n\n and \n\n\ns\n\n\n2\n\n\n. The divergence employed is based on the Pearson (PE) divergence\u00a0(Pearson, 1900) which is defined as \n\n(1)\n\n\nP\nE\n\n(\np\n\u2225\nq\n)\n\n=\n\u222b\nq\n\n(\nx\n)\n\n\n\n\n\n\n\np\n\n(\nx\n)\n\n\n\nq\n\n(\nx\n)\n\n\n\n\u2212\n1\n\n\n\n\n2\n\n\nd\nx\n.\n\n\n\nPE divergence is 0 when \np\n and \nq\n are the same. Eq.\u00a0(1) requires estimating \np\n and \nq\n directly. A faster approach is to estimate only their ratio at time step \ni\n, \n\n(2)\n\n\nr\n\n(\nx\n)\n\n=\n\n\np\n\n(\nx\n)\n\n\n\nq\n\n(\nx\n)\n\n\n\n,\n\n\n\nusing a least-squares optimization (uLSIF) fit to the observations. To avoid a potential division by zero (when \n\nq\n\n(\nx\n)\n\n=\n0\n\n), RuLSIF employs a modified approach that estimates the \n\u03b1\n-relative probability density ratio, defined as \n\n(3)\n\n\n\n\nr\n\n\n\u03b1\n\n\n\n(\nx\n)\n\n=\n\n\np\n\n(\nx\n)\n\n\n\np\n\n\nq\n\n\n\u03b1\n\n\n\n(\nx\n)\n\n\n\n,\n\n\n\nwhere the \n\u03b1\n-mixture density between \np\n and \nq\n is \n\n(4)\n\n\np\n\n\nq\n\n\n\u03b1\n\n\n\n(\nx\n)\n\n=\n\u03b1\np\n\n(\nx\n)\n\n+\n\n(\n1\n\u2212\n\u03b1\n)\n\nq\n\n(\nx\n)\n\n.\n\n\n\nThis corresponds to the \n\u03b1\n-relative PE-divergence: \n\n(5)\n\n\nP\n\n\nE\n\n\n\u03b1\n\n\n\n(\np\n\n\u2225\n\nq\n)\n\n=\n\u222b\np\n\n\nq\n\n\n\u03b1\n\n\n\n(\nx\n)\n\n\n\n\n\n\n\nr\n\n\n\u03b1\n\n\n\n(\nx\n)\n\n\u2212\n1\n\n\n\n\n2\n\n\nd\nx\n,\n\n\n\nwhich can be approximated given \n\n\n\nr\n\n\n\u03b1\n\n\n\n(\nx\n)\n\n\n, which is modeled as a linear combination of Gaussian kernels with kernel width \n\u03c3\n with \nC\n random centers, sampled from the observations. The weights \n\n\u03b8\n\u2208\n\n\nR\n\n\nC\n\n\n\n in this linear combination are the solution to a quadratic programming problem, regularized by a coefficient \n\n\u03bb\n\u2208\nR\n\n. Finally, the score \n\nR\nS\n\n assigned by RuLSIF at each time step \ni\n is: \n\n(6)\n\n\nR\n\n\nS\n\n\ni\n\n\n=\nP\n\n\nE\n\n\n\u03b1\n\n\n\n\np\n\u2225\nq\n\n\n+\nP\n\n\nE\n\n\n\u03b1\n\n\n\n\nq\n\u2225\np\n\n\n\n\n\n\n\nFollowing the methodology employed by\u00a0Liu et al. (2013), we set the parameter \n\u03b1\n to 0.1 in all of our experiments with RuLSIF. Experiments by\u00a0Liu et al. (2013) and\u00a0Yamada et al. (2013) have shown that RuLSIF is not extremely sensitive to changes in \n\u03b1\n. However, research shows that higher values of \n\u03b1\n yield slightly better performance when the dimensionality of samples is high\u00a0(Yamada et al., 2013). We restrict the Gaussian kernel width \n\u03c3\n and the regularization coefficient \n\u03bb\n to certain ranges defined in\u00a0Liu et al. (2013).\nUsing the \n\u03b1\n-relative PE-divergence provides an improvement in robustness compared to using PE-divergence for change-point detection, at the cost of introducing another parameter. The \n\u03b1\n-relative PE-divergence is always bounded, making it easier to employ the kernel model for \n\n\n\nr\n\n\n\u03b1\n\n\n\n(\nx\n)\n\n\n. This makes RuLSIF more robust than its precursor uLSIF, in which the density ratio can go to infinity if the denominator is 0. Note that the \n\u03b1\n-relative probability density ratio reduces to the ordinary density ratio when \n\u03b1\n is \n0\n.\nVarying the packing parameter \nk\n between \n2\n and \n\n10\n\n did not significantly affect the computed scores. However, the window size \nw\n is important, because it impacts both algorithm runtime and the delay before a crossing is detected. Larger windows provide more context, but they require a longer runtime to analyze.\n\nFig.\u00a02 illustrates how RuLSIF assigned high scores near the anomalous regions mentioned in Section\u00a03.\n\n\n4.2\nState change detection with hidden Markov models\nBoundary crossings can also be detected as a change in the state of the environment surrounding the spacecraft. In this approach, we model the spacecraft observations as samples obtained from one of a discrete set of states. Our goal is to estimate the (unknown) state at each time point, given the observations. When the estimated state changes from time \n\ni\n\u2212\n1\n\n to time \ni\n, a boundary crossing is detected.\nTo generate the state model, we employ a Hidden Markov Model (HMM). An HMM models the observations as coming from one of a set of hidden (unknown) states\u00a0(Baum and Petrie, 1966). Transition probabilities between states are modeled as those in a Markov chain: they depend only on the state before the current state, and not on any previous history of transitions. Further, the probability density of observations only depends on the current state, not on any prior history. These assumptions enable the efficient learning of HMM parameters (given an observation sequence \ny\n) by casting the problem as a likelihood maximization problem. State assignments are optimized using a method such as the iterative Baum\u2013Welch algorithm (Baum and Petrie, 1966). The likelihood \nL\n of an HMM parameterized by \n\u03b8\n, given \nn\n observations \ny\n, is defined as \n\n(7)\n\n\nL\n\n(\n\u03b8\n\u2223\ny\n)\n\n=\n\n\n\u220f\n\n\nt\n=\n1\n\n\nn\n\n\np\n\n(\n\n\ny\n\n\nt\n\n\n\u2223\n\u03b8\n)\n\n,\n\n\n\nwhere \np\n is the observation probability density (using e.g.,\u00a0a Gaussian or mixture of Gaussians) and \n\u03b8\n is the set of the HMM transition probabilities between states and the emission (observation) probabilities for each state.\nEach state \n\n\ns\n\n\nj\n\n\n is modeled by a \nd\n-dimensional Gaussian distribution with mean \n\n\n\u03bc\n\n\n\n\ns\n\n\nj\n\n\n\n\n and covariance matrix \n\n\n\u03a3\n\n\n\n\ns\n\n\nj\n\n\n\n\n, learned from the data. We compute a dissimilarity matrix \n\nD\nS\n\u2208\n\n\nR\n\n\ns\n\u00d7\ns\n\n\n\n, where \ns\n is the number of states and each entry is defined as: \n\n(8)\n\n\nD\n\n\nS\n\n\n\n\ns\n\n\n1\n\n\n,\n\n\ns\n\n\n2\n\n\n\n\n=\nK\nL\n\n\nN\n\n\n\n\n\u03bc\n\n\n\n\ns\n\n\n1\n\n\n\n\n,\n\n\n\u03a3\n\n\n\n\ns\n\n\n1\n\n\n\n\n\n\n\u2225\nN\n\n\n\n\n\u03bc\n\n\n\n\ns\n\n\n2\n\n\n\n\n,\n\n\n\u03a3\n\n\n\n\ns\n\n\n2\n\n\n\n\n\n\n\n\n+\n\n=\n\nK\nL\n\n\nN\n\n\n\n\n\u03bc\n\n\n\n\ns\n\n\n2\n\n\n\n\n,\n\n\n\u03a3\n\n\n\n\ns\n\n\n2\n\n\n\n\n\n\n\u2225\nN\n\n\n\n\n\u03bc\n\n\n\n\ns\n\n\n1\n\n\n\n\n,\n\n\n\u03a3\n\n\n\n\ns\n\n\n1\n\n\n\n\n\n\n\n\n\n\n\n using the symmetrized \n\nK\nL\n\n-divergence between the learned distributions. The HMM anomaly detection score at time step \ni\n is: \n\n(9)\n\n\nH\nM\n\n\nM\n\n\nt\n\n\n=\n\n\n\u03b4\n\n\ni\n\n\nT\n\n\nD\nS\n\n\n\n\u03b4\n\n\ni\n\n\n\n\n\nwhere \n\n\n\n\u03b4\n\n\ni\n\n\n=\n\n|\n\n\np\n\n\ni\n\n\n\u2212\n\n\np\n\n\ni\n\u2212\n1\n\n\n|\n\n\n, the absolute value of the difference in posterior state distributions \n\n\n\np\n\n\ni\n\n\n\u2208\n\n\nR\n\n\ns\n\n\n\n. The use of the dissimilarity matrix \n\nD\nS\n\n assigns more weight to transitions between dissimilar states.\n\nNote that the standard HMM framework requires the number of states to be specified before learning. Fig.\u00a03(a) shows that the \u201cvanilla\u201d HMM learns a reasonable segmentation when the number of states is close to the number of different types of observation sequences. For this observation, two states suffice to describe the distribution of counts well. However, if we specify five states, the HMM learns redundant states and produces a poor segmentation, because several of the learned states are highly similar in terms of emission parameters, as shown in Fig.\u00a03(b). Note that the detection scores shown in the bottom panel of these figures are influenced by the dissimilarity between states (Eq.\u00a0(8)), so scores are higher for transitions between dissimilar states and lower for transitions between similar (possibly redundant) states. Setting the number of states correctly requires either prior knowledge about the underlying generative process or the use of heuristics (such as the Akaike Information Criterion\u00a0(Akaike, 1998) or the Bayesian Information Criterion\u00a0(Schwarz, 1978)) to determine the number of states. Such heuristics are not always interpretable, and they are often misleading in the presence of noisy data.\nWe do not know a priori how many HMM states are appropriate for this task, because there can be significant variation in measurements even when the spacecraft does not cross over a magnetic field boundary. Thus, a single magnetic field region may be modeled by multiple hidden states in the HMM. We therefore employed a more powerful approach (Bayesian nonparametrics) that augments the vanilla HMM with the ability to learn the appropriate number of states. This method is the Hierarchical Dirichlet Process (HDP) HMM\u00a0(Teh et al., 2006). We also employ an extension called the \u201csticky HDP-HMM\u201d, which favors self-transitions over transitions to other states, thus reducing the number of extraneous states learned by the HDP-HMM\u00a0(Fox et al., 2008).\n\nLearning with the HDP-HMM requires us to specify an upper bound on the number of states to be learned, rather than the exact number. Fig.\u00a04 shows the sticky HDP-HMM output on the same observation from Fig.\u00a03. The algorithm selected two as the appropriate number of states, given an upper bound of eight states.\nAdditionally, we impose a constraint based on the nature of the physical environment from which the data was collected. We disallow short hidden state subsequences in which the predicted state has a duration of less than \n5\n min, since it is physically implausible that the spacecraft would transition between regions within such a short time period. If a state sequence of less than \n5\n min is predicted, we zero out the probability of the predicted state in the probability distribution over all states and renormalize the distribution so that the sum of probabilities over all states sums to \n1\n.\nAs seen in Fig.\u00a03 and Fig.\u00a04, HMM scores are more crisply localized than those of RuLSIF, due to the scoring mechanism and the discrete number of states in the HMM models.\n\n\n4.3\nAnomaly detection with HOT SAX\nThe events of interest (boundary crossings) are expected to be rare with respect to the full data set. We therefore explored whether anomaly detection methods could successfully identify crossings as anomalous observations.\nThe HOT SAX (Heuristically Ordered Time Series with Symbolic Aggregate Approximation)\u00a0(Keogh et al., 2005) algorithm identifies discords within a time series. Discords are defined as subsequence(s) with the largest nearest neighbor distance, when only non-self matches (no overlap with the subsequence) are considered.\nFormally, let \n\nX\n\u2208\n\n\nR\n\n\nn\n\u00d7\nd\n\n\n\n be a \nd\n-dimensional time series of length \nn\n and \n\n\n\nX\n\n\ni\n,\nw\n\n\n\u2208\n\n\nR\n\n\nw\n\u00d7\nd\n\n\n\n be a z-normalized\n3\n\n\n3\nNormalized to have zero mean and unit variance.\n subsequence of length \nw\n starting at time \ni\n. The HOT SAX score for time step \ni\n is the minimum distance between the current subsequence \n\n\nX\n\n\ni\n,\nw\n\n\n and every other non-overlapping subsequence of the same length. \n\n(10)\n\n\nH\n\n\nS\n\n\ni\n\n\n=\n\n\nmin\n\n\n\n\n\n|\ni\n\u2212\nj\n|\n\n\u2265\nw\n\n\n0\n\u2264\nj\n\u2264\nn\n\u2212\nw\n\n\n\n\n\n\n\n\n\n\nX\n\n\ni\n,\nw\n\n\n\u2212\n\n\nX\n\n\nj\n,\nw\n\n\n\n\n\n\nF\n\n\n,\n\n\n\nwhere \n\n\n\n\n\u22c5\n\n\n\n\nF\n\n\n represents the Frobenius norm.\nA brute-force algorithm to compute the discords in a time series is straightforward, but prohibitively expensive in terms of runtime, with a quadratic time-complexity. HOT SAX improves upon the brute-force algorithm by converting the real-valued time series to a symbolic representation using Symbolic Aggregate Approximation (SAX), which enables the use of heuristic to improve the order in which sequences are searched to identify discords, allowing for early termination when the current candidate subsequence has distance to another subsequence that is less than the largest nearest neighbor distance seen so far and therefore is definitely not a discord. Although HOT SAX will still take quadratic time in the worst case, the empirical speedups are often very large, around \n\n2\n\u2212\n3\n\n orders of magnitude.\nThe HOT SAX algorithm operates on subsequences of a given input size \nw\n. Following\u00a0Keogh et al. (2005), we set the SAX alphabet length to \n3\n. We also set the z-normalization threshold on the variance to be 0.01. Subsequences with variance less than this value are not re-scaled to achieve a variance of 1.0, to avoid highlighting small variations in nearly constant subsequences.\nThe original HOT SAX algorithm was intended for unidimensional time series. To extend it to the \n\n32\n\n-dimensional CAPS ELS data, we implemented the SAX-ZSCORE\u00a0(Mohammad and Nishida, 2014), SAX-REPEAT\u00a0(Mohammad and Nishida, 2014), and SAX-INDEPENDENT (Minnen et al., 2007) methods as well as a novel SAX-ENERGY method.\nSAX-ZSCORE modifies the z-normalization for multi-dimensional sequences and changes the way real values are mapped to symbols (SAX) to take the average along dimensions. SAX-REPEAT performs the standard SAX mapping independently for each dimension, so that each dimension employs a distinct symbol alphabet. Each possible combination of \nd\n symbols at each time step is assigned its own meta-symbol, and the resulting set of meta-symbols is clustered into the desired number of distinct symbols to achieve the final representation. SAX-INDEPENDENT also generates per-dimension SAX mappings, but instead of clustering the symbols, the strings for each dimension are concatenated, yielding \nd\n symbols per time step. SAX-ENERGY applies the standard SAX mapping independently on the vector for each sample (time step). Note that this approach treats the energy bin values as a sequence, but it is a sequence in the energy domain rather than the time domain. The SAX symbols (across the energy bins) are concatenated to create a SAX string for each time step.\nSince all four HOT SAX methods work on subsequences of the original \n\n32\n\n-dimensional time series, the detected discords should be identical. The difference is in their SAX representations of the data, which determines the order in which they are examined and therefore which opportunities arise for early loop termination. Hence, the difference between these methods manifests in their runtime.\nApplying dimensionality reduction methods, such as Principal Component Analysis (PCA) or Independent Component Analysis (ICA), to the data prior to running HOT SAX can help reduce runtime. This is because HOT SAX uses nearest-neighbor distances, and in higher dimensional spaces, under some weak conditions, the nearest neighbor distance approaches the average neighbor distance, diminishing the ability of HOT SAX to discriminate between discords and \u201cnormal\u201d subsequences.\nWe tested the effect of an additional pre-processing step in which we reduced the starting dimensionality from \n\nd\n=\n32\n\n to \n\n5\n,\n10\n,\n15\n\n or \n\n20\n\n using PCA. Across all methods, runtimes were lower when the time series had lower dimensionality (but detection sensitivity might suffer). Our experiments showed that SAX-ZSCORE was the fastest on average across the different choices of dimensionality after PCA.\n\n\nFig.\u00a05 shows HOT SAX detections in the running example of CAPS ELS data. Given the discords and their nearest-neighbor distances as identified by HOT SAX, we assign a common score \n\nH\n\n\nS\n\n\ni\n\n\n\n to all time steps \nj\n covered by each discord starting at time step \ni\n (specifically, \n\ni\n\u2264\nj\n<\ni\n+\nw\n\n). The remaining time steps are assigned a score of \n0\n. Due to this scoring mechanism, HOT SAX scores are much sparser than those of RuLSIF. Compared to RuLSIF, the order of observations within a subsequence of the time series, and not just the values within the observations, is important for HOT SAX. As a result, HOT SAX emphasizes more \u201cstructurally\u201d different subsequences than RuLSIF. Unlike the HMM models, HOT SAX does not model the observations as being generated from an underlying latent state, and hence, has no explicit \u201cmemory\u201d of previous transitions.\n\n\n4.4\nAnomaly detection with the matrix profile\nThe Matrix Profile\u00a0(Yeh et al., 2016) is an extension of HOT SAX. It computes the nearest neighbor distance (NNdist in the equation below) for each subsequence, ignoring trivial matches (subsequences that overlap). This is different from HOT SAX, which computes the nearest neighbor distances only for the discord subsequences.\nFormally, let \n\n\n\nX\n\n\n\n(\nk\n)\n\n\n\n\u2208\n\n\nR\n\n\nn\n\n\n\n be the \nk\nth channel of a multidimensional time-series \n\nX\n\u2208\n\n\nR\n\n\nn\n\u00d7\nd\n\n\n\n of length \nn\n, window size \nw\n, and dimensionality \nd\n. Let \n\n\n\nX\n\n\ni\n,\nw\n\n\n\n(\nk\n)\n\n\n\n\u2208\n\n\nR\n\n\nw\n\n\n\n be a z-normalized sub-sequence of length \nw\n, starting at time \ni\n, within \n\n\nX\n\n\n\n(\nk\n)\n\n\n\n. Then, the Matrix Profile (\n\nM\nP\n\n) is computed for all time steps \n\n0\n\u2264\ni\n\u2264\nn\n\u2212\nw\n\n by comparing the values observed as: \n\n(11)\n\n\nM\n\n\nP\n\n\ni\n\n\n\n\n(\nk\n)\n\n\n\n=\n\n\nmin\n\n\n\n\n\n|\ni\n\u2212\nj\n|\n\n\u2265\nw\n\n\n0\n\u2264\nj\n\u2264\nn\n\u2212\nw\n\n\n\n\ndist\n\n(\n\n\nX\n\n\ni\n,\nw\n\n\n\n\n(\nk\n)\n\n\n\n,\n\n\nX\n\n\nj\n,\nw\n\n\n\n\n(\nk\n)\n\n\n\n)\n\n\n\n\n\n\nThe requirement that \n\n\n|\ni\n\u2212\nj\n|\n\n\u2265\nw\n\n ensures that the nearest-neighbor window does not overlap with the window under consideration. The Euclidean distance metric is used in the formulation above.\nThe Matrix Profile offers some advantages over HOT SAX. First, the Matrix Profile supplies information about all subsequences, instead of just a certain set of subsequences. This could enable identification of other features of interest in the time series, such as motifs, which correspond to repeated features in the time series, providing a unique perspective to visualize a time series. Second, some implementations of the Matrix Profile (SCRIMP++ (Zhu et al., 2018)) are anytime algorithms that can be stopped at any time to give an approximation of the Matrix Profile. For an onboard setting with limited computation, this flexibility is very valuable.\nThe original Matrix Profile algorithm was intended for unidimensional time series. Although extensions to multidimensional time series have been seen in the literature\u00a0(Yeh et al., 2017), these methods use distance metrics that would hide discords that span only a subset of dimensions in the resulting Matrix Profile, because they take a minimum over all dimensions. Instead, we developed our own extension (the \u201cdiscord-tuned\u201d Multidimensional Matrix Profile), consisting of the following steps:\n\n\n\n1.\nCompute the individual Matrix Profiles \n\nM\n\n\nP\n\n\n\n\n(\nk\n)\n\n\n\n\n for each dimension \nk\n. This takes \n\nO\n\n(\n\n\nn\n\n\n2\n\n\n)\n\n\n time for each dimension (using the fastest Matrix Profile algorithms) for a total of \n\nO\n\n(\n\n\nn\n\n\n2\n\n\nd\n)\n\n\n time.\n\n\n2.\nCompute the sum at each time step, over all dimensions. This takes \n\nO\n\n(\nd\n)\n\n\n time for each time step, for a total of \n\nO\n\n(\nn\nd\n)\n\n\n time. \n\n(12)\n\n\nM\nM\n\n\nP\n\n\ni\n\n\n=\n\n\n\u2211\n\n\nk\n=\n1\n\n\nd\n\n\nM\n\n\nP\n\n\ni\n\n\n\n\n(\nk\n)\n\n\n\n.\n\n\n\n\n\n\n\n\nOverall, this is a \n\nO\n\n(\n\n\nn\n\n\n2\n\n\nd\n)\n\n\n time algorithm for the discord-tuned Multidimensional Matrix Profile.\n\nThe key to the Multidimensional Matrix Profile is that discords are highlighted along each dimension they are anomalous over, as the above equation shows. Similar to\u00a0Yeh et al. (2017), certain dimensions can be excluded based on prior domain knowledge. Further, step \n1\n can be modified to compute approximate Matrix Profiles, which can often be order of magnitudes faster, in order to compute the approximate Multidimensional Matrix Profile. Here, however, we use the exact Matrix Profiles.\nWe also implemented noise correction\u00a0(Paepe et al., 2019), which was crucial for improving the sensitivity of the Matrix Profile. Noise correction subtracts out the expected contribution of Gaussian noise in the distance computation along each dimension. Fixing a dimension \nk\n, we replace the pairwise subsequence distance \n\n\ndist\n\n\ni\nj\n\n\n by the noise-corrected distance \n\n\ndist\n\n\ni\nj\n\n\n\u2032\n\n\n where: \n\n\n\n\n\ndist\n\n\ni\nj\n\n\n=\ndist\n\n(\n\n\nX\n\n\ni\n,\nw\n\n\n\n(\nk\n)\n\n\n\n,\n\n\nX\n\n\nj\n,\nw\n\n\n\n(\nk\n)\n\n\n\n)\n\n\n\nstd\n\n\ni\n\n\n=\nstd\n\n(\n\n\nX\n\n\ni\n,\nw\n\n\n\n(\nk\n)\n\n\n\n)\n\n\n\nstd\n\n\nj\n\n\n=\nstd\n\n(\n\n\nX\n\n\nj\n,\nw\n\n\n\n(\nk\n)\n\n\n\n)\n\n\n\ndist\n\n\ni\nj\n\n\n\u2032\n2\n\n\n=\n\n\ndist\n\n\ni\nj\n\n\n2\n\n\n\n\u2212\n\n\n(\n2\nw\n+\n2\n)\n\n\u00d7\n\n\n\n\n\n\n\n\nstd\n\n\nn\no\ni\ns\ne\n\n\n\n\nmax\n\n(\n\n\nstd\n\n\ni\n\n\n,\n\n\nstd\n\n\nj\n\n\n)\n\n\n\n\n\n\n\n2\n\n\n\n\n\n\n\n\nstd\n\n(\n)\n\n\n refers to the standard deviation of the subsequence along this dimension. Noise correction requires another input parameter \n\n\nstd\n\n\nn\no\ni\ns\ne\n\n\n, the standard deviation of noise, referred to as the noise correction factor. Note that the standard deviation of each subsequence can be precomputed, in linear time. This means that noise correction does not change runtimes asymptotically. A comparison between Fig.\u00a06(a) and Fig.\u00a06(b) shows the impact of noise correction; noise-corrected Matrix Profile highlights features of interest, while the standard Matrix Profile does not. Another advantage of the Multidimensional Matrix Profile over HOT SAX is that noise correction is much more efficient, since it already computes the pairwise distances \n\n\nd\n\n\ni\nj\n\n\n along individual dimensions in an optimized manner internally.\nWe assign the score at each time step \ni\n as the Multidimensional Matrix Profile value for the subsequence starting at that time step.\n\n\n4.5\nTransition detection via Euclidean distance\n\nWe also employed a simple baseline method that computes a transition score based on the Euclidean distance between adjacent observations. Given the observation of counts at each time step \ni\n as the \nd\n-dimensional vector \n\n\n\nX\n\n\ni\n\n\n\u2208\n\n\nR\n\n\nd\n\n\n\n, the baseline algorithm computes the score \n\n\nB\n\n\ni\n\n\n at each time step as the Euclidean distance between the current and the previous observations: \n\n\n\n\n\nB\n\n\ni\n\n\n=\n\n\n\u2016\n\n\n\nX\n\n\ni\n\n\n\u2212\n\n\nX\n\n\ni\n\u2212\n1\n\n\n\n\u2016\n\n\n2\n\n\n\n\n\n\nFig.\u00a07 shows the scores of the baseline method on an example observation. We see that the baseline scores, similar to those from RuLSIF, seem to peak near the transitions between visually different observations, but are noisier.\n\n\n\n5\nAlgorithm evaluation\n\n5.1\nTraining and evaluation sets\nTo assess the ability of each algorithm to generalize to future observations, we divided the data into training and evaluation sets according to the time of acquisition.\nOur training set was defined as all of the CAPS ELS observations from 2004, comprising over \n\n180\n\n hours of data with \n\n35\n\n BS and \n\n25\n\n MP crossings. Observations from the remaining years (2005 through 2012) comprise our significantly larger held-out test set with \n\n601\n\n BS and \n\n1678\n\n MP crossings. The precise breakdown of boundary crossing events across these years is provided in Table\u00a01.\nTo handle the data irregularities, we applied the pre-processing steps from Section\u00a02 to all observations. Then, for each event type, we selected the best parameters for each algorithm using the evaluation procedure over the training set, described next in Section\u00a05.2. Finally, we report the performance on the unseen test set for these parameter choices in Section\u00a05.3.\n\n\n5.2\nEvaluation procedure\nWe conducted a retrospective evaluation by measuring the reliability of each algorithm\u2019s detection scores at each time step in the held-out test set. First, we fixed a \u2018time tolerance\u2019 of \n\n\nt\n\n\ntol\n\n\n minutes. Then, given the scores assigned by the algorithm to each time step, we generated a list of detections that exceeded a given score threshold \n\n\ns\n\n\nthres\n\n\n and then retained only the local peak detections (those with scores greater than all other detections within \n\n\u00b1\n\n\n\n\nt\n\n\ntol\n\n\n\n\n2\n\n\n\n s).\nWe defined true positives as detections within a window that spans \n1\n min before and \n\n\nt\n\n\ntol\n\n\n minutes after a labeled event, false positives as detections outside of all such windows, and false negatives as labeled events that lack a detection within the time tolerance window.\n\nThe use of a window in the definition of true positives above handles the imprecision in the labels, as mentioned in Section\u00a02.5. Further, detections not too long after a true crossing event are also operationally useful: the time tolerance \n\n\nt\n\n\ntol\n\n\n quantifies exactly how much to wait for a valid detection after a crossing event. Based on the estimate in Section\u00a01 of the duration of the transition mode on Europa Clipper as around \n\n48\n\n min, we set \n\n\nt\n\n\ntol\n\n\n as \n\n20\n\n min. Fig.\u00a08 illustrates these time tolerance windows around a few labeled events from a CAPS ELS observation.\nWe computed precision and recall at each score threshold using their standard definitions based on TP (number of true positives), FP (number of false positives), and FN (number of false negatives): \n\n(13)\n\n\nPrecision\n=\n\n\nT\nP\n\n\nT\nP\n+\nF\nP\n\n\n\nRecall\n=\n\n\nT\nP\n\n\nT\nP\n+\nF\nN\n\n\n\n\n\n\n\n\nFor each algorithm, we chose the best parameters based on the maximum recall attained at precision \n\n\u2265\n0\n.\n8\n\n on the training set. For this application, in which a spacecraft instrument will change its observing mode based on event detections, precision is very important since false detections will cause unwanted mode changes. The selected parameters are shown in Table\u00a02.\n\n\n5.3\nTransition detection performance\n\n\n\nEvaluating all algorithms with the chosen parameters on the test set gives the results in Fig.\u00a09, which is summarized in Table\u00a03. We found that the bow shock crossings were consistently much easier to reliably detect than the magnetopause crossings. Over all bow shock crossings over all years, the Matrix Profile performed the best, followed by the non-Bayesian HMM. At a precision of \n\n0\n.\n8\n\n, the Matrix Profile achieved a recall of \n\n0\n.\n63\n\n6, and the non-Bayesian HMM had a recall of \n\n0\n.\n583\n\n (see Table\u00a03). However, as shown in Fig.\u00a010, performance varies significantly across different years of the mission, indicating that concept drift is a challenge for generalization using parameters optimized from a single year (2004). For 2012, both the Matrix Profile and the non-Bayesian HMM achieved a recall of \n\n0\n.\n923\n\n, while in 2006, HOT SAX achieved a recall of \n\n0\n.\n750\n\n while the Matrix Profile (\n\n0\n.\n083\n\n) and non-Bayesian HMM (\n\n0\n.\n583\n\n) recall scores were much lower, given a precision threshold of \n\n0\n.\n8\n\n. One of the reasons for this variation is that Cassini employed different orbits of Saturn across the years, leading to different interactions with the magnetic field boundaries.\nFor magnetopause detection, the overall best-performing method was the non-Bayesian HMM. Unfortunately, no methods achieved \n\n0\n.\n8\n\n precision across all years (Fig.\u00a09(b)). The best result observed was for 2006, in which HOT SAX achieved a recall of \n\n0\n.\n146\n\n at precision threshold \n\n0\n.\n8\n\n\n (Fig.\u00a010(b)). As discussed before, magnetopause events are subtler, and characterized by more diffuse boundaries. Additionally, it is common to have a \u2018boundary layer\u2019 (Masters et al., 2011) adjacent to either the magnetopause or the bow shock, representing a region containing mixed plasma populations, which can result in a less than sharp transition signature. The list we employ from\u00a0Jackman et al. (2019) does not specifically identify boundary layer transitions. The result is that magnetopause events can be difficult to identify visually from CAPS ELS data alone. Human experts use complementary data from other instruments (such as the magnetometer) to achieve confident detections.\n\n\n5.4\nRuntimes\n\n\nWe compared all algorithms in terms of processing time on a \n\n2\n.\n1\n\n\u00a0GHz CPU. For each algorithm, we recorded the time it took to process each CAPS ELS file in the test set; each CAPS ELS file contains six hours of observations (e.g.,\u00a0\n\n10\n,\n800\n\n time steps with a \n2\n-second cadence). Table\u00a04 reports the mean and one standard deviation for the time (in seconds) required to process a six-hour observation file. Fig.\u00a011 shows the complete distribution of runtimes on the test set. As expected, the baseline (L2-Diff) ran the fastest, while the Hidden Markov Models and HOT SAX also finished quickly. Even the slowest algorithm (RuLSIF magnetopause crossing detector) on average finished in less than six hours, so all algorithms would generally be able to keep up with the rate of data acquisition, given a \n\n2\n.\n1\n\n\u00a0GHz CPU. The modern RAD750 processor used on many recent missions runs at approximately \n\n200\n\n MHz, a factor of \n\n10\n\n slower. Therefore, for onboard processing, we would choose to constrain the parameter search for RuLSIF to reduce its runtime. As shown in Table\u00a02, RuLSIF employed a large window size (\n\n20\n\n to \n\n60\n\n\u00a0s) and, for the magnetopause detector, a packing factor \nk\n of \n5\n, which increases data dimensionality by a factor \n5\n and therefore impacts runtime significantly.\n\n\n\n6\nFuture directions\n\n6.1\nOnline adaptation of parameters\nTo tackle the inherent variation of Cassini\u2019s interactions with Saturn\u2019s magnetosphere across years, we are investigating the benefit of \u2018online\u2019 adaptation of parameter settings based on the most recent observations to improve generalization, as compared to the \u2018offline\u2019 setting investigated here, where algorithm parameters are fixed throughout evaluation. RuLSIF and the baseline L2-Diff can operate natively as online algorithms, because they only use preceding time steps to compute their respective scores. There exist sliding-window based extensions (e.g.\u00a0Chau et al. (2018)) for all of the other algorithms that would render them suitable for the onboard setting.\n\n\n6.2\nEuropa Clipper\nNASA\u2019s Europa Clipper is an upcoming mission to Jupiter inspired heavily by Cassini that could potentially benefit from the data analysis techniques evaluated in this work. As of this writing, Clipper has a planned launch date of October 2024. The mission will assess the habitability of Jupiter\u2019s icy moon Europa and provide reconnaissance information for a potential future lander mission\u00a0(Howell and Pappalardo, 2020). The Clipper spacecraft will be equipped with a suite of imagers (wide and narrow angle visible, near-IR, and UV), a mass spectrometer, a dust analyzer, an ice-penetrating radar, a magnetometer, and a plasma instrument.\n\nThe Plasma Instrument for Magnetic Sounding (PIMS) is a Faraday cup-based electrometer that will measure the density and charge of ions encountered in the plasma around Europa and throughout the Jovian magnetosphere\u00a0(Westlake et al., 2016; Grey et al., 2018). Because the ion and electron plasma populations vary significantly as a function of distance from Europa, PIMS will operate in four different modes (survey, magnetospheric, transition, and ionospheric) depending on Clipper\u2019s distance to Europa (see Fig.\u00a012). Each mode will operate with a different energy range and fidelity, optimized for the anticipated plasma conditions in each region. Because the precise spatial boundaries of these plasma regions are unknown, the current plan is to switch from one mode to the next based solely on distance from Europa and update the mode transition points for subsequent flybys using observations from the previous flybys. The transition mode is specifically implemented to accommodate uncertainty in the true boundary location by interleaving magnetospheric and ionospheric mode sweeps so that the anticipated plasma populations are not missed. The instrument is expected to experience transition mode for an average of 48\u00a0min per flyby, while it will function in the magnetospheric and ionospheric modes for approximately 220 and 20\u00a0min per flyby, respectively.\nAn alternative approach would be to detect any plasma transitions onboard the spacecraft and autonomously react to them by altering the instrument parameters accordingly. Although PIMS will not have this processing capability within the instrument, there is a possible operational scenario in which the main spacecraft processor could access data generated by PIMS before it is downlinked to Earth. In this setting, it would be possible to provide plasma anomaly triggers to the PIMS instrument that indicate events such as plumes, ionospheric boundary crossings, Jovian plasma sheet crossings, or negative ion populations. Anomaly detections could even lead to the discovery of unknown scientific phenomena, that might otherwise not be detectable by a non-responsive instrument.\nAlthough the design of the CAPS ELS differs significantly from PIMS, the Saturn boundary crossings investigated here serve as analogues for the events that PIMS is expected to respond to, as they arise from similar electromagnetic phenomena.\nIn the Jovian system, indirect evidence of water-vapor plumes on Europa, similar to those at Enceladus, has been discovered\u00a0(Roth et al., 2014; Sparks et al., 2016). One of the goals of the PIMS instrument will be to identify Europa\u2019s contribution to Jovian plasma, further strengthening our understanding of Jupiter\u2019s magnetosphere, much like Cassini has already done with Saturn. The goal of detecting unseen phenomena highlights the need for responsive instruments that can identify domain shifts in these complex, dynamic environments.\nWe would also like to explore the inclusion of known labeled events as partial supervision that could increase precision\u00a0(Yeakel et al., 2018). It is possible that the unsupervised approaches are detecting additional kinds of events that may also be of scientific interest. An interesting direction for future analysis is to see if the false detections made by the bow shock detectors are potentially magnetopause crossing events, and vice versa. A closer look at the detections currently designated as false positives could yield new insights into Saturn\u2019s magnetosphere. Such a study would provide an avenue to see if any events were accidentally mislabeled or missed by domain experts. A deeper comparison of the kinds of errors made by these algorithms in contrast to humans could potentially allow these methods to augment human experts in the labeling process.\n\n\n\n7\nConclusions\nWe evaluated four approaches to unsupervised time series analysis to aid in the identification of boundary crossing events in plasma spectrometer data. This work is motivated by the desire to enable responsive instrument data collection such that when a boundary crossing is detected, the instrument can select the observing mode most suitable to collecting high-quality data in the new region. While time series analysis is a well established area, to our knowledge this is the first application of unsupervised methods to the goal of onboard analysis and response. The evaluation was conducted on data collected by the Cassini CAPS ELS instrument using existing annotations of Saturn bow shock and magnetopause events by domain experts. Overall, we found that the non-Bayesian Hidden Markov Model approach yielded the best combination of detection performance and speed. We also found that variability between years (or mission phases) significantly affects algorithm performance, leading to the recommendation to enable adaptation to changes in data distribution over time. The techniques explored here can potentially be employed to adaptively switch data-collection modes for improving the responsiveness of the PIMS instrument on the upcoming Europa Clipper mission.\n\n\n8\nComputer code availability\n\n\n\n\u2022\n\nName of Code: Detecting Magnetic Field Transitions in CAPS ELS Data.\n\n\n\u2022\n\nDevelopers: Ameya Daigavane, Kiri L. Wagstaff, and Gary Doran.\n\n\n\u2022\n\nContact Details: Machine Learning and Instrument Autonomy Group (MLIA), Jet Propulsion Laboratory, California Institute of Technology.\n\n\n\u2022\n\nEmail: ameya.d.98@gmail.com, kiri.l.wagstaff@jpl.nasa.gov, gary.b.doran.jr@jpl.nasa.gov\n\n\n\n\u2022\n\nYear First Available: 2020.\n\n\n\u2022\n\nHardware Required: None.\n\n\n\u2022\n\nSoftware Required: Python 2.7 distribution (available for Windows, Linux, and macOS).\n\n\n\u2022\n\nProgram Language: Python 2.7.\n\n\n\u2022\n\nProgram Size: 500 KB.\n\n\n\u2022\n\nDetails on how to access the source code: See GitHub repository at https:\/\/github.com\/JPLMLIA\/CAPS-ELS-Transition-Detection.\n\n\n\n\n","90":"","91":"","92":"","93":"\n\n1\nIntroduction\nLaminae or layers are a dominant type of texture in geoscience from the micro to the macro scale. Several types of geoscientific studies require the quantitative analysis of laminated structures: for example stalagmite\u00a0(Baker et al., 2021), tree-ring, coral\u00a0(Isdale et al., 1998), mollusk\u00a0(Arkhipkin et al., 2018), lake sediment\u00a0(Brauer et al., 1999; Gan and Scholz, 2013) and ice core\u00a0(Sigl et al., 2016) paleoclimate reconstructions have all employed annual laminae counting methods to create or support chronologies\u00a0(Butler et al., 2013; Hopley et al., 2018). In many cases, lamina structures can be blurry, irregular, or discontinuous. When this is the case, manual identification and counting are usually the most accurate means of characterizing these structures, but it is also very time consuming and can involve multiple operators.\nThis task can be supported by automatic counting techniques based on signal processing\u00a0(Lotter and Lemcke, 1999; Taylor et al., 2004). These approaches consider an image containing laminations. The expert draw a transect perpendicular to the laminae to extract a time series of the intensity signal. The time series is then processed to identify peaks corresponding to each lamina. This can be based on band-pass filters and minima\/maxima detection\u00a0(Weber et al., 2010; Nagra et al., 2017), fuzzy logic\u00a0(Ebert and Trauth, 2015), and wavelet transform to extract the component of the signal with a prescribed frequency (e.g.\u00a0annual periodicity)\u00a0(Smith et al., 2009). These techniques require repeating the analysis using multiple transects to account for the laminae variability and sometime involve a preliminary assumption on the growth rate. A more sophisticated approach, tested with success on varve sediments\u00a0(Fabija\u0144ska et al., 2020), uses a deep neural network to identify laminae in the target images. This has the advantage of being capable to analyze entire 2D images, but it requires a large training dataset of similar lamination patterns and an accurate setup phase. While these kinds of approaches are promising to assist the counting of laminated structures, there is considerable scope for improvement in developing user-friendly efficient methods. For example: simplification of the preliminary operations to extract the time series, reduction in the need for user-led assumptions in the setup, and techniques to accommodate variable laminae size and density.\nAn alternative approach to analyze different structured signals, as laminated patterns, is Dynamic Time Warping (DTW). Originally introduced in the late 1970\u2019s\u00a0(Sakoe and Chiba, 1978), DTW is a technique used to compare different time series or more generally data vectors, and apply continuous deformations to align them to match common features such as similar sequences of peaks. Based on the amount of deformation applied to match the data series, the level of similarity of the signals is quantified. This approach has been recently used for a variety of applications in geoscience: to estimate travel time in hydrological time series\u00a0(Claure et al., 2018), to identify common features in physical logs\u00a0(Silversides and Melkumyan, 2016), for seismic pattern classification\u00a0(Orozco-Alzate et al., 2015), in correlations of stratigraphic sections\u00a0(Lallier et al., 2013, 2016), and to compare paleoclimate proxy time series\u00a0(Ajayi et al., 2020; Burstyn et al., 2021).\nUsing the potential of the DTW technique, we present here a novel semi-automatic tool called WlCount. The goal is to assist scientists through the detection and counting process with a simple workflow supported by a graphical user interface (GUI). In contrast to existing techniques, WlCount extract information from a whole given image instead of a given transect. To do that, we introduce an initial automatic stage where, using DTW, the raster image is deformed to reach the vertical alignment of the laminae. The pixel rows in the image are then summed along the column axis to obtain a time series of the signal along the laminae growth direction, from which a chronology may be derived. This avoids the need for a user to choose a particular transect for counting, which may be ambiguous, and reduces uncertainty by retaining only the persistent laminated structure from the local noise. In a second stage, the time series is decomposed with the wavelet transform and, with the help of the GUI, the user can isolate the signal component which corresponds to the laminae structure. The extracted signal is then automatically thresholded to identify the laminae, whose location is marked in real time in the image. This way, the user can repeat the process to improve the laminae detection.\nThe proposed approach is primarily tested on a series of pictures obtained by synchrotron radiation x-ray fluorescence mapping (SR-XFM) over annually laminated stalagmites (Golgotha Cave, Yonderup Cave, and Harrie Wood Cave, Australia). The obtained automatic count is compared both visually and statistically with a manual count. The main goal is to assess the ability of the method to reproduce the manual count over a selection of maps in which the laminae vary in size, clarity and amplitude. To test the potential for other applications in the geoscience domain, an additional laminae-detection test is performed on an tree-ring image from a dendrochronological tree stem sample.\nThe paper is structured as follows: Section\u00a02 presents the method and the implementation, in Sections\u00a03 and 4 the images used and the test methods are described, the results are presented in Section\u00a05 and discussed in Section\u00a06, and concluding remarks are given in Section\u00a07.\n\n\n2\nMethods\nWlCount is a novel approach based on image analysis and signal processing with the goal of assisting geoscientists in the identification and counting of laminae represented in raster images. The depicted laminae may have a variable orientation, but preferably sub-vertical (see point 3.). The workflow, implemented in python, is composed of the following steps:\n\n\n\n\nInput\n\n: raster image of size \nn\n rows \n\u00d7\n\n\nm\n columns, which can be multivariate (Fig.\u00a01a).\n\n\n1.\nIf the image is multivariate (e.g.\u00a0RGB or multiband), the mean value across all variables for each pixel is computed to obtain a univariate image (Fig.\u00a01b).\n\n\n2.\nA fixed 5-pixel moving-average filter can be applied as optional preprocessing step to remove small-scale noise and improve the visibility of relevant structures (Fig.\u00a01c).\n\n\n3.\nUsing the dynamic time-warping technique (DTW, Fig.\u00a02), each pixel row is deformed to obtain a vertical alignment of the laminae (Fig.\u00a01d), preserving the structure coherence from row to row. To assure convergence in the DTW alignment, the deformation is by default limited in a radius of 10 pixels. In preliminary tests (not shown here) this limitation assured a correct alignment in all cases showing a continuous laminae deformation not exceeding +-45 degrees from the column axis.\n\n\n4.\nThe aligned image is averaged along the column axis (along lamina) to obtain a unique time series (Fig.\u00a01d, red line) of length \nm\n, showing the amplitude structures related to the seasonal information, observed along the row direction. In the time series, each lamina is represented by a wave of a certain length and magnitude.\n\n\n5.\nThe time series is decomposed using the discrete wavelet transform\u00a0(Daubechies, 1988; Mallat, 1989): the signal is convoluted with a wavelet, i.e.\u00a0a theoretical function defined over a limited support and characterized by a certain scale length. The convolution allows obtaining the signal component of the time series in the same scale length as the wavelet. The wavelet is defined for different scale lengths and convoluted each time over the time series. The result is a 2D image where each row represents the intensity of the signal component for a certain scale length (Fig.\u00a01e).\n\n\n6.\nThe original image, the aligned image with the associated time series, and its wavelet transform are shown in an interactive GUI: at this point the user can draw, with the mouse pointer from left to right, a section over the wavelet image (Fig.\u00a01e, dashed red line) to select the specific signal component corresponding to the annual laminae wavelength. If the section is drawn in the upper part of the wavelet image, dense\/small laminae structures are detected, while large-scale laminations are detected if the section is drawn in the lower part (the scale length is indicated in the \ny\n axis). The line can be composed of multiple contiguous segments, which allows detection of laminae that are varying in thickness across the image. As explained in the following step, the result of this operation in terms of detected laminae can be immediately checked and repeated if necessary.\n\n\n7.\nAt every segment drawn, the corresponding portions of wavelet signal are retrieved (Fig.\u00a01f, red curve). Its mean intensity is computed and used as threshold to identify the positive half waves indicating the laminae presence in the image. If the selected wavelet component portion is close to stationary, this simple threshold method is usually sufficient to detect the laminae. The mid-point of each positive half wave is marked as a lamina with a cross in the aligned image (Fig.\u00a01f, red crosses). The user can undo\/redo each segment to improve the result.\n\n\n8.\nOnce the process reaches a point that the user is satisfied with the laminae identification, the section can be ended by a keyboard command, which displays the counts. Multiple lines can be drawn in the wavelet image to assess the uncertainty over different counts.\n\n\n\nOutput\n\n: .dat file containing the coordinates of each count series, python .npy files for the extracted time series and aligned image (points 3. and 4.), and a .pdf screenshot of the GUI.\n\n\n\n\n\n\n\n2.1\nLaminae alignment by DTW\nIn the WlCount workflow, every pixel row composing the image intersects the subvertical laminae structures, thus constitutes a data vector containing the laminae fluctuations. DTW (dtaidistance implementation\u00a0Meert et al., 2020) is applied iteratively to deform all pixel rows to obtain a vertical alignment of the laminae structure in the image (Section\u00a02 step 3.). This is applied to pairs of rows in a hierarchical fashion (Fig.\u00a02): DTW is applied to the first row of the image to match the second one, then to the third to match the fourth, and so on. This way, all the pairs of adjacent rows are aligned with each other (if the number of rows are odd the last one is discarded). At this point, the numerical values in the two rows of each pair are averaged to obtain a unique row. Applying the averaging to all pairs results in an image composed by half the number of pixel rows. Next, DTW is applied again, aligning the new row pairs and averaging to obtain half the number of rows. The process is repeated until one single pixel row remains, consisting in the time series of step 4. in the WlCount workflow. For display purposes, every original pixel row is again deformed to match this time series and obtain a visible aligned image (Fig.\u00a01d).\n\n\n\n\n\n\n3\nUsed data\nThe imagery used to test the WlCount technique consists of a series of SR-XFM generated maps\u00a0(XFM Australian Synchrotron, Howard et al., 2020) obtained from polished slabs sectioned along the growth axis of cave stalagmites, sampled from underground caves in New South Wales (NSW) and South West Western Australia (SWWA). For all images (Fig.\u00a03), the Strontium (Sr) relative element concentration map is used for the automatic counting since in this case it shows the laminated structure most clearly. The 8-bit RGB intensity levels are based on the instrumental levels of light detection, with zones of high element concentration displayed with light shades. The maps length varies from 4 to 26\u00a0mm with a pixel resolution of 2\u201310 \n\u03bc\nm (Table\u00a01). The represented laminated textures are of different types, including thin well-defined laminae (1562), thick irregular (23265), barely visible (1563) or disturbed by porosity (23261 and 23262).\n\n\nThe tree-ring sample used in the last experiment is the test image used in the demonstration of the MtreeRing software\u00a0(Shi et al., 2019). The images used in this analysis are increment core samples from coniferous species (Larix gmelinii) in the northern Greater Khingan Mountains (Heilongjiang Province, NE China).\n\n\n4\nValidation\nThe proposed technique is tested by operating the WlCount software on all test images and performing 3 counts for each image. The laminae are counted and their position marked in the aligned section with a series of cross pointers. The position (x coordinate) and number of the detected laminae is then compared to a manual count (MC), used as reference. Being the MC repeated by different operators, an uncertainty value (+- integer) is related to the number of laminae which have not been constantly detected over repeated counts, being weakly prominent or discontinuous. Following\u00a0Faraji et al. (2021), three codes for uncertain counts are considered: code 0 if a lamina presents a relative (0\u20131) detection frequency in the range \n\n[\n0\n.\n96\n\u2212\n1\n]\n\n, code 1 for the range \n\n[\n0\n.\n51\n\u2212\n0\n.\n95\n]\n\n and code 2 for the range \n\n(\n0\n.\n06\n\u2212\n0\n.\n50\n]\n\n. Code 0 is considered a detection with zero error, while for the other codes the corresponding error on the count of each uncertain lamina is determined as \n\n1\n\u2212\n\n(\np\n1\n+\np\n2\n)\n\n\/\n2\n\n, with \n\n[\np\n1\n,\np\n2\n]\n\n as the frequency range. Therefore, for every code-1 and code-2 lamina, the error contribution is respectively 0.27 and 0.72. The sum of these unitary contributions constitutes the MC error.\nTo make a quantitative comparison between the automatic and manual counts, two statistical indicators have been considered: (i) the number of laminae counted with WlCount is compared to the MC and its uncertainty range and (ii) the average distance (\n\nD\na\n\n) is computed between the manually counted laminae to the closest laminae detected with WlCount. \n\nD\na\n\n allows assessing whether the spatial laminae distribution obtained from WlCount is similar to the one from the MC. If this distance is lower than the mean laminae thickness (\n\nT\na\n\n), this suggests the spatial distribution of the detected laminae follows the actual laminae distribution. \n\nT\na\n\n is estimated as \n\n\n(\n\n\nx\n\n\ns\nt\na\nr\nt\n\n\n\u2212\n\n\nx\n\n\ne\nn\nd\n\n\n)\n\n\/\n\n(\nMC\n\u2212\n1\n)\n\n\n, where \n\n\nx\n\n\ns\nt\na\nr\nt\n\n\n and \n\n\nx\n\n\ne\nn\nd\n\n\n are the first and last lamina \nx\n coordinates detected by MC.\n\n\n5\nResults\nAs visual examples of the WlCount software GUI, the output display for the counts on sections 1562 and 23265 are shown in Fig.\u00a04 (all output displays attached as supplemental material). The output display shows the original image (mean of all variables if multivariate) at the top, the aligned image in the center together with the time series describing the laminae variations along the \nx\n axis, and the wavelet transform of the same time series at the bottom. Section 1562 (Fig.\u00a04 a) presents a thin laminae structure in varying density and signal intensity. Avoiding the MA preprocessing (Section\u00a02 step 2.), allows preserving the fine-scale details for the alignment (Fig.\u00a04 a, center image). The alignment process improves the visibility and continuity of thin and barely visible vertical structures, whose signal is contained in the final time series (red line). The wavelet transform of this time series (Fig.\u00a04 a, bottom image) shows a marked periodicity in the wavelet scale range of 3\u20136 pixels (y axis), which corresponds to the laminae structure. By drawing segmented lines in the wavelet transform image, the user isolates this signal component. It is then used by the software to detect the laminae presence in the upper image and return the amount (235\u2013293 bands).\nThe second example (Fig.\u00a04 b) shows the output display for section 23262. In this case, the laminae structure appears thicker because the section is approximately half the length with respect to section 1562 (the image is stretched to occupy the whole display). The user choose to apply the MA preprocessing which removed the small-scale noise, resulting in a smoother image and time series. In the wavelet transform image (Fig.\u00a04 bottom), the choice of the wavelet component with scale length of 2 pixels allowed detecting 99 laminae, marked in the aligned image. The image alignment improved again the continuity of the laminae structures. Note that the three lines in the wavelet transform are superposed but they are formed by different segments. This allows breaking up the wavelet signal in different parts, for which their threshold corresponds to their mean value. The final laminae count is equivalent but small differences appear in the laminae locations (crosses of different colors in Fig.\u00a04 b, center image).\n\n\nFig.\u00a05 shows all the test sections with reference manual laminae count (MC) and the WlCount estimations (three attempts per section). All laminae detected in the MC (blue crosses) are linked to the closest WlCount detection by a dashed white segment. For all sections, the WlCount laminae distribution is overall similar to the MC, but some mismatch in the detection can occur, as visible in Fig.\u00a05 b and e: Wlcount may count twice an MC lamina or vice versa.\n\nThis result is confirmed by the statistical scores in Table\u00a02. Sections 1562 and 1563 present respective MC values of 246 and 131 with a relatively small uncertainty (\n+\n-12 and \n+\n-19), while the WlCount estimations are in part within and in part outside this uncertainty range (Table\u00a02 bold values). For sections 23261 and 23262 the laminae are dense, less continuous, and more disturbed by porosity (Fig.\u00a05c and d). For these images, the MC uncertainty is higher (\n+\n-43 and \n+\n-85) and all WlCount estimations lie within the same range. Section 23265 presents better defined laminae for which the MC and Wlcount match well with very low uncertainty (98+-5 and 99 respectively).\n\nFor all counts, the average distance between every MC detection and the closest WlCount detection (\n\nD\na\n\n in Table\u00a02) is sensibly lower than the average laminae thickness (Table\u00a02\n\n\nT\na\n\n). This suggests that the laminae locations detected by WlCount match on average the MC locations.\nThe last test image shows a ring lamination from a cross section of tree-ring sample. With respect to the previous stalagmite images, the laminated pattern here is more regular and the border clearer, which makes the detection and counting task easier. As confirmed by the visual output (Fig.\u00a06), WlCount is able to detect efficiently all the rings and provide the exact count. Note that here, the laminae thickness is larger and the associated wavelet frequency is represented in the central part of the wavelet transform image, which has been captured with a unique threshold line. The presence of discontinuities in the ring border (thin stripes in blue shaded zones) does not affect the ring detection.\n\n\n\n\n\n6\nDiscussion\nIn contrast to previous techniques for geoscientific laminae counting (Section\u00a01), which are mainly based on time series analysis from linear transects, the workflow introduced here is, to our knowledge, the first laminae-counting tool which extracts information from a whole given image with no training phase. The pairwise hierarchical application of the DTW alignment (Section\u00a02 step 3.) allows estimating the laminated structure based on the average thickness and location of signals persistent along the column direction. Both counts displayed in Fig.\u00a04 show how the hierarchical DTW not only aligns the subvertical or curved structures, but allows increasing their continuity over local disturbances, which shrink and tend to disappear in the aligned images. The results show that this approach is adaptive to textures of different thickness, including discontinuous, disturbed, or barely visible laminae, which can constitute a primary limitation for pattern-recognition techniques\u00a0(see e.g. Fabija\u0144ska et al., 2020).\nWlCount is designed to be supervised with a graphical interface, relying on the scientist knowledge on the studied morphology. This requires the user to become familiar with the wavelet analysis and the choice of the appropriate component from the wavelet image. This stage is needed since the choice of the frequency representative of the lamination cannot become automatic for different reasons: the laminae signal can vary sensibly in wavelength and intensity, it is not necessarily the most intense signal present, it may be strongly non-stationary, or it may not cover the whole section. To deal with this complexity, automatic algorithms usually require a numerical setup (see Section\u00a01) and a supervised trial and error approach. Conversely, with the proposed technique, the process is more intuitive since the laminae periodicity displayed matches their spatial distribution and it is selected manually with a trace. The resulting laminae detection appears on the display, allowing quick corrections.\nThe optional MA preprocessing, e.g.\u00a0used for section 23265 (Fig.\u00a04b), can improve the visibility of the structures by removing small-scale noise, which can generate vertical artifacts in the alignment image. However, this noise is usually filtered out in the wavelet analysis by choosing the appropriate wavelet component. The MA preprocessing is not suggested if thin (\n<\n5 pixels) laminae are present, e.g.\u00a0section 1562 (Fig.\u00a04a).\nThe results suggest that the WlCount estimation is comparable to the manual count for both the number of detected laminae and their position (Table\u00a02). The uncertainty of the WlCount estimation can be given by tracing multiple trajectories in the wavelet space, to explore the zone representing the laminae fluctuation. In case of high uncertainty, the WlCount estimation can constitute a baseline assessment which can be further examined and manually edited using the output files.\nTested on high-resolution images of \n\n1\n\n\n0\n\n\n6\n\n\n\n pixels (Table\u00a01), the technique took up to 5\u00a0min to generate one aligned image on a personal computer. This phase can be skipped when the software is run again on the same image, by using the previous alignment files which are automatically stored and recalled from the local folder. The alignment algorithm (Section\u00a02 step 3.) is mainly non-sequential, so it can be parallelized for cluster computing if needed. Examining the images and choosing the wavelet component takes usually some minutes, while manual count can take up to hours for one single count.\nWe now compare WlCount to other lamina counting approaches and consider its applicability in geoscience. Previously proposed approaches based on Markov chains\u00a0(Winstrup et al., 2012) allow a more rigorous quantification of the uncertainty in the layer identifications. Nevertheless, they require statistical assumptions on the layer thickness and a set of layer templates to calibrate the algorithm. This probabilistic approach, tested on chemical time series from ice cores, is convenient for long and stationary scan-line data exhibiting relatively regular lamination structures, for which an accurate inspection is impractical. Conversely, WlCount requires to visualize entirely the data under form of 2D images, but it is adaptive to more diverse and non-stationary lamination structures, with no required calibration phase and a relatively fast operation.\nThe test on the tree-ring image (Fig.\u00a06) shows that the proposed technique can easily detect tree-ring laminations. With respect to the MtreeRing algorithm, tested on the same image\u00a0(Shi et al., 2019) with comparable results to the commercial software WinDENDRO\u2122, WlCount does not require tracing a scan line perpendicular to the ring borders. This is not necessary since the average laminae location and thickness is represented in the aligned image (Fig.\u00a01d). Moreover, the WlCount approach is less affected by discontinuity in the ring patterns, since the time series extracted from the image is not based on segmentation as in MtreeRing.\nThe proposed approach can also have a potential in the field of sclerochronology\u00a0(Hudson et al., 1976), which analyzes laminar growth patterns in skeletal tissue samples of mollusks and fishes\u00a0(Sch\u00f6ne, 2013). These laminae, often curved and poorly defined, can benefit from the hierarchical DTW technique, which enhances the visibility of the persistent features in the vertical laminae alignment. This can also reduce potential bias due to the discard of samples presenting discontinuous or unclear lamination, which can be relevant since possibly correlated to environmental conditions\u00a0(Peharda et al., 2021).\nIn the present all-purpose implementation, WlCount outputs the coordinates of the lamina centers, which allows obtaining the local laminae thickness by calculating the difference between adjacent center coordinates. This will not define the exact position of each lamina border, but it will allow analyzing the laminae thickness variation along the section. Depending on the process which generated the lamination, the border of a lamina can be defined as the mid point between two local maxima in the signal or a break point in the signal slope. In future research, the WlCount implementation can be tailored for specific applications by including this information in the output data.\n\n\n7\nConclusions\nThe analysis of laminated structures in geoscientific imagery is a common task to acquire information on geochronology and periodic natural processes. In this paper, we introduced WlCount as a novel semi-automatic laminae counting method, with the goal of speeding up and supporting laminae detection counting operations. With respect to the available assisted or automatic count approaches, the novelty of WlCount resides in the extraction of the average features of the laminated structure from a whole given image, without the need of tracing and comparing multiple line scans and minimizing the effect of disturbances and discontinuities.\nBy applying Dynamic Time Warping to pairs of pixel rows in the image, the technique aligns vertical structures and extracts a time series of the longitudinal variations. This time series is then decomposed using the wavelet transform, which generates a 2D image describing the longitudinal variations at different scales. From the GUI, the user selects the wavelet component representing the longitudinal variations corresponding to the laminated structures. This component is then automatically thresholded to detect the laminae position in the section image.\nTested on a series of different images from cave stalagmites, WlCount returned a laminae number and position similar to the reference manual count for both the laminae number and their location. The uncertainty of the count can be assessed by performing different wavelet component selections in the scale range representative of the laminae fluctuations. This allows comparing the number and positions of the detected laminae for different counts. The output detection data can be further examined and manually edited if needed. The last test on a tree-ring image suggests the potential of WlCount in other geoscientific fields as the one of dendrochronology, but further testing is needed and possibly an adaptation of the produced output data.\nOverall, WlCount presents as a flexible tool to make a quick assessment on the number and spatial distribution of the laminae on the studied images, with an accuracy and reliability comparable to the manual count, but a minimal user intervention. Future development will include further testing and adaptation of the implementation to specific applications.\n\n\nCode availability section\nName of the code\/library: WlCount\nContact: Fabio Oriani, University of Lausanne, fabio.oriani@protonmail.com\n\nSuggested Hardware requirements: processor frequency 3.60+ GHz, 8+ GB memory\nProgram language: Python 3\nSoftware required: Phyton 3 with additional open-source packages (see repository)\nProgram size: 1.2\u00a0GB (whole repository)\nThe source codes are available for downloading at the link:\n\nhttps:\/\/bitbucket.org\/orianif\/wlcount\/src\/master\/\n\n\n\nCRediT authorship contribution statement\n\nFabio Oriani: Development of the main ideas, Implementation, Experiment conception, Data analysis and interpretation, Manuscript writing and revision. Pauline C. Treble: Development of the main ideas, Experiment conception, Data acquisition, Analysis and interpretation, Manuscript revision. Andy Baker: Development of the main ideas, Experiment conception, Data interpretation, Manuscript revision. Gregoire Mariethoz: Development of the main ideas, Experiment conception, Data interpretation, Manuscript revision.\n\n","94":"","95":"","96":"\n\n1\nIntroduction\nGlobally-distributed Earth observation data is today available across many disciplines as low-Earth-orbit satellite missions, in combination with worldwide ground-based observing networks, provide a continuous stream of survey data. Such observations provide information and constraints on problems ranging from the impact of human activities on the Earth\u2019s surface and atmosphere\u00a0(e.g. Jeong et al., 2017; Jun and Stein, 2008) to inferring the structure and dynamics of the Earth\u2019s interior\u00a0(e.g. Meschede and Romanowicz, 2015; Save et al., 2016; Gillet et al., 2013). Common to such global problems is the need for analysis and interpretation on approximately spherical surfaces. Many analysis problems of this type can be formulated in terms of a linear inverse problem which connects observations, \nd\n, to model parameters, \nm\n, through a linear forward operator \nG\n, i.e.\u00a0\n\nd\n=\nG\nm\n\n. We refer to solving such problems as linear inversion. Solution methods are traditionally based on least square methods\u00a0(e.g. Menke, 2018) and have in recent years been developed to include probabilistic solutions based on Bayesian methods\u00a0(e.g. Tarantola and Valette, 1982; Tarantola, 2005). In the Bayesian formulation one seeks to estimate the posterior probability density function (pdf) of the model parameters, proportional to the product of an a-priori pdf and a likelihood function. Probabilistic solutions to inverse problems with non-Gaussian prior information are often obtained using sampling methods such as the Metropolis algorithm, but this becomes very expensive when working with high dimensional model spaces. Here we present an alternative approach to generating realizations of the posterior pdf for linear inverse problems in spherical geometry, based on observations related to the model by a linear averaging kernel, that can account for non-Gaussian prior distributions for the model parameters. We also include the capability of using point data concerning the model parameters and refer to these as direct observations.\nIn geostatistics, solutions based on point data (where the model parameters \n\n\nm\n\n\ni\n\n\n are known at some locations) are often obtained using kriging, i.e.,\u00a0interpolation through Gaussian process modelling conditioned on prior covariances, which provides the best linear unbiased predictions based on observed point data\u00a0(Journel and Huijbregts, 1978; Deutsch and Journel, 1998). Such kriging schemes can be extended to systems where there is a combination of point data and observations related to the model parameters by a linear averaging kernel, and to cases where only the latter are available\u00a0(Hansen et al., 2006). When the observation noise and a-priori pdfs are Gaussian, the posterior solution is provided by simple kriging through the mean and variance of a Gaussian estimate for each model parameter. Extensions of this framework, whereby the form of the posterior pdf can be obtained beyond means and covariances, is possible by sequentially simulating model parameters through sampling of local distributions conditional on prior information\u00a0(Soares, 2001). Direct sequential simulation was applied to combinations of point and weighted linear average observations in Cartesian geometry in the VISIM algorithm of\u00a0Hansen and Mosegaard (2008).\nIn spherical geometry, to the best of our knowledge, no implementation of direct sequential simulation algorithms yet exists for obtaining probabilistic solutions of linear inverse problems, although the underlying methods are well understood. Recent work by\u00a0Alegr\u00eda et al. (2020) introduced a method for simulating Gaussian random fields on the d-dimensional unit sphere which is computationally very efficient but does not possess the non-Gaussian capabilities of direct sequential simulation.\u00a0Gneiting (2013) analyses valid positive definite correlation functions on spheres which may be used to generate the necessary spherical covariance models. Spherical harmonics are a well known method to represent continuous fields on the sphere\u00a0(Wieczorek and Meschede, 2018); they also provide a means to specify isotropic prior covariance functions on a sphere which have an exact correspondence to the spherical harmonic power spectra\u00a0(e.g. Moritz, 1980; Jackson, 1994; Hipkin, 2001). Building on the work of\u00a0Hansen and Mosegaard (2008) in Cartesian geometry, and making use of covariance models linked to spherical harmonic spectra, we implement direct sequential simulation in spherical geometry with the aim of providing a new tool for Earth and Space science problems. While we focus here on working with non-Gaussian posterior pdfs, sequential Gaussian simulation using point data is also possible with the tool presented. We illustrate our method by obtaining a probabilistic solution for the geomagnetic field at the Earth\u2019s core\u2013mantle boundary, taking prior information from geodynamo simulations and observations from real satellites. Our Spherical Direct Sequential Simulation (SDSSIM) algorithm enables probabilistic solutions to this problem without assuming a priori that the model parameters are Gaussian distributed.\nIn Section\u00a02 we describe the linear forward problem, \n\nd\n=\nG\nm\n\n, focusing on its discretization in spherical geometry. We next review the basic principles of Gaussian process based least-squares solutions to the inverse problem, sequential Gaussian simulation methods, and the theory of the direct sequential simulation method\u00a0(Soares, 2001; Oz et al., 2003). Section\u00a03 gives a detailed description of the implementation of our SDSSIM algorithm. In Section\u00a04 we present the results of tests on both synthetic and real data, based on the geophysical problem of inferring the Earth\u2019s magnetic field at the core\u2013mantle boundary from remote, noisy, satellite observations. We also demonstrate classic direct sequential simulation by using synthetic direct observations from a known simulation of the core\u2013mantle boundary radial field. Finally we discuss the strengths and limitations of our method, along with our conclusions and some perspectives for future steps in 5.\n\n\n2\nTheory\n\n2.1\nThe linear forward problem in spherical geometry\nIn spherical geometry, given observations, \n\nd\n\n(\nr\n)\n\n\n, and a model on a spherical surface, \n\nm\n\n(\ns\n)\n\n\n, related through a forward kernel operator \n\nG\n\n(\nr\n,\ns\n)\n\n\n we consider a linear forward problem of the form \n\n(1)\n\n\nd\n\n(\nr\n)\n\n=\n\n\n\u222b\n\n\nS\n\n\nG\n\n(\nr\n,\ns\n)\n\nm\n\n(\ns\n)\n\nd\nS\n\n\n\nwhere \n\nd\nS\n=\nsin\n\n\n\u03b8\n\n\n\u2032\n\n\nd\n\n\n\u03b8\n\n\n\u2032\n\n\nd\n\n\n\u03d5\n\n\n\u2032\n\n\n\n, with \n\nr\n=\n\n(\nr\n,\n\u03b8\n,\n\u03d5\n)\n\n\n indicating locations of the observations, and \n\ns\n=\n\n(\n\n\nr\n\n\n\u2032\n\n\n,\n\n\n\u03b8\n\n\n\u2032\n\n\n,\n\n\n\u03d5\n\n\n\u2032\n\n\n)\n\n\n indicating locations on a spherical surface of radius \n\n\nr\n\n\n\u2032\n\n\n. This system describes observations that are related to the model by a linear averaging kernel. The integral equation in (1) may be approximated numerically via quadrature rules; here we use a Gauss\u2013Legendre quadrature scheme appropriate for spherical geometry\u00a0(e.g. Atkinson, 1982; Wieczorek and Meschede, 2018), in which the integration is carried out on a \n\n\n(\n2\n\n\nN\n\n\nq\n\n\n\u2212\n1\n)\n\n\u00d7\n\n\nN\n\n\nq\n\n\n\n grid, where \n\n\nN\n\n\nq\n\n\n is the number of latitudinal nodes. \n\ncos\n\n\n\u03b8\n\n\n\u2032\n\n\n\n are then the Gauss\u2013Legendre nodes on the interval \n\n[\n\u2212\n1\n,\n1\n]\n\n, with corresponding integration weights, \n\n\nw\n\n\ns\n\n\n. \n\n\n\u03d5\n\n\n\u2032\n\n\n is chosen such that the points are equally spaced with separation \n\n\u03c0\n\/\n\n(\n\n\nN\n\n\nq\n\n\n\u2212\n1\n\/\n2\n)\n\n\n on the interval \n\n[\n0\n,\n2\n\u03c0\n\n[\n\n\n. For model parameters on a sphere distributed according to Gauss\u2013Legendre quadrature rules, this numerical integration is exact for polynomials of degrees less than \n\n2\n\n\nN\n\n\nq\n\n\n\n\u00a0(Atkinson, 1982). The integral (1) may then be discretized according to\u00a0Eq.\u00a0(2). \n\n(2)\n\n\nd\n\n(\nr\n)\n\n=\n\n\n\u03c0\n\n\n\n\nN\n\n\nq\n\n\n\u2212\n1\n\/\n2\n\n\n\n\n\u2211\n\n\ni\n=\n1\n\n\n\n\nN\n\n\nm\n\n\n\n\n\n\nw\n\n\ni\n\n\nG\n\n(\nr\n,\n\n\ns\n\n\ni\n\n\n)\n\nm\n\n(\n\n\ns\n\n\ni\n\n\n)\n\n\n\n\nwhere \n\n\n\nN\n\n\nm\n\n\n=\n\n(\n2\n\n\nN\n\n\nq\n\n\n\u2212\n1\n)\n\n\u00d7\n\n\nN\n\n\nq\n\n\n\n is the number of model parameters. For a series of \n\n\nN\n\n\nd\n\n\n observations, \n\nd\n=\n\n\n\n[\n\n\n\nd\n\n\n1\n\n\n\n(\n\n\nr\n\n\n1\n\n\n)\n\n,\n\u2026\n,\n\n\nd\n\n\ni\n\n\n\n(\n\n\nr\n\n\ni\n\n\n)\n\n,\n\u2026\n,\n\n\nd\n\n\n\n\nN\n\n\nd\n\n\n\n\n\n(\n\n\nr\n\n\n\n\nN\n\n\nd\n\n\n\n\n)\n\n\n]\n\n\n\nT\n\n\n\n, with a vector of model parameters, \n\nm\n=\n\n\n\n[\n\n\n\nm\n\n\n1\n\n\n\n(\n\n\ns\n\n\n1\n\n\n)\n\n,\n\u2026\n,\n\n\nm\n\n\ni\n\n\n\n(\n\n\ns\n\n\ni\n\n\n)\n\n,\n\u2026\n,\n\n\nm\n\n\n\n\nN\n\n\nm\n\n\n\n\n\n(\n\n\ns\n\n\n\n\nN\n\n\nm\n\n\n\n\n)\n\n\n]\n\n\n\nT\n\n\n\n; we absorb the constant, \n\n\n\u03c0\n\n\n\n\nN\n\n\nq\n\n\n\u2212\n1\n\/\n2\n\n\n, and integration weights \n\n\nw\n\n\ni\n\n\n into the elements of a matrix \nG\n (size \n\n\n\nN\n\n\nd\n\n\n\u00d7\n\n\nN\n\n\nm\n\n\n\n) such that \n\n(3)\n\n\n\n\nG\n\n\ni\nj\n\n\n=\n\n\n\u03c0\n\n\n\n\nN\n\n\nq\n\n\n\u2212\n1\n\/\n2\n\n\n\n\nw\n\n\ni\n\n\nG\n\n(\n\n\nr\n\n\nj\n\n\n,\n\n\ns\n\n\ni\n\n\n)\n\n\n\n\nOther grids on the sphere could alternatively be used, along with suitable quadrature weights. We adopted the Gauss\u2013Legendre grid for simplicity and due to the ease of transforming to a spherical harmonic representation. Any linear forward problem in spherical geometry may then be written in the familiar form \n\n(4)\n\n\nd\n=\nG\nm\n\n\n\nHere we are concerned with the inverse problem of how best to estimate \nm\n, a vector of parameter values on a spherical surface grid, given noisy observed data \nd\n linearly related to the model, along with suitable prior information regarding the model.\n\n\n2.2\nEquivalent least-squares solution to the linear inverse problem\nA simple solution to the above inverse problem exists if we are able to assume the spherical surface model parameters can be represented by a Gaussian probability density function (pdf) with a-priori mean \n\n\n\u03bc\n\n\n0\n\n\n and covariance \n\n\nC\n\n\nm\n\n\n, while the observations, \nd\n, represent realizations of Gaussian random variables with data error covariance \n\n\nC\n\n\ne\n\n\n\u00a0(Tarantola, 2005). The least-squares solution is then also a Gaussian pdf with mean \n\n(5)\n\n\n\n\n\n\nm\n\n\n\u02c6\n\n\n\n\nL\nS\nQ\n\n\n=\n\n\n\u03bc\n\n\n0\n\n\n+\n\n\nC\n\n\nm\n\n\n\n\nG\n\n\nT\n\n\n\n\nS\n\n\n\u2212\n1\n\n\n\n(\n\nd\n\u2212\nG\n\n\n\u03bc\n\n\n0\n\n\n\n)\n\n\n\n\nand covariance \n\n(6)\n\n\n\n\n\n\nC\n\n\n\u02c6\n\n\n\n\nL\nS\nQ\n\n\n=\n\n\nC\n\n\nm\n\n\n\u2212\n\n\nC\n\n\nm\n\n\n\n\nG\n\n\nT\n\n\n\n\nS\n\n\n\u2212\n1\n\n\nG\n\n\nC\n\n\nm\n\n\n\n\n\nwhere \n\n(7)\n\n\nS\n=\n\n\nC\n\n\ne\n\n\n+\nG\n\n\nC\n\n\nm\n\n\n\n\nG\n\n\nT\n\n\n\n\n\nThis solution is identical to the solution of a simple kriging system\u00a0(Hansen et al., 2006). Before presenting spherical direct sequential simulation (Section\u00a02.4) as an alternative solution method which avoids these often restrictive Gaussian assumptions, we first briefly describe the method of sequential Gaussian simulation on the sphere.\n\n\n2.3\nSequential Gaussian simulation on the sphere\nThe method of sequential simulation\u00a0(e.g. Deutsch and Journel, 1998; Hansen and Mosegaard, 2008) involves inferring Gaussian posterior realizations \n\n\nm\n\n\n\u02c6\n\n\n of random variables \nm\n, from the observations \nd\n. For a joint distribution of \n\n\nN\n\n\nm\n\n\n random variables, \n\n\nm\n\n\ni\n\n\n, conditioned on a set of known observations, \nd\n, the \n\n\nN\n\n\nm\n\n\n variate cumulative distribution function (cdf) is \n\n(8)\n\n\n\n\n\nF\n\n\nm\n\n\n\n(\n\n\nm\n\n\n1\n\n\n,\n\u2026\n,\n\n\nm\n\n\n\n\nN\n\n\nm\n\n\n\n\n|\nd\n)\n\n=\nP\n\n{\n\n\nm\n\n\ni\n\n\n\u2265\n\n\n\n\nm\n\n\n\u02c6\n\n\n\n\ni\n\n\n,\ni\n=\n1\n,\n\u2026\n,\n\n\nN\n\n\nm\n\n\n|\nd\n}\n\n=\nP\n\n{\n\n\nm\n\n\n1\n\n\n\u2265\n\n\n\n\nm\n\n\n\u02c6\n\n\n\n\n1\n\n\n|\nd\n}\n\nP\n\n{\n\n\nm\n\n\n2\n\n\n\u2265\n\n\n\n\nm\n\n\n\u02c6\n\n\n\n\n2\n\n\n|\nd\n,\n\n\n\n\nm\n\n\n\u02c6\n\n\n\n\n1\n\n\n}\n\n\n\n\n\n\n\n\n\u2026\nP\n\n{\n\n\nm\n\n\nN\n\n\n\u2265\n\n\n\n\nm\n\n\n\u02c6\n\n\n\n\nN\n\n\n|\nd\n,\n\n\n\n\nm\n\n\n\u02c6\n\n\n\n\n1\n\n\n,\n\n\n\n\nm\n\n\n\u02c6\n\n\n\n\n2\n\n\n,\n\u2026\n,\n\n\n\n\nm\n\n\n\u02c6\n\n\n\n\nN\n\u2212\n1\n\n\n}\n\n\n\n\n\nwhere \nP\n denotes probability. Sequential simulation involves drawing an \n\n\nN\n\n\nm\n\n\n variate sample based on (8) making use of the product rule of probability, such that each probability term on the right-hand side is sampled in succession\u00a0(Deutsch and Journel, 1998). Realizations are thus obtained in a series of \n\n\nN\n\n\nm\n\n\n sequential steps, gradually increasing the conditioning, beginning with the observations, \nd\n.\nFor a Gaussian random field, drawing samples satisfying (8) equates to drawing from the Gaussian pdf, \n\nN\n\n(\n\n\n\u03bc\n\n\nk\n\n\n,\n\n\n\u03c3\n\n\nk\n\n\n2\n\n\n)\n\n\n, where \n\n\n\u03bc\n\n\nk\n\n\n and \n\n\n\u03c3\n\n\nk\n\n\n2\n\n\n are the kriging mean and variance found by solving the kriging system\u00a0(e.g. Journel and Huijbregts, 1978; Deutsch and Journel, 1998; Hansen and Mosegaard, 2008), which in our notation is \n\n(9)\n\n\n\n\n\u2211\n\n\ni\n=\n1\n\n\n\n\nN\n\n\nv\n\n\n\n\n\n\nC\n\n\nv\n\n\n\n(\n\n\nv\n\n\ni\n\n\n,\n\n\nv\n\n\nj\n\n\n)\n\n\n\n\u03bb\n\n\ni\n\n\n=\n\n\nc\n\n\nv\nm\n\n\n\n(\n\n\nv\n\n\ni\n\n\n,\n\n\n\n\nm\n\n\n\u02c6\n\n\n\n\nk\n\n\n)\n\n\n\u2200\nj\n=\n1\n,\n\u2026\n,\n\n\nN\n\n\nv\n\n\n\nor\n\n\n\nC\n\n\nv\n\n\n\u03bb\n=\n\n\nc\n\n\nv\nm\n\n\n\n\n\nwhere \n\n\nv\n\n\ni\n\n\n is a member of the \n\n\nN\n\n\nv\n\n\n available conditional variables in each step (observations, point data, and previously simulated model parameters), \n\n\n\n\nm\n\n\n\u02c6\n\n\n\n\nk\n\n\n is the model parameter currently being simulated, and \n\n\n\u03bb\n\n\ni\n\n\n are known as the kriging weights which determine the desired Gaussian pdf, \n\nN\n\n(\n\n\n\u03bc\n\n\nk\n\n\n,\n\n\n\u03c3\n\n\nk\n\n\n2\n\n\n)\n\n\n. \n\n\n\nC\n\n\nv\n\n\n\n(\n\n\nv\n\n\ni\n\n\n,\n\n\nv\n\n\nj\n\n\n)\n\n\n are the a-priori covariances between conditional variables including any measurement error covariance, and \n\n\n\nc\n\n\nv\nm\n\n\n\n(\n\n\nv\n\n\ni\n\n\n,\n\n\nm\n\n\nt\n\n\n)\n\n\n are the covariances between conditional variables and the target model parameter. Solving Eq.\u00a0(9) for the kriging weights \n\u03bb\n, the kriging mean and variance are \n\n\n(10)\n\n\n\n\n\u03bc\n\n\nk\n\n\n=\n\u03bb\n\u22c5\n\n(\nv\n\u2212\n\n\n\u03bc\n\n\n0\n\n\n\n\ne\n\n\n\u0304\n\n\n)\n\n+\n\n\n\u03bc\n\n\n0\n\n\n\nwhere\n\n\n\ne\n\n\n\u0304\n\n\n=\n\n\n\n[\n\n1\n,\n\u2026\n,\n1\n\n]\n\n\n\nT\n\n\nof\u00a0length\u00a0\n\n\nN\n\n\nv\n\n\n\n\n\n\n(11)\n\n\n\n\n\u03c3\n\n\nk\n\n\n2\n\n\n=\n\n\n\u03c3\n\n\n0\n\n\n2\n\n\n\u2212\n\u03bb\n\u22c5\n\n\nc\n\n\nv\nm\n\n\n\n\n\n\n where \n\n\n\u03bc\n\n\n0\n\n\n and \n\n\n\u03c3\n\n\n0\n\n\n2\n\n\n are a-priori estimates of the mean and variance of the model parameters.\nCovariances between observation pairs and observation\/model parameter pairs can be obtained from the forward relation between the observation and model parameters defined in (2) and (4), and a covariance measure, \n\nC\n\n\n\n\nm\n\n\n0\n\n\n\n(\n\n\ns\n\n\ni\n\n\n)\n\n,\n\n\nm\n\n\n0\n\n\n\n(\n\n\ns\n\n\nj\n\n\n)\n\n\n\n\n, based on a-priori information. With the covariance of an observation pair defined by (2), and again absorbing the constant and integration weights into \n\nG\n\n(\n\n\nr\n\n\np\n\n\n,\n\n\ns\n\n\ni\n\n\n)\n\n\n, we have \n\n(12)\n\n\n\n\n\nC\n\n\nd\nd\n\n\n\n{\n\nd\n\n(\n\n\nr\n\n\np\n\n\n)\n\n,\nd\n\n(\n\n\nr\n\n\nq\n\n\n)\n\n\n}\n\n=\nC\n\n{\n\n\n\n\u2211\n\n\ni\n=\n1\n\n\n\n\nN\n\n\nm\n\n\n\n\nG\n\n(\n\n\nr\n\n\np\n\n\n,\n\n\ns\n\n\ni\n\n\n)\n\n\n\nm\n\n\n0\n\n\n\n(\n\n\ns\n\n\ni\n\n\n)\n\n,\n\n\n\u2211\n\n\nj\n=\n1\n\n\n\n\nN\n\n\nm\n\n\n\n\nG\n\n(\n\n\nr\n\n\nq\n\n\n,\n\n\ns\n\n\nj\n\n\n)\n\n\n\nm\n\n\n0\n\n\n\n(\n\n\ns\n\n\nj\n\n\n)\n\n\n}\n\n=\n\n\n\u2211\n\n\ni\n=\n1\n\n\n\n\nN\n\n\nm\n\n\n\n\n\n\n\u2211\n\n\nj\n=\n1\n\n\n\n\nN\n\n\nm\n\n\n\n\nG\n\n(\n\n\nr\n\n\np\n\n\n,\n\n\ns\n\n\ni\n\n\n)\n\nG\n\n(\n\n\nr\n\n\nq\n\n\n,\n\n\ns\n\n\nj\n\n\n)\n\nC\n\n{\n\n\n\nm\n\n\n0\n\n\n\n(\n\n\ns\n\n\ni\n\n\n)\n\n,\n\n\nm\n\n\n0\n\n\n\n(\n\n\ns\n\n\nj\n\n\n)\n\n\n}\n\n\n\n\n\nand \n\n(13)\n\n\n\n\nC\n\n\nd\nm\n\n\n\n{\n\nd\n\n(\n\n\nr\n\n\np\n\n\n)\n\n,\n\n\nm\n\n\n0\n\n\n\n(\n\n\ns\n\n\nq\n\n\n)\n\n\n}\n\n=\n\n\n\u2211\n\n\ni\n=\n1\n\n\n\n\nN\n\n\nm\n\n\n\n\nG\n\n(\n\n\nr\n\n\np\n\n\n,\n\n\ns\n\n\ni\n\n\n)\n\nC\n\n{\n\n\n\nm\n\n\n0\n\n\n\n(\n\n\ns\n\n\ni\n\n\n)\n\n,\n\n\nm\n\n\n0\n\n\n\n(\n\n\ns\n\n\nq\n\n\n)\n\n\n}\n\n\n\n\nAll required covariances are thus available given an a-priori model covariance function based on prior information regarding the random variables on the spherical surface and knowledge of the forward problem. The kriging system (9) can therefore be expanded as follows, in order to explicitly show the contributing parts of the covariance matrix \n\n(14)\n\n\n\n\n\n\n\n\n\nC\n\n\nd\nd\n\n\n+\n\n\nC\n\n\ne\n\n\n\n\n\n\nC\n\n\nd\nm\n\n\n\n\n\n\n\n\nC\n\n\nd\nm\n\n\nT\n\n\n\n\n\n\nC\n\n\nm\n\n\n\n\n\n\n\n\u03bb\n=\n\n\n\n\n\n\n\nc\n\n\nd\nm\n\n\n\n\n\n\n\n\nc\n\n\nm\nm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC\n\n\nd\nd\n\n\n is a matrix of observation to observation covariances computed using (12), \n\n\nC\n\n\ne\n\n\n holds observation data error covariances. \n\n\nC\n\n\nm\n\n\n and the vector \n\n\nc\n\n\nm\nm\n\n\n contain covariances between previously simulated model parameters and between previously simulated model parameters and the target model parameter; both are obtained directly from the a-priori model covariance. \n\n\nC\n\n\nd\nm\n\n\n and \n\n\nc\n\n\nd\nm\n\n\n are covariances from observations to previously simulated model parameters, and to the target model parameter respectively, as given by (13).\nSolving the kriging system sequentially using the above covariances, results in a sequential Gaussian simulation model realization \n\nN\n\n(\n\n\n\u03bc\n\n\nk\n\n\n,\n\n\n\u03c3\n\n\nk\n\n\n2\n\n\n)\n\n\n. Example model realizations are drawn from the posterior distribution by visiting model parameters in a random order for each realization.\nSuch sequential Gaussian simulation schemes are well-known and widely used in geostatistics. However many physical processes in Earth and Space physics are fundamentally nonlinear which results in non-Gaussian statistics for the model parameters \nm\n. In the next section we extend the above treatment to permit non-Gaussian model parameter distributions, based on the method of direct sequential simulation\u00a0(Journel, 1994; Tran et al., 2001; Oz et al., 2003). This approach ensures the linear relationship (4) between \nd\n and \nm\n is preserved, which is not the case for sequential Gaussian simulations after transforming \nd\n and\/or \nm\n to Gaussian variables. In sequential Gaussian simulation such a transformation destroys the linear relationship so the necessary covariance matrices cannot be expressed simply as a function of \n\n\nC\n\n\nm\nm\n\n\n and \nG\n.\n\n\n2.4\nDirect sequential simulation for non-Gaussian fields on a sphere\nThe system described in Section\u00a02.3 allows one to sequentially simulate model parameters on a spherical surface given observations, leading to a realization of a Gaussian random field which fits the observations to within measurement error, and as far as this fit allows, reproduces the mean and covariance of the prior information. We now go further and simulate non-Gaussian random fields using direct sequential simulation with histogram reproduction for a given a-priori model, following the methods proposed by\u00a0Journel (1994), Tran et al. (2001), and\u00a0Oz et al. (2003).\nA normal-score transform of an a-priori model \n\n\nm\n\n\n0\n\n\n to variables, \ny\n, that follow a standard Gaussian distribution (i.e.\u00a0zero mean, variance one), and the associated back-transformation to original values may be performed through (15) and (16). \n\n\n(15)\n\n\ny\n=\n\n\nH\n\n\n\u2212\n1\n\n\n\n(\nF\n\n(\n\n\nm\n\n\n0\n\n\n)\n\n)\n\n\n\n\n\n(16)\n\n\n\n\nm\n\n\n0\n\n\n=\n\n\nF\n\n\n\u2212\n1\n\n\n\n(\nH\n\n(\ny\n)\n\n)\n\n\n\n\n\n where \n\n\nH\n\n\n\u2212\n1\n\n\n is a standard Gaussian quantile function with cdf, \nH\n, \nF\n is the a-priori model cdf with quantile function \n\n\nF\n\n\n\u2212\n1\n\n\n. This transformation describes the connection between random variables with a Gaussian distribution, and non-Gaussian distributions defined by the a-priori model. It allows one to generate a collection of non-Gaussian cdf\u2019s through which the sampling in the sequential simulation steps described in\u00a0Eq.\u00a0(8) can occur. Generating a non-Gaussian cdf is achieved by substituting the standard Gaussian representation, \ny\n, with a Gaussian distribution, \n\n\ny\n\n\nn\n\n\n, with mean, \n\n\n\u03bc\n\n\nn\n\n\n, and variance \n\n\n\u03c3\n\n\nn\n\n\n2\n\n\n. This can be accomplished as follows \n\n(17)\n\n\n\n\ny\n\n\nn\n\n\n=\n\n\nH\n\n\n\u2212\n1\n\n\n\n(\nu\n)\n\n\n\n\u03c3\n\n\nn\n\n\n+\n\n\n\u03bc\n\n\nn\n\n\n\n\n\nthrough inverse transform sampling using a vector, \nu\n, of \n\n\nN\n\n\nu\n\n\n uniformly spaced quantiles between zero and one, which divides the Gaussian distribution into intervals of equal probability. The ranges of the mean and variance should cover approximately \n\n[\n\u2212\n3\n.\n5\n,\n3\n.\n5\n]\n\n and \n\n[\n0\n,\n2\n]\n\n respectively, to fully utilize the prior information in characterizing conditional distributions\u00a0(Oz et al., 2003). A variance range of [0,2] is used in order to alleviate cases where the obtained kriging variances lie outside the domain of the generated local conditional distributions as shown by\u00a0Deutsch et al. (2001). Performing transformation with \n\n\ny\n\n\nn\n\n\n as in (16) results in a discrete vectorized quantile function, \n\n\nq\n\n\nn\n\n\n, conditional on the prior information and with length \n\n\nN\n\n\nu\n\n\n describing a distribution with mean, \n\n\n\u03bc\n\n\ni\n\n\n, and variance, \n\n\n\u03c3\n\n\ni\n\n\n2\n\n\n, \n\n(18)\n\n\n\n\nq\n\n\nn\n\n\n=\n\n\nF\n\n\n\u2212\n1\n\n\n\n(\nH\n\n(\n\n\ny\n\n\nn\n\n\n)\n\n)\n\n\n\n\nfrom which a sample, \n\n\nz\n\n\ni\n\n\n, can be drawn using a uniform distribution, \nU\n, discretized in \n\n\nN\n\n\nu\n\n\n intervals \n\n(19)\n\n\n\n\nz\n\n\ni\n\n\n=\n\n\nq\n\n\nn\n\n\n\n(\n\nU\n\n\n\n(\n0\n,\n\n\nN\n\n\nu\n\n\n)\n\n\n\ni\n\n\n\n)\n\n\n\n\nSolving the kriging system yields an estimated mean and variance of the local Gaussian distributions and a distribution is then assigned from \n\n\nq\n\n\nn\n\n\n based on the mean, \n\n\n\u03bc\n\n\ni\n\n\n, and variance, \n\n\n\u03c3\n\n\ni\n\n\n2\n\n\n, closest to the kriging mean and variance. In this step a distance measure must be used and our implementation is described in Section\u00a03.3.2. We refer to the chosen distributions collectively as the local distributions. However, reproduction of the a-priori model is only ensured if the applied local distribution has mean and variance equal to the kriging mean and variance\u00a0(Journel, 1994). This further requires that the local distributions are scaled to have exactly the kriging mean and variance. For a value sampled from one of the local distributions, this is achieved by \n\n(20)\n\n\n\n\n\n\nm\n\n\n\u02c6\n\n\n\n\nk\n\n\n=\n\n(\n\n\nz\n\n\ni\n\n\n\u2212\n\n\n\u03bc\n\n\ni\n\n\n)\n\n\u22c5\n\n\n\n\n\u03c3\n\n\nk\n\n\n\n\n\n\n\u03c3\n\n\ni\n\n\n\n\n+\n\n\n\u03bc\n\n\nk\n\n\n\n\n\nwhere \n\n\n\n\nm\n\n\n\u02c6\n\n\n\n\nk\n\n\n is a final simulated model parameter value in the vector of model parameters \n\n\n\n\n\nm\n\n\n\u02c6\n\n\n\n\nD\nS\nS\n\n\n=\n\n\n\n[\n\n\n\n\n\nm\n\n\n\u02c6\n\n\n\n\n1\n\n\n,\n\u2026\n,\n\n\n\n\nm\n\n\n\u02c6\n\n\n\n\nk\n\n\n,\n\u2026\n,\n\n\n\n\nm\n\n\n\u02c6\n\n\n\n\n\n\nN\n\n\nm\n\n\n\n\n\n]\n\n\n\nT\n\n\n\n for a given realization. A probabilistic solution is then the collection of \n\n\nN\n\n\np\n\n\n complete realizations of the model parameters in the matrix, \n\n\n\n\nM\n\n\n\u02c6\n\n\n\n\nD\nS\nS\n\n\n, and computing the sample covariance as follows \n\n(21)\n\n\n\n\n\n\nC\n\n\n\u02c6\n\n\n\n\nD\nS\nS\n\n\n=\n\n\n1\n\n\n\n\nN\n\n\np\n\n\n\u2212\n1\n\n\n\n(\n\n\n\n\n\nM\n\n\n\u02c6\n\n\n\n\nD\nS\nS\n\n\n\u2212\n\n\n\n\n\u03bc\n\n\n\u02c6\n\n\n\n\nD\nS\nS\n\n\n\n\ne\n\n\n\u0304\n\n\n\n)\n\n\n\n\n(\n\n\n\n\n\nM\n\n\n\u02c6\n\n\n\n\nD\nS\nS\n\n\n\u2212\n\n\n\n\n\u03bc\n\n\n\u02c6\n\n\n\n\nD\nS\nS\n\n\n\n\ne\n\n\n\u0304\n\n\n\n)\n\n\n\nT\n\n\n\nwhere\n\n\n\ne\n\n\n\u0304\n\n\n=\n\n[\n\n1\n,\n\u2026\n,\n1\n\n]\n\nof\u00a0length\u00a0\n\n\nN\n\n\np\n\n\n\n\n\n where \n\n\n\n\n\u03bc\n\n\n\u02c6\n\n\n\n\nD\nS\nS\n\n\n is an \n\n\nN\n\n\nm\n\n\n length column vector of the model parameter means. This procedure ensures that simulations represent samples from the posterior probability density function of the model parameters based on the a-priori mean, variance, covariance structure, and histogram of model parameters, while honouring the data\u00a0(Tran et al., 2001; Oz et al., 2003).\n\n\n\n3\nImplementation\nWe have implemented the methods described in Section\u00a02 as a Python repository called Spherical Direct Sequential Simulation, which is available on Github at github.com\/mikkelotzen\/spherical_direct_sequential_simulation. The implementation includes five modules. (i) The geometry of the problem and the forward operator, relating the observations to the model parameters, (ii) the prior information, (iii) the measured observations, (iv) the simulation itself, and (v) the posterior pdf output. Fig.\u00a01 gives an overview of these modules; their content is described in more detail below. The propagation of information is shown with arrows.\n\n\n\n\n\n3.1\nGeometry and the forward problem\nIn SDSSIM the spherical polar coordinates for the surface points to be modelled and observations are stored in arrays of length \n\n\nN\n\n\nm\n\n\n and \n\n\nN\n\n\nd\n\n\n respectively, with the radius of the points to be modelled on the spherical surface of interest being a constant, \n\n\nr\n\n\n\u2032\n\n\n. Eq.\u00a0(1) defines the spherical linear inverse problem by connecting random variables on a spherical surface to observations. This spatial structure is illustrated with an example in Fig.\u00a02, where a unit radius Gauss\u2013Legendre quadrature (GLQ) grid is displayed among equal area distributed observations with varying radii. The discretized forward operator, \nG\n, which defines the connection between the observations and the model in Eqs.\u00a0(3)\u2013(4), is an array of size \n\n(\n\n\nN\n\n\nd\n\n\n,\n\n\nN\n\n\nm\n\n\n)\n\n, and its construction is problem dependent. In Section\u00a04 a specific example of a forward operator is presented.\n\n\n3.2\nObservations\nWe store observations, \n\nd\n\n(\nr\n)\n\n\n, in the vector array \nd\n of length \n\n\nN\n\n\nd\n\n\n with one value for each observation coordinate \n\nr\n=\n\n(\nr\n,\n\u03b8\n,\n\u03d5\n)\n\n\n. These values are noisy and linearly related to the desired model parameters being simulated as described by the forward problem in (1)\u2013(4). In case of point data, i.e.\u00a0direct observations of the model parameters themselves, these are simply added to the model parameter vector during simulation with the associated error added to the corresponding covariance indices.\n\n\n3.3\nPrior information\nAn a-priori model, \n\n\nm\n\n\n0\n\n\n, provides a distribution, with a-priori mean, \n\n\n\u03bc\n\n\n0\n\n\n, and variance, \n\n\n\u03c3\n\n\n0\n\n\n2\n\n\n. The prior distribution is also used as conditioning for a range of possible local distributions and further prior information in the form of a covariance model. Included in the implementation is the possibility of semi-variogram modelling. In our implementation this allows for estimating a covariance model from an a-priori model based on the assumption that it is second-order stationary and isotropic. It is also possible to use an a-priori power spectrum to specify the covariance model. The structure of the a-priori model is problem dependent, we provide examples in Section\u00a04.\n\n3.3.1\nCovariance model\nFor an isotropic field one can compute the spherical harmonic power spectrum and use this to define the covariance model, and hence the covariance matrix\u00a0(e.g. Jackson, 1994; Hipkin, 2001 in the geomagnetic framework). Given any such covariance matrix, \n\n\nC\n\n\nm\n\n\n, the data to data and data to model parameter covariances are computed through (12) and (13) respectively. The observation to observation data error covariance, \n\n\nC\n\n\ne\n\n\n, will later be added to \n\n\nC\n\n\nd\nd\n\n\n; this is a diagonal array of the data error covariance level.\n\n\n3.3.2\nLocal distributions conditional on prior\nLocal distributions conditional on the a-priori model used to sample model parameters are implemented as a lookup-table (LUT) based on Eqs.\u00a0(15)\u2013(18). The procedure is shown as pseudo-code in algorithm 1. The first input is the a-priori model and the second is the number of local distribution quantiles, \n\n\nN\n\n\nu\n\n\n. The number of quantiles is chosen by the user and should at most equal the size of the a-priori model, as it controls the level of detail expressed in the conditional distributions, which cannot exceed the level of detail in the histogram on which they are based. Two further inputs are the discretization levels, \n\n\nN\n\n\n\u03bc\n\n\n and \n\n\nN\n\n\n\u03c3\n\n\n, the ranges of the mean and standard deviation used in generating Gaussian distributions as shown in\u00a0Eq.\u00a0(17). The discretization level of the mean and standard deviation range determines the number of local distributions in the LUT. From these inputs, a uniformly spaced array, \nu\n, in the range zero to one is generated containing \n\n\nN\n\n\nu\n\n\n equally spaced values. \nu\n and the range of mean and standard deviation values determined by \n\n\nN\n\n\n\u03bc\n\n\n and \n\n\nN\n\n\n\u03c3\n\n\n are then used iteratively in\u00a0Eqs.\u00a0(17) and (18) to generate the local distribution LUT, \nQ\n, of size \n\n(\n\n\nN\n\n\nu\n\n\n,\n\n\nN\n\n\n\u03bc\n\n\n,\n\n\nN\n\n\n\u03c3\n\n\n)\n\n. In our implementation the Python package scikit-learn\u00a0(Pedregosa et al., 2011) is used to handle the normal-score and inverse transformations, \n\n\n\nF\n\n\n\u2212\n1\n\n\n\n(\n)\n\n\n and \n\n\n\nH\n\n\n\u2212\n1\n\n\n\n(\n)\n\n\n.\nHaving generated \nQ\n, we require a measure for finding the nearest local distribution given a kriging mean and variance, \n\n\n\u03bc\n\n\nk\n\n\n and \n\n\n\u03c3\n\n\nk\n\n\n2\n\n\n, such that a simulated model parameter (20) can be computed. This is achieved using an array \n\u03a8\n of measures \n\n(22)\n\n\n\u03a8\n=\n\n|\n\n\nQ\n\n\n\u03bc\n\n\n\u2212\n\n\n\u03bc\n\n\nk\n\n\n\n\nE\n\n\n\u0304\n\n\n|\n\n\/\n\u0394\n\n\nm\n\n\n0\n\n\n+\n\n|\n\n\nQ\n\n\n\n\n\u03c3\n\n\n2\n\n\n\n\n\u2212\n\n\n\u03c3\n\n\nk\n\n\n2\n\n\n\n\nE\n\n\n\u0304\n\n\n|\n\n\/\n\n\n\u03c3\n\n\n0\n\n\n2\n\n\n\n\n\nwhere \n\n\u0394\n\n\nm\n\n\n0\n\n\n\n is \n\nm\na\nx\n\n(\n\n\nm\n\n\n0\n\n\n)\n\n\u2212\nm\ni\nn\n\n(\n\n\nm\n\n\n0\n\n\n)\n\n\n, and \n\n\n\u03c3\n\n\n0\n\n\n2\n\n\n is the a-priori model variance. \n\n\nQ\n\n\n\u03bc\n\n\n and \n\n\nQ\n\n\n\n\n\u03c3\n\n\n2\n\n\n\n\n are arrays of the mean and variance for each local distribution, their sizes are \n\n(\n\n\nN\n\n\n\u03bc\n\n\n,\n\n\nN\n\n\n\u03c3\n\n\n)\n\n, and \n\n\nE\n\n\n\u0304\n\n\n is an array of ones matching their size. \n\u03a8\n is then likewise of size \n\n(\n\n\nN\n\n\n\u03bc\n\n\n,\n\n\nN\n\n\n\u03c3\n\n\n)\n\n and the index of the minimum value indicates the required nearest local distribution. Note that \n\n\nN\n\n\n\u03bc\n\n\n and \n\n\nN\n\n\n\u03c3\n\n\n determines the size of the above computation. Setting these to very large values can lead to heavy computational costs. \n\n\n\n\n\n\n\n\n3.4\nSpherical direct sequential simulation\nExpanding on the outline of SDSSIM given in the algorithm flowchart of Fig.\u00a01, the algorithm contains the following steps.\n\n\n\n1.\nDetermine a random path through the model parameters on the spherical surface.\n\n\n2.\nAt each location in the random path solve Eq.\u00a0(9) with the appropriate covariance matrices based on all available observations and previously simulated values. The kriging mean, \n\n\n\u03bc\n\n\nk\n\n\n, and variance, \n\n\n\u03c3\n\n\nk\n\n\n2\n\n\n, are then determined through (10) and (11).\n\n\n3.\nThe nearest local distribution in \nQ\n is found through (22). This provides the local distribution closest to the kriging mean and variance.\n\n\n4.\nDraw a sample from this nearest local distribution.\n\n\n5.\nScale the sample through (20) such that it originates from a local distribution with mean and variance exactly equal to the kriging mean and variance.\n\n\n6.\nAdd the scaled sample to the list of previously simulated model values for use in the rest of the simulation.\n\n\n7.\n2.-6. is repeated until all model parameters have been visited.\n\n\n\nPerforming the above with different random paths each time yields an ensemble of realizations from the posterior pdf, which are collected in the matrix \n\n\n\n\nM\n\n\n\u02c6\n\n\n\n\nD\nS\nS\n\n\n, and can then be used to estimate statistics such as the sample mean and covariance. Algorithm 2 shows pseudo-code for the SDSSIM algorithm in the case of computing \n\n\nN\n\n\np\n\n\n realizations with the outputs \n\n\n\n\nM\n\n\n\u02c6\n\n\n\n\nD\nS\nS\n\n\n, \n\n\n\n\nC\n\n\n\u02c6\n\n\n\n\nD\nS\nS\n\n\n and \n\n\n\n\n\u03bc\n\n\n\u02c6\n\n\n\n\nD\nS\nS\n\n\n. While not shown in algorithm 2, the implementation includes an option of skipping step 3\u20135 in the above, which results in spherical sequential Gaussian simulation. \n\n\n\n\n\n\n\n\n4\nA case study from geophysics: Core-mantle boundary magnetic field estimation\nWe now demonstrate SDSSIM on an example spherical linear inverse problem, estimating the radial component of Earth\u2019s magnetic field on the approximately spherical core\u2013mantle boundary (CMB) from globally-distributed satellite magnetic observations\u00a0(e.g. Langel, 1987; Bloxham et al., 1989; Gubbins, 2004; Finlay, 2020).\nAs a-priori models we use a training ensemble of \n\n487\n\n instances of the core\u2013mantle boundary field (up to spherical harmonic degree 30) from a numerical model of the magnetic field generating dynamo process in Earth\u2019s outer core\u00a0(Aubert et al., 2017). These dynamo fields contain highly localized field structures that lead to a more Laplacian than Gaussian distribution of the radial field at the CMB.\nFirst we test our method on a synthetic case considering two distinct scenarios where different types of observations are generated from a known source, referred to below as the synthetic truth. In Section\u00a04.1.1 we consider synthetic satellite observations and in Section\u00a04.1.2 we consider direct observations of some of the model parameters (i.e.\u00a0of the synthetic truth radial field at the CMB with added noise). The synthetic truth is chosen to be another snapshot from the dynamo model, not included in the prior set.\nHaving validated the method in this test case we go on to use real satellite magnetic field observations from Swarm Alpha, one satellite from ESA\u2019s Swarm constellation mission\u00a0(e.g. Friis-Christensen et al., 2008). Swarm data are freely available and were downloaded through the Swarm Virtual Research Environment.\n1\n\n\n1\nSwarm Virtual Research Environment swarm-vre.readthedocs.io.\n\n\nThe geomagnetic forward problem is of the form (1) with a forward operator based on the Green\u2019s function describing a potential field solution to Laplace\u2019s equation in spherical geometry for internal source Neumann boundary conditions\u00a0(e.g. Gubbins and Roberts, 1983; Hammer and Finlay, 2019). For simplicity we consider only observations of the radial component of the field; the forward operator linking the radial geomagnetic field at an observation location, \n\nr\n=\n\n(\nr\n,\n\u03b8\n,\n\u03d5\n)\n\n\n, to the radial field at a source location on the spherical core\u2013mantle boundary, \n\ns\n=\n\n(\n\n\nr\n\n\n\u2032\n\n\n,\n\n\n\u03b8\n\n\n\u2032\n\n\n,\n\n\n\u03d5\n\n\n\u2032\n\n\n)\n\n\n, is then \n\n(23)\n\n\nG\n=\n\n\n1\n\n\n4\n\u03c0\n\n\n\n\n\n\nh\n\n\n2\n\n\n\n(\n1\n\u2212\n\n\nh\n\n\n2\n\n\n)\n\n\n\n\n\nf\n\n\n3\n\n\n\n\n\n\n\nwhere \n\n\n\nh\n=\n\n\n\n\nr\n\n\n\u2032\n\n\n\n\nr\n\n\n,\n\nf\n=\n\n\n\n\n\n\nr\n\n\n2\n\n\n+\n\n\n\n\nr\n\n\n\u2032\n\n\n\n\n2\n\n\n\u2212\n2\nr\n\n\nr\n\n\n\u2032\n\n\ncos\n\u03a5\n\n\n\n\nr\n\n\n\nwith\n\ncos\n\u03a5\n=\ncos\n\u03b8\ncos\n\n\n\u03b8\n\n\n\u2032\n\n\n+\nsin\n\u03b8\nsin\n\n\n\u03b8\n\n\n\u2032\n\n\ncos\n\n(\n\u03d5\n\u2212\n\n\n\u03d5\n\n\n\u2032\n\n\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe represent the radial magnetic field in physical space at the CMB at a radius of \n\n\n\nr\n\n\n\u2032\n\n\n=\n3480\n.\n0\n\nkm\n\n, on a Gauss Legendre quadrature grid with \n\n\n\nN\n\n\nq\n\n\n=\n31\n\n latitudinal nodes and thus have \n\n\n\nN\n\n\nm\n\n\n=\n1891\n\n model parameters. This allows accurate transformation to a spherical harmonic representation up to degree \n\nn\n=\n30\n\n.\nWhen only satellite observations are available we use the training ensemble of dynamo model realizations to specify an a-priori covariance function for the CMB radial magnetic field model. Assuming isotropy and stationarity over the spherical surface the covariance function for the radial magnetic field may be written\u00a0(Jackson, 1994; Hipkin, 2001) \n\n(24)\n\n\n\n\nC\n\n\n\n\nB\n\n\nr\n\n\n\n\n\n(\n\u03a5\n)\n\n=\n\n\n\u2211\n\n\nn\n=\n1\n\n\n\u221e\n\n\n\n\nn\n+\n1\n\n\n2\nn\n+\n1\n\n\n\n\nR\n\n\nn\n\n\n\n(\n\n\nr\n\n\n\u2032\n\n\n)\n\n\n\nP\n\n\nn\n\n\n\n(\ncos\n\u03a5\n)\n\n\n\n\nwhere \n\n\n\nR\n\n\nn\n\n\n\n(\n\n\nr\n\n\n\u2032\n\n\n)\n\n=\n\n(\nn\n+\n1\n)\n\n\n\n\n\n\n\na\n\n\n\n\nr\n\n\n\u2032\n\n\n\n\n\n\n\n\n2\nn\n+\n4\n\n\n\n\n\u2211\n\n\nm\n=\n0\n\n\nn\n\n\n\n\n\n(\n\n\ng\n\n\nn\n\n\nm\n\n\n)\n\n\n\n2\n\n\n+\n\n\n\n(\n\n\nh\n\n\nn\n\n\nm\n\n\n)\n\n\n\n2\n\n\n\n is the Lowes spherical harmonic power spectrum\u00a0(Lowes, 1966) at \n\n\nr\n\n\n\u2032\n\n\n, with \n\n\ng\n\n\nn\n\n\nm\n\n\n and \n\n\nh\n\n\nn\n\n\nm\n\n\n Schmidt quasi-normalized spherical harmonic coefficients of degree \nn\n and order \nm\n, \na\n is the reference radius of the spherical harmonics, and \n\n\nP\n\n\nn\n\n\n are Legendre polynomials of degree \nn\n. We used (24) to compute an a-priori model covariance matrix between all grid points on the CMB that is consistent with the isotropic second order statistics given by the power spectra of the realizations in the dynamo model training ensemble. The mean of these defines our a-priori model covariance matrix, \n\n\nC\n\n\nm\n\n\n. The a-priori models in the dynamo training ensemble are provided to spherical harmonic degree \n\nn\n=\n30\n\n, whereas Eq.\u00a0(24) involves a summation to infinity. Generating a covariance matrix using (24) based on truncation to degree 30 can thus result in non-positive-definite covariance matrices due to the truncation. To avoid this problem we implement a function that gradually tapers the spectra to zero beyond degree \n\n30\n\n, using \n\n\n\nf\n\n\nt\na\np\ne\nr\n\n\n=\n0\n.\n5\n\n\ne\n\n\n\u2212\n5\nn\n\n\n+\n0\n.\n5\n\n\ne\n\n\n\u2212\n2\nn\n\n\n\n. The a-priori local distribution for the model parameters is obtained by concatenating histograms from the training ensemble of numerical dynamo simulations of the CMB radial field. This is \n\n\nm\n\n\n0\n\n\n as described in Section\u00a03. Fig.\u00a03(b) shows the individual ensemble histograms and (c) shows the ensemble of Lowes power spectra used to generate the a-priori covariance model.\nFor the test based on direct observations (i.e.\u00a0on sampled values of the radial magnetic field at the CMB with noise added), the a-priori covariance is instead specified using an exponential semi-variogram model estimated from the available direct observations and a-priori local distributions are generated from their distribution shape.\nNote that in the figures of this section, histograms are normalized with respect to the total frequency of each distribution and displayed as a percentage of the total.\n\n4.1\nProbabilistic inversion of synthetic test data\n\n4.1.1\nSynthetic satellite observations\nFor this test we used \n\n\n\nN\n\n\np\n\n\n=\n2773\n\n synthetic observations located at satellite altitude at positions sampled along real Swarm Alpha orbits taken from April\u2013June 2018. To the synthetic radial field computed on the satellite orbits (based on the synthetic truth model) we add zero mean random Gaussian noise with std.dev. of \n\n2\n\nnT\n\n. In the simulation we characterize this with a diagonal data error covariance matrix of \n\n\n\n(\n2\n\nnT\n)\n\n\n\n2\n\n\n.\n\nFig.\u00a03 summarizes simulation diagnostics after 1000 posterior realizations are generated. The residual histograms in (a) demonstrate the posterior realizations fit the observations to a level similar to the added noise. In (b) the histogram of synthetic truth model values is well reproduced by the posterior realizations, and the posterior is within the training ensemble. Some posterior realizations exhibit larger magnitudes than the synthetic truth as seen by the distribution tails. The power spectra in (c) displays close agreement between the posterior and synthetic truth until around degree 8. At higher degrees the posterior is more broadly distributed around the synthetic truth, reaching similar width as the training ensemble by degree 17. Already at degree 15 the spectrum of the posterior mean starts to drop, indicating a point at which the small scales due to the prior start to average out across realizations. Sample maps of the posterior realizations as well as the posterior mean, standard deviation, and the synthetic truth are shown on Fig.\u00a04. As expected we see differences in the small scale features between posterior realizations. In comparison the synthetic truth is smoother, except the high amplitude features which are more concentrated. These results make it clear that with the prior covariance model, number of synthetic observations, and Gaussian noise used in this test, we are able to retrieve the synthetic truth only up to spherical harmonic degree 15. We further observe that the posterior standard deviation obtained with globally distributed synthetic satellite observations is uniform.\nA characteristic output of SDSSIM is the local marginal posterior distribution for each model parameter in physical space on the spherical surface. We refer to these as the marginal posterior distributions. They each contain all the generated values from the posterior realizations at a specific location. Examples are presented in Fig.\u00a05, selected based on their departure from a Gaussian distribution, as measured by the Kullback\u2013Leibler (KL) divergence\u00a0(Kullback and Leibler, 1951), \n\n(25)\n\n\n\n\nD\n\n\nK\nL\n\n\n=\n\n\n\u2211\n\n\n\n\n\n\nm\n\n\n\u02c6\n\n\n\n\nk\n\n\n\n\nP\n\n(\n\n\n\n\nm\n\n\n\u02c6\n\n\n\n\nk\n\n\n)\n\nlog\n\n(\n\n\n\nP\n\n(\n\n\n\n\nm\n\n\n\u02c6\n\n\n\n\nk\n\n\n)\n\n\n\nQ\n\n(\n\n\n\n\nm\n\n\n\u02c6\n\n\n\n\nk\n\n\n)\n\n\n\n\n)\n\n\n\n\nwhere P is here the marginal posterior distribution and Q is a Gaussian distribution with equal mean and variance sampled the same number of times. This allows us to identify marginal posterior distributions most similar and dissimilar to Gaussian distributions. Example marginal posterior distributions with low and high KL-divergence are selected in respectively blue and orange, along with their equivalent Gaussian distributions in black. Here it is clear that the marginal posteriors contain both distributions close to Gaussian and distributions with much sharper peaks and longer tails. SDSSIM is thus clearly capable of generating non-Gaussian probabilistic model parameter estimates.\n\n\n4.1.2\nSynthetic direct observations of the CMB magnetic field\nValidation tests based on synthetic direct observations of the model parameters are common in geostatistical studies. We report here briefly the results of such a test in order to demonstrate that our method can also be used within a more conventional geostatistical setup . To obtain direct observations, we sampled 27% of the synthetic truth radial magnetic field at the CMB and added zero mean random Gaussian noise with a std.dev. of \n\n2\n\nnT\n\n. 1000 posterior realizations are generated using the covariance model and a-priori histogram described above. In Fig.\u00a06 we display the resulting posterior mean and std. deviation maps of the radial magnetic field at the CMB. The large scale structures are well reproduced for areas containing direct observations, whereas areas without direct observations lack the structures present in the synthetic truth (bottom of Fig.\u00a04). The posterior std. deviation clearly aligns with the locations of the sampled direct observations and show that these areas are more informed. The histogram and semi-variogram reproduction is successful (not shown). This test shows that SDSSIM is capable of performing classical direct sequential simulation using only direct observations of the model.\n\n\n\n4.2\nProbabilistic inversion of real satellite magnetic observations\nWe now move on to the more interesting case of inferring the radial magnetic field at the CMB using real satellite data. We use data from Swarm Alpha sampled at 5\u00a0min intervals, during dark times, over one year from 01-11-2018 to 01-11-2019, applying standard data selection criteria to remove observations collected during periods of high solar-driven field disturbances\u00a0(Kauristie et al., 2017; Finlay, 2020).\nThis resulted in \n\n\n\nN\n\n\nd\n\n\n=\n4884\n\n observations at altitudes between \n\n432\n\nkm\n\n and \n\n452\n\nkm\n\n. In order to isolate the core field, we removed the LCS-1\u00a0(Olsen et al., 2017) model of the lithospheric magnetic field and the magnetospheric field and secular variation (changes over time of the core field) predicted by the CHAOS-7.2 model\u00a0(Finlay et al., 2020). The resulting radial field observations are presented in Fig.\u00a07. For simplicity we assume the data error to be independent, Gaussian, with zero mean, and a standard deviation of \n\n6\n\nnT\n\n at all latitudes.\nSimulation diagnostics based on 500 posterior realizations are presented in Fig.\u00a08. In (a) we see a fit of the posterior realizations to the observations with a mean RMSE of \n\n1\n.\n83\n\nnT\n\n, and also the fit of the posterior mean model, which was not explicitly constructed to fit the data. These are well below the assumed data error level of \n\n6\n\nnT\n\n suggesting this was over-estimated. (b) shows the histograms of the radial field at the core\u2013mantle boundary from posterior realizations compared to the posterior mean and training ensemble. The posterior distribution is narrower than the training ensemble with smaller tail amplitudes. In (c) we compare power spectra of the posterior realizations and posterior mean to those of the training ensemble and the internal (lithosphere and core) part of the CHAOS-7 model for 01-11-2018. The posterior realizations agree very closely with CHAOS-7 until degree 10 at which point the spread in the ensemble of posterior realizations begins to broaden. The posterior mean matches CHAOS-7 until degree 13, after which it diverges as it contains the lithospheric field. The posterior mean retains power until degree 15, beyond this it loses power since smaller scales in the posterior realizations are largely based on the prior covariance function which average out.\nExample posterior realizations, as well as the posterior mean and standard deviation are shown on Fig.\u00a09. The posterior mean is smooth in comparison to the realizations since it has little power beyond degree 15. The observations used in this experiment clearly do not constrain the posterior beyond degree 15. Fig.\u00a010 compares maps of the CMB radial field from CHAOS-7, maps of the equivalent least-squares solution and mean of the posterior realizations, and a map collecting the radial field values from the maximum of the marginal posterior distribution at each grid point. CHAOS-7 is truncated as is conventional at degree 13 while the other models are visualized after truncating at degree 30. Our results show slightly higher power and more detailed structures, particularly the maximum of the marginal posterior. There are similarities to structures seen in other studies which have attempted to infer the core field above degree 13\u00a0(e.g. Baerenzung et al., 2020; Aubert, 2020). In particular the strong flux patch in the equatorial Atlantic is split into two as also seen by\u00a0Baerenzung et al. (2020) while e.g.\u00a0above and slightly to the west of this patch, small scale patches are present which were also seen in\u00a0Aubert (2020). The difference seen in the maximum of the marginal posterior map compared to the equivalent LSQ and posterior mean solutions indicate the presence of non-Gaussian features in the posterior realizations.\nAlthough the differences between the maps of the maximum of the marginal posterior and the posterior mean are minor, and less than the differences between either of them and traditional spherical harmonic-based models such as CHAOS-7, there are nevertheless some interesting features. We note that the maximum of the marginal posterior shows higher amplitude features in the regions under the South Atlantic south-west of Africa; such features are important for understanding recent changes in the South Atlantic weak field anomaly at Earth\u2019s surface\u00a0(Finlay, 2020). Higher amplitude features are seen in the central Pacific region, around latitude 15 degrees South, longitude 100 degrees West. Finally there is a noticeable East\u2013West elongation of a positive flux feature South East of Madagascar around latitude 30 degrees South and 60 degrees East. Overall the map of the maximum of the marginal posterior shows generally sharper features than the map of the posterior mean or that from CHAOS-7.\n\nAn example of a probabilistic investigation of CMB radial field structures is shown in Fig.\u00a011. This presents histograms of the integrated radial magnetic field inside the cylinder tangent to the Earth\u2019s solid inner core\u00a0(Livermore et al., 2017), separated into normal and reversed polarities, in the north and south hemispheres. This analysis demonstrates the northern hemisphere has with high probability more reversed magnetic flux and weaker normal flux, a result of importance in geodynamo studies that has been difficult to quantify with conventional field models.\n\n\n\n\n\n\n5\nDiscussion and conclusions\nIn the case studies presented we found that the posterior mean is close to the equivalent least-squares solution. Why then go to all the trouble of generating posterior realizations? A key point here is that marginal posterior distributions at particular locations can still be non-Gaussian (see e.g.\u00a0Fig.\u00a05) and hence are not necessarily well described by the posterior mean and variance. The importance of this has previously been highlighted in the Cartesian case\u00a0(Hansen and Mosegaard, 2008) and will doubtless also prove crucial for some applications in spherical geometry, particularly when the prior model distributions are strongly non-Gaussian.\nIn the presented applications we have routinely transformed from the simulated grids in physical space to spherical harmonic representations. This was found to be useful for comparisons with existing geomagnetic field models and for visualization, but care is needed with this procedure. The transform from the grid in physical space to spherical harmonics is only exact for real square-integrable functions and when the spherical harmonic degree of the underlying function is limited to the level of chosen Gauss\u2013Legendre quadrature grid\u00a0(e.g. Wieczorek and Meschede, 2018). We observe a smoothing\/loss of power on the grid scale compared to the originally simulated values in some of the results presented here. This is acceptable if one wishes to compare models only up to some specific spherical harmonic degree, but for applications with covariance functions that allow discontinuities between neighbouring grid points, it is recommended to work instead with the simulated grids in physical space. There may be important advantages to working directly in the physical domain because the a-priori covariance information can then be allowed to vary with position. For geophysical problems involving Earth\u2019s lithosphere and upper mantle it may for instance be important to allow different covariance models for positions in the continents versus oceans, or to use locally defined information based on auxiliary variables such as geological composition or features. Allowing spatial variations in the a-priori covariance models is an obvious next step for the framework presented here. The presented geomagnetic application was somewhat limited in the sense it involved only sources at one depth and ignored any time dependence. The extension to sources at multiple depths (ideally with independently specified prior information) can be achieved by superposing the sources and visiting the model parameters at all depths during the sequential simulation. Similarly the model grids could be extended to a sequence of times in order to account for time-dependent source processes, provided the necessary time-dependent covariance matrices are specified\u00a0(see e.g. Gillet et al., 2013; Ropp et al., 2020; Baerenzung et al., 2020).\nA major limitation of the present implementation of the SDSSIM algorithm is the use of two-point statistics (covariances) for describing the a-priori conditional relationship between the model parameters. Given the complexity of natural phenomena on the sphere, these are not capable of fully capturing all the essential details. In order to move beyond this limitation, similar algorithms in Cartesian geometry have utilized multiple-point statistics\u00a0(Strebelle, 2002; Gravey and Mariethoz, 2020). Use of multiple-point statistics in spherical geometry is not yet well developed, but would certainly be of interest for improving on the results obtained here.\nThe SDSSIM scheme is in principle applicable to a wide variety of problems involving linear inversion or interpolation on a sphere. For example, possible applications could involve meteorological data such as the case presented by\u00a0Jun and Stein (2008) where non-stationary covariance models are used to analyse global ozone levels or\u00a0Jeong et al. (2017) where isotropic and non-stationary covariance models are used with global surface temperature data. Extensions to 3D using grids at different radii and radial covariance functions is also possible, e.g.\u00a0for inversion problems in seismology\u00a0(Meschede and Romanowicz, 2015) or gravity\u00a0(Save et al., 2016). The success in such applications will rely on the availability of suitable prior information, for example in the form of a-priori models or covariance functions. In such cases SDSSIM may allow for an improved exploitation of prior information and probabilistic descriptions of models in spherical geometry.\n\n\nCRediT authorship contribution statement\n\nMikkel Otzen: Conceptualization, Methodology, Software, Writing \u2013 original draft, Review, Editing. Christopher C. Finlay: Supervision, Conceptualization, Methodology, Funding acquisition, Writing \u2013 review & editing. Thomas Mejer Hansen: Supervision, Conceptualization, Writing \u2013 review & editing.\n\n","97":"","98":"\n\n1\nIntroduction\nClassical geostatistics can be used in many applied cases where the values under study show isotropic or regular trends of preferential directions. However, in complex scenarios the results obtained can be unrealistic since the underlying phenomena shows highly anisotropic trends which cannot be reproduced by the classical approach. These kind of complex scenarios arise in geological modelling of faults and veins in mineral reserves, sedimentary deposits in oil and gas reservoirs, environmental modelling of pollution spread, rain fall patterns or animal migration, and mobility patterns in highly populated urban areas. Additional computational issues arise when large-scale domains should be analysed.\nConcretely, this work is an effort to advance the state of the art in the field of Sequential Simulation algorithms, both for classical and LVA-based implementations, such as described previously, by presenting the following contributions:\n\n\n\n\u2022\nA novel parallel neighbour search is presented which introduces a new performance improvement for a well-known bottleneck in sequential simulation.\n\n\n\u2022\nParallelization of two well-known classical sequential simulation algorithms (SGSIM and SISIM), which were already parallelized in\u00a0Peredo et al. (2018), but contained a major bottleneck specifically in the neighbour search strategy.\n\n\n\u2022\nParallelization of two well-known LVA-based sequential simulation algorithms (SGS and SISIM), with remarkable speedups, specially in SISIM.\n\n\n\u2022\nPerformance evaluation is presented using different three-dimensional scenarios.\n\n\n\nThis article is organized as follows: Section\u00a02 describes the theoretical background of LVA geostatistics and its main difference with classical geostatistics. Section\u00a03 contains all aspects of the baseline sequential implementations. Section\u00a04 shows the parallelization strategies explored in this work. Sections 5 and 6 summarize the numerical results with a final analysis. Section\u00a07 presents conclusions and future work.\n\n\n2\nTheoretical background\nAnisotropy manifests itself as preferential directions of continuity in the underlying phenomena, i.e.\u00a0properties are more continuous in one orientation than in another. If constant anisotropy is present, a single trend or drift can be observed in the sampled data set or secondary sources of information\u00a0(Isaaks and Srivastava, 1990). In the case of local anisotropy, each location of the domain in study presents different preferential directions of continuity\u00a0(Boisvert, 2010; Boisvert and Deutsch, 2011), which is commonly known as Locally Varying Anisotropy (LVA).\nThe LVA geostatistical approach sets the initial path for future developments in terms of numerical implementations that can potentially scale to large scenarios. However, no further analysis or open source code improvements were developed in previous years. This is in part because non-standard algorithms, acceleration and distribution techniques must be applied to the inner kernels of the proposed LVA codes for geostatistical analysis. These inner kernels can be clustered in two groups: the classical geostatistical inner kernels, such as variogram computing, kriging estimation and sequential simulation (Deutsch and Journel, 1998; Chil\u00e8s and Delfiner, 1999); and dimensionality reduction techniques in high-dimensional spaces\u00a0(Huo et al., 2004). Both groups are connected by the property of positive definite covariance functions\u00a0(Curriero, 2006). In order to use the classical methods in contexts where LVA is present, non-euclidean distances must be used instead of straight lines connecting different points in the domain. The non-linear path that connects two points is the shortest path that follows the underlying LVA field. By computing the non-euclidean distances between each pair of points in the domain of study, a distance matrix is obtained. This matrix should be embedded into a similar distance matrix generated in a higher-level space using euclidean distances. If large domains are being studied in \n\n\nR\n\n\nd\n\n\n with \n\nd\n\u2208\n\n{\n2\n,\n3\n}\n\n\n, a prohibitive amount of computational resources will be necessary to obtain a sequential simulation using an LVA field as proxy of the underlying anisotropy. Additionally, a common bottleneck for non LVA and LVA approaches is related with the neighbourhood search to perform kriging interpolation across the random path selected for the sequential simulation. This step can be prohibitive in terms of computational resources if large search windows are used.\nRegarding previous works related to accelerating large scale geostatistical simulations, novel attempts in isotropic modelling have been reported in\u00a0Vargas et al. (2007), Nunes and Almeida (2010), Peredo et al. (2015) and Rasera et al. (2015), in order to accelerate classical methods using different algorithmic approaches combined with multi-core and distributed architectures, particularly MPI and OpenMP. A recent work described in\u00a0Peredo et al. (2018) follows the same path, preserving the original values of the single-core execution by splitting the neighbour search and simulation steps. In the same track,\u00a0Nussbaumer et al. (2018) shows a similar approach using a constant path for multiple simulations, proposing a parallel search for neighbour, as first task, followed by the simulation step, with focus on execution of multiple realizations.\nIn this work, a deep dive into a specific algorithm for parallel neighbour search is presented, which is also coupled with a previous work from the same authors. The proposed parallelization is applied to four different scenarios, non LVA and LVA based, and also using sequential Gaussian and sequential indicator simulation codes as baseline implementations.\n\n\n3\nSequential implementation\nRegarding classical algorithms, the baseline implementation used in this work was proposed by\u00a0Peredo et al. (2018), with baseline codes for sequential Gaussian simulation (SGSIM) and sequential indicator simulation (SISIM). Regarding LVA-based algorithms, the baseline implementation used in this work was proposed by\u00a0Boisvert and Deutsch (2011),\n1\n\n\n1\n\nhttp:\/\/www.ualberta.ca\/~jbb\/LVA_code.html.\n with baseline codes for variogram computation, kriging estimation and sequential Gaussian simulation (SGS).\u00a0Gutierrez and Ortiz (2019) contributed posteriorly with the implementation of sequential indicator simulation (SISIM). All of these codes are based on the well known GSLIB code base\u00a0(Deutsch and Journel, 1998).\nThe existing LVA implementations are based on the L-ISOMAP manifold learning method\u00a0(Tenenbaum et al., 2000). In this method, landmark or \u201danchor\u201d points are used to approximate non-euclidean distances in an origin space by euclidean distances in a higher or lower dimensional destination space. The mapping of each origin space point to the destination space is denoted as the embedding\n\nZ\n.\nThe steps of the sequential classical SGSIM and SISIM algorithms can be reviewed in\u00a0Peredo et al. (2018), and briefly in Algorithm 1 from line 6 to 16 (removing non used parameters such as \nZ\n, \n\n\nk\n\n\nc\no\nv\na\n\n\n and \n\n\nk\n\n\ns\ne\na\nr\nc\nh\n\n\n). For the sake of simplicity, the inner loop in lines 12 to 14 was left as is, which is the case of LVA-based SGS, but it is worth to mention that the actual codes can use this simulation loop outside of the domain loop of line 9 as well, even including a new random path for each simulation, which is the case of LVA-based SISIM, non LVA-based SISM ans SGSIM. The steps of the sequential LVA-based SGS and SISIM algorithms are depicted in Algorithm 1 from steps 1 to 16. The first steps of the algorithm (lines 2, 3 and 4) correspond to the L-ISOMAP algorithm, using specific LVA parameters such as the LVA field \nF\n, the graph connectivity policy \n\u03c0\n and the number of dimensions \nk\n of the resulting embedding \nZ\n. With \nZ\n computed, the next steps of the algorithm are standard sequential simulation, which consists in running an inner loop for multiple simulations using a single random-path (lines 6 to 15). Specifically, we can observe that the baseline implementation contains 4 main parts: graph building, distance matrix building, embedding building and standard sequential simulation routines (SGS and SISIM contain different methods to simulate). Each one contains several subroutines and code parts that were released by the authors without specific focus on large-scale usage. Additionally, the baseline implementations were based on Fortran 90 coupled with C++ code through system calls and disk I\/O communications, which entangles the code readability and exposes potential performance issues. In this section, a brief summary of the baseline implementation features of each part is included. \n\n\n\n\n\n\n3.1\nGraph building routines\nAs depicted in Algorithm 1, the first step of the L-ISOMAP routines adapted for LVA-based sequential simulations consists in calculating the connectivity graph (line 2) for the domain \n\u03a9\n. The domain must be a regular grid, which is a constraint of the current implementation. The inner steps of the routine build_connectivity_graph are depicted in Algorithm 2. The inputs of this algorithm are the domain \n\u03a9\n and specific LVA parameters. These parameters are the LVA field \nF\n and the graph connectivity policy \n\u03c0\n.\n\n\n\n\n\n\n\n\n\nThe parameter \n\u03c0\n is used to define the neighbourhood \nN\n for each domain point which will be considered in the connectivity graph, i.e.\u00a0for each neighbour an edge will be added to the graph (line 3). In practical terms, the policy \n\u03c0\n consists of a value \n\u0394\n which sets the number of separation edges (\u201chops\u201d) in the regular grid. For instance, \n\n\u0394\n=\n1\n\n will set a neighbourhood \nN\n of at most 6 points located at 1 hop of separation. The LVA field \nF\n is defined for each domain point as a tuple of five values, namely the angles azimuth (or strike) \n\u03b1\n, dip \n\u03b2\n and plunge \n\u03c6\n, and the directional ratios \n\n\nr\n\n\n1\n\n\n and \n\n\nr\n\n\n2\n\n\n, representing the ratios between axis X and Y, and axis Z and Y respectively. Using this tuple, a rotation matrix \n\nR\n\u2254\nR\n\n(\n\u03b1\n,\n\u03b2\n,\n\u03c6\n,\n\n\nr\n\n\n1\n\n\n,\n\n\nr\n\n\n2\n\n\n)\n\n\n is computed for each domain point in order to calculate the local anisotropic distance in each cell of the gridded domain (line 4). With the neighbourhood and rotation matrix computed, for each neighbour neig, an edge is added to the graph, defined as \n\ne\n=\n\n{\n\ni\nx\ny\nz\n\n,\n\nn\ne\ni\ng\n\n}\n\n\n. Each edge has weight equal to \n\nd\n=\n\n\n\n\nh\n\n\nT\n\n\n\n\nR\n\n\nT\n\n\nR\nh\n\n\n\n with \nh\n lag vector between the edge endpoints (lines 5 to 9). The last step of the algorithm performs a removal of redundant edges already computed, since the connectivity graph is undirected (line 10). The resulting graph \nG\n is stored in disk in file grid.out (line 12). \n\n\n\n\n\n\n\n3.2\nDistance matrix building routines\nThe second step of the L-ISOMAP routines adapted for LVA-based sequential simulation is the computation of the distance matrix (line 3 of Algorithm 1). This matrix is computed between domain and landmark points using the connectivity graph \nG\n computed in the previous step. The inner steps of the routine build_distance_matrix are depicted in Algorithm 3.\nThe baseline implementation reads two files from disk: nodes2cal.out (landmark points list) and grid.out (connectivity graph) (lines 3 and 4). With both files loaded into memory, for each landmark point a shortest path calculation must be performed through the connectivity graph \nG\n (line 6). This step is computed using Dijkstra\u2019s shortest path algorithm\u00a0(Dijkstra, 1959), implemented in the C++ Boost Library\u00a0(Boost.org, 2012). All distances from a landmark to all graph nodes are appended to the file dist_cpp.out (line 7). As mentioned before, the baseline implementation performs a system call to launch the execution of a compiled C++ code with the Boost Library call to Dijkstra routine, and the data transfer between Fortran and C++ is performed through expensive disk I\/O communication. \n\n\n\n\n\n\n\n3.3\nEmbedding building routines\nBased on the distance matrix \nD\n, the third step of the L-ISOMAP routines is the computation of the embedding \nZ\n (line 4 of Algorithm 1). The inner steps of the routine build_embedding are depicted in Algorithm 4. The input of the algorithm is file dist_cpp.out, computed in Algorithm 3, that contains the shortest path distances between landmark and domain points.\nFirst, the file dist_cpp.out is loaded in matrix \nD\n (line 2) and transformed into \nB\n (line 3). After this step, matrix \n\n\n\nB\n\n\nT\n\n\nB\n\n is factorized and the largest positive \n\nk\n\u2264\nn\n\n eigenvalues are selected, being \nn\n the number of landmark points. Finally, the embedding \nZ\n is defined as the rows of the matrix \nY\n with columns \n\n\n\n\u03bb\n\n\ni\n\n\n\u2212\n1\n\n\nB\n\n\nv\n\n\ni\n\n\n\n for \n\ni\n\u2208\n\n{\n1\n,\n\u2026\n,\nk\n}\n\n\n (lines 5 and 6), being \n\n\nv\n\n\ni\n\n\n the corresponding eigenvector of \n\n\n\u03bb\n\n\ni\n\n\n. \n\n\n\n\n\n\n\n3.4\nSequential simulation routines\nThe final routines are related to the classical Sequential Simulation algorithm, as described in\u00a0(Deutsch and Journel, 1998; Chil\u00e8s and Delfiner, 1999). The random path \nP\n (line 6, Algorithm 1) represents a random re-ordering of the domain point indexes from 1 to \n\n|\n\u03a9\n|\n\n. We will assume that the embedding \nZ\n is a subset of \n\n\nR\n\n\nk\n\n\n, with \n\nk\n\u2264\nn\n\n and \nn\n the number of landmark points. Regarding the neighbour search, represented by the routine search_neighbours (line 10), spiral search is implemented in the baseline non-LVA code, and two alternatives are implemented in the baseline LVA code, exhaustive search and KDTree-based search\u00a0(Kennel, 2004), being the last one the most efficient in large-scale scenarios. The parameters of this routine are the domain point index \n\n\nP\n\n\n\ni\nx\ny\nz\n\n\n\n, local interpolation parameters \n\u03ba\n (such as maximum\/minimum number of neighbours for further kriging interpolation, number of sample data values and previously simulated values, and maximum search distance) and a pseudo-random number generator seed \n\u03c4\n. Additional parameters only for LVA codes are the embedding \nZ\n and LVA parameters represented as \n\n\nk\n\n\ns\ne\na\nr\nc\nh\n\n\n and \n\n\nk\n\n\nc\no\nv\na\n\n\n. A key parameter in this routine is the number of dimensions used for search \n\n\nk\n\n\ns\ne\na\nr\nc\nh\n\n\n. This parameter controls which dimensions of the embedding \nZ\n will be used to perform distance comparisons to identify proximity. As discussed by\u00a0Boisvert (2010), Boisvert and Deutsch (2011), reducing the number of dimensions in \n\n\nk\n\n\ns\ne\na\nr\nc\nh\n\n\n can impact negatively in the accuracy of the obtained results, with a trade-off in speed of execution. Once the neighbours are computed, a simulation can be performed in the domain point, represented by the routine simulate (line 12). This routine is different in SGS, SGSIM and SISIM implementations. In SGS, the routine is based on the classical Fortran 90 GSLIB routine ktsol, which solves a universal kriging linear system through Gaussian elimination. In SGSIM, one execution of the GSLIB routine ksol is computed, which solves a kriging linear system. In SISIM, several executions of the GSLIB routine ksol are computed, one for each category (indicator) defined in the inputs, and solving a kriging linear system of equations each time. As with the previous search_neighbours routine, a key parameter in these routines is the number of dimensions used for covariance distance-based estimation \n\n\nk\n\n\nc\no\nv\na\n\n\n, only used in LVA codes. Similarly to \n\n\nk\n\n\ns\ne\na\nr\nc\nh\n\n\n, the reduction of this number impacts negatively in the accuracy of the obtained results with a trade-off in the execution time. The parameter \n\n\nk\n\n\nc\no\nv\na\n\n\n controls which dimensions of the embedding \nZ\n will be used to compute distance differences for covariance models implemented also following the classical GSLIB routine cova3. Finally, simulate is executed \nS\n times per point (\nS\n stochastic simulations), using the same random path for all simulations (line 6). As mentioned in the beginning if this section, non-LVA codes SGSIM and SISIM does not work in this order, with the simulation loop placed outer-most and using different random paths for each simulation. The final simulation results are stored in a matrix \n\n\nV\n\n\nt\nm\np\n\n\n, which is saved in file output.txt.\n\n\n\n4\nAccelerated\/parallel implementation\nThe first step that must be performed in order to accelerate or parallelize an application is to get an elapsed time profile of each part of the code. Based on that information, further code modifications are prioritized. In Tables\u00a01 and 2 we can observe the percentage of elapsed time in each set of routines, using four scenarios denoted sgsim, sisim, swiss-roll and escondida (described in Section\u00a05). For the non-LVA scenarios, sgsim and sisim, elapsed time spent on neighbours calculation and simulation can be splitted, since the baseline code already separates these tasks. We can observe that \n\n\n60\n%\n\n and \n\n\n26\n%\n\n correspond to neighbours calculation and \n\n\n34\n%\n\n and \n\n\n72\n%\n\n correspond to simulation, based on execution using 16 threads for parallel processing. For the LVA scenarios, no task separation is present in the code, so the major portion of elapsed time corresponds to the simulation routines (neighbour calculation + simulation) with \n\n\n79\n%\n\n and \n\n\n96\n%\n\n respectively, followed by the embedding and the distance matrix building. In the next subsections we will describe different strategies applied on these routines.\nAn initial code optimization step is applied to the LVA scenarios, since disk I\/O communication and C++ code execution is performed in the L-ISOMAP routines. Software refactoring tasks are applied to the corresponding code in order to optimize the execution. The proposed refactoring changes are in favour of a unified in-memory execution (sequential and parallel) which improves performance, code development, debugging and allows future modifications more easily. These modifications which avoid launching other processes and communication through disk I\/O are not described in this paper and they can be analysed by the reader directly in the available code.\nAs result of all refactor, acceleration and parallelization strategies applied, the proposed implementation can be reviewed in Algorithm 5. For non-LVA scenarios, steps 1 to 4 can be skipped, and also LVA parameters such as \nZ\n, \n\n\nk\n\n\ns\ne\na\nr\nc\nh\n\n\n and \n\n\nk\n\n\nc\no\nv\na\n\n\n are not used. In the next sections, a detailed explanation of each aspect of this algorithm is included. \n\n\n\n\n\n\n4.1\nParallel neighbour search\nFollowing the profiling from Tables\u00a01 and 2, the first routine group that should be accelerated corresponds to neighbour search and sequential simulation routines as described in Section\u00a03. Code modifications are applied in lines 9 to 14 of Algorithm 1, in case decoupling of both parts is needed (LVA scenarios), and subsequently accelerate each with different strategies. The framework used in this work follows an exact path-level parallelization of non-LVA codes based on\u00a0Peredo et al. (2018), which was adapted to the LVA codes, as shown in Algorithm 5.\n\n\n\n\n\n\n\n\n\n\nIn case of neighbour search routines, the proposed parallelization is based on a combination of: an optimized version of sequential KDTree neighbour search from\u00a0Kennel (2004), and a parallel implementation presented in Algorithm 6, which is used in line 9 of Algorithm 5. These search routines were implemented on the four codes in study, non LVA-based SGSIM and SISIM, and LVA-based SGS and SISIM.\nIn LVA-based SGS, the neighbour search can be applied using KDTree-based search originally. However, in LVA-based SISIM and non LVA-based SGSIM and SISIM, the only option implemented in the original baseline code was the exhaustive search or spiral search. Thus in these cases the first task was to adapt the original code to include the KDTree method. KDTree search is algorithmically faster than exhaustive search (\n\nO\n\n(\nlog\nN\n)\n\n\n for KDTree-based search and \n\nO\n\n(\nN\n)\n\n\n for exhaustive search), so no further analysis was performed for the last method. Spiral search on the other hand, as described in Deutsch and Journel (1998), is an efficient method to search for neighbours in gridded data, however its sequential nature makes it difficult to parallelize it efficiently.\nIn the existing implementation of KDTree search, the inner-most part of the computation should calculate distances between the query point and all points inside a terminal node of the tree. The points that are inside a fixed-size ball around the query points are marked as neighbours until the maximum number is reached. Using the line profiler tool of gprof\u00a0(Graham et al., 2004), we identify lines in KDTree code which are top contributors in the sequential part of execution. The optimization applied is based on the unrolling of the loop that computes the squared distance between the query point and each potential neighbour, which reduces the number of conditional evaluations and branch instructions processed by the CPU.\nThe parallel implementation is based on a modified OpenMP-compliant version of KDTree search and a block cyclic decomposition strategy of the random path. The block cyclic approach is necessary since an underlying unbalance exists in the amount of work for neighbour search (early points require more effort than later points). In order to use the existing KDTree implementation, a private variable used internally in the corresponding Fortran module should be declared as threadprivate with an OpenMP directive. With this change, different threads can create private trees and search independently for neighbours on different points without sharing data structures (line 6 of Algorithm 6). Since the neighbour search should be compliant with the sequential simulation search, only previously simulated nodes (marked points) or initial conditioning data can be considered as valid neighbours. This constraint is applied in lines 12 to 14 of Algorithm 6, by setting as marked all previous nodes for each thread of execution. By combining this strategy with a block cyclic distribution of iterations, the final workload is balanced through all threads, as shown in Fig.\u00a01. The final steps, depicted in lines 23 to 27 of Algorithm 6, use the computed neighbours to infer the level of each point, which indicates the degree of dependency of that point on previously simulated or initial conditioning points. Specifically, initial conditioning points are level 0 and a point is level \nn\n if the maximum level of all its neighbours is \n\nn\n\u2212\n1\n\n.\nIn Section\u00a05 we include performance tests of this specific algorithm applied to non-LVA and LVA scenarios.\n\n\n\n\n\n\n4.2\nParallel sequential simulation\nAs mentioned in the previous section, code modifications are applied in lines 9 to 14 of Algorithm 1, in order to decouple neighbour search and simulation parts for LVA scenarios. Regarding acceleration of the simulation part, an exact path-level parallelization is applied, following an exact path-level strategy described in\u00a0Peredo et al. (2018). We refer the reader to that article for further details of the existing parallel algorithm, but essentially it allows to simulate in parallel every point located in the same level of the random path, as described in the previous section.\nThe steps of this strategy applied to LVA-based SGS and SISIM are depicted in Algorithm 7. The reader can refer to\u00a0Peredo et al. (2018) for further reference of the inner routines and arrays used in this algorithm. \n\n\n\n\n\n\n\n\n\n\n\n\n4.3\nParallel embedding building\nThe second group of routines with large percentage of elapsed time is related to the assembling of the embedding \nZ\n, which is only relevant for LVA scenarios. After the software refactoring was completed, code optimizations can be applied properly on this part. By examining Algorithm 4, several matrix algebraic operations are involved in these routines. In order to accelerate them, Intel Math Kernel Library\u00a0(Intel, 2020) was selected as optimized implementation for Matrix-Vector (DGEMV), Matrix-Matrix (DGEMM) and Eigenvalue solver computation (DSYEV). Additionally, several memory accesses were modified from row-major to column-major order, which is the optimal memory access pattern in Fortran code.\n\n\n4.4\nParallel distance matrix building\nRegarding the computation of the distance matrix, also relevant for LVA scenarios only, several executions of Dijkstra\u2019s shortest path Boost implementation are launched. These executions are orchestrated by the routine dijkstra_cpp, integrated in the source after the software refactoring. Since C++ code can be parallelized with OpenMP\u00a0(OpenMP Architecture Review Board, 2008) in a straightforward way, we add parallel pragmas to this code, in order to parallelize the landmark loop of lines 5 to 7 of Algorithm 3.\n\n\n\n5\nResults\nThis section is divided in two subsections. The first one shows performance tests of the proposed implementation from Section\u00a04.1 using two scenarios extracted from Peredo et al. (2018), with adapted parallel versions of non LVA-based SGSIM and SISIM respectively. In the second subsection, performance tests of the proposed implementation are presented for two LVA-based scenarios, using parallel LVA-based versions of SGS and SISIM codes.\n\n5.1\nPerformance tests for parallel non LVA-based codes\nIn order to measure the performance of the proposed parallel algorithm, simulations are generated from non LVA-based codes SGSIM and SISIM. Both scenarios are denoted sgsim and sisim respectively. Scenario sgsim uses an initial 3D dataset of 2376 diamond drill-hole samples with information of copper grade composites. Scenario sisim uses a synthetic 3D dataset of 3000 random samples with 10 categories. The parameters in each scenario can be viewed in Table\u00a03.\n\nAll runs were executed in a single-node machine with Ubuntu 18.04.5 LTS with 2\u00a0\u00d7\u00a010-cores Intel(R) Xeon(R) CPU Silver 4210R at frequency 2.40\u00a0GHz and a main memory of 128\u00a0GB RAM. All Fortran programs were compiled using GNU Fortran version 4.8.5 supporting OpenMP version 3.1, with options -cpp -O2 -ffast-math -ftree-vectorize. Execution time results are depicted in Figs.\u00a02 and 3, and speedup results are depicted in Figs.\u00a04 and 5.\nIn sgsim scenario depicted in Fig.\u00a04 we can observe that the speedup increased consistently across all tests, with improvements using 16 threads (this number of threads is selected in order to compare results with previous work from the same authors). From Fig.\u00a02, speedups using 16 threads between the proposed implementation against the baseline version without parallel neighbour search are \n\n1\n.\n33\n\u00d7\n\n, \n\n1\n.\n79\n\u00d7\n\n, \n\n1\n.\n85\n\u00d7\n\n and \n\n2\n.\n14\n\u00d7\n\n in cases with 16, 32, 48 and 64 maximum neighbours for simulation respectively. However, it is important to notice that using lower numbers of neighbours, such as 16 and 32, the execution time is considerably lower in the baseline scenario using less than 8 and 4 threads of execution respectively. The reason for this behaviour is the amount of work that needs to be done to initialize the KDTree parallel structures, which in these cases is higher than the baseline search methods (the reader can check srchsupr method from\u00a0Deutsch and Journel (1998)). On all other cases the parallel adaptation has lower execution time than the baseline code.\nIn the sisim scenario depicted in Fig.\u00a05 we can observe no significative differences in speedup trends between the baseline and parallel adaptation. From Fig.\u00a03, speedups using 16 threads between the proposed implementation against the baseline version without parallel neighbour search are \n\n1\n.\n55\n\u00d7\n\n, \n\n2\n.\n42\n\u00d7\n\n, \n\n3\n.\n06\n\u00d7\n\n and \n\n4\n.\n11\n\u00d7\n\n in cases with 16, 32, 48 and 64 maximum neighbours for simulation respectively. In this scenario the execution time is consistently lower in the proposed implementation. This can be explained since this scenario involves more work (10 kriging linear systems should be solved for each domain point versus only one linear system in sgsim), so the initialization of KDTree structures is not significative in the execution time.\nFinally, if we compare the aggregated contribution to speedup of the new parallel neighbour search and optimizations, plus the previous parallelization work from\u00a0Peredo et al. (2018), against a single thread execution of the previous parallelization work, the obtained speedups using 16 threads for sgsim scenario are \n\n2\n.\n2\n\u00d7\n\n, \n\n5\n.\n0\n\u00d7\n\n, \n\n7\n.\n6\n\u00d7\n\n and \n\n11\n.\n9\n\u00d7\n\n, using 16, 32, 48 and 64 maximum neighbours respectively. Similarly for sisim scenario, speedups using 16 threads are \n\n7\n.\n8\n\u00d7\n\n, \n\n20\n.\n3\n\u00d7\n\n, \n\n32\n.\n7\n\u00d7\n\n and \n\n50\n.\n4\n\u00d7\n\n, using 16, 32, 48 and 64 maximum neighbours respectively.\n\n\n5.2\nPerformance tests for parallel LVA-based codes\nThe first scenario, namely swiss-roll, is based in the swiss roll testing scenario, which is extensively used in the Machine Learning community\u00a0(Tenenbaum et al., 2000). In our case, a 3D swiss roll is prepared, which is posteriorly transformed into a 3D LVA field. A synthetic dataset of 17280 samples is designed and attached to the domain as sample data. The second scenario, namely escondida, is based on real mining 3D data of 2376 diamond drill-hole samples with information of copper grade composites and lithologies per sample. In this case, a synthetic fold-like LVA field is used for simulation. The parameters of each scenario are detailed in Table\u00a04. A schema of the LVA fields and the drillhole data are depicted in Figs.\u00a06 and 7. In Figs.\u00a08 and 9 we can observe simulated domains in the swiss-roll scenario using LVA-based SGS. In the first figure, 6 slices of simulated domains are presented, each one generated with different values of \n\n\nr\n\n\n1\n\n\n ratio. The second figure shows two 3D simulated domains generated with LVA fields with different \n\n\nr\n\n\n1\n\n\n ratio values. In Figs.\u00a010 and 11 we can observe simulated domains in the escondida scenario using LVA-based SISIM, with 4 categorical values. As in the previous figures, the first one shows 3 slices of simulated domains each one generated with different values of \n\n\nr\n\n\n1\n\n\n ratio. The second one shows a 3D simulated domain generated with an LVA field.\n\nAll runs were executed in a single-node machine with Ubuntu 18.04.5 LTS with 2\u00a0\u00d7\u00a010-cores Intel(R) Xeon(R) CPU Silver 4210R at frequency 2.40\u00a0GHz and a main memory of 128\u00a0GB RAM. All Fortran programs were compiled using Intel ifort version 13.1.1 supporting OpenMP version 3.1, with options -fpp -mkl -openmp -O3 -mtune=native -march=native. All C++ programs were compiled using GCC g++ version 6.2.0 supporting OpenMP version 4.5, with options -Ofast -fopenmp -funroll-loops -finline-functions -ftree-vectorize.\n\nFig.\u00a012 shows execution time and speedup of the LVA-based SGS parallel code for the swiss-roll scenario, using \n\n\n\nk\n\n\nc\no\nv\na\n\n\n=\n\n\nk\n\n\ns\ne\na\nr\nc\nh\n\n\n=\n32\n\n and \n\n\n\nk\n\n\nc\no\nv\na\n\n\n=\n\n\nk\n\n\ns\ne\na\nr\nc\nh\n\n\n=\n1000\n\n control dimensions. Fig.\u00a013 shows execution time and speedup of the LVA-based SISIM parallel code for the escondida scenario, using \n\n\n\nk\n\n\nc\no\nv\na\n\n\n=\n\n\nk\n\n\ns\ne\na\nr\nc\nh\n\n\n=\n42\n\n and \n\n\n\nk\n\n\nc\no\nv\na\n\n\n=\n\n\nk\n\n\ns\ne\na\nr\nc\nh\n\n\n=\n1344\n\n control dimensions. On both figures, speedup is computed using the baseline sequential code (middle plot) and fully optimized code (bottom plot). Differences in speedup values on both plots, can be explained by large differences in elapsed time of the original baseline code and the optimized single threaded execution of the OpenMP code. In case of SISIM, this effect reduces three orders of magnitude the baseline execution time, mostly due to the refactoring of neighbour search using KDTree instead of exhaustive search.\nFinally, to illustrate the contribution of the neighbour search acceleration and parallelization to the overall speedup, in Table\u00a05 we can observe execution time and speedup against the baseline code version for LVA-based SGS and SISIM using \n\n\n\nk\n\n\n\u2217\n\n\n=\n1000\n\n and \n\n\n\nk\n\n\n\u2217\n\n\n=\n1344\n\n respectively. The purpose of this information is to show the relevance of the acceleration and parallelization of the neighbour search, which allows to improve the speedup an order of magnitude for each scenario. Special relevance has the inclusion of KDTree search in LVA-based SISIM which delivered a single contribution of two orders of magnitude in a sequential execution.\n\n\n\n\n\n\n6\nAnalysis\nAccording to the results of Section\u00a05, two aspects of the proposed implementation are reviewed in detail in this section: accuracy and efficiency.\n\n6.1\nAccuracy\nIn terms of accuracy, all parallel codes match exactly the baseline results (assuming the same pseudo-random number generator seed and the same neighbour search method), regardless of the number of threads used in the execution. This level of accuracy is obtained as result of the explicit replication of the random path simulations, although re-ordering non-conflicting nodes in order to allow parallel simulation of nodes in the same level.\nParticularly for LVA-based codes, by decreasing the values of control variables \n\n\nk\n\n\ns\ne\na\nr\nc\nh\n\n\n and \n\n\nk\n\n\nc\no\nv\na\n\n\n (which define the number of dimensions to use for neighbour search and covariance distance calculation), both parallel and baseline codes can achieve faster results, but with lower accuracy values (compared against larger values of control variables). It will be a final user\u2019s decision if he or she can tolerate lower levels of accuracy. In case of LVA-based SGS (Fig.\u00a012), the accuracy loss comparing executions with \n\n\n\nk\n\n\n\u2217\n\n\n=\n1000\n\n versus \n\n\n\nk\n\n\n\u2217\n\n\n=\n32\n\n is 71%, measured as the average relative error at node level . In case of LVA-based SISIM (Fig.\u00a013), the accuracy loss comparing executions with \n\n\n\nk\n\n\n\u2217\n\n\n=\n1344\n\n versus \n\n\n\nk\n\n\n\u2217\n\n\n=\n42\n\n is 15%, measured as the average categorical mismatch at node level. Nonetheless, the proposed parallel codes deliver the same results as the baseline for any fixed value of the control variables. Note that using the new parallel codes, simulations are executed much faster. Therefore, performing computations for low values of the control variables in search of a reduction of the execution time becomes meaningless when the accuracy loss is high.\n\n\n6.2\nEfficiency\nIn terms of the achieved efficiency by the proposed parallelization, in Tables\u00a06 and 7 we can observe a detailed profile of the refactored codes using 16 OpenMP threads in sgsim and sisim, and 20 OpenMP threads in swiss-roll and escondida scenarios, all executions using 48 maximum neighbours for simulation, and \n\n\n\nk\n\n\n\u2217\n\n\n=\n1000\n\n and \n\n\n\nk\n\n\n\u2217\n\n\n=\n1344\n\n respectively on each LVA-based scenario. It is worth mentioning that these tables are very different from the initial baseline profiling in Tables\u00a01 and 2, where the most relevant part was the neighbour calculation plus simulation as a whole. Now, the relevant parts for non LVA-based scenarios are neighbours calculations for SGSIM and again simulation for SISIM. For LVA-based scenarios, the relevant parts are distance calculation, neighbours calculation and simulation (embedding building is not relevant after applying optimizations in matrix operations and memory accesses). On non LVA-based scenarios, both results have efficiencies of 62% and 63% respectively, measured as the percentage of the theoretical maximum speedup achieved. On LVA-based scenarios, both results have efficiencies of 42% and 44% respectively. These results are acceptable for these applications since the baseline code and algorithms were not designed originally to run on parallel architectures. Additionally, Figs.\u00a014 and 15 contain efficiency percentages for non LVA-based scenarios for values of \n\nP\n\u2264\n16\n\n and LVA-based scenarios for values of \n\nP\n\u2264\n20\n\n.\n\n\nThe achieved speedup and efficiency obtained on both scenarios can be explained mostly by three factors: 1) intrinsic efficiency of external libraries (C++ Boost and Intel MKL), 2) different sizes in the initial conditioning datasets, and 3) amount of work performed in the simulation step.\nFactor 1 impacts only on LVA-based scenarios, and depends explicitly on the performance delivered by the external libraries. Even though further optimizations can be done in these libraries, it is left out of the scope of this work. In any case, on both scenarios we obtained similar performance since these executions only depend on the number of domain points, number of landmarks and LVA additional parameters, which remain on the same orders of magnitude across scenarios.\nRegarding factor 2, also impacting only on LVA-based scenarios, in Fig.\u00a016 we can observe the number of points assigned to each level, using a maximum of 48 neighbour points per simulation for both scenarios. In the swiss-roll scenario, 396 initial conditioning points are used, which results in a larger point-level curve, and translates into more overhead (resulting in less speedup and efficiency) due to multi-thread contingency and context switching from level to level. On the contrary, in the escondida scenario, 2313 initial conditioning points are used, which results in a shorter curve, and consequently less overhead (resulting in more speedup and efficiency). We can infer that a large number of initial conditioning data will generate a short point-level curve with better speedup and efficiency at lower execution time. In Fig.\u00a017 we can observe different point-level curves for the same scenario with equal parameters, except the number of initial conditioning data points. Curves using 48 maximum neighbours and 4% and 0.25% of initial conditioning data (continuous blue and orange curves) exemplify this phenomenon.\nRegarding factor 3, which impacts on both non LVA and LVA-based scenarios, we can identify two cases: keeping constant versus increasing the maximum number of neighbours to use for simulation. Assuming the maximum number of neighbours does not change, the amount of work in the simulation step can be increased by assembling and solving more kriging linear systems per simulation point. This situation occurs on non LVA-based and LVA-based SISIM implementations, since for each category, a kriging linear system should be assembled and solved per each simulation point. In sisim scenario, 10 categories are simulated, which result in \n\n10\n\u00d7\n\n more work per simulation compared against sgsim scenario. In escondida scenario, 4 categories are simulated, which results in \n\n4\n\u00d7\n\n more work per simulation point compared against swiss-roll scenario. The increment of work per simulation point does not change the form of the point-level curve, but it will impact on performance by delivering better speedup and efficiency values, at higher execution time. Multi-threaded execution becoming more efficient as the problem becomes larger (more computing needed) is a well-known behaviour denoted weak scalability (the reader can refer to the definition of Gustafson\u2019s law from\u00a0Hennessy and Patterson (2012) or the original reference from\u00a0Gustafson (1988)). On the other hand, if the maximum number of neighbours increases, the amount of work in the simulation step will be increased automatically, since the size of the kriging linear systems will increase accordingly. Fig.\u00a018 shows the speedups obtained using 32 and 64 neighbours as the maximum values for sgsim and sisim scenarios. Similarly, Fig.\u00a019 shows the equivalent using 48 and 96 as the maximum for the swiss-roll and escondida scenarios. On all figures we can observe an increment of the speedup values when higher number of neighbours are used. A slight decline in the speedup trend can be observed only for escondida using 96 neighbours, which is caused by overhead in the execution due to multicore contingency and higher memory usage overall.\n\n\n\n7\nConclusions and future work\nThe proposed implementation is able to speed-up the execution using code optimizations and allowing parallel execution using OpenMP. Compared against the sequential baseline codes, using 16 OpenMP threads it delivers speedups of \n\n12\n\u00d7\n\n and \n\n50\n\u00d7\n\n for non LVA-based SGSIM and SISIM respectively, and using 20 OpenMP threads it delivers speedups of \n\n56\n\u00d7\n\n and \n\n1822\n\u00d7\n\n for LVA-based SGS and SISIM respectively. Results for SISIM codes were obtained by also refactoring the baseline code neighbour search to allow KDTree-based search. The code of KDTree search was also optimized to reduce overhead due to large number of conditional evaluations and branching in the CPU. On LVA-based codes, by decreasing the parameters \n\n\nk\n\n\nc\no\nv\na\n\n\n and \n\n\nk\n\n\ns\ne\na\nr\nc\nh\n\n\n, the execution speed can be increased, with a trade-off in the accuracy obtained. In any case, the proposed implementation obtains the same results as the baseline implementation, regardless of the number of parallel threads used in the execution. This level of accuracy is possible since the same random path of simulation is used on both implementations, preserving neighbours calculations and the order of simulated values (assuming the same pseudo-random number generator seeds and neighbour search method). This is exactly the main purpose of using an exact path-level approach for parallelization, as discussed in previous sections.\nWe expect that this implementation will allow many researchers and practitioners to improve their tasks in stochastic resource simulations. Since the code will be released publicly, every aspect can be modified and improved by the community of interested users and institutions.\nIn terms of future work and next steps, two aspects will be explored: performance and usability improvements. In the first aspect, approximate computing approaches will be explored in order to reduce the execution time keeping statistically similar results. Additionally, improved techniques to accelerate the distance matrix building on LVA scenarios. In the second aspect, usability improvement can be achieved by allowing Python or R wrappers to the main execution parts of the code.\n\n\n8\nSource code\nThe current version of the code is available in the following links\n\nhttps:\/\/github.com\/operedo\/parallel-sgs-lva\n\n\nhttps:\/\/github.com\/operedo\/parallel-sisim-lva\n\n\n\nCRediT authorship contribution statement\n\nOscar F. Peredo: Study, Conception, Design, Acquisition\/generation of data, Analysis and interpretation of results, Parallel design, Writing \u2013 original draft, Writing \u2013 review & editing. Jos\u00e9 R. Herrero: Parallel design, Writing \u2013 original draft, Writing \u2013 review & editing.\n\n","99":"\n\n1\nIntroduction\nThe 3D magnetotelluric (MT) is a sounding technique to obtain the electrical resistivity distribution of the subsurface from simultaneous measurement of naturally induced electromagnetic (EM) fields in the Earth\u2019s subsurface. The EM response to the natural excitation sources depends on the electrical resistivity of the geological structures. From this dependence, it is possible to extract useful subsurface information to improve and reinforce geophysical characterization and interpretation. As a result, the 3D MT method has been widely used to mapping subsurface conductivity\/resistivity variations at different scales (e.g.,\u00a0from lithospheric and crustal to near surface studies\u00a0Ledo et al., 2002; Campany\u00e0 et al., 2012) and in diverse geophysical applications (e.g.,\u00a0hydrocarbon exploration, mineral mining, CO\n\n\n\n2\n\n\n sequestration, geothermal reservoir characterization\u00a0Queralt et al., 2007; Vilamaj\u00f3 et al., 2013; Pi\u00f1a-Varas et al., 2015).\nGeophysics EM modeling is an active research area, and several 3D modelers have been developed to study and understand MT responses. For the solution of Maxwell\u2019s equations, these modeling routines are typically based on four major numerical approaches: integral equation (IE;\u00a0Colton and Kress, 2013), finite difference (FD;\u00a0Kunz and Luebbers, 1993), finite volume (FV;\u00a0Eymard et al., 2000), and finite elements (FE;\u00a0Jin, 2015).\nOne of the first attempts of 3D MT modeling using the IE method dates to the 1980s, when\u00a0Ting and Hohmann (1981) presented their results for the solution of theoretical surface anomalies because to 3D conductive bodies buried in a half-space Earth. Other applications of the IE method for 3D MT modeling were presented by\u00a0Wannamaker (1991) and\u00a0Avdeeva et al. (2015). In all of them, the IE method results in a dense linear system of equations and works efficiently for simple layered models. However, its major drawback is the expensive computational cost when the model complexity increases (e.g.,\u00a0models with several layers).\nThe FD method arises as one of the most commonly used approaches for 3D MT modeling. Its main advantage is the comparably reduced implementation effort. On the other hand, the major disadvantage of FD schemes is their inability to work on unstructured meshes. As a result, FD methods can only approximate complex geometries using a stair-case strategy. FD modeling algorithms for MT datasets are those developed by\u00a0Mackie et al. (1994), Siripunvaraporn et al. (2002), Kun et al. (2013), Kelbert et al. (2014), Singh et al. (2017), Varilsuha and Candansayar (2018), among others.\nLike the FD approach, the FV scheme combines the advantages of a straightforward mathematical formulation and computational implementation. Given its reduced implementation effort, the FV method has recently been employed for 3D MT modeling, either for 2D problems on arbitrary topographies (e.g.,\u00a0Du et al., 2016) or 3D Earth models with general anisotropy (e.g.,\u00a0Guo et al., 2020). But, although it supports unstructured grids, the accuracy of FV solutions is, in general, inferior to FE computations when comparing meshes with similar characteristics\u00a0(Bondeson et al., 2012; Jahandari et al., 2017).\nThe FE method can overcome the issue mentioned above regarding structured grids due it has full flexibility concerning complex geometrical structures using unstructured grids. Also, the FE schemes offers a good trade-off between accuracy and the number of degrees of freedom (dof). One of the early challenges of EM modeling using FE was the possible jump of normal components across material interfaces\u00a0(B\u00f6rner, 2010). The nodal-based FE cannot reproduce the physical behavior of field discontinuities, leading to spurious solutions\u00a0(Jin, 2015). The introduction of edge FE, also referred to as N\u00e9d\u00e9lec elements\u00a0(N\u00e9d\u00e9lec, 1980), resolved this issue. The edge FE family provides stable numerical solutions through proper discretization of the curl space to which the EM field belongs. The basis functions of edge elements can perfectly treat EM fields\u2019 discontinuities across material interfaces (e.g.,\u00a0ensuring tangential continuity of the fields, while the normal components are allowed to be discontinuous). Also, since edge elements belong to the FE class, the accuracy of the solution can be extremely improved by using adaptive mesh refinement (\nh\n-refinement) and polynomial degree refinement (\np\n-refinement). Considering its advantages for EM fields, the edge FE method has recently been employed for 3D MT modeling, either using hexahedral meshes (e.g.,\u00a0Nam et al., 2007, 2008; Farquharson and Miensopust, 2011; Kordy et al., 2016; Rivera-Rios et al., 2019; Zhang et al., 2021) or tetrahedral meshes (e.g.,\u00a0Liu et al., 2008; Nam and Kim, 2010; Xiao et al., 2018; Zhu et al., 2021). However, with the sole exception of the algorithms developed by\u00a0Rivera-Rios et al. (2019) and\u00a0Grayver and Kolev (2015) none support high-order edge vector basis, and they are either sequential or black-box packages. Also, only modeling routines implemented by\u00a0Kordy et al. (2016) and\u00a0Zhu et al. (2021) supports parallel computations on modest multi-core architectures.\nThis paper presents a high-order edge FE method (HEFEM) algorithm for the efficient solution of arbitrary 3D MT modeling problems under anisotropic conductivities. To model realistic-world 3D MT datasets, our modeling tool supports tailored and unstructured tetrahedral meshes (\nh\n-refinement), high-order polynomial variants (global \np\n-refinement for \n\np\n=\n1\n,\n2\n,\n3\n,\n4\n,\n5\n,\n6\n\n), and massively parallel computations. Also, we investigate the impact of mesh quality on the EM responses by testing our previously published meshing rules in\u00a0Castillo-Reyes et al. (2019), where a tetrahedral adaptive-meshing strategy has been developed and studied over controlled-source electromagnetic method (CSEM) scenarios. Our implementation is based on and extends the Parallel Edge-based Tool for Geophysical EM Modeling (PETGEM;\u00a0Castillo-Reyes et al., 2018). With the inclusion of this new high-order 3D MT routine, the upgraded PETGEM is well suited to simulate 3D MT\/CSEM survey data on realistic models containing dipping layers, large conductivity\/resistivity contrasts, multiple-scale structures, and wide range of periods.\nThe rest of the paper is organized as follows. Section\u00a02 provides a comprehensive description about the mathematical background of our modeling routine. In Section\u00a03, we perform PETGEM simulations for challenging 3D MT setups presented in the literature and analyze their EM responses versus numerical tests. In Section\u00a04, we discuss important points to control the performance and suitability of our modeling tool, including considerations about the mesh design. Finally, Section\u00a05 provides summary remarks.\n\n\n2\nProblem statement\nThe 3D MT method is mathematically described in spatial coordinates \n\nx\n\u2208\n\u03a9\n\n by the frequency-domain Maxwell\u2019s equations in diffusive form, written as \n\n\n(1)\n\n\n\u2207\n\u00d7\nE\n=\ni\n\u03c9\n\u03bc\nH\n+\nK\n,\n\n\n\n\n(2)\n\n\n\u2207\n\u00d7\nH\n=\nJ\n+\n\n(\n\u03c3\n+\ni\n\u03c9\n\u03b5\n)\n\nE\n,\n\n\n\n\n where \nE\n, \nH\n and \nJ\n, \nK\n are the electric and magnetic fields and sources respectively; \ni\n is the imaginary unit; \n\u03c9\n is the angular frequency; \n\u03bc\n is the magnetic permeability for Earth materials\u00a0(Chave and Jones, 2012); \n\u03b5\n denotes the constant model permittivity; and \n\u03c3\n is the variable electric conductivity tensor. Fig.\u00a01 depicts a sketch of the computational domain \n\u03a9\n.\nWhen MT methods are considered, natural electric and magnetic fields of the Earth subsurface are measured and no external sources are generated, thus \n\nK\n=\nJ\n=\n0\n\n. Imposing additionally the usual assumption \n\n\u03c3\n\u226b\n\u03c9\n\u03b5\n\n,\u00a0Eqs.\u00a0(1) and (2) can be rewritten only in terms of the electric field using the Helmholtz form of the Maxwell\u2019s equations, that is \n\n(3)\n\n\n\u2207\n\u00d7\n\u2207\n\u00d7\nE\n\u2212\ni\n\u03c9\n\u03bc\n\u03c3\nE\n=\n0\n\nin\u00a0\n\u03a9\n.\n\n\n\nThe approximation of the electric field by the HEFEM requires the variational (weak) form of Eq.\u00a0(3). Let us define the space of curl-conforming basis functions that we use for the approximation of \nE\n as \n\n(4)\n\n\nH\n\n(\ncurl\n,\n\u03a9\n)\n\n\u2254\n\n\nw\n\u2208\n\n\n\n[\n\n\nL\n\n\n2\n\n\n\n(\n\u03a9\n)\n\n]\n\n\n\n3\n\n\n|\n\u2207\n\u00d7\nw\n\u2208\n\n\n\n[\n\n\nL\n\n\n2\n\n\n\n(\n\u03a9\n)\n\n]\n\n\n\n3\n\n\n\n\n.\n\n\n\nWe follow the Galerkin method\u00a0(Jin, 2015), testing with an appropriate weighting function \n\nv\n\u2208\nH\n\n(\ncurl\n,\n\u03a9\n)\n\n\n over the whole \n\u03a9\n, so \n\n(5)\n\n\n\n\n\u222b\n\n\n\u03a9\n\n\nv\n\u22c5\n\n\n\u2207\n\u00d7\n\u2207\n\u00d7\nE\n\u2212\ni\n\u03c9\n\u03bc\n\u03c3\nE\n\n\n\nd\n\u03a9\n=\n0\n.\n\n\n\nTaking into account the vector calculus identities \n\n\n(6)\n\n\nv\n\u22c5\n\n\n\u2207\n\u00d7\nF\n\n\n=\nF\n\u22c5\n\n\n\u2207\n\u00d7\nv\n\n\n+\n\u2207\n\u22c5\n\n\nF\n\u00d7\nv\n\n\n,\n\n\n\n\n(7)\n\n\nn\n\u22c5\n\n\nF\n\u00d7\nv\n\n\n=\nv\n\u22c5\n\n\nn\n\u00d7\nF\n\n\n,\n\n\n\n\n with \n\nF\n=\n\u2207\n\u00d7\nE\n\n and \nn\n as the outward-pointing normal to a surface, and using the divergence theorem on the resulting divergence term, finally yields, for all the test functions \n\nv\n\u2208\nH\n\n(\ncurl\n,\n\u03a9\n)\n\n\n, the weak form \n\n(8)\n\n\na\n\n(\nE\n,\nv\n)\n\n=\nl\n\n(\nv\n)\n\n,\n\n\n\nwith bilinear form \n\na\n\n(\n\u22c5\n,\n\u22c5\n)\n\n\n and linear form \n\nl\n\n(\n\u22c5\n)\n\n\n defined by \n\n\n(9)\n\n\na\n\n(\nE\n,\nv\n)\n\n=\n\n\n\u222b\n\n\n\u03a9\n\n\n\n\n\u2207\n\u00d7\nv\n\n\n\u22c5\n\n\n\u2207\n\u00d7\nE\n\n\n\u2212\ni\n\u03c9\n\u03bc\n\u03c3\nv\n\u22c5\nE\n\nd\n\u03a9\n,\n\n\n\n\n(10)\n\n\nl\n\n(\nv\n)\n\n=\n\u2212\ni\n\u03c9\n\u03bc\n\n\n\u222b\n\n\n\u0393\n\n\nv\n\u22c5\n\n\nn\n\u00d7\n\n\nH\n\n\n\u0302\n\n\n\n\n\nd\n\u0393\n,\n\n\n\n\n where \n\n\nH\n\n\n\u0302\n\n\n is the magnetic field imposed on the boundary \n\u0393\n of the domain. Note from Fig.\u00a01 that \n\n\u0393\n=\n\u2202\n\u03a9\n=\n\n\n\u22c3\n\n\ni\n=\n1\n\n\n6\n\n\n\n\n\u0393\n\n\ni\n\n\n\n.\n\n\n\n\n2.1\nBoundary conditions\nThe boundary conditions for the MT problem in Eq.\u00a0(8) are generated by specifying the value of the magnetic field \n\n\nH\n\n\n\u0302\n\n\n imposed on each surface \n\n\n\u0393\n\n\ni\n\n\n, \n\ni\n=\n1\n,\n\u2026\n,\n6\n\n, see Fig.\u00a01, and then computing the integral term in Eq.\u00a0(10). Given that usual quantities of interest for MT applications (e.g.,\u00a0apparent resistivity) require the computation of the model impedance, both electric \nx\n-polarization and \ny\n-polarization computations are required, see\u00a0Appendix\u00a0A for more details. These modes impose a magnetic field \n\n\n\nH\n\n\n\u0302\n\n\n=\n\n[\n0\n,\n\n\nH\n\n\n\u0302\n\n\n\n(\nz\n)\n\n,\n0\n]\n\n\n and \n\n\n\nH\n\n\n\u0302\n\n\n=\n\n[\n\n\nH\n\n\n\u0302\n\n\n\n(\nz\n)\n\n,\n0\n,\n0\n]\n\n\n for the \nx\n and \ny\n-polarization case respectively.\nWithout loss of generality, a constant value \n\n\n\nH\n\n\n\u0302\n\n\n=\n1\n\n is imposed on the surface \n\n\n\u0393\n\n\n1\n\n\n (see Fig.\u00a01) because only the ratio between electric and magnetic fields determines the impedance property. Also, the natural damping of the EM field imposes \n\n\n\nH\n\n\n\u0302\n\n\n=\n0\n\n on \n\n\n\u0393\n\n\n6\n\n\n. On both surfaces \n\n\n\u0393\n\n\n1\n\n\n and \n\n\n\u0393\n\n\n6\n\n\n (i.e.,\u00a0air and deep subsoil layers respectively) the model conductivity is assumed to be constant. On the rest of surfaces, the conductivity is considered variable only along the \nz\n coordinate and the scalar magnetic component \n\n\n\nH\n\n\n\u0302\n\n\n\n(\nz\n)\n\n\n is evaluated by solving the 1D Maxwell\u2019s model particularized as \n\n(11)\n\n\n\n\n\nd\n\n\n\n2\n\n\n\n\nH\n\n\n\u0302\n\n\n\n(\nz\n)\n\n\n\nd\n\n\nz\n\n\n2\n\n\n\n\n+\ni\n\u03c9\n\u03bc\n\u03c3\n\n(\nz\n)\n\n\n\nH\n\n\n\u0302\n\n\n\n(\nz\n)\n\n=\n0\n\nin\u00a0\n\n(\n\n\nz\n\n\n\nm\ni\nn\n\n\n\n,\n\n\nz\n\n\n\nm\na\nx\n\n\n\n)\n\n,\n\n\n\n\n\n\n\n\nH\n\n\n\u0302\n\n\n\n(\n\n\nz\n\n\n\nm\na\nx\n\n\n\n)\n\n=\n1\n,\n\n\n\n\n\n\n\n\nH\n\n\n\u0302\n\n\n\n(\n\n\nz\n\n\n\nm\ni\nn\n\n\n\n)\n\n=\n0\n,\n\n\n\n\nwhere, for a given surface, \n\n\nz\n\n\n\nm\ni\nn\n\n\n\n and \n\n\nz\n\n\n\nm\na\nx\n\n\n\n are the lower and higher values of the \nz\n coordinate respectively, and \n\n\u03c3\n\n(\nz\n)\n\n\n corresponds to the vertical conductivity profile. For more concise details on the computation of the integration terms in Eqs.\u00a0(9) and (10) using the HEFEM, see\u00a0Appendix\u00a0B.\n\n\n\n3\nNumerical validation\nTo verify the robustness of the upgraded PETGEM version, we simulate different and relevant scenarios of the 3D MT problem. We chose some of the models that the MT community has developed in the frame of several workshops: \nmt3dinvin\n (Dublin-2008), \nmt3dinv2\n (Dublin-2011), and \nmt3dinv3\n (Bari2016). The main results of the chosen models are reported and discussed in\u00a0Miensopust et al. (2013). Also, in the corresponding workshop web-page, several synthetic MT responses can be downloaded for comparison purposes. Each model presents a particular numerical modeling challenge, being a suitable approach to study the code capabilities (e.g.,\u00a0discretization of boundaries, mesh quality, run-time). In the next sections, we will discuss the most relevant cases. Further, readers interested in the complete set of PETGEM solutions are referred to Section \u201cCode and data availability\u201d.\nFrom the computational point of view, we use a standard continuous FE approximation for solving the 1D Eq.\u00a0(11) with an element size 10 times smaller than the one used in HEFEM for solving the 3D model in Eq.\u00a0(8). For all test cases, we use Gmsh\u00a0(Geuzaine and Remacle, 2008) to perform the tailored mesh generation. This process is based on the strategy proposed by\u00a0Castillo-Reyes et al. (2019) and accomplished using simple Python scripts and calls to routines from Gmsh. We use the multifrontal solver MUMPS\u00a0(Amestoy et al., 2006) to solve the proposed 3D MT setups and study the parallel scalability of the code. This solver is supported by PETGEM via the PETSc interface\u00a0(Balay et al., 2016). Furthermore, in all experiments, we consider run-time as the elapsed real-world time from start to end of assembling and solving the sparse linear system. Memory refers to the maximum peak of memory consumption at any point of solving region. All simulations have been performed on Marenostrum IV supercomputer using \n\n240\n\n CPUs.\n\n3.1\nTailored meshes for a 3D hill model\nAs the first example, we investigate the performance of our tailored meshing strategy using high-order edge elements. The purpose of this experiment is to validate our implementation and determine a suitable approach for the truncation of the computational domain, which is usually large enough so that the approximate solution at the boundaries matches the reference solution (i.e.,\u00a0determining how many skin-depths (\n\u03b4\n) are required for the boundary to be sufficiently far away from the inhomogeneities in every direction). Therefore, we perform a set of simulations to study the impact of tailored meshes and how they can significantly reduce the computational domain size.\nWe consider the 3D trapezoidal hill model introduced by\u00a0Nam et al. (2007), which is a homogeneous half-space model with \n\n\n\n\u03c1\n\n\nearth\n\n\n=\n100\n\n\u03a9\n\n\u00a0m as host resistivity and \n\n\n\n\u03c1\n\n\nair\n\n\n=\n1\n\u00d7\n10\n\n\n\n8\n\n\n\n\u03a9\n\n\u00a0m as the free-space resistivity. The trapezoidal hill, centered at the computational domain, has a height of 0.45 km with a hill-top square of \n\n0\n.\n45\n\n\u00d7\n\n0\n.\n45\n\n km, and a hill-bottom square of 2\u00a0\u00d7\u00a02 km. A cross-section view of the model under consideration is depicted in Fig.\u00a02. By using the horizontal components of the EM fields at 2\u00a0Hz, we compute the apparent resistivities for the \n\n\n\u03c1\n\n\nx\ny\n\n\n and \n\n\n\u03c1\n\n\ny\nx\n\n\n components. \n\n41\n\n sites at \n\ny\n=\n0\n\n km and along the \nx\n-axis are arranged equidistant spacing over the interval \n\nx\n=\n\n[\n\u2212\n2\n,\n2\n]\n\n\n km.\n\n\n\n\n3.1.1\nImpact of boundaries placed at different number of skin-depths\nAs the first part of this test, we truncate the computational domain using different number of skin-depths (\n\n\nn\n\n\n\u03b4\n\n\n) and study its impact on the accuracy of the obtained EM responses. For this model, the skin-depth is approximately \n\n\u03b4\n=\n3\n.\n5\n\n km in terms of the host resistivity of \n\n\n\u03c1\n\n\nearth\n\n\n. Based on this parameter, we design a set of tailored meshes for \n\np\n=\n1\n,\n2\n\n. In particular, twelve numerical solutions have been computed on different meshes using \n\n\n\nn\n\n\n\u03b4\n\n\n=\n1\n,\n2\n,\n4\n,\n6\n,\n8\n,\n10\n\n and \n\np\n=\n1\n,\n2\n\n, respectively. The resulting mesh hierarchies are shown in Table\u00a01.\n\nThe obtained apparent resistivities (\n\n\n\u03c1\n\n\nx\ny\n\n\n and \n\n\n\u03c1\n\n\ny\nx\n\n\n) and phases (\n\n\n\u03d5\n\n\nx\ny\n\n\n and \n\n\n\u03d5\n\n\ny\nx\n\n\n) for \n\np\n=\n1\n,\n2\n\n are shown in Fig.\u00a03. A close inspection of both numerical approximations allows us to observe two patterns in the EM responses. First, the apparent resistivities and phases at measuring stations located far from the hill, corresponds to the solution of a homogeneous flat Earth model (e.g.,\u00a0apparent resistivity of \n\n100\n\n\u03a9\n\n\u00a0m with phase of \n\n45\n\u00b0\n\n). Second, for stations located over the hill-top, the apparent resistivities and phases are significantly altered by the presence of the surface topography slope. More concretely, the apparent resistivity component \n\n\n\u03c1\n\n\nx\ny\n\n\n shows oscillations while for the resistivity component \n\n\n\u03c1\n\n\ny\nx\n\n\n decreases. The phases over the hill-top are about \n\n5\n\u00b0\n\n higher in comparison with that obtained over the hill-base. This general EM behavior is consistent with other low-order FE solutions previously published\u00a0(Nam et al., 2007, 2008; Ren et al., 2013; Zhu et al., 2021). However, we point out that although both orders of polynomial basis functions can obtain the EM mapping as expected, the \n\np\n=\n2\n\n produces better numerical solutions than \n\np\n=\n1\n\n (e.g.,\u00a0EM fields obtained with \n\np\n=\n2\n\n are less oscillating and closer to the theoretical definition).\n\nAdditionally, we consider an \n\n\nL\n\n\n2\n\n\n-norm to quantify the errors (\n\u03b5\n) of each numerical approximation with respect to a reference solution computed on a very \nh\n-fine mesh with \n\np\n=\n6\n\n and outer boundaries places at \n\n\n\nn\n\n\n\u03b4\n\n\n=\n20\n\n. It is worth mentioning that the numerical results and conclusions derived below remain valid for \n\np\n=\n1\n\n. Still, to preserve brevity, we focus on analyzing the \n\np\n=\n2\n\n approximations with \n\n\n\nn\n\n\n\u03b4\n\n\n=\n1\n,\n4\n,\n8\n\n. Fig.\u00a04 shows the obtained misfits for the resistivity component \n\n\n\u03c1\n\n\ny\nx\n\n\n, where it can be seen that the least accurate solutions are those that were computed on meshes with boundaries placed at \n\n\n\nn\n\n\n\u03b4\n\n\n=\n1\n\n away (this effect is also observable in apparent resistivities and phases depicted in Fig.\u00a03). However, for meshes with boundaries placed at \n\n\n\nn\n\n\n\u03b4\n\n\n=\n4\n,\n8\n\n, the numerical solutions exhibit better agreement with respect to the reference. More concretely, the misfit drops between 2 and 3 orders of magnitude for cases \n\n\n\nn\n\n\n\u03b4\n\n\n=\n4\n,\n8\n\n with respect to the case with boundary placed at \n\n\n\nn\n\n\n\u03b4\n\n\n=\n1\n\n. This pattern is similar for the resistivity component \n\n\n\u03c1\n\n\nx\ny\n\n\n, which has been omitted to avoid over-plotting. Finally, we point that although the \n\n\n\nn\n\n\n\u03b4\n\n\n=\n10\n\n case produces the most accurate apparent resistivity and phase values, the cases with boundaries at \n\n\n\nn\n\n\n\u03b4\n\n\n=\n4\n,\n6\n,\n8\n\n can be competitive in function of the desirable accuracy.\n\n\n\n\n\n3.1.2\nImpact of high-order discretizations\nAs the second part of this experiment, we focus on studying the impact of high-order polynomial basis functions on meshes with an equivalent number of dof. Thus, we design a set of non-tailored meshes for \n\np\n=\n1\n,\n2\n,\n3\n,\n4\n\n and with the outer boundaries placed at \n\n\n\nn\n\n\n\u03b4\n\n\n=\n4\n\n. The resulting number of dof for all meshes is about \n\n8\n\u00d7\n10\n\n\n\n4\n\n\n\n. These meshes have the same quality, same geometry and use the same meshing algorithm with slight variations in nodal positions. Again, we obtain the misfit ratios for each numerical scheme by direct comparison against a reference solution computed on \nh\n-fine mesh with \n\np\n=\n6\n\n and outer boundaries places at \n\n\n\nn\n\n\n\u03b4\n\n\n=\n10\n\n.\nThe obtained misfit ratios for apparent resistivities (\n\n\n\u03c1\n\n\ny\nx\n\n\n) and phases (\n\n\n\u03d5\n\n\ny\nx\n\n\n) are shown in Fig.\u00a05. It can be seen the positive impact of high-order variants in the reduction of the numerical error. The most pronounced improvement occurs between \n\np\n=\n1\n\n and \n\np\n=\n2\n\n. From there, the error reduction continues for \n\np\n=\n3\n,\n4\n\n. Fig.\u00a05 also depicts the error for each order of polynomial basis function, where it can be seen that the high-order basis exhibits a favorable impact on error control when meshes with the equivalent number of dof are employed. Again, this behavior is similar for the resistivity component \n\n\n\u03c1\n\n\nx\ny\n\n\n.\nGiven the results in these experiments, we conclude that the implementation of our 3D MT routine is correct. Also, our numerical results confirm that high-order polynomials can be up to ten times more accurate compared to first-order polynomials when the same number of dof are used (see Fig.\u00a05). Given the reasonable misfit ranges for each \np\n order and each \n\n\nn\n\n\n\u03b4\n\n\n value, we conclude that \n\np\n=\n2\n\n with \n\n\n\nn\n\n\n\u03b4\n\n\n=\n4\n\n provides the best compromise between misfit ratios and the number of dof. This conclusion is similar to that described in the context of active-source EM modeling\u00a0(Schwarzbach et al., 2011; Grayver and Kolev, 2015; Castillo-Reyes et al., 2019; Rochlitz et al., 2019). Nonetheless, we acknowledge that our conclusions should be further corroborated for more complex modeling setups. Therefore, in the following experiments, we focus on completing the analysis of our parallel and high-order 3D MT algorithm.\n\n\n\n\n\n\n3.2\nDublin test model 1 (DTM1)\nAs a second example, we consider the DTM1 proposed in the first 3D MT Dublin workshop. This 3D MT setup is suitable for verifying the code capabilities to modeling extreme situations such as a very wide range of periods and strong resistivity contrasts. The DTM1 consists of three different resistivity blocks in a homogeneous \n\n\n\n\u03c1\n\n\nearth\n\n\n=\n100\n\n\u03a9\n\n\u00a0m half-space. The free-space resistivity is set to \n\n\n\n\u03c1\n\n\nair\n\n\n=\n1\n\u00d7\n10\n\n\n\n8\n\n\n\n\u03a9\n\n\u00a0m. The Fig.\u00a06 depicts a sketch of the DTM1.\nWe consider the period range of 0.1 s to \n\n10\n\n000\n\n s taking four periods per decade. Following the recommendation for comparison proposed by the workshop, to perform the PETGEM simulations, we design tailored meshes for each period. We chose \n\u2248\n7.5 points per skin-depth to control the characteristic element size in the computational grid and the boundaries are placed at eight skin-depth far away from the region of interest. The resulting mesh statistics are shown in Table\u00a02.\n\n\nThe PETGEM solutions for the central station (\n\nx\n=\ny\n=\n0\n\n km) are shown in Fig.\u00a07. Here, we compare our results against the unique numerical responses for that model computed with a low-order FE code presented in\u00a0Miensopust et al. (2013). This FE reference solution was computed by Nuree Han and Tae Jong Le using the FE code by\u00a0Nam et al. (2007). Overall, in Fig.\u00a07 it can be seen an excellent match between both numerical solutions. The discrepancies are in diagonal elements of apparent resistivities and phases (\n\n\n\u03c1\n\n\nx\nx\n\n\n, \n\n\n\u03c1\n\n\ny\ny\n\n\n, \n\n\n\u03d5\n\n\nx\nx\n\n\n, and \n\n\n\u03d5\n\n\ny\ny\n\n\n) for periods below 1 s where resistivity values are very small (more than eight orders of magnitude smaller that off-diagonal elements). As a result, the numerical solutions cannot capture them due to numerical errors. In fact, the diagonals elements contain structural information when the 3D effects are important. For the DTM1 and the station position under consideration, these 3D effects are present for periods above 1 s. We point out that these discrepancies are also showed in\u00a0Miensopust et al. (2013) for other low-order numerical schemes (e.g.,\u00a0FD and IE methods). These results confirm that the EM response discrepancies for early periods are independent of the numerical method and its order.\n\nIt is worth mentioning here that the use of tailored meshes in conjunction with high-order polynomials can be beneficial for the solution of the problem under consideration. A close inspection of Table\u00a02 shows that the number of elements and dof in the grid remains constant for all periods. Furthermore, the run-time to obtain the solution for each period is also constant. Last but not least, we point out the difference of elements between the reference grid and our tailored meshes (e.g.,\u00a0\n\n69\n\n936\n\n elements for reference solution against \n\n\u2248\n61\n\n000\n\n elements for PETGEM computations). Furthermore, since we use polynomial basis functions \n\np\n=\n2\n\n, the resulting linear system of equations is much larger than those obtained in the reference. Given the results in these experiments, we conclude that our 3D MT modeling routine is robust and capable of dealing with complex and realistic setups.\n\n\n3.3\nDublin test model 2 (DTM2)\nAs a third example for our modeling algorithm, we chose the DTM2 which was proposed in the second 3D MT Dublin workshop. The original design purpose of this setup was to investigate how well the galvanic effects are dealt with in the modeling routines. The DTM2 corresponds to the model introduced by\u00a0Groom and Bailey (1991), which is composed by an hemisphere \n\n\n\n\u03c1\n\n\n1\n\n\n=\n10\n\n\u03a9\n\n\u00a0m of radius \n\nR\n=\n5\n\n km embedded in a homogeneous \n\n\n\n\u03c1\n\n\nearth\n\n\n=\n300\n\n\u03a9\n\n\u00a0m half-space. The free-space resistivity is set to \n\n\n\n\u03c1\n\n\nair\n\n\n=\n1\n\u00d7\n10\n\n\n\n8\n\n\n\n\u03a9\n\nm. The Fig.\u00a08 depicts a sketch of the DTM2. Again, we consider a very wide period range (from 0.01 to 10000 s, with four periods per decade). As in the previous experiment, we design tailored meshes with \n\u2248\n3 points per skin-depth for each period. Also, the boundaries are placed at eight skin-depth far away from the region of interest. The resulting mesh statistics are shown in Table\u00a03.\n\n\nFor the DTM2,\u00a0Miensopust et al. (2013) reported a vast responses comparison for two stations at the inner and outer boundary of the hemisphere (referred to as station 10 inside and station 18 outside in\u00a0Miensopust et al. (2013), renamed here as station 1 and 2, respectively). We compare PETGEM responses against a low-order FE solution computed by\u00a0Franke et al. (2007). It is important to state that the chosen FE reference solution corresponds to the more accurate approximation reported in\u00a0Miensopust et al. (2013). More concretely, in\u00a0Miensopust et al. (2013), the authors compared the synthetic responses against the analytic one at the galvanic limit. Therefore, we consider that comparing PETGEM responses against this FE reference follows a rigorous methodology.\nThe PETGEM solutions for station 1 and station 2 are shown in Fig.\u00a09. For both stations, it can be seen an excellent agreement between PETGEM responses and the reference solution. For this modeling test, the positive impact tailored meshes and high-order polynomials remain valid. The number of elements and dof reported in Table\u00a03 is constant for all periods. Consequently, the required run-time to obtain the solution is also constant. Again, it is worth to mention here the difference in number of elements between the reference grid and the PETGEM tailored meshes (e.g.,\u00a0\n\n\u2248\n300\n\n000\n\n elements for reference solution against \n\n\u2248\n125\n\n000\n\n elements for PETGEM). In view of these numerical results, we conclude that our algorithm can solve 3D MT setups with both non-structured geometries and in the presence of galvanic effects.\n\n\n\n\n\n3.4\nParallel performance analysis\nFinally, the fourth test focuses on studying the parallel performance of the presented MT routine. We consider a fine mesh for the COMMEMI model introduced by\u00a0Zhdanov et al. (1997), which is composed by three resistivity layers: \n\n\n\n\u03c1\n\n\n1\n\n\n=\n10\n\n\u03a9\n\n\u00a0m, \n\n\n\n\u03c1\n\n\n2\n\n\n=\n100\n\n\u03a9\n\n\u00a0m, and \n\n\n\n\u03c1\n\n\n3\n\n\n=\n0\n.\n1\n\n\u03a9\n\n\u00a0m. Over imposed to this layered model, a resistive block \n\n\n\n\u03c1\n\n\n4\n\n\n=\n1\n\n\u03a9\n\n\u00a0m is embedded in the first layer (\n\n\n\u03c1\n\n\n1\n\n\n). The dimensions of resistive block \n\n\n\u03c1\n\n\n4\n\n\n are \n\n20\n\n km in \nx\n-direction, \n\n40\n\n km in \ny\n-direction, and \n\n10\n\n km in \nz\n-direction. The free-space resistivity is set to \n\n\n\n\u03c1\n\n\nair\n\n\n=\n1\n\u00d7\n10\n\n\n\n8\n\n\n\n\u03a9\n\n\u00a0m. The Fig.\u00a010 depicts a sketch of the COMMEMI model. In this case, we compute the solution for the horizontal components of the EM fields at \n\n0\n.\n1\n\nHz\n\n on a tailored mesh with \n\n68619\n\n elements. We consider the basis orders \n\np\n=\n1\n,\n2\n,\n3\n,\n4\n\n, resulting in \n\n86606\n\n, \n\n456834\n\n, \n\n1316541\n\n, and \n\n2871584\n\n dof, respectively.\n\nWe evaluate the code scalability on distributed-memory platforms by running the same problem size for a different number of CPU. We compute the speed-up ratios through \n\n(12)\n\n\nS\n=\n\n\n\n\nT\n\n\ns\n\n\n\n\n\n\nT\n\n\nN\n\n\n\n\n,\n\n\n\nwhere \n\n\nT\n\n\ns\n\n\n is the serial run-time; \n\n\nT\n\n\nN\n\n\n is the parallel run-time; and \nN\n is the total number of CPUs. Furthermore, we measure the fraction of time for which a CPU is usefully utilized. This performance metric, also referred to as parallel efficiency, corresponds to the ratio of \nS\n with respect to \nN\n. We compute the parallel efficiency through \n\n(13)\n\n\nE\n=\n\n\nS\n\n\nN\n\n\n=\n\n\n\n\nT\n\n\ns\n\n\n\n\nN\n\u22c5\n\n\nT\n\n\nN\n\n\n\n\n.\n\n\n\nThe obtained speed-up and parallel efficiency ratios for the MUMPS solver are shown in Table\u00a04. In our experiments, the serial run-time used as reference is the resulting from computations with 48 CPUs (1 computing node). The excellent performance ratios for high-order simulations can be seen due to the higher workload per CPU (e.g.,\u00a0the quadrature order for numerical integration increases in a proportional ratio to the polynomial basis order). Fig.\u00a011 depicts the obtained speed-up and the parallel efficiency for polynomial basis functions of order \n\np\n=\n1\n,\n2\n,\n3\n,\n4\n\n. We obtained a near-linear speed-up growth for up to \n\n1\n\n008\n\n CPUs. From this number of CPUs, the speed-up ratios stop its almost linear gain due to the execution becomes dominated by the communication between processing units (e.g.,\u00a0messages exchange to perform the parallel solution of the linear system of equations). Nevertheless, the speed-up ratios keep increasing constantly, and we obtained significant run-time reductions for more than a thousand CPUs. Furthermore, Fig.\u00a011 shows the percent of parallel efficiency for each order of polynomial basis function. It can be seen that although higher-order polynomials increase the run-time, they offer better parallel efficiency ratios (see Table\u00a04). For example, a 15.13 speed-up on \n\n1008\n\n CPUs for \n\np\n=\n1\n\n corresponds to a efficiency of \n\n72\n.\n05\n%\n\n. This ratio means that, on average, over the course of the execution, each of the CPUs is idle about \n\n28\n%\n\n. For basis function \n\np\n=\n2\n\n it is a bit different, where a speed-up of 19.19 on the same number of CPUs is obtained, resulting in an efficiency ratio of \n\n91\n.\n86\n%\n\n which indicates that each CPUs is idle about \n\n8\n%\n\n during the execution. This conclusion is consistent for a still higher number of CPUs and polynomial orders \n\np\n=\n3\n,\n4\n\n. We state that the reported run-time depends mostly on the solver-type. Since we use general-purpose solver implementations, no special efforts were undertaken to minimize run-time, as this is an entire different task.\n\n\n\n\n\n\n\n4\nDiscussion\nThe development of 3D MT modeling routines has increased in the last decade. As a result, today, there are several codes available to solve arbitrarily setups of the 3D MT problem and obtain reasonable-looking results. However, most of the current algorithms for MT modeling lack an open-source development environment, which makes it hard for other users to study, adapt, and extend the code features to their own needs. Also, regardless of the type of meshes used, most of these algorithms use low-order numerical methods, and few of them support parallelism on modest multi-core architectures. These are the core motivations for this study and the introduction of a new PETGEM version.\nTo demonstrate the robustness of our high-order algorithm, we compute the solutions for a set of challenging 3D MT setups. Overall, it can be seen an excellent match between reference solutions and PETGEM responses. The first test is based on the 3D hill model, and its main purpose is to study the impact of tailored meshes, the distance between computational core and artificial boundaries, and the accuracy of high-order polynomial basis functions. The obtained EM responses have mostly a relative error of less than \n\n1\n\u2212\n3\n%\n\n. The PETGEM simulations on tailored meshes with boundaries placed at different skin-depths yield a general EM behavior consistent with previously computed FE solutions. However, the most accurate solutions correspond to that with boundaries placed from at least four skin-depths. These results confirm the importance of the distance between the artificial boundaries and the region of interest. Furthermore, the 3D hill model simulations performed on meshes with an equivalent number of dof yields a positive impact of high-order variants in reducing the numerical error. More concretely, in our experiments, it can be observed that high-order polynomial basis functions can be up to ten times more accurate compared to low-order polynomial basis functions (see Fig.\u00a05). The second model under consideration is the DTM1, which is more realistic than the previous one in terms of scale and physical parameters (e.g.,\u00a0large resistivity contrasts and large periods of time). For this model, we design tailored meshes for each period. The cross-validation between PETGEM and the low-order FE code by\u00a0Nam et al. (2007) yields a similar EM pattern. The main discrepancies between both approximations are in diagonal elements of apparent resistivities and phases for early periods (e.g.,\u00a0below 1 s). However, these differences are also presented in other numerical solutions reported in\u00a0Miensopust et al. (2013). Therefore, we consider these results to be correct because comparing different codes which use different numerical methods and different grids is ideal to address the topic of validation. The third test case is the DTM2 which also exhibits large periods of time. We compute the apparent resistivities and phases for two stations using tailored meshes. Here, we compare our results against the low-order FE code by\u00a0Franke et al. (2007). The cross-validation between both numerical approximations yields an excellent agreement.\nAccording to one of the main findings of this study, the mesh design is a complex and time-consuming process, but its know-how is fundamental for future modeling tasks and developments (e.g.,\u00a0EM inversion routines). As a result, adaptive meshing has already been investigated in geophysical electromagnetics\u00a0(Plessix et al., 2007; Schwarzbach et al., 2011; Key, 2016; Castillo-Reyes et al., 2018, 2019). Such meshing rules take into account the physics of EM fields computation with its diffusive behavior. The main conclusions of these works confirm that ad-hoc meshing strategies are needed to increase the modeling routine\u2019s flexibility and provide accurate solutions in a feasible run-time. Therefore, we built tailored meshes based on the rules proposed by\u00a0Castillo-Reyes et al. (2019), which were originally designed and evaluated for active-source electromagnetic modeling (e.g.,\u00a0CSEM). The core of this meshing process follows the skin-depth principle as the main quality criteria to determine the characteristic mesh sizes for each order of polynomial basis functions and each period of time. Our numerical results confirm that the rigorous application of these meshing rules can be beneficial for 3D MT modeling in terms of run-time and accuracy (e.g.,\u00a0for DTM1 and DTM2 the number of dof and run-time remains constant for each period, see Tables\u00a02 and 3). Also, we point out the difference of number of elements between the reference grids and our ad-hoc meshes for DTM1 and DTM2. Using tailored meshes and HPC, we achieved a considerable reduction in the run-times compared with those reported in\u00a0Miensopust et al. (2013). We acknowledge that this run-time improvement is due to the use of HPC, one clearly evident differentiator in the state-of-the-art of 3-MT modeling.\nIn our numerical experiments, the accuracy obtained with each basis function is consistent with the theoretical definition. Also, the high-order polynomial degrees require fewer dof to attain a given error level in comparison with the low-order case. However, this accuracy improvement has a cost. The computational implementation of HEFEM and its parallelization are technically complex. Also, the use of high-order elements decrease the sparsity pattern of the resulting linear system due the number of dof per element is larger. Then, high-order schemes demands more memory. Furthermore, the computation of the element integrals is more expensive for high-order basis functions (e.g.,\u00a0the quadrature order for numerical integration increases in a proportional ratio to the polynomial basis order). Consequently, the run-time for linear system assembly is also increased. However, the used solver is the main discriminator in terms of run-time, memory consumption, and parallel efficiency. In this paper, we use the MUMPS direct solver to obtain the solutions for the proposed 3D MT models. Furthermore, we investigate the computational efficiency of the code for the solution of large-scale modeling setups. When high-order polynomial basis functions and more than a thousand CPUs are employed, the performance scalability study shows that PETGEM offers excellent parallel efficiency.\nWe state that the used performance metrics to compare our numerical solutions are those available in\u00a0Miensopust et al. (2013) (e.g.,\u00a0run-time and number of mesh cells). The authors did not report the memory needs. Validating the correctness and efficiency of 3D codes is a difficult task, and it is essential to have easily accessible benchmark models with reliable and reproducible solutions. Therefore, in our study, we promote open practices, including sharing of code and data. We hope that these results may be useful for the entire MT community and more robust comparisons in the future (e.g.,\u00a0modeling tests that include real data).\n\n\n5\nConclusions\nWe have presented a new high-order and parallel modeling routine for arbitrary 3-D MT setups. This algorithm is based on and extends the PETGEM code, which was initially developed for 3-D marine CSEM problems. To verify the robustness, accuracy, and computational efficiency of this new version code, we solve a set of reference models within the MT community. These models exhibit large resistivity contrasts, a wide range of periods, and relatively complex geometries, making them challenging and ideal to address the topic of validation.\nA 3-D hill model is used as first example to verify our implementation and study the impact of tailored meshes and high-order discretizations. The second and third example correspond to realistic and complex 3-D MT setups, namely the DTM1 and DTM2. These models and its corresponding MT responses are open-source, allowing an independent cross-validation of our numerical results. In all test cases, the high-order discretizations in conjunction with tailored meshes show excellent accuracy. Furthermore, by using tailored meshes and HPC, the obtained run-times have proven to be highly competitive for the solution of realistic synthetic 3D MT setups. Then, we conclude that a proper discretization is crucial not only for accurate results but also to obtain solutions in a feasible run-time. Also, a scalability test demonstrates that PETGEM offers an excellent efficiency in HPC clusters. Nevertheless, we state that code performance depends on the input model, solver-type, and computational architecture.\nIn view of our numerical results, we conclude that PETGEM features satisfy modeling requirements of challenging and arbitrarily 3-D MT setups using both modest multi-core architectures and large-scale parallel computing clusters. We believe that the upgraded version of our modeling tool and numerical experiments prove useful for geophysicists interested in arbitrary passive-source EM modeling (also applicable for active-source EM methods). Our future research aims to compare synthetic 3-D MT responses against experimental data in a geothermal exploration context. Also, we intend to perform simulations for models that include anisotropy.\n\n\nCRediT authorship contribution statement\n\nOctavio Castillo-Reyes: Conceived the study, Contributed to the parallel implementation, math background, mesh generation, PETGEM simulations, Wrote the manuscript. David Modesto: Contributed to the magnetotelluric math background, Critically revised the manuscript. Pilar Queralt: Analyzed and interpreted the modeling results, Supervised the overall study, Critically revised the manuscript. Alex Marcuello: Analyzed and interpreted the modeling results, Supervised the overall study, Critically revised the manuscript. Juanjo Ledo: Analyzed and interpreted the modeling results, Supervised the overall study, Critically revised the manuscript. Adrian Amor-Martin: Contributed to the numerical background about high-order edge finite element method. Josep de la Puente: Contributed to the magnetotelluric math background, Critically revised the manuscript. Luis Emilio Garc\u00eda-Castillo: Contributed to the numerical background about high-order edge finite element method.\n\n","100":"\n\n1\nIntroduction\nTime-lapse gravimetry is extensively employed in the monitoring of subsurface fluid flow. Examples includes monitoring of volcanic unrest (Carbone et al., 2017), assessment of CO2 sequestration (Appriou et al., 2020), and oil reservoirs exploitation (Krahenbuhl et al., 2011). The gravity changes associated with these applications can vary between a few and a few hundred microGal (e.g., Van Camp et al., 2017; 1 \u03bcGal\u00a0=\u00a010\u22128\u00a0ms\u22122). Portable, relative gravimeters like the Scintrex CG-5 and CG-6 are commonly employed in these applications thanks to their compact shape, limited size\/weight, and automated features. If selected in the interface of the instruments, field measurements (taken at 6\u00a0Hz sampling rate) can be corrected in real time for tilt, internal temperature, solid Earth tides, seismic noise, and instrumental drift to obtain a gravity measurement and its standard deviation. The manufacturer specifies a nominal repeatability of 5 \u03bcGal for the CG-5 and less than 5 \u03bcGal for the CG-6 (Scintrex System, Operation Manual, 2014, 2018; Liu et al., 2019). Real-time corrections are useful to verify first-order results in the field, but post-processing is needed to achieve higher accuracy or for surveys where multiple gravimeters are used. Residual instrumental drift, the lack of correction for ocean loading and measurement noise from the field work itself (e.g., car transportation on dirt roads or rough terrains, or walking with the instrument in a backpack) can degrade the gravity measurements by some tens of \u03bcGal (Reudink et al., 2014) requiring more complete data corrections during post-processing to reach the desired precision.\ngTOOLS is open-source software for the processing of relative gravity data. The software runs under Windows OS, but it can be modified to run under Linux or Mac OS. The software is available both in MATLAB and as a compiled executable to be run under the free MATLAB Runtime (https:\/\/www.mathworks.com\/products\/compiler\/matlab-runtime.html). The software binds together single-task processing modules within a very simple user interface that is based on one text file (Fig. 1\n). The modular structure allows the software to be easily updated. The code allows for the speedy processing of raw gravity data from a network of gravity benchmarks composed of a base station and multiple monitoring sites. Data processing includes the removal of measurements noise, correction for solid Earth tides, ocean loading and residual instrumental drift. The residual instrumental drift, the relative gravity differences between the base station and monitoring sites, and associated errors, are estimated by a weighted least square analysis of the data set. The software allows the automatic processing of a gravity campaign spanning multiple days in a single run.\nEarlier efforts to develop codes to reduce campaign gravity data, include CG3TOOLS (Gabalda et al., 2003), GravProcess (Cattin et al., 2015), and PyGrav (Hector and Hinderer, 2016). The first two programs are coded in MATLAB, the third in Python. Data reduction, analysis, and representation in GravProcess (Cattin et al., 2015), and PyGrav, Hector and Hinderer (2016), are implemented by a graphical user interface. GravProcess (Cattin et al., 2015) implements free-air and terrain corrections as well. Recently, Kennedy (2020) released GSadjust. This software is a graphical user interface (GUI), built on PyGrav (Hector and Hinderer, 2016), that offers several plotting and analysis tools to correct, adjust and integrate relative and absolute campaign gravity data. All software employs similar reduction and analysis algorithms. The major differences between gTOOLS and the other software are: (a) gTOOLS is specifically designed for time lapse (temporal) gravity monitoring; (b) we spent significant efforts rigorously testing and verifying the code; (c) gTOOLS can be run by editing a single text command file and (d) The software allows the automatic processing of a gravity campaign spanning multiple days in a single run.\nHere, we first introduce the general characteristics of the program (Section 2), then we present the calibration module (Section 3). Corrections for solid Earth tides (\n\nE\nT\n\n), ocean loading (\n\nO\nL\n\n) and residual instrumental drift (\n\nR\nD\n\n), and the weighted least-square system solved to obtain adjusted gravity measurements and their uncertainties are introduced in Section 4. The post-processing module is reviewed in Section 5. Because of the complexity of the development of gTOOLS, testing the software to detect existing logical or mathematical bugs is indispensable. We show the results of four dynamic tests in Section 6. Finally, we present the case study of time-lapse gravity monitoring of the unrest at Cotopaxi volcano, Ecuador (Section 7).\n\n\n2\nGravity reduction\nGravity measurements processing in gTOOLS comprises three main modules: (1) gravimeter calibration; (2) automatic processing of gravity data to find adjusted gravity differences with a base station; and (3) post processing of results. Each module is optional and runs independently from the others. The different processing steps (shown in Fig. 2\n) are controlled by a single input text file (Fig. 1). These steps are performed by individual subroutines called by the main program modules.\ngTOOLS requires two input data \ufb01les:\n\n1.\na calibration \ufb01le, with the gravimeter(s) constant scale correction factor \n\nk\n\n.\n\n\n2.\ngravity data \ufb01les (CSV format text file) in the Scintrex CG-5 or CG-6 format for each gravity meter and day of field work.\n\n\n\nIn the Scintrex acquisition data mode, unprocessed 6\u00a0Hz data (gravity, tilt-x, tilt-y, and temperature) are stored in memory. The built-in software can average the readings over a specific time interval, subtract the solid Earth tides from the data and correct the data for a linear drift. Gravity data can be dumped from the gravity meter to a PC using the Scintrex Data logger software, a serial dump program that can be used to export data as text files with the columns in the following order LATITUDE, LONGITUDE, ALTITUDE., TIDE CORRECTED GRAVITY, ST DEV, TILTX, TILTY, TEMPERATURE, EARTH TIDE, DURATION, MEASUREMENTS REJECTED, TIME, DEC. TIME\u00a0+\u00a0DATE, TERRAIN CORRECTION, DATE. Examples of input and output \ufb01le formats are available in the gTOOLS software release.\nRelative gravity values at gravity benchmarks can be described by the equation (Fig. 3\n):\n\n(1)\n\n\nk\n\u22c5\n\ng\n\u02c6\n\n\n(\nt\n)\n\n=\ng\n\n(\nt\n)\n\n\u2212\nE\nT\n\n(\nt\n)\n\n\u2212\nO\nL\n\n(\nt\n)\n\n+\nR\nD\n\n(\nt\n)\n\n+\n\u03b5\n\n\n\nwhere \n\nt\n\n is the time of the measurement, \n\nk\n\u22c5\n\ng\n\u02c6\n\n\n(\nt\n)\n\n\n is the gravity reading, \n\n\ng\n\u02c6\n\n\n the nominal instrument measurement, and \n\nK\n\n the scale correction factor (Scintrex System, Operation Manual, 2014, 5\u201337). \n\ng\n\n(\nt\n)\n\n\n is the adjusted gravity value \u2013 the gravity value of interest for monitoring applications, \n\nE\nT\n\n(\nt\n)\n\n\n the solid Earth tide correction, \n\nO\nL\n\n(\nt\n)\n\n\n the ocean loading correction; \n\nR\nD\n\n(\nt\n)\n\n\n the residual instrumental drift, and \n\n\u03b5\n\n the residual instrumental\/operator\/site noise (Fig. 3). If the uncertainty for the scale correction factor (\n\n\n\u03c3\nk\n\n\n) and the residual instrumental\/operator\/site noise (\n\n\u03b5\n\n) are such that \n\n\n\u03c3\nk\n\n\u223c\n\u03b5\n<\n\n\u03c3\ng\n\n\n, where \n\n\n\u03c3\ng\n\n\n is the uncertainty of the adjusted gravity value \n\ng\n\n(\nt\n)\n\n\n, then \n\ng\n\n(\nt\n)\n\n\n and \n\n\n\u03c3\ng\n\n\n are given by the equation:\n\n(2)\n\n\n\n\n\n\ng\n\n(\nt\n)\n\n\u2243\nk\n\u22c5\n\ng\n\u02c6\n\n\n(\nt\n)\n\n+\nE\nT\n\n(\nt\n)\n\n+\nO\nL\n\n(\nt\n)\n\n\u2212\nR\nD\n\n(\nt\n)\n\n\n\n\n\n\n\n\n\u03c3\ng\n\n\u2243\n\n\n\nk\n2\n\n\u22c5\n\n\u03c3\n\ng\n\u02c6\n\n2\n\n+\n\n\u03c3\n\nE\nT\n\n2\n\n+\n\n\u03c3\n\nO\nL\n\n2\n\n+\n\n\u03c3\n\nR\nD\n\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3\nGravity meter calibration\nThe U.S. Geological Survey checks the calibrations of gravity meters by measuring mountain gravity loops (calibration lines) in Mauna Loa (HI), Mt Hood (OR) and Mt Hamilton (CA); see Battaglia et al. (2018). Mountain gravity loops allow comparison of the gravity changes measured by an instrument against know gravity measurements over a large range of gravity values. The occupation of calibration lines provides at small cost information about changes in time and non-linearity of the calibration factor, unusual drift behavior and other instrument defects.\nScintrex gravity meters have a linear response between spring acceleration and gravity, so that the relation between nominal instrument readings and actual gravity values can be represented by a single calibration constant GCAL1. Scintrex calibrates the instrument with a relative precision of 0.01%, equivalent to an error of 0.01 mGal (10 \u03bcGal) in 100 mGal. To decrease the error to 0.001 mGal (1 \u03bcGal) in 100 mGal, equivalent to a relative error of 0.001%, users must estimate a scale correction factor of the main calibration constant GCAL1 with a relative error of 10\u22125 (Scintrex, 2014, p. 5\u201337; (Valiant, 1991)see also Battaglia et al., 2018).\nFor this reason, we consider the manufacturer's calibration (i.e., GCAL1 in the case of a Scintrex CG-5 or CG-6) as an approximation of the calibration constant. The scale correction factor \n\n(\nk\n)\n\n simulates the divergence from this approximation and must be estimated by the operator. We model the linear response of the gravity meter (Fig. 4\n) as\n\n(3)\n\n\nC\nA\nL\nG\nR\nA\nV\nI\nT\nY\n=\ni\nn\nt\ne\nr\nc\ne\np\nt\n+\nk\n\u00d7\nL\nS\nG\nR\nA\nV\nI\nT\nY\n\n\n\n\n\nThe slope of the straight-line model above is the scale correction factor k; the intercept represents eventual systematic errors in the gravity measurements. If the uncertainties associated with the gravity measurements LSGRAVITY are much smaller than the errors associated with the calibration gravity measurements CALGRAVITY, then the slope and intercept of the straight-line model can be found solving a linear regression problem (Press et al., 1992, equation 15.2.6) \u2013 Fig. 4 (left). When the order of magnitude of the uncertainties of LSGRAVITY and CALGRAVITY can be compared then the slope and intercept of the straight-line model can be found by solving a non-linear regression problem (Press et al., 1992, equation 15.3.2) \u2013 Fig. 4 (right).\n\n\n4\nProcessing\nAdjustments to the field gravity measurements (solid Earth tides and ocean loading, the correction for the residual instrumental drift, weighted least square solution) are applied to measurements from each gravity meter by the module runAutProc.m (Fig. 2).\nAlthough designed to process gravity data from a double occupation of a gravity network (the so-called double loop; see Chapter 7), the software can process data from a single loop as well.\n\n4.1\nFilter\nMost gravimeters display a stabilization period, i.e., a non-linear evolution of the gravity value until it becomes stable. This stabilization period can last from a few minutes to tens of minutes, depending on the environmental noise and time and type of transportation between gravity sites. Once the gravimeter is stable, the occupation time and the read time should be programmed considering the balance between a read time short enough to give many readings and an occupation time long enough to give stable mean values (see section 7).\nAs a \ufb01rst step, the code reads gravity measurements (gravity), adjusts the measurements by the main calibration constant's scale correction factor \nk\n and computes the weighted mean (GRAVITY) and standard deviation (SIGMA) of gravity readings at each gravity benchmark. The weights represent the reciprocal of the root mean square errors from the CG-5 (or CG-6) output files. Measurements that fall outside of one SIGMA from the weighted mean GRAVITY are discarded.\nResults of this initial filtering are plotted for all the sites to help identify unstable readings, or outliers (Fig. 5\n). The code allows to manually discard any reading that might be considered unstable, or an outlier, by commenting the appropriate line in the text data file.\n\n\n4.2\nSolid earth tides and ocean loading correction\nThese corrections are performed for each measurement using its specific location and time stamp. Scintrex follows the approach proposed by Longman (1959) to correct for solid Earth tides. We use the same approach but extend it to also a) compute the Moon and Sun longitudes with the original equations by Bartels (1957, p. 747), b) employ updated values from USNO (2011) for the astronomical constants and c) consider anelastic effects on tides (Agnew, 2007) - see Fig. 6\n.\nThe ocean loading correction OTL(t) module calls the Fortran code HARDISP by Petit and Luzum (2010) to compute the effect of ocean loading on local gravity (Fig. 7\n). Ocean loading coefficients in the example included with the code are from the TOPEX9.2a model (Egbert and Erofeeva, 2002). However, users can provide coef\ufb01cients from any model they prefer. The ocean loading coef\ufb01cients implemented in gTOOLS are from the Bos and Scherneck's ocean loading provider (available on-line at http:\/\/holt.oso.chalmers.se\/loading\/).\n\n\n4.3\nLinear instrumental drift correction\nIn a time-lapse gravity survey, a double loop with two instruments is ideal and recommended (e.g., Jachens et al., 1981; Battaglia et al., 2008; Carbone et al., 2017). Every benchmark of the monitoring network is measured with at least two gravimeters during two complete loops through the network per day, with the base station remeasured three times.\nTo estimate the daily, linear instrumental drift parameters, the code first removes the average gravity value \n\n\ng\n\u203e\n\n\n from each benchmark, so that all the occupations are centered around zero (Fig. 8\n). The average value \n\n\ng\n\u203e\n\n\n is the solution of the weighted least square system\n\n(4)\n\n\n\n\n\n\n\n\ng\n\u203e\n\ni\n\n=\n\n\n[\n\n\nS\n\ni\nk\n\n\n\nW\n\nk\nj\n\n\n\nS\n\nj\ni\n\n\n\n]\n\n\n\u2212\n1\n\n\n\nS\n\ni\nk\n\n\n\nW\n\nk\nj\n\n\n\n\ng\n\u02dc\n\nj\n\n\n\n\n\n\n\n\n\ni\n=\n1\n,\n\u2026\n,\nN\n\n\n\n\n\n\nj\n,\nk\n=\n1\n,\n\u2026\n,\nM\n\n\n\n\n\n\n\n\n\n\n\n\n\n(5)\n\n\n\n\n\n\n\n\ng\n\u203e\n\ni\n\n=\n\n[\n\n\n\n\n\ng\n\u203e\n\n0\n\n\n\n\n\n\n\ng\n\u203e\n\n1\n\n\n\n\n\n\u22ee\n\n\n\n\n\u22ee\n\n\n\n\n\n\ng\n\u203e\n\nN\n\n\n\n\n]\n\n\n\n\n\n\n\n\n\nb\na\ns\ne\n\n\n\n\n\n\ns\ni\nt\n\ne\n1\n\n\n\n\n\n\n\u22ee\n\n\n\n\n\u22ee\n\n\n\n\n\ns\ni\nt\n\ne\nN\n\n\n\n\n\n,\n\n\n\n\n\nS\n\ni\nj\n\n\n=\n\n[\n\n\n\n1\n\n\n1\n\n\n1\n\n\n0\n\n\n0\n\n\n\u22ef\n\n\n\u22ef\n\n\n0\n\n\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n\u22ef\n\n\n\u22ef\n\n\n0\n\n\n\n\n\u22ee\n\n\n\n\n\n\n\u22f1\n\n\n\n\u22ee\n\n\n\n\n\u22ee\n\n\n\n\n\n\n\n\u22f1\n\n\n\u22ee\n\n\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n1\n\n\n\n]\n\n,\n\n\n\n\n\n\n\n\n\n\n(6)\n\n\n\n\n\n\n\nW\n\nk\nj\n\n\n=\n\n[\n\n\n\n\nw\n1\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nw\n2\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nw\n3\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nw\n1\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nw\n2\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u22f1\n\n\n\n\n\n\n\n\n\n\n\n\n\nw\n1\nN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nw\n2\nN\n\n\n\n\n]\n\n,\n\n\n\n\n\n\ng\n\u02dc\n\nj\n\n=\n\n[\n\n\n\n\n\ng\n\u02dc\n\n1\n0\n\n\n\n\n\n\n\ng\n\u02dc\n\n2\n0\n\n\n\n\n\n\n\ng\n\u02dc\n\n3\n0\n\n\n\n\n\n\n\ng\n\u02dc\n\n1\n1\n\n\n\n\n\n\n\ng\n\u02dc\n\n2\n1\n\n\n\n\n\n\u22ee\n\n\n\n\n\n\ng\n\u02dc\n\n1\nN\n\n\n\n\n\n\n\ng\n\u02dc\n\n2\nN\n\n\n\n\n]\n\n,\n\n\n\n\n\n\n\nb\na\ns\n\ne\n1\n\n\n\n\n\n\n\nb\na\ns\n\ne\n2\n\n\n\n\n\n\n\nb\na\ns\n\ne\n3\n\n\n\n\n\n\n\ns\ni\nt\n\ne\n1\n1\n\n\n\n\n\n\n\ns\ni\nt\n\ne\n2\n1\n\n\n\n\n\n\n\u22ee\n\n\n\n\n\ns\ni\nt\n\ne\n1\nN\n\n\n\n\n\n\n\ns\ni\nt\n\ne\n2\nN\n\n\n\n\n\n\n\n\n\n\n\nwhere \n\n\n\ng\n\u02dc\n\nj\n\n\u2243\nk\n\u22c5\n\n\ng\n\u02c6\n\nj\n\n+\nE\n\nT\nj\n\n+\nO\n\nL\nj\n\n\n and \n\n\n\n\u03c3\n\u02dc\n\nj\n\n\u2243\n\n\n\nk\n2\n\n\u22c5\n\n\n(\n\n\u03c3\n\ng\n\u02c6\n\nj\n\n)\n\n2\n\n+\n\n\n(\n\n\u03c3\n\nE\nT\n\nj\n\n)\n\n2\n\n+\n\n\n(\n\n\u03c3\n\nO\nL\n\nj\n\n)\n\n2\n\n\n\n\n are the gravity measurements at each benchmark and their errors, \n\n\nS\n\ni\nj\n\n\n\n is a sparse matrix describing the benchmark occupations, \n\n\nW\n\nk\nj\n\n\n\n is the diagonal matrix of weights \n\n\nw\nj\n\n=\n\n1\n\/\n\n\n\u03c3\n\u02dc\n\nj\n2\n\n\n\n, \n\nN\n\n is the number of benchmarks, and \n\nM\n\n the number of gravity measurements (or benchmark occupations) with \n\nN\n<\nM\n\n.\nOnce the average value is removed from all gravity measurements, the residual instrumental drift (Fig. 7) is given by the least square solution\n\n(7)\n\n\n\np\n\n1\n\u2212\n2,1\n\n\n=\n\n\n[\n\n\nM\n\n1\n\u2212\n2\n,\nj\n\n\n\nM\n\nj\n,\n1\n\u2212\n2\n\n\n\n]\n\n\n\u2212\n1\n\n\n\n[\n\n\n\ng\n\u02dc\n\nj\n\n\u2212\n\n\u03a0\n\nj\n,\ni\n\n\n\n\ng\n\u203e\n\ni\n\n\n]\n\n\n\n\nwhere \n\n\nM\n\nj\n,\n1\n\u2212\n2\n\n\n\n is a rectangular matrix whose first column is the time \n\n\nt\nj\n\n\n of the measurement and the second column is the unit vector, \n\n\np\n\n1,1\n\u2212\n2\n\n\n\n is a column vector whose first element is the instrumental drift rate and the second element models the instrument noise\n\n(8)\n\n\n\n\n\n\n\n\ng\n\u02dc\n\nj\n\n\u2212\n\n\u03a0\n\nj\n,\ni\n\n\n\ng\n\u203e\n\n=\n\n[\n\n\n\n\n\n\ng\n\u02dc\n\n1\n0\n\n\u2212\n\n\ng\n\u203e\n\n0\n\n\n\n\n\n\n\n\n\ng\n\u02dc\n\n2\n0\n\n\u2212\n\n\ng\n\u203e\n\n0\n\n\n\n\n\n\n\n\n\ng\n\u02dc\n\n3\n0\n\n\u2212\n\n\ng\n\u203e\n\n0\n\n\n\n\n\n\n\n\n\ng\n\u02dc\n\n1\n1\n\n\u2212\n\n\ng\n\u203e\n\n1\n\n\n\n\n\n\n\n\n\ng\n\u02dc\n\n2\n1\n\n\u2212\n\n\ng\n\u203e\n\n1\n\n\n\n\n\n\n\u22ee\n\n\n\n\n\n\n\ng\n\u02dc\n\n1\nN\n\n\u2212\n\n\ng\n\u203e\n\nN\n\n\n\n\n\n\n\n\n\ng\n\u02dc\n\n2\nN\n\n\u2212\n\n\ng\n\u203e\n\nN\n\n\n\n\n\n]\n\n\n\n\n\n\nM\n\nj\n,\n1\n\u2212\n2\n\n\n=\n\n[\n\n\n\n\nt\n1\n\n\n\n1\n\n\n\n\n\nt\n2\n\n\n\n1\n\n\n\n\n\nt\n4\n\n\n\n1\n\n\n\n\n\nt\n4\n\n\n\n1\n\n\n\n\n\nt\n5\n\n\n\n1\n\n\n\n\n\u22ee\n\n\n\u22ee\n\n\n\n\n\nt\n\nM\n\u2212\n1\n\n\n\n\n1\n\n\n\n\n\nt\nM\n\n\n\n1\n\n\n\n]\n\n\n\n\n\n\n\n\n\n\nGravProcess (Cattin et al., 2015) and PyGrav (Hector and Hinderer, 2016) employ a similar approach to eliminate the linear instrumental drift, i.e., the drift is estimated by a least-square fit of the weighted time series. DSAdjust (Kennedy, 2020) implements four different reduction schemes.\n\n\n4.4\nMean gravity measurements\nThe final adjusted relative gravity values \n\n\u0394\n\ng\ni\n\n\n is the solution of the weighted least square inversion\n\n(9)\n\n\n\n\n\n\n\u0394\n\ng\ni\n\n=\n\n\n[\n\n\nS\n\ni\nk\n\n\n\nW\n\nk\nj\n\n\n\nS\n\nj\ni\n\n\n\n]\n\n\n\u2212\n1\n\n\n\nS\n\ni\nk\n\n\n\nW\n\nk\nj\n\n\n\n(\n\n\ng\nj\n\n\u2212\n\ng\n0\n\n\n)\n\n\n\n\n\n\n\n\n\ni\n=\n1\n,\n\u2026\n,\nN\n\n\n\n\n\n\nj\n,\nk\n=\n1\n,\n\u2026\n,\nM\n\n\n\n\n\n\n\n\n\n\nwhere \n\n\nS\n\ni\nj\n\n\n\n is the sparse matrix from equation (5), \n\n\nW\n\nk\nj\n\n\n\n is the diagonal matrix of weights \n\n\nw\nj\n\n=\n\n1\n\/\n\n\u03c3\nj\n2\n\n\n\n, \n\n\ng\nj\n\n\n and \n\n\n\u03c3\nj\n\n\n are is adjusted gravity from equation (2), and \n\n\ng\n0\n\n\n is the average adjusted value at the base station. According to Gurland and Tripathi (1971), the errors \n\n\n\u03c3\ni\n\n\n can be estimated using the unbiased standard deviation\n\n(10)\n\n\n\n\n\n\n\n\u03c3\ni\n\n=\n\ns\ni\n\n\n[\n\n1\n+\n\n0.25\n\n\nN\ns\n\n\u2212\n1\n\n\n\n]\n\n,\n\n\n\n\n\ns\ni\n\n=\n\n\n\n\u03c7\nv\n2\n\n\n\u03a3\n\ni\ni\n\n\n\n\n,\n\n\n\n\n\n\n\n\nN\ns\n\n=\n3\n\n\n\n\nb\na\ns\ne\n\n\n\n\n\n\n\nN\ns\n\n=\n2\n\n\n\n\ns\ni\nt\ne\n\n\n\n\n\n\n\n\n\n\n\n\nThe chi-square per degree of freedom \n\n\n\u03c7\nv\n2\n\n\n and the covariance matrix \n\n\n\u03a3\n\ni\nj\n\n\n\n are given by the solution of (9).\n\n\n\n5\nPost processing\nThe runPostProc.m module takes the results from the previous gravity adjustment procedure (see section 4) to estimate the average adjusted relative gravity value \n\n\u0394\n\ng\ni\n\n\n at each benchmark for the survey and plot the scatter of the adjusted gravity measurements \n\n\u0394\n\ng\ni\n\n\n at each site, including uncertainties \u2013 see Fig. 9\n.\n\n\n6\nSoftware verification and limitations\n\n6.1\nVerification\nDue to the complexity of the mathematical models involved in the development and operation of gTOOLS, testing the software to detect existing subtle faults is critical (Farrell et al., 2011). We run four dynamic tests to verify the code:\n\n1.\nwe verified the Earth Tide correction (EarthTide.m) against the FORTRAN 77 routine tideg by Plouff (2000) \u2013 Fig. 6.\n\n\n2.\nwe verified the Ocean Loading correction (OCVLoading.m) against the synthetic tide routine of TSOFT (Van Camp and Vauterin, 2005) \u2013 Fig. 7.\n\n\n3.\nwe verified the theoretical model in (1) against experimental data (Fig. 3).\n\n\n4.\nwe verified the algorithm of the gravity adjustment module, runAutProc.m, using a synthetic data set based on equation (1), and the time stamps and benchmark locations of the 2016 survey of the gravity monitoring network of Mount St Helens (WA; Battaglia et al., 2018) \u2013 Fig. 9.\n\n\n5.\nfinally, we tested the entire code against GSadjust (Kennedy, 2020) and PyGrav (Hector and Hinderer, 2016) \u2013 Table 1\n.\n\n\n\n\n\n6.2\nLimitations\nThe present version of the software has one significant computational disadvantage compared to other software. The ocean loading correction employs the compiled version of the HARDISP Fortran code by Petit and Luzum (2010). The compiled version of HARDISP depends on the operating system. The version available in gTOOLS has been compiled under Windows and will not run under MacOS or Linux. GravProcess (Cattin et al., 2015) and PyGrav (Hector and Hinderer, 2016) employ translations into MATLAB (or Python) of the SPOTL code (Agnew, 2012).\ngTOOLS, GravProcess (Cattin et al., 2015) and PyGrav (Hector and Hinderer, 2016) only eliminate the linear instrument drift, while DSAdjust (Kennedy, 2020) uses four different reduction schemes.\n\n\n\n7\nGravity monitoring of Cotopaxi volcano\nRegarded as one of the most dangerous volcanic systems in Latin America, the monitoring of Cotopaxi volcano is done by the Instituto Geofisico de la Escuela Politecnica Nacional (IG-EPN) using infrasound, seismic and geodetic monitoring networks (Bernard et al., 2016; Gaunt et al., 2016; Hidalgo et al., 2016; Mothes et al., 2017). Cotopaxi experienced a new period of heightened activity in April 2015, which peaked during the eruptive activity of August 14, 2015 (Mothes et al., 2017). Summit ash emissions continued until November 2015. Monitoring parameters decreased until they returned to background levels in March 2016 (Hidalgo et al., 2018). Although several plausible sources for the unrest were inferred from modelling of monitoring data (Bernard et al., 2016; Morales Rivera et al., 2017; Hidalgo et al., 2018), and from petrological analysis (e.g., Gaunt et al., 2016; Troncoso et al., 2017), these techniques did not uniquely constrain sub-surface mass change. To constrain sub-surface mass movements, an initial plan for time-lapse gravity monitoring was implemented at Cotopaxi by installing 3 survey stations in June 2015. The gravity network was expanded in October 2015 with the addition of 5 survey stations and an additional far-off base station (site OVC; Fig. 10\n). Gravity measurements were performed bi-monthly during the heightened activity, while the frequency of measurements was decreased after the end of unrest in March 2016 (Calahorrano-Di Patre et al., 2019).\nSurveys at Cotopaxi were performed using a single CG-5 Scintrex relative gravimeter. Survey day loops were designed using the so-called \u201cstep method\u201d (Greco et al., 2012), where two or more stations, including the base station, were repeated at least twice in a day (e.g., stat1 \u2192\u00a0stat2\u00a0\u2192\u00a0stat3\u00a0\u2192\u00a0stat2\u00a0\u2192\u00a0stat1). To minimize the possibility of including data with systematic instrumental errors, several identical loops were repeated in one survey and considered independent from each other. The change between gravity differences in one survey was considered the final measurement error. Other environmental effects (such as pressure changes and temperature) were monitored during data collection. These effects were also minimized with the placement of an insulating foam box around the gravity meter during measurements along with the use of a strong portable windbreak. Finally, since the travel time between stations at Cotopaxi is relatively long (typically more than 2\u00a0h), it was not unusual to observe a hysteresis effect (intrinsic to the Scintrex CG-5, e.g., Klees et al., 2017; Seigel, 1995) in the gravity data. Therefore, at least 20\u00a0min of stabilization time (measurements not considered as valid in the final mean) were required for each hour of careful transport, after which an additional 15-min period of valid measurements were collected to detect any lingering relaxation or data tares.\nUntil July 2018, a total of 16 surveys were completed at Cotopaxi volcano. Data from these surveys were corrected and processed in its entirety using gTOOLS. Basic corrections to gravity data included removing tidal effects, filtering outliers, and calculating and reducing the daily instrumental drift. The final error for the calculated gravity differences was estimated using the standard deviation in the data, the residuals from the drift calculation, and the repeatability for each survey. Although normally time-lapse gravity data is corrected for height changes in each station due to ground displacement, the free-air gravity effect caused by inflation at Cotopaxi did not surpass instrumental error, and thus is not considered for the calculation of residual gravity. An example of data reduction step by step is presented in Table 2\n and Table 3\n. The results from the gTOOLS processing are visualized in Fig. 11\n, with station CAME as reference for the period before October 2015, and station OVC as reference after its installation in October 2015.\nTwo periods of gravity change, coinciding with volcanic activity, are shown in Fig. 10, notably June\u2013August 2015 (pre-eruption) and October 2015\u2013March 2016 (post-eruption). Although gravity changes are also observed in periods of inactivity (e.g., January\u2013September 2017), the rate of change is not comparable to that observed during and immediately after eruptive activity. For example, during the period of Oct 2015 to March 2016, a gravity decrease was observed in almost all measured sites except REF, with the largest change observed at station VC1 of \u221272 \u03bcGal. During the period of January\u2013September 2017, a gravity decrease was again observed at several stations at Cotopaxi (Fig. 10), with the largest change in this period of \u221235 \u03bcGal at station VC1. Therefore, the maximum rate of change was more than double during periods of volcanic unrest (\u223c15 \u03bcGal\/month) in comparison with periods of relative volcanic inactivity (\u223c5 \u03bcGal\/month). Gravity changes from the period of October 2015\u2013March 2016 were therefore interpreted as being related to volcanic activity and were inversely modelled to explore sub-surface mass movement post-eruptive activity. Because of a lack of coverage, it is not possible to invert model data collected before the eruptive activity and forward modelling of sources inferred from deformation data was inconclusive (Calahorrano-Di Patre et al., 2017). The post-eruptive gravity changes at Cotopaxi were inversely modelled using several magmatic sources, assuming both an increase and a decrease of sub-surface mass into the modelled source (Calahorrano-Di Patre et al., 2019). The best model contemplated the migration of hydrothermal fluids for a deeper annulus-like aquifer to a cylindrical shallow, perched aquifer (Fig. 12\n).\nWith the addition of the inverse modelling of the gravity data to the already existing information from geochemistry, petrology, and seismology we were able to propose a conceptual model for this phase of Cotopaxi's activity. (Fig. 12; Calahorrano-Di Patre et al., 2019): a A magma body (best modeled by a spheroidal source) intruded approx. 12 km b.s.l. and triggered the measured pre-eruptive deformation and seismicity. b) A smaller source of magma, inferred from SO2 and petrological data, ascended from a deeper reservoir and intermingled with the hydrothermal system, starting the eruptive activity. c) Hydrothermal fluids moved upwards from a deeper aquifer to a shallower level of the aquifer because of the source\u2019s heat, and this mass movement was detected by gravity measurements. d)\u00a0A new equilibrium was reached, and all geophysical signals returned to background levels (e.g., SO2 and seismicity), or stabilized at a new level (e.g., deformation and gravity).\n\n\n8\nSummary and conclusions\nA fast, easy to use and reliable program for processing gravity data is a valuable tool for volcano observatories and scientists since time-lapse gravity monitoring of active volcanoes provides information on sub-surface mass change in times of unrest. For example, gravity data from the unrest of Cotopaxi (Ecuador), Yellowstone (WY) and Laguna del Maule (Chile) have been processed in their entirety using the gTOOLS software. (Calahorrano-Di Patre et al., 2017; Poland and de Zeeuw\u2010van Dalfsen, 2019; Trevino et al., 2021). Although designed for volcano observatories, the software can be readily employed in any field that monitors sub-surface mass flow.\nThe program reads input data files from Scintrex CG-5 and CG-6, but the input module can be easily modified to read data files from other gravimeters. Data processing includes the removal of measurement noise, and correction for residual instrumental drift, ocean loading and Earth tides. The code can process both single and double loop gravity surveys and allows the automatic processing of relative gravity data from a campaign spanning multiple days in a single run.\nThe main limitation of the present version of gTOOLS is that the ocean tide loading correction employs the compiled version of the HARDISP Fortran code by Petit and Luzum (2010). The version available in gTOOLS has been compiled under Windows and will not run under MacOS or Linux. Furthermore, the code corrects the effect of daily, linear instrument drift.\nOn the other hand, the software is open-source, stable, thoroughly verified and tested, and can be downloaded from a public repository managed by the US Geological Survey.\n\n\nComputer code availability\n\n\n\n\u2022\nName of code: gTOOLS\n\n\n\u2022\ndevelopers and contact address\n\no\nMaurizio Battaglia, US Geological Survey, Volcano Disaster Assistance Program, PO Box 158, NASA Ames Research Center, Bldg. 19, 2nd floor, Moffett Field CA 94035. (650) 439\u20132629. mbattaglia@usgs.gov\n\n\n\no\nAshton F. Flinders, US Geological Survey, Hawaiian Volcano Observatory, 1266 Kamehameha Ave Suite A5 Hilo HI 96720. aflinders@usgs.gov\n\n\n\n\n\n\n\u2022\nyear first available: 2012\n\n\n\u2022\nhardware required: PC with 8\u00a0GB of RAM\n\n\n\u2022\nsoftware required: MATLAB 2020a or higher to run the open-source code; the free MATLAB Compiler Runtime R2021a (9.10) - Windows OS 64-bit for the compiled code\n\n\n\u2022\nprogram language: MATLAB (R2021a)\n\n\n\u2022\nprogram size: 30\u00a0Mb\n\n\n\u2022\nhow to access the source code:\n\n\u2192 we waive copyright and related rights in the work worldwide through the CC0 1.0 Universal public domain dedication.\n\n\n\u2192 https:\/\/code.usgs.gov\/vsc\/publications\/gtools\n\n\n\n\n\n\n\n\n\nLink to the code\n\nhttps:\/\/code.usgs.gov\/vsc\/publications\/gtools.\n\n\nAuthorship statement\nMaurizio Battaglia developed the code. Antonina Calahorrano-Di Patre applied gTOOLS to the gravity monitoring of the unrest at Cotopaxi volcano (Ecuador). Ashton F. Flinders verified the code and developed parts of the code. All the authors contributed equally in writing the manuscript.\n\n","101":"","102":"","103":"","104":"","105":"\n\n1\nIntroduction\nWith countless sensors deployed all over the globe, human knowledge about earth systems is growing explosively. Every day these sensors capture huge amounts of geolocated data to help us gain a deeper understanding of the natural environment, human society, and outer space. The information is critical to (1) learn and understand natural systems, (2) foresee trends and consequences of human activities, and (3) assess hazards to human society and the Earth. Despite numerous tools, methods, and theories, we are still incapable of efficiently and fully utilizing this huge data mine. Current theories about how the earth will respond to global change are full of unrealistic and subjective assumptions due to the manual configuration and handling of data.\nArtificial intelligence (AI) models have outperformed conventional data handling in many cases, like recognizing street views, extracting roads, and comprehending medical images. The first generation of AI research in the 1980s resulted in many classic theories and methods, but the earliest models took too long to train due to computing limitations. With the recent rapid development of hardware and software, AI has accelerated scientific advances and discoveries in medicine, biology, and economics.\nNowadays, AI is no longer a lab concept but used practically in many daily scenarios such as banking, camera object identification, telecommunications, household robot cleaners, recommendation systems, autonomous driving, self-checkout, etc. All of these applications depend on computer algorithms that digest information and solve problems by mimicking brain nervous systems. However, unlike human brains that can differentiate many objects by only deductively learning one object, AI algorithms must learn thousands of patterns before making accurate decisions (Qiu et al., 2016). Owing to the vital role big data plays in building AI, manipulating big data is critical to designing reliable AI-based workflows (Mayer-Sch\u00f6nberger and Cukier, 2013).\nGeoscientists led the development of tools bridging gaps between geoscientific data and AI models (Fig. 1\n). Here, we probed the modern computing workflows, storage needs, and revolutionary cyberinfrastructure for conducting AI research in geosciences. The breakthroughs in both theory and infrastructure will carry geoscience into the next phase: Earth Artificial Intelligence (Earth AI). We envision Earth AI to be a huge combination of systems to automatically monitor and forecast nature, help adapt human society to environmental changes, guide humans to make planet-wise policies and decisions, and protect us from geohazards. Earth AI will be a significant tool to confront grand challenges such as exploding population, food security, and climate change. This paper will overview the current status of Earth AI, list the grand challenges, and foresee the big opportunities in Earth sciences. Section 2 describes the popular AI techniques at present, and their applications in geosciences will be introduced in Section 3. Section 4 summarizes the generic steps in Earth AI workflows, and section 5 talks about the useful tools and services. Section 6 discusses the primary challenges Earth AI practitioners face and the opportunities coming along, and it is concluded in section 7.\n\n\n2\nAI techniques\nThe term AI, a buzzword used in so many different places, can be confusing for geoscientists. The scope of AI techniques is vastly bigger than the popular ones like machine learning (ML) and deep learning (DL). Generally, ML is a subset of AI, and DL is a subset of ML. Since it is impractical to cover the entire AI universe, this section will briefly introduce the milestone techniques that are widely used in geosciences.\n\n2.1\nKnowledge-based system\nBefore ML became viral, rule-based systems dominated data digesting and decision support techniques, and still perform critical data analysis today. Rule-based approaches rely on a set of rules, each depicting some contextual knowledge (Clancey, 1983), typically appearing as IF\/THEN expressions. For example, if the river reaches an action (flood) stage, the weather agency must take mitigation action in preparation for possible significant hydrologic activity (NWS, 2021). As the rules are common knowledge and contain less ambiguous judgment, rule-based systems have very good stability and certainty and are commonly seen in many industries.\n\n\n2.2\nProbabilistic machine learning\nProbabilistic ML offers a practical method for engineering machines that can evolve by learning realistic data (Ghahramani, 2019). Most ML models are using probabilistic theory to tackle uncertainty challenges. Probability theory can be utilized to express many forms of variances and noises and prevent excessive errors in prediction (Ghahramani, 2019). In ML, a probabilistic reasoner can infer the probability function given input data and eventually make predictions with control over uncertainty (Pearl, 1988).\n\n\n2.3\nUnsupervised learning\nUnsupervised learning searches for hidden patterns in a dataset with neither annotations nor intervention (Ferran et al., 2013). Different from supervised learning heavily subject to manual labels, unsupervised learning probes the general probability densities simply based on the inputs. One of the common examples is clustering analysis entrenched in Earth scientific analysis, e.g., geochemical sample grouping (Templ et al., 2008). The clusters are automatically grouped using distance metrics like Euclidean distance in a feature space and algorithms like K Means, Hidden Markov, etc.\n\n\n2.4\nSupervised learning\nMost current AI applications involve supervised learning which builds a transformer connecting outputs with inputs. It can be further categorized into two subtypes: regression and classification. Regression could output any continuous number in a range (such as atmospheric pressure, surface temperature, precipitation). Classification model outputs are limited to a collection of pre-fixed numbers. Supervised learning has an extensive method collection including K nearest neighbor (KNN) (Henley and Hand, 1996), Decision Tree (DT) (Safavian and Landgrebe, 1991), Support Vector Machine (SVM), Random Forest (RF) (Breiman, 2001), Artificial Neural Network (ANN) (Gurney, 2014), etc. Meta algorithms like Bagging (bootstrapping) (Breiman, 1996) or Boosting (i.e., AdaBoost) can be used to further advance accuracy and stability (Freund and Schapire, 1997).\n\n\n2.5\nDeep learning\nDeep learning (DL) refers to a powerful group of neural networks with more hidden layers and complex architecture compared to their ancestors (i.e. Multilayer Perceptron). DL can be used in supervised, unsupervised, and semi-supervised fashion. Deep convolutional neural networks (DCNN) are commonly used for feature extraction and dimensionality reduction (Krizhevsky et al., 2012). The power of CNNs in learning representation usually results in a better performance on prediction. However, superior performance comes with a limitation that DL is more data-hungry and its application is often limited to cases when large amounts of high-quality labeled data are available (Mousavi et al., 2019).\nAccording to data flows, DL can be generally bifurcated into two main branches: feedforward neural networks (FNN), and recurrent neural networks (RNN). The former is simple with information moving in one single forward direction. The latter has information moving in a circle, meaning the output of the previous step shall be inputted to the ongoing step. Each branch has numerous variants and forms a wide variety of advanced networks such as ResNet (He et al., 2016), U-Net (Ronneberger et al., 2015), PSP (Zhao et al., 2017), SegNet (Badrinarayanan et al., 2017), VGG-16, DenseNet (Iandola et al., 2014), YOLO (Redmon and Farhadi, 2018), R\u2013CNN (Girshick, 2015), Mask RCNN (He et al., 2017), DeepLab (Chen et al., 2017).\n\n\n2.6\nReinforcement learning\nReinforcement learning finds an optimal way to maximize a numerical reward signal (Sutton and Barto, 2018). The learning module must select actions by its own decisions to find the best path (not unique) with the most reward. It differs from supervised and unsupervised learning requiring neither training dataset nor finding hidden structure in collections of unlabeled data. A key feature is that it explicitly considers goal-directed problems by agents interacting with an uncertain environment and countless potential solutions. The term \u201cagent\u201d is not necessarily a real robot but could be a virtual program to explore data. Reinforcement learning is suitable for situations where it is unrealistic to retrieve data of desired behaviors that are both correct and holistic for all the possibilities that the agents might act.\n\n\n\n3\nExisting Earth AI research\n\n3.1\nGeosphere\nHuman population growth raises daunting challenges in requiring natural resources to sustain the population but increases vulnerability by exposing more people to natural (e.g. tectonic earthquakes, volcanos, landslides) and anthropogenic (e.g. induced earthquakes, dam failures) geohazards. Sustaining infrastructure in the face of these challenges requires a deeper understanding of these phenomena and the physical mechanisms behind them, provided by earth scientists. Although it is far from becoming fully realized, AI is now becoming widespread in all areas of geology, including the search for minerals (Saliu et al., 2020) and energy (Koroteev and Tekic, 2021).\nHere is an overview of major practices in applying AI toward this goal (Table 1).\n\n(1)\nEarthquake\n\n\n\nDespite their frequency and devastating consequences, much remains unknown about earthquake generation mechanisms and effects. Earthquake forecasting, the Grail of Seismology, has been a topic of interest for extensive applications of AI techniques. Feedforward (Lin and Chiou, 2019) and recurrent neural networks (Adeli and Panakkat, 2009) are among the most used ML approaches for this task. In these approaches, neural networks predict the magnitude and location of future earthquakes (Karas\u00f6zen and Karas\u00f6zen, 2020)- in a time or spacetime window - often based on the time series of previous earthquake characteristics such as occurrence time, magnitude, or focus location. Despite recent progress in developing advanced DL, there are still challenges as to how it will be effectively applied to AI-based earthquake predictions (Mignan and Broccardo, 2020). This is caused by the fact that most earthquake catalogs are recorded in plain tabular format and limited features are available for training more complex models. However, DL methodologies have accelerated the development of more reliable and efficient algorithms for earthquake monitoring (Mousavi et al., 2020). AI-based earthquake monitoring methods can result in advancing seismic hazard safety in two folds: by empowering Earthquake Early Warning (EEW) systems (Bose et al., 2008) with faster and more reliable estimations of earthquake parameters and by providing more complete and precise earthquake catalogs used for improving long-term seismic hazard assessments (Mousavi and Beroza, 2018).\n\n(2)\nVolcano\n\n\n\nIn volcanology, manual analyses of gas emissions, deformation measurement, and seismic signals have been used for decades to monitor, mitigate, and minimize risks associated with volcanic hazards (Tilling, 1989). A major application of AI in volcano monitoring is discriminating between seismic volcanic tremors and similar events including earthquakes, landslides, lava fountains, wind, and thunder. The successfully tested ML techniques include ANN (Scarpetta et al., 2005), SVM (Masotti et al., 2006), Hidden Markov models (Beyreuther et al., 2008), and Fuzzy Logic (Hibert et al., 2014). Short-period forecasting of sudden steam-driven eruptions can also be done using AI\/ML by detecting precursors from the streaming seismic data (Dempsey et al., 2020). The capability of AI in identifying the energy bursts happening from a few hours to several days ahead of large eruptions is enlightening and has proven that ML could issue life-saving short-term volcano alerts in future.\n\n(3)\nLandslides\n\n\n\nLandslides in mountainous areas cause billions of dollars in losses annually. AI applications in landslide studies have been mainly devoted to risk estimation efforts (Mousavi et al., 2011). Landslide susceptibility mapping has experimented with ML approaches like logistic regression (Umar et al., 2014), ANN (Nefeslioglu et al., 2008), and SVM (Peng et al., 2014). A set of control variables like land slope, vegetation cover, precipitation, soil mass, and hydrologic setting, are measured and used as ML inputs to calculate landslide likelihoods. Another group of AI applications is the automation of landslide identification on remote sensing (RS) imagery. For instance, CNN is evaluated in accomplishing automatic landslide detection in Nepal, concluding that CNN is \u201cstill in its infancy\u201d for landslide detection (Ghorbanzadeh et al., 2019). Accurately predicting the place and time of landslides remains a vital challenge (Korup and Stolle, 2014). Although our knowledge about the underlying mechanism of slope failure could be weaved into physics models, the inadequate high-resolution observation of soil and groundwater restricts us from effectively running the models or enhancing the precision. Input data quality and potential overfitting remain major issues influencing the accuracy of models in real-world forecasting scenarios. Nevertheless, data mining and ML methods are increasingly popular in addressing landslide forecasting.\n\n\n3.2\nHydrosphere\nHydrosphere research has greatly benefitted from AI methods and applications (Hu et al., 2018; Kratzert et al., 2018; Mo et al., 2019; Mohajerani et al., 2019; Naganna et al., 2019; Shen, 2018). This section will elaborate on three aspects: rainfall, surface water, and groundwater.\n\n(1)\nRainfall\n\n\n\nRainfall forecasting involves learning complex nonlinear patterns in the data. Methods proposed for rainfall forecasting include using the combinations of RNNs and SVMs (Hong, 2008; Lin et al., 2009) or Singular Spectrum Analysis (SSA) and SVMs (Sivapragasam et al., 2001). This multi-model approach was extended to include ANN, KNN, and radial basis SVM to forecast the daily or monthly precipitation (Sumi et al., 2012). Other examples include the use of convolutional LSTMs (Shi et al., 2015), RF to retrieve rainfall rates from optical satellite images (K\u00fchnlein et al., 2014), and the combination of ANN, SVM, and DT for short-term rainfall prediction (Ingsrisawang et al., 2008).\n\n(2)\nSurface water\n\n\n\nAI-based methods have been frequently exercised on modeling non-linear hydrological problems (Fathian et al., 2019; Yaseen et al., 2015). ML-based approaches like neuron-wavelet hybrid systems show similar performances for predicting streamflow (Anctil and Tape, 2004), monitoring coastal water quality (Kim et al., 2014), and discovering complex relationships between water level and discharge (Bhattacharya and Solomatine, 2005). FNN, generalized regression NN, and Fuzzy Logic are also helpful to populate the under-measured water-level data (Turan and Yurdusev, 2009). River researchers use ANN, adaptive network-based fuzzy inference system (ANFIS), and wavelet-coupled NN for predicting sediment load (Olyaie et al., 2015) and water level (Seo et al., 2015), and finding that ML techniques are more efficient. Coupled approaches like the ensemble of ANN, Bayesian, and Genetic Algorithms (GA) are tested and yield improvement (by 3\u201311%) (Perea et al., 2019). RNN like LSTM was used in discovering polluting substances in water (Wang et al., 2019b). Remote sensing data like Landsat 8 images provide rich data sources for ML to quantify concentrations of different surface water quality parameters (Sharaf El Din et al., 2017). Considering water-society research, ML models have been utilized successfully in forecasting water consumption around Indianapolis (Shah et al., 2018) and many other scenarios.\n\n(3)\nGroundwater\n\n\n\nAs groundwater is hard to measure at scale, AI-based algorithms are useful in deriving information and making predictions crucial for groundwater management. ML has successfully created ground water management maps (Barzegar et al., 2018), assessing risks of nitrate contamination (Nolan et al., 2015; Sajedi-Hosseini et al., 2018) and predicting groundwater levels (Sahoo et al., 2017). ML models including SVM, RF, and GA optimized random forest, can assess groundwater potential by locations (Naghibi et al., 2017). It noticed that RF outperforms classification and regression trees (CART) in large-scale nitrate concentration prediction (Knoll et al., 2019). Ensembled ML models are practical alternatives to sophisticated conventional models to perceive the subsurface water patterns. Regarding city underground water networks, ML (e.g., extreme learning machine - ELM) (Sattar et al., 2019) can help in estimating the potential failures on individual pipes to prevent future tragic events.\n\n\n3.3\nAtmosphere\nThis section highlights the progress of AI development in atmospheric phenomena. In addition to addressing the specific atmospheric geohazards below, AI is of growing importance in essentially all aspects of meteorology, especially for improving the skill and efficiency of numerical weather forecasting, and in assimilating and interpreting the huge amounts of data contained in weather satellite observations (Boukabara et al., 2021).\n\n(1)\nHurricane\n\n\n\nTropical cyclones (hurricanes, typhoons, etc.) are amongst the most costly of all the disasters (Klotzbach et al., 2018). ML was used to predict hurricane path and assess damage using reanalysis data (Giffard-Roisin et al., 2018) and satellite images (Cao and Choe, 2020; Yu et al., 2019). The damage annotation ML model achieved >97% accuracy for Hurricane Harvey. Time-series forecasting models like RNN and ConvLSTM can learn hurricane behavior and calculate trajectories (Alemany et al., 2019; Kim et al., 2019). Extensive experiments using 20 years of climate reanalysis data show that ConvLSTM has higher accuracy than other approaches. Other data sources like passive microwave satellite data are also used together with DL for monitoring tropical cyclones (Wimmers et al., 2019). To simplify the problem by removing small-scale low-impact events, DL has successfully detected only severe storms (Maskey et al., 2018). From the social impact perspective, some researchers used ML to rapidly identify hurricane-critical Tweets (Shams et al., 2019).\n\n(2)\nMeteorological Drought\n\n\n\nDrought is a complex natural hazard causing tremendous global economic, social and environmental damages every year (Wilhite, 2016). Efforts have applied ML for drought prediction in Africa (Belayneh et al., 2016), Australia (Deo and \u015eahin, 2015), the USA (Agana and Homaifar, 2018), and China (Chen et al., 2012). Some studies used ML to predict drought indicators (Sutanto et al., 2019), such as SPEI and SPI (Belayneh and Adamowski, 2012; Maca and Pech, 2016) and estimate drought severity at ungauged sites (Sadri and Burn, 2012). ML-powered a high-resolution drought forecasting model using remote sensing data (Rhee and Im, 2017). On product processing, different ML methods are compared in downscaling hourly reanalysis precipitation to monthly data, and relevance vector machines work best (Sachindra et al., 2018).\n\n(3)\nWildfire\n\n\n\nWildfires are increasing in many countries, imposing adverse effects on human health and the economy. Early fire detection and intervention are vital for wildfire damage minimization. Various AI\/ML methods have been applied to improve fire detection and prediction (Jain et al., 2020), classify and map wildfire severity (Brewer et al., 2005), and automatically detect wildfires on UAVs or satellite images (Zhao et al., 2018). High-profile studies used AI in improving smoke plume forecasting combining ML with satellite (e.g., CALIPSO) observations (Yao et al., 2018) and infer ozone expansion and distribution (Watson et al., 2019). Other applications include identifying wildfires on RS images (Sayad et al., 2019) and assessing human health issues connected to poor air quality (Reid et al., 2016). Meanwhile, scientists use ML to trace human-caused wildfires and found RF is currently the most accurate among those tested (Rodrigues and de la Riva, 2014).\n\n(4)\nDust storm\n\n\n\nDust sources are associated with multiple health effects and socioeconomic impacts, including infectious diseases (Tong et al., 2017) and highway safety (Ashley et al., 2015). ML is increasingly used to detect dust sources, transport, and wind erosion susceptibility at various scales (Boloorani et al., 2022; Gholami et al., 2021; Lin et al., 2020). ML was utilized in inverse emission modeling to improve accuracy and outperformed a traditional chemical transport model (Jin et al., 2020). A Dust Source Susceptibility Map (DSSM) was developed using RS and ML to show dust sources 2005\u20132016 in Iran (Boroughani et al., 2020). Various ML models were benchmarked to investigate soil susceptibility to dust, finding RF performs best (Gholami et al., 2021). On a global scale ML is still applicable (Lee et al., 2021).\n\n(5)\nAnthropogenic Air Pollutants\n\n\n\nAir pollution is associated with over seven million premature deaths each year (WHO, 2021). A majority of them stem from exposure to O3 (ozone) and PM2.5 (fine particles). However, the ever-changing dynamics make it extremely difficult for computer models to predict air quality. AI has been involved to address these challenges, particularly for predicting O3, PM2.5, and nitrogen oxides, a precursor chemical that contributes to the formation of O3 and PM2.5 (Nowack et al., 2018; Wang et al., 2003; Wu et al., 2017; Zhang et al., 2012). Earlier works often utilize neural network methods to improve air quality forecasting (Abdul-Wahab and Al-Alawi, 2002; Kolehmainen et al., 2001; Ruiz-Suarez et al., 1995). Recently, more advanced ML algorithms are used to enhance O3 and NO2 prediction and SVM is better than NN in predicting daily maximum O3 concentrations (Chelani, 2010). For small-grain air quality forecasting, DL can complete common tasks like mosaicking, inserting missing values, or selecting features (Du et al., 2018; Fan et al., 2017; Qi et al., 2018).\n\n3.3.1\nBiosphere\nThe biosphere represents the living parts of the Earth system. This section briefly introduces the status of AI in life sciences under three themes: plant, animal, and microorganism.\n\n(1)\nPlant (Botany)\n\n\n\nPhytogeography, the study of plant distribution, is an active area in Earth AI research, and using RS imagery and ML, especially DL, has become the mainstream technique due to the low cost and the high accuracy of ML classification. AI-derived maps are proliferating in biogeographic studies. DCNN, trained on a public dataset of leaves to distinguish fourteen crops and twenty-six diseases, can achieve 99.35% accuracy (Mohanty et al., 2016; Sun et al., 2019a). Agriculture has many profound use scenarios for AI like disease detection, crop yield prediction, and irrigation recommendation (Kamilaris et al., 2017). Coupled RNN-CNN model can predict corn yield in the midwestern U.S (Sun et al., 2019b) and can be a low-cost reliable alternative in guiding irrigation (Vij et al., 2020).\n\n(2)\nAnimal (Zoology)\n\n\n\nAdvances in sensing technologies provide big data of animals like GPS and video surveillance. Together with data manually collected by professionals and citizen scientists, a huge dataset exists on wild animals' location, movements, behaviors, and well-being. Similarly, big data is becoming a norm in animal agriculture (Neethirajan, 2020). Based on these datasets, the application of AI in zoology focuses on detecting, counting, and describing animals and their behavior from images. DL has been proven efficient in recognizing wild animals on camera-trap imagery (Chen et al., 2014), attributing wildlife behaviors (Norouzzadeh et al., 2018), detecting ultrasonic calls of bats (Mac Aodha et al., 2018), and projecting diving of cormorants (Browning et al., 2018). For urban animals, DL can analyze city audio data (Fairbrass et al., 2019) and animal trajectories (Maekawa et al., 2020). However, despite progress, AI in zoology is still in an experimental phase and hasn't fully penetrated the zoological community.\n\n(3)\nMicroorganisms\n\n\n\nSimilar to zoology, AI is intensively studied in microbiology (Egli et al., 2020). DL has identified 30 common bacterial pathogens (Ho et al., 2019), detected pathogenic bacteria in food and water on time-lapse holograms (Wang et al., 2020a), and achieved an overall accuracy of 99% for 80-diatom classification (Kloster et al., 2020; Pedraza et al., 2017). DL-driven workflow can automatically recognize microscopic images of viruses, bacteria, fungi, and parasites (Zhang et al., 2021). Scientists also use AI in predicting the evolution of microorganisms, estimating optimal growth temperature for bacteria, archaea, and microbial eukaryotes, (Li et al., 2019), and predicting sgRNA activity in Escherichia coli (Wang and Zhang, 2019). However, since ML requires a lot of work to obtain adequate training labels, pre-trained models can be repurposed to classify environmental microorganisms to lower the cost (Kosov et al., 2018).\n\n\n3.3.2\nCryosphere\nPolar science studies the Earth's frozen zones, which are more highly subject to environmental changes than the planet as a whole. Despite years of efforts on modeling, precisely forecasting changes and consequences is still an unsolved challenge for the cryosphere community.\n\n(1)\nSea Ice\n\n\n\nAI\/ML has been used to map the ice shelves in Antarctica from Sentinel-1 (Baumhoer et al., 2019), estimate Arctic sea ice thickness (Tiemann et al., 2018), and evaluate its melting speed on SAR images (Lee et al., 2016; Wang et al., 2016) and distinguish water from ice (Leigh et al., 2013). It can help identify ages\/types of sea ice as radar backscattering signals of sea ice are composed of scattering from both the rough surface as well from underneath ice according to radar signal penetration (Ghanbari et al., 2019; Lohse et al., 2019; Park et al., 2020). GNSS images and ML can be harmonized for sea ice detection (Yan and Huang, 2018). The ambiguous connections between microseisms and sea ice activities are also suitable for AI\/ML (Cannata et al., 2019).\n\n(2)\nSnow\n\n\n\nSnow research has two main indicators: snow water equivalent (SWE) and snow depth; both can be monitored and forecasted by AI\/ML with decent reliability (Holt et al., 2015; Wang and Zhang, 2019). SVM-derived snow depth products from microwave satellites can pass the validation tests by stationary observation with higher precision while effectively suppressing the saturation effects (Xiao et al., 2018). Advanced DL methods such as deep residual networks show excellence over RF, SVM, and NN in snow detection from satellite imagery (Xia et al., 2019). Meantime, AI\/ML is intensively experimented to differentiate snow from cloud at the pixel level (Zhan et al., 2017).\n\n\n\n3.4\nOceanography\nThe turbulent ocean contains small-scale eddies that imprint on oceanographic observables like sea surface height (SSH), color, roughness, and temperature (SST). Identifying these features with ML is a hot study area. Oceanic mesoscale eddies (\u223c300\u00a0km diameter) are usually identified by physics-based algorithms and previous seminal work produced an eddy database (Chelton et al., 2011) as a robust benchmark for ML. So far, CNN has been used in eddy identification with SSH (Franz et al., 2018; Santana et al., 2020), SAR images (Du et al., 2019; Huang et al., 2017), high-frequency radar (HFR) data (Liu et al., 2021) and SST images (Moschos et al., 2020).\nSAR provides unprecedented detail of ocean surface roughness at fine resolutions (\u223c10\u201325\u00a0m). With higher-quality Sentinel-1 succeeding the earlier Radarsat-1 and Envisat missions, ML efforts are increasing on SAR ocean imagery to identify and map many surface features beyond eddies (Wang et al., 2019a). Submesoscale eddies (on the order of 5\u201330\u00a0km diameter) are more fully captured on standard SAR imagery in coastal regions under low to moderate wind speeds due to multiple dark, curvilinear slicks within each eddy. An early application of ML in SAR ocean detection was mapping oil spills arising out of petroleum seeps (Garcia-Pineda et al., 2009, 2013).\nSatellite ocean surface observations are intrinsically gappy due to cloud cover or sparse ground tracks such as the conventional nadir altimeters and upcoming SWOT (Durand et al., 2010) altimeter mission. AI\/ML can address the gappy issue in synthetic SWOT SSH data demonstrating the feasibility of AI-based interpolation algorithms in filling gaps containing small-scale ocean eddies (Manucharyan et al., 2020). One step further, the CNN-based algorithm can be applied to reconstruct fluxes induced by those eddies (Bolton and Zanna, 2019; George et al., 2021). These algorithms will be useful in parameterizing eddy fluxes not resolved in coarse resolution climate models.\nSince ocean circulation is three-dimensional, AI-based algorithms can also retrieve deep-ocean information based on surface satellite fields (Ali et al., 2007; Cheng et al., 2021; Wang et al., 2021). Other methods include a self-organizing map (Chapman and Charantonis, 2017; Wu et al., 2012), CCN (Han et al., 2019), neural net with fruit fly optimization algorithm (Bao et al., 2019), and RF (Su et al., 2018).\nOceanography is transiting from a state of data scarcity to a state of extreme data abundance. How to utilize a sea of data on a scale of petabytes and distill useful information for either new scientific discoveries or applications of a direct societal impact on the \u201cblue economy\u201d is a new challenge for the community (Watson-Wright and Snelgrove, 2021). AI-based algorithms will foreseeably play a compelling role in the transition.\n\n\n\n4\nWorkflow\n\n4.1\nData preparation\nIn most supervised ML research, a training dataset includes two components: input observations and associated labels. Inputs are fully observed and cyclic data sources like RS images, stationary data, model simulations, etc. Output variables are usually less-observed but critical for understanding Earth system processes, such as emissions, land cover, soil moisture, etc. Several problems arise in the process:\n\n(1)\nTime Series\n\n\n\nThe time axis is a fundamental characteristic of Earth data for trend analysis and forecasting. Earth observations are discrete sequences of numbers (e.g., samples per second, minute, hour, etc) in which data gaps and time-varying noise are common. Bandpass filtering, down sampling, up sampling, detrending, interpolation, and smoothing are commonly applied to preprocess time series data.\n\n(2)\nFormat\n\n\n\nAlmost every major data provider or professional software has an exclusive self-defined format. For example, HDF is the official format in NASA, NetCDF is commonly used in NOAA and climate communities, and GeoTiff is popular for georeferenced imagery. Furthermore, each format has various versions that might cause compatibility issues in I\/O programs. Libraries like GDAL\/OGR and NCO could address these problems. However, disparate formats still create a headache in aggregating multiple source datasets, requiring extra effort.\n\n(3)\nProjection & Grid\n\n\n\nMultisource datasets usually have various coordinate systems. NASA products use Sinusoidal projection, netCDF uses a 4-D Grid space system, OpenStreetMap uses EPSG:5070, and many public datasets use WGS84 (EPSG:4326). To integrate data from different sources within the same region\/location, data needs to be re-projected or re-gridded into the same coordinate system. Any displacement can result in erroneous misleading conclusions. GDAL, Proj4, ESMF Regridding Toolkit, are common tool solutions for re-projection and re-gridding.\n\n(4)\nMetadata\n\n\n\nMetadata is an important part of data acquisition and sharing. By providing information like naming conventions, variable units, resolution, projection, observation time, contact information, and data file versions in a comprehensive and standardized manner, can potentially enable more efficient reuse of datasets. However, if metadata is not standardized, the underlying datasets may be misused if users are unfamiliar with the data or don't fully understand the provenance of the data contained within files (e.g., that precipitation is reported in inches or centimeters) (Mons 2020). A recent survey suggests that most researchers do not use or are unfamiliar with metadata standardization protocols for their disciplines (Tenopir et al., 2020).\n\n\n4.2\nModel building\nBuilding an appropriate ML model for a specific problem in Earth sciences is tricky, requiring much comparison and experimentation. Specialists must gain expertise with several models and compare their performance characteristics before choosing one best meeting their objectives.\nAs an example, given a problem description, there is no generic methodology to assess a priori about the optimal setup of neurons and layers for an ANN model. A common approach starts with a rough guess based on prior experience about networks employed on similar problems. This supposition could be user's experience, or second\/third-hand experience learned from a training course, blog, or research paper. At that point, the researcher may try some variations and carefully assess the performance of the model before deciding on a strategy. The size and depth of neural networks interact with other hyperparameters and changing one variable can affect the other hyperparameters. A simple stepwise guide is:\n\n\u25cf\nCreate a network with hidden layers of similar size to the input.\n\n\n\u25cf\nTry varying network widths and depths.\n\n\n\u25cf\nTry dropping out some nodes and other solutions (e.g., dropout, learning rate decay, regularization, optimization algorithm, loss function, etc).\n\n\n\u25cf\nAfter a few adjustments, settle on an overall better model.\n\n\n\nUsers shouldn't get lost in tuning ML models as there will always be better models. Exploring the data helps form a reasonable expectation of accuracy. Attempt simple linear approaches first to create benchmarks to surpass. Considering a different ML algorithm may be mind-changing, faster, and more effective than your original pick.\n\n\n4.3\nTraining, testing & validation\nMost ML models need three datasets: training, validation, and testing. In practice, the overall dataset is first fractionated into the learning dataset and test dataset. The learning data is further split into a training dataset and validation dataset. Training datasets are used to fit the model. Validation datasets provide a real-time evaluation of the model during training. Test datasets provide an out-of-box evaluation of the final model. There is no fixed optimal ratio to allocate the three datasets. To ensure the model is unbiased, the splitting is repeated N times, and the accuracy is averaged, which is called N-fold cross-validation.\n\n\n4.4\nSensitivity analysis\nSensitivity analysis is a series of methods used to quantify ML uncertainty. It studies the feature importance of each input variable for the outputs. To measure the influence of each input variable, a comparison is made on model outputs with all variables in place and the model with one variable excluded or fixing the values of all other variables, only tuning the weight of one input factor to discover how the model output changes. Sensitivity analysis is mandated for practical use of ML in the real world; it explicitly reveals the level of dependence of model output on each variable, and hands more control to practitioners, especially when the new observations are extreme events and could be extra outliers exceeding the prediction capacity of models.\n\n\n\n5\nTooling and services\nThe big data nature of Earth science and the high complexity of AI algorithms demand powerful computing. This section overviews popular hardware and software for Earth AI.\n\n5.1\nComputing device\nCommonly used ML devices are Central Processing Unit (CPU), Graphics Processing Unit (GPU), Field-Programmable Gate Array (FPGA), and specialized accelerators (e.g., TPU - Tensor Processing Unit). GPUs are dominant due to their performance in speeding up the calculation of convolution and matrix operation. In DL the weights are updated in every cycle and are stored in a memory or local cache to be carried over from iteration to iteration. GPUs have higher memory bandwidths than CPUs and are optimized for more intensive workloads and streaming memory models.\nIn addition, scientists actively explore the next revolution in AI computing. After R. Feynman proposed the idea of a quantum computer, quantum computing is believed to be the next potential big breakthrough by producing the statistical patterns that are computationally difficult for a classical computer to produce (Biamonte et al., 2017; Deutsch, 1985; Feynman, 2018). Edge computing is another way around by leveraging the Internet of Things (e.g., endpoints, gateways, smart watches, smartphones, sensors, etc) with embedded AI techniques to process data locally without transmitting much data, which can reduce reliance on networks and increase the AI's resilience and practicality (Li et al., 2018).\nIndividual researchers can set up their workstations by assembling GPUs into a computer. Research groups and institutes can purchase more powerful pre-built servers configured by professionals. Self-maintaining workstations cost less if the experiments will last long. However, maintainers are required to build and sustain the rig. They need to find appropriate GPUs, compatible motherboard, CPU, and memory and fix any problems observed like GPU collapse, memory leak, disk failures, etc. This solution is suggested for people with experiences on servers.\n\n\n5.2\nCyberinfrastructure\nManipulating large-scale high-resolution Earth datasets requires massive computational power beyond the capacity of personal computers or even self-built DL workstations. Private companies with large computing power have developed some public cyberinfrastructure as the ultimate solutions. One typical example is Google Earth Engine (GEE) (Gorelick et al., 2017), which has digested petabyte-scale archives of publicly available RS imagery and model-simulated data. It optimizes Google's computational infrastructure for the parallel processing of geospatial data. Utilizing provided APIs with basic ML algorithms in Javascript and Python, GEE has powered many breakthroughs in RS-based Earth scientific researches like natural resource management, climate change monitoring, and disaster prediction and evaluation (Amani et al., 2020; Campos-Taberner et al., 2018; Tamiminia et al., 2020).\nTo exercise AI techniques on GEE, Colab (Bisong, 2019), a Jupyter-notebook-like interactive coding environment, can be used to program deep neural networks or other complicated ML models. Colab allows people to write and execute Python in web browsers with zero configuration required and easy sharing. With Colab, Earth scientists now can work with large datasets, build complex AI models, train them at lower cost and share the results seamlessly with others.\nAs a major competitor to the GEE ecosystem, Amazon is developing AI capability rooted in its AWS (Amazon Web Service) ecosystem. SageMaker (Januschowski et al., 2018) is their recent product and advertised as a managed web service to create and deploy ML models faster. SageMaker could be considered as an AutoML solution for scientists who are less technical and want less coding.\n\n\n5.3\nSoftware\nThe recommended operating system is Linux-derived systems with active long-term technical support. At present Ubuntu is the bellwether with many built-in dependencies for AI. It is easy for users to install GPU drivers, like CUDA (a software allowing coding for NVIDIA GPUs), and Python package manager (i.e., Conda, Pip) can facilitate the package installation. To interact with the machines remotely, Jupyter server (Kluyver et al., 2016) (either of notebook, Lab, or Hub) is highly recommended. It allows Earth scientists to create and share their experiments, from codes to full result reports in one single document to streamline their work and enable more productivity and easy collaboration.\nThe dominance of Python in the AI world is largely credited to its thriving, openly accessible, and pro-collaborative library ecosystem. Table 2\n lists some widely used open-source libraries. Generally, those tools can be categorized into six types: DL, non-DL ML, non-ML AI, data manipulation, parallel computing, and visualization. These tools play a significant role in recent scientific breakthroughs, i.e., plotting the first blackhole photo (Numpy, 2020), confirming the existence of the gravitational waves (Biwer et al., 2019), the mission to fly a helicopter on Mars (Vaughan-Nichols, 2021), etc. Many tools are for processing Earth scientific datasets, such as Rasterio, Shapely, Geopandas, ESMPy, which make the infusion between Earth science and AI techniques possible.\n\n\n\n6\nChallenges and opportunities\nThis section highlights some major challenges and potential opportunities (shown in Fig. 2\n).\n\n6.1\nModel development\nModel development is the process of choosing one suitable model or customizing a coupled model for one or multiple training datasets. Candidate off-the-shelf models include single models such as Neural Network, SVM, and Decision Tree, as well as ensemble models like RF, XGBoost, and most DL models. Finding optimal models or coupling new models is time-consuming and might never be satisfactory, which created a strong demand for AutoML that does not require expert knowledge or manual tuning. For example, OptiML, AutoScikit-learn, and AutoWeka use Bayesian parameter optimization for predicting the model's performance on a given dataset, assuming the performance of an ML algorithm is data-dependent. For instance, OptiML, after automatically trying a few models, can learn a regression model to predict the performance of other not yet tested models to save time. Auto-sklearn's hyperparameter tuning also uses Bayesian optimization, meta-learning, and ensemble construction. However, unsolved serious issues remain. First, the best metrics used for selecting models should be different according to various use cases. Second, the cross-validation technique performs poorly on big data training. Third, performance on accuracy should not be the only factor: stability, reliability, computational cost, and generalizability are all very important and often overlooked in seeking solutions.\nA good AutoML solution should automatically produce a model addressing all the concerns on scenario adaption, big data, and comprehensive metrics besides accuracy performance. The shortage of ML experts in industry and academia has been widely acknowledged, yet highly skillful ML experts are rare to find and hard to train. AutoML can bridge that gap and could derive many new opportunities in the AI job market, including Earth science. With AutoML, model selection would be easy and quick, and the barrier of shopping around ML models will be greatly reduced. AI-powered value-added services would no longer be the privilege of tech giants. Small groups will also be able to quickly put solid models together to simulate the real world, extract actionable information, and guide climate and environmental policy making. New doors will be opened for the next generation of Earth AI.\n\n\n6.2\nData preparation\nThe majority of an Earth AI project is typically spent on data preparation. Acquiring a large-scale labeled dataset in Earth science is very costly as labeling is usually manually done by in-house labor. A popular tactic is to crowdsource hand labeling tasks using services like Amazon Mechanical Turk. Despite unprecedented amounts of data to analyze, a lack of openly available curated and labeled training data is an obstacle to realizing efficient AI in the earth sciences (Maskey et al., 2018; Reichstein et al., 2019). Standardized training and testing datasets launched the AI revolution in other disciplines (e.g., imagnet, MNIST), yet training datasets capturing the diversity of geoscience data are being developed, and where they exist are heavily used. For example, Spacenet, an online hub for satellite imagery, algorithms, and tools, provides RS data with labeled information for ML. This leaves inexperienced modelers with the time-consuming and difficult task of locating, integrating, and labeling disparate datasets. Other times earth scientists must go outside of their domains to train their models. Right now, the incentive structure has scientists focused on \u2018building a better algorithm\u2019 rather than curating datasets (Hutchinson et al., 2021).\nWith more data producers, repositories, and publishers embracing calls for FAIR data, community-developed data standards (Sansone et al., 2019) are being developed where no international standards exist. OGC standards have been developed by international members to make geospatial information and services FAIR. There is a movement in the Earth and Environmental Sciences to create libraries of standardized and benchmark datasets (ESIP, 2021). These benchmark datasets can be used to efficiently evaluate how newly developed algorithms perform compared to already existing models on a common, standardized dataset. Standardization of benchmark datasets can lift data curation burdens by offering ready-to-use data for modelers (Reichstein et al., 2019).\n\n\n6.3\nTraining optimization\nTuning AI models is an essential but painful experience step for many beginners. It is a process of adjusting hyperparameters to minimize the cost function. Optimizers are algorithms for changing the attributes like weights and learning rates to lower the losses. Commonly used optimizers include Gradient Descent, Nesterov's Accelerated Gradient, Adaptive Moment Estimation, AdaDelta, etc. One common challenge of gradient-based optimizers is that most found minimum points are local minima. The global minima are hard to locate as the gradient becomes smaller when the training goes further and the learning rate is too large to get closer to the right answer. Another way is the genetic algorithm that applies the theory of evolution to ML. The process is repeated many times and only the best models survive at the end of the process. All the optimization methods have flaws. No one-size-for-all method can adapt to any dataset and speed up the learning to reach minima faster. An ideal ultimate solution should make the training quickly converge to the point with minimum loss within fewer iterations\/epochs. The gradient vanishing problem (the gradient is too small to update the weight in the next loop) should be well addressed.\n\n\n6.4\nParallel computing\nParallel computing, which improves the efficiency of AI training and running, is a valuable tool in Earth AI. The first reason is the ever-increasing size of available Earth data due to advances in both RS techniques and numerical Earth simulation. For example, total available climate data may increase exponentially from 100\u00a0PB in 2020 to about 350\u00a0PB in 2030 (Overpeck et al., 2011). The second reason is the increasing complexity of AI models. Advances in ML models, especially DL models, are more and more complex to achieve prediction accuracy. For instance, the Turing natural language generation model from Microsoft has 17 billion parameters. Because of the two reasons, it could take weeks or even months for a complex AI model to train without parallelization (Johnsirani Venkatesan et al., 2019).\nThere have been many efforts on studying how to support parallel ML from different perspectives. (Verbraeken et al., 2020; Wang et al., 2020b). We summarize three opportunities for parallel ML below, the first general to all ML tasks, the second and third unique to Earth AI. The first opportunity is the requirement of developing a unified system combining parallel hyperparameter tuning and parallel deep model training. Currently, these two tasks are often done via different systems, for example, Spark for parallel hyperparameter tuning and Tensorflow supporting parallel DL. A more integrative way\/platform supporting both efficiently is still needed. The second opportunity is the support for parallel learning on top of array-based Earth system datasets including HDF and NetCDF. Xarray and Dask are recent community efforts on accessing\/processing HDF and NetCDF datasets efficiently. But it is still not clear how to integrate these techniques with machine\/DL. The last opportunity is parallel ML support for spatiotemporal data, typifying Earth system datasets. Unlike traditional independent and identically distributed (IID) datasets, partitioning spatiotemporal data would break their spatial\/temporal correlation and dependence. Therefore, special attention should be considered for parallel ML with spatiotemporal Earth data.\n\n\n6.5\nExplainable AI\nCompared to basic or tree-structured ML models (e.g. linear regression, DT, Bayesian, RF), complex ML models (e.g., DNN, SVM) cannot provide a self-explainable theory for their results. Many Earth scientists called for adding explanation into ML models to facilitate understanding of the ML models and build user trust. Explainable AI (XAI) tools provide a way to look into the original \u201cblack-box\u201d model with \u201cexplanations\u201d providing a qualitative understanding of relationships between model features and predictions. This process answers questions about the model, such as what features are the most important and why some features are more responsible for driving decisions than others. It also provides insights allowing for meaningful changes to the models. An overview of common explainable methods can be found in Molnar et al. (2020). Decisive factors in selecting XAI methods may include the need for model-agnostic or model-specific methods, the extent of the explanation required, and spatiotemporal or computational constraints.\nLimitations of current XAI methods include that they cannot tell the problems in the training dataset, and they focus on RGB images and are user-friendly for high dimensional images (Krishnan, 2019). Despite the problems, opportunities exist with XAI for improving geoscientific models. Artifacts that create errors in numeric models could be revealed by XAI.\n\n\n6.6\nGeneralization\nThe conventional goal of generalization is to make trained AI models perform better on the test data. However, it becomes complicated as the Earth dataset is tremendous and the training dataset is only a tiny portion. In Earth AI, it is no longer simply finding a balancing point between overfitting and underfitting: models trained in one place at one time may not apply in another place at another time. However, a root cause of common AI failures is that current empirically trained models do not generalize well on new samples with different distributions. Finding a good generalization strategy to make models fit beyond the training dataset is a major bottleneck for applying AI in Earth science. The developing field of generalization theory may hold promise in solving these problems.\nAI generalization has been studied for decades. Ockham's Razor principle (Ariew, 1976) proves the less complex a model is, the more likely a good empirical result is not just due to the peculiarities of the chosen samples. The edges between under-learning and over-learning the training samples are obscure. One of the classic methods to detect underfitting or overfitting is to separate samples into two parts: training subset and testing subset. During each iteration in the training, the program will run the trained model on the testing subset to calculate the prediction accuracy on samples that are outside the original training pool. If the accuracy of testing data starts to gradually decrease, it means the model is overfitting. On the contrary, if the testing accuracy hasn't reached the peak, it means the model is still underfitting. A method is needed to find a balance between bias (underfitting) and variance (overfitting). One common solution is cross-validation to ensure no coincidental training bias is in place. Regularization is another technique used to make the learning algorithm generalizes better. It focuses on reducing the impacts of noise samples that don't reflect the real characteristics of the dataset, but random errors and coincidences. It discourages training a more sophisticated model to reduce the risk of poor generalization. Dropout is a recently proposed approach dedicated for neural networks to randomly drop units to force the subsequent layers to rely on all their connections to previous layers. However, no method can avoid intensive endless tuning to optimize the model with better generalization.\nAn attractive feature of Artificial Intelligence is that model performance will improve when a model is fed with larger datasets. However, it will eventually reach some limits posed by the model capacity that is capable of learning. Many DL models are over-parameterized and likely to become biased after learning more noise samples. Addressing the generalization problem will make AI models of the Earth system much more stable and noise-proof in a long-time operational run. A future solution would be to run an automatic algorithm to self-adjust in adopting samples by judging their quality. Those samples which might destabilize the model should be automatically given less consideration in the propagation and their impacts on the future updating should be reduced.\n\n\n6.7\nUncertainties\nML models are fundamentally algorithms composed of a set of rules, which involve random number generation and optimization to determine model parameters. Therefore, ML models developed on the same dataset are almost always different. The uncertainty of ML applications is a combination of uncertainties from two sources: data and knowledge. The uncertainty associated with the inherent noise of the real data is also known as aleatory uncertainty, which is not caused by the model but irreducible (H\u00fcllermeier and Waegeman, 2021). The uncertainty caused by inadequate knowledge and data is also called epistemic uncertainty, which is often a result of the mismatch between the data in model training and prediction.\nTo quantify the aleatory uncertainty, we need to estimate the uncertainty of all the inputted data of ML models and understand how uncertainty propagates through the model. This can be challenging for DL models because of the high model complexity. A small permutation in input data for a DL model can lead to notable changes in final model outputs. The epistemic uncertainty is related to the issue of generalization. Most ML applications are developed based on a specific set of data, thus the model may not be easily generalized to other conditions that are not covered in the original dataset. Because of the lack of representation in the original data set, it can be very challenging to accurately quantify the uncertainty related to generalization.\nAccurate uncertainty quantification is essential to enhance users\u2019 trust and increase the usability of ML applications. To address uncertainty quantification (UQ), many statistical and computational methods have been proposed. The most commonly used methods can be grouped into two categories \u2013 Bayesian UQ and ensemble UQ. Bayesian UQ approaches focus on approximating the posterior probability distribution given the training dataset (Abdar et al., 2021). Ensemble UQ means training multiple models, calculating their synthesized prediction (e.g., mean), and measuring uncertainty using deviation. Recently, there have been different variations of Monte Carlo (MC) simulation (Ferrenberg and Swendsen, 1989) for UQ, such as MC Dropout (Gal, 2016), to characterize prediction uncertainty more efficiently.\n\n\n6.8\nIntegration with physics-based models\nModel-driven solutions based on known physical laws have long been the main trend in applied sciences. Numerical modeling plays a dominant role in Earth system science on scales ranging from performing density functional theory (DFT) calculations to predict properties of molecules, to studying the climate using general circulation models (Han and Zhang, 2020). However, difficulties remain within developing efficient and accurate models. Unlike traditional physics-based Earth science models requiring high flops and massive CPU cores, ML, especially DL, can parallelize its processing by simply using GPU, or custom processing units like TPU to achieve the same effects as a stack of massive CPUs. Currently, there are two main trends in approaching this problem: 1) partial use of AI or AI-platforms (like Tensorflow and PyTorch) within traditional modeling frameworks to improve computational efficiency and performance accuracy (Xu et al., 2020); 2) incorporating physics laws into ML-based approaches to improve the interpretability of data-driven models (Raissi et al., 2020). In both cases, ML offers unprecedented opportunities for empowering modeling capability in approximating complex functions. The emergence of the physics-informed ML model (Kashinath et al., 2021) underscores the importance of advancing cutting-edge algorithms.\n\n\n6.9\nProvenance, reproducibility, replicability, & reusability\n\n\n\nFour broad and interrelated concerns for Earth AI research include:\n\n\n\u25cf\n\nProvenance: Where did the training data, AI model, software, and hardware originate, and what transformations have the data undergone before the findings were reported?\n\n\n\u25cf\n\nReproducibility: Can an independent party replicate the precise AI workflow and reported results, using the same data and algorithms?\n\n\n\u25cf\n\nReplicability: Can an independent party run similar (but not identical) ML analyses on similar (but not necessarily the same) data and come to the same conclusions?\n\n\n\u25cf\n\nReusability: How easily can the trained AI models be applied to new data or other new situations?\n\n\n\nEarth scientists have proposed standards to document the provenance of both data and scientific workflows (Sun et al., 2020a) including ISO 19115:2003 and ISO 19115\u20132:2009, the Open Provenance Model (Moreau et al., 2008), the data service standards of the Open Geospatial Consortium, and the Provenance Ontology of W3C (Hills et al., 2015; Lebo et al., 2013; Sun et al., 2013; Tilmes et al., 2013; Zhang et al., 2020).\nSoftware like Docker, Helm, Conda\/Anaconda-project, Prov, MetaClip, and Geoweaver can be used to record the AI workflow being used so that it can be made available for later retrieval to understand, replicate, reproduce, and reuse the trained AI models. As Earth scientists increasingly embrace open data and managed workflow platforms, the topics of provenance, reusability, replicability, and reproducibility have received increased attention (Gil et al., 2019; Kedron et al., 2021). Provenance is critical for Earth AI models to be understood and trusted by the public, and a standardized provenance framework for AI would be an ideal solution to address these concerns. Another challenge for reusability is ensuring that data used for training and evaluating algorithms are openly accessible (Neylon, 2012; Tenopir et al., 2020). As a step toward more open data, researchers should archive their data in a long-term repository (Duerr et al., 2018). Many of these repositories provide templates and tools to enable the submission of metadata that describes the data being archived and AI practitioners may benefit from guidelines that suggest which files are most important to submit to long-term repositories.\n\n\n6.10\nFull-stack workflow automation\nAI engineering is an inclusive discipline involving many technologies, algorithms, tools, libraries, and its product pipelines are composed of a series of links ranging from hardware to software, from a raw data repository to actionable information dissemination, from web services to endpoint software. Manually managing all the portions is unrealistic. Automating all the processing steps are required to make Earth AI practical in real-world scenarios. However, full-stack automation of Earth AI workflows is still under development. To maintain AI adoption and scale, the Earth science community needs a better way to holistically deploy and manage the lifecycle of deployed ML models.\nMLOps (ML DevOps) is the process of deploying an experimental ML model into a production web system. It manages the deployment, monitoring, managing, and governing of production-level ML models. There are many opportunities ahead for open source software developers to take on this task. Ongoing projects within the NASA Earth community like Geoweaver (Sun et al., 2020b) already realized this challenge and are working to deliver practically stable software as a solution.\nRunning efficient and productive Earth AI models requires the collaboration of various entities and resources, and involves various programs, scripts, libraries, software, and platforms from automation of data preparation, indigestion, training, validation, testing, deployment, and production. It requires building a workflow, meaning a logically chained flow of multiple processes to complete a big mission. Workflow orchestration could be conducted in many ways, e.g., writing a Python notebook, a Shell script, or using workflow management software like Cylc. The basic components of workflows are similar. All the workflows have atomic processes and connections among them. Once the workflow is started, all the atomic processes will be executed automatically without asking, which is called workflow automation. There are many workflow management software (WfMS) developing to enable automation, i.e., Apache Airflow, Cylc, Galaxy, Pegasus-WMS, Geoweaver (Sun et al., 2020b), and so forth. These WfMS can not only automate the process but also record the provenance to improve the replicability and reproducibility of Earth AI discovery.\n\n\n6.11\nAI ethics\nEarth AI is designed to protect us with an unseen powerful capability of forecasting the Earth's future and navigating natural hazards and resources in advance to save people and conserve the environment. However, the power has a limit and it cannot save everyone equally, for instance, in a geohazard or disruptive event. What if Earth AI miscalculates the situation, misses a region\/group, underestimates the harm, and results in more fatalities or greater damage? Earth AI is intelligent but still a lifeless system, which is not a legal entity. Yet its decisions impact society, and it behaves on a certain level of self-will.\nThere is a wealth of research focused on the ethical problems caused by AI when it is in operation (Jobin et al., 2019). Critics have examined the relationship between the role cultural bias plays in algorithmic inequality (Eubanks, 2018) and how AI systems oppress racial minorities and reinforce existing discrimination (Buolamwini and Gebru, 2018). We can foresee many regulations and laws regarding Earth AI ethics soon. Here, we outline several of the many paths toward more ethical AI in the earth and environmental sciences that include more open datasets and unbiased algorithms. Engineers should develop Earth AI ethics-related logic by partnering with social scientists, ethicists, and philosophers who have been studying the social implications of AI in the domain of policing, law, finance. This includes developing a guideline for ML researchers to engage with ethics as not only a philosophical project but also a pragmatic one where the collection of data and the use of particular models over others have direct impacts on ecosystems and humans. Last but not least, we believe that communicating one's application of any ML or AI application to the broader community it impacts (for example, if an automated method for developing land cover maps will directly impact on representations of Indigenous land) will be necessary for achieving a fair and ethical movement in AI in geosciences.\n\n\n6.12\nOperation management\nOperationalizing AI service cannot be simply fulfilled by one scientist or one small Earth research group. AI products need maintainers and customer service after being deployed. A large corporation could produce gigantic volumes of business and log data. Transition to and sustainment of AI operations is complicated by the rapid pace of technological evolution. However, DevOps practices, which emphasize close coordination between developers and operations, can mitigate these difficulties, and even provide useful AI feedback from operations into model evolution in some cases. Another potentially effective technique is internal capacity building, such as training the operations staff in the basics of the AI technology in use, so that they can better recognize issues and provide support to customers.\n\n\n\n7\nSummary\nFocusing on applications to geosciences, this paper overviews the cutting-edge technology and progress of AI research. Breakthroughs in Earth AI theory and infrastructure will carry geoscience into the next phase: Earth AI. The geoscience community must catch up with the pace of exploding observational datasets and quickly build useable AI models at an affordable cost promptly with adequate accuracy. The research and development of Earth AI are still at the infancy stage, and all the grand challenges ranging from data to model to operation can derive numerous opportunities in all sectors from academia to government and industry. The future of Earth AI is bright and dramatically beneficial to the entire human society and Earth system and should advance our civilization into its next epic phase and transform the Earth into a more sustainable, healthy planet.\n\n\nFunding sources\nThis work is sponsored by NASA ACCESS (#80NSSC21M0028 and #80NSSC21M0027, 2020), DOE (DE-AC02-05CH11231), NSF EPSCoR (#2019609, 2020), NSF Geoinformatics program (EAR-1947893 & EAR-1947875, 2020), NASA Health and Air Quality project (#80NSSC21K0512, 2018), NASA PO program (#80NM0018D0004), NSF Cybertraining (#2117834, 2021), NSF CSSI (#1835717, 2018), NSF EarthCube (#2126315, 2021). This paper has gone through NASA internal review and been cleared for publication.\n\n\n\n8\nAuthorship contribution statement\nZiheng Sun drafted the initial version, coordinated and contributed to the edits of all the sections, and prepared the manuscript; Laura Sandoval contributed to section 6.5, 6.1 to 6.4, coordinated the edits to all the sections and prepared the manuscript; Rob Crystal-Ornelas contributed to section 4.1, 6.2, 6.5, 6.9, 6.11, and did major editorial changes to the paper; S. Mostafa Mousavi contributed to section 2.1, 3.5, 4.1, and 6.8; Thomas E. Gill contributed to section 2.3 and 6.8 and did a thorough editorial improvement on the entire paper; Jinbo Wang contributed to section 2.2 and 2.6; Cindy Lin contributed to section 2.3, 6.2, 6.3, and 6.11; Javier Orduz contributed to section 2.1, 5.5, and 6.8; Peisheng Zhao contributed to section 4.1, 5.1, 6.1, 6.2, and 6.4; Chandana Gangodagemage contributed to section 2.1, 2.2, 5.1, and 6.1; James Bednar contributed to section 5.1 and 6.9, Pablo Rivas contributed to section 6.5 and 6.11; Wendy Carande contributed to section 6.2, 6.3, 6.5, 6.6, 6.7, 6.8, 6.9, 6.10, and 6.11; Amanda Tan contributed to section 6.8, 6.10, and 6.11; Jianwu Wang contributed to section 5.3 and 6.4; Benjamin Holt contributed to section 2.6; Sanjay Purushotham contributed to section 6.5 and 2.3; Nicoleta Cristea contributed to section 1, 2.2, 2.5 and 3.4; Daniel Tong contributed to section 2.3, 3.1, 4.4, and 6.8; Daniel Howard contributed to section 5.2, 5.3, 5.5, and 6.4; Julien Chastang contributed to section 4.2 and 6.9; Yuhan Rao contributed to section 3.2, 3.3, 6.6, and 6.11, Xiaogang Ma contributed to section 6.5 and 6.9; Zack Chester contributed to section 2.3 and 5.5; Aji John contributed to section 2.2, 4.1, and 4.3.\n\n\nComputer code availability\nNot applicable.\n\n","106":"","107":"\n\n1\nIntroduction\nThree main sources of geospatial big data may be identified, namely, volunteered geographic information (VGI), data generated by mobile devices, and data generated by earth observation and modelling infrastructures. The first two types of data have usually the form of vector features, i.e.,\u00a0data about geospatial entities. On the other hand, a large amount of the environmental data that is currently being generated comes from remote sensing devices and environmental modelling processes, and it has the form of very large raster coverages, i.e.,\u00a0very large numerical arrays with spatial and temporal dimensions.\nThe open data infrastructures used in the geospatial\u00a0(Foley, 2009) and environmental domain\u00a0(Nativi et al., 2015) provide data discovery facilities and data access mechanisms for vector features\u00a0(Vretanos, 2010) and raster coverages\u00a0(OGC, 2018; Baumann, 2009). Important to achieve interoperability in these infrastructures is the definition of standard interfaces and formats, like those proposed by the Open Geospatial Consortium (OGC)\n1\n\n\n1\n\nhttps:\/\/www.ogc.org\/.\n and UNIDATA.\n2\n\n\n2\n\nhttps:\/\/www.unidata.ucar.edu\/.\n The use of these infrastructures is mainly restricted to experts in Geographic Information Systems (GIS) and to scientists of the environmental domain.\nGeneral purpose open data infrastructures, on the other hand, are based on semantic web and linked data\u00a0(Bizer et al., 2009) standards defined by the World Wide Web Consortium (W3C).\n3\n\n\n3\n\nhttps:\/\/www.w3.org\/.\n Therefore, to enable geospatial and environmental data to be accessible through these infrastructures and used by ITC practitioners, both their data models (Resource Description Framework\u2014RDF\u00a0(Schreiber and Raimond, 2014) and Web Ontology Language\u2014OWL\u00a0(Hitzler et al., 2012)) and their data access interface (SPARQL Protocol and RDF Query Language\u00a0Harris and Seaborne, 2013) must be extended with geospatial capabilities. Many applications could benefit from the available geospatial and environmental data if they were appropriately accessible through standard Linked Data technologies. One such example is tourism, where already existing linked data repositories might be combined with geospatial entity-based data of locations, hotels, restaurants, or site seeing and with meteorological predictions modelled as large spatiotemporal coverages. Queries such as \u201cWhat is the predicted average of temperature of each municipality of Spain for the next week?\u201d might be formulated and resolved by combining vector feature sources (municipality geometries), raster coverage sources (meteorological data), and linked data sources like DBPedia.\nGeospatial vector features may already be represented and manipulated using a geospatial extension of SPARQL, called GeoSPARQL\u00a0(Perry and Herring, 2012), proposed by the OGC. Efficient implementations of GeoSPARQL are also available\u00a0(Kyzirakos et al., 2012), which leverage mature spatial database technologies. Raster coverage data access through linked data infrastructures is usually restricted to the querying of their vector-based metadata\u00a0(Koubarakis et al., 2012). Some specific extensions\u00a0(Andrejev et al., 2013; Homburg et al., 2020) have been designed to query raster data with SPARQL. However, their raster capabilities rely on collections of raster-specific functions defined on a raster data type, which demand specific training to use them, narrowing their use again to geographic and environmental data related experts.\nIn this paper, we present the design and first prototype implementation of a GeoSPARQL query processing approach for scientific raster array data, called GeoLD. Raster coverage data is mapped to RDF, without requiring any additional data type, using coverage to RDF mapping solutions, which are based on the already existing mapping approaches for relational data proposed by the W3C, namely direct mapping\u00a0(Arenas et al., 2012) and R2RML\u00a0(Das et al., 2012). Data querying is done with GeoSPARQL statements, without the need for any additional raster or array function or predicate. Therefore, it is expected that it may be used without any additional training. Enabling raster coverage access in GeoSPARQL automatically provides support for federated vector\u2013raster geospatial querying in SPARQL. However, further work is still required to achieve an efficient implementation of such federated querying. The efficient access to raster coverage data is based on the incorporation of a new operator in the SPARQL query engine, which enables the delegation of part of the query in standard Web Coverage Processing Services (WCPS)\u00a0(Baumann, 2009, 2010). As it was expected, the implementation outperforms the reference GeoSPARQL implementation\u00a0(Kyzirakos et al., 2012). GeoLD is also more efficient than currently available SPARQL raster extensions such as SciSPARQL\u00a0(Andrejev and Risch, 2012; Andrejev et al., 2013) and GeoSPARQL\n+\n\u00a0(Homburg et al., 2020) in the data access stage, despite not requiring specific array syntax in its language, due to the lack of efficient array data storage and access approaches in their current implementations.\nThe main contributions of this work are the following: (i) the definition of coverage data to RDF mapping solutions, based on W3C standards; (ii) the definition of a SPARQL query processing approach in the form of a new SPARQL operator that enables the delegation of raster data access in OGC standard WCPS services; and (iii) the definition of a query optimization strategy that identifies maximum SPARQL query trees to be replaced by WCPS operator instances.\nThe rest of the paper is organized as follows. Section\u00a02 discusses related work. Section\u00a03 describes the main components of the system architecture. Two approaches to map raster coverages to RDF are discussed in Section\u00a04. The query GeoSPARQL query processing solution is explained in Section\u00a05. Section\u00a06 shows performance evaluation results, and Section\u00a07 concludes the paper and outlines future work.\n\n\n2\nRelated work\nDiscovering, accessing, and browsing are key functionalities that must be provided by open data infrastructures to enable data-driven decision making. Semantic web technologies play a key role in the enablement of those functionalities by supplying amongst others: (i) ground data representation models and languages, namely RDF\u00a0(Schreiber and Raimond, 2014) and OWL\u00a0(Hitzler et al., 2012); (ii) ontologies to represent different types of metadata, such as DCAT\u00a0(Albertoni et al., 2020) for general dataset metadata, PROV-O\u00a0(Belhajjamey et al., 2013) for provenance metadata, and DQV\u00a0(Debattista et al., 2016) for quality metadata; (iii) a set of specifications and languages called SPARQL that include a query language\u00a0(Harris and Seaborne, 2013), formats for query results, a syntax for federated queries over external data sources\u00a0(Prud\u2019hommeaux and Buil-Aranda, 2013), a transactional language to perform updates and HTTP based protocols for client-service communications; and (iv) languages to support the mapping between the RDF and the relational model\u00a0(Das et al., 2012; Arenas et al., 2012).\nSpatial Data Infrastructures (SDIs)\u00a0(Foley, 2009) must also provide services and\/or components to support data discovery, access, and browsing. ISO and OGC standards are commonly adopted to achieve interoperable SDIs. ISO 19115\u00a0(ISO, 2014) is used for geographic metadata representation and the OGC CSW\u00a0(Nebert et al., 2016) service specification for metadata querying. Geospatial vector features are accessed through web services with WFS\u00a0(Vretanos, 2010) interface, and their geometric properties are usually implemented following the Simple Feature Access (SFA)\u00a0(Herring, 2011). Basic geospatial raster coverage data access support is given by the WCS\u00a0(OGC, 2018) web service interface, and more complex queries with some declarative processing are supported by the Web Coverage Processing Service (WCPS)\u00a0(Baumann, 2009, 2010). Two recent examples of projects that include amongst their objectives the development of Spatial Data Infrastructures are TRAFAIR (http:\/\/trafair.eu\/) and RADAR-ON-RAIA (http:\/\/radaronraia.eu\/), the former in the scope of urban scale air quality observation and prediction and the later for HF-Radar ocean data.\nChallenges for the application of semantic technologies in the geospatial domain have already been identified\u00a0(Patroumpas et al., 2014), and the importance of their incorporation in SDIs has already been stressed ten years ago\u00a0(Janowicz et al., 2010). During the last years, many efforts were devoted to the creation of ontologies, and it is expected that new projects will focus more on their use to solve the problems that arise during data discovery and access\u00a0(Narock and Wimmer, 2017). Examples of ontologies in the geospatial domain arise in many different application domains, including hydrology\u00a0(Essawy et al., 2017), geology\u00a0(Ma et al., 2011) or mining\u00a0(Ma et al., 2010). More generic are the attempts to achieve geospatial provenance metadata ontologies\u00a0(Ma et al., 2014; Jiang et al., 2018) and their use for the representation of image classification rules\u00a0(Andr\u00e9s et al., 2017). The Semantic Web for Earth and Environment Technology Ontology (SWEET)\u00a0(Raskin and Pan, 2005), created by NASA, is an example of a general-purpose vocabulary in the environmental domain.\nInitial solutions that apply semantic technologies to the data discovering stage have already been reported in the literature\u00a0(Zhao et al., 2009; Yue et al., 2007; Stock et al., 2012; Athanasis et al., 2009; Gahegan et al., 2009). In general, geospatial metadata is represented and encoded with ontologies, and semantic technologies are next applied to address semantic matching with query specifications. Additionally, the semantic representation of the metadata enables a more precise semantic interpretation of the retrieved datasets. Regarding data access, the main advantage is that new possibilities are open for the resolution of semantic conflicts during data integration\u00a0(Wang et al., 2018; Buccella et al., 2009; Lutz et al., 2009; Graybeal et al., 2012; Regueiro et al., 2015, 2017).\nRecently, challenges related to the development of geospatial semantic technologies have been identified in the scope of the construction of smart environmental open data infrastructures\u00a0(Viqueira et al., 2020). Regarding data representation and querying, RDF support for geospatial vector features and related query capabilities is already provided by the GeoSPARQL standard\u00a0(Perry and Herring, 2012), which is implemented by some existing data management tools\u00a0(Kyzirakos et al., 2012). The RDF Cube Vocabulary\u00a0(Cyganiak and Reynolds, 2014) has been designed to represent multidimensional data in data warehouses; however, it is not adequate to efficiently support the sampling spatial and temporal dimensions of coverages. GeoSPARQL may be used to represent and query geospatial coverage metadata, but it is not suitable for their data\u00a0(Koubarakis et al., 2012). Finally, to the best of our knowledge, only SciSPARQL\u00a0(Andrejev and Risch, 2012; Andrejev et al., 2013) and GeoSPARQL\n+\n\u00a0(Homburg et al., 2020) aim at providing SPARQL support for scientific array data and raster coverages. Raster arrays are represented with new array or raster data types. Such a nested representation of the arrays into a conventional data model has also been adopted by spatial DBMSs, which show poor performance when they are compared with specialized raster engines. Besides, the user must know a set of array data operators, in addition to the underlying SPARQL syntax, to be able to perform queries. Regarding data integration and federated querying, already existing relational mappings\u00a0(Das et al., 2012; Arenas et al., 2012) and relevant geospatial extensions may be used either to query directly spatial databases with SPARQL\u00a0(Bereta et al., 2019) or to generate RDF from geospatial formats\u00a0(Kyzirakos et al., 2014). Few geospatial federated query solutions have been proposed\u00a0(Green et al., 2008), and they are also restricted to vector feature data.\n\n\n3\nSystem architecture\nThe architecture of GeoLD, the extended SPARQL query engine designed in the present work, is graphically shown in Fig.\u00a01. As with any other query engine, first the query must be parsed to generate an appropriate tree of RDF algebra operations. Next, an optimizer chooses the best evaluation plan, i.e.,\u00a0a tree of operator algorithms that it is estimated to reach the best performance. Finally, the evaluation engine executes the query plan to generate the expected results from the input data. Few more details of each of the query engine layers are described below, focusing on the contributions of the present work with respect to already existing SPARQL query engines.\n\nThe SPARQL Query Parser is a conventional SPARQL parser that identifies the user defined functions and predicates specified in the GeoSPARQL standard\u00a0(Perry and Herring, 2012). The output of this component is a tree whose nodes represent SPARQL algebra operators\u00a0(Harris and Seaborne, 2013). Examples of these operators are the BGP, which generates an RDF triple (subject, predicate, object) sequence from a Basic Graph Pattern, and JOIN, which enables the combination of two sequences of triples.\nThe SPARQL Optimizer processes the SPARQL algebra tree generated by the parser to obtain an efficient query evaluation plan. An evaluation plan is also a tree, but now each node contains an algorithm that provides an implementation for a specific SPARQL algebra operator. A main contribution of the present work in this component is the implementation of an optimizer (WCPS Query Generator) that identifies parts of the evaluation plan that may be delegated to be executed by an underlying OGC WCPS service. Those parts are rewritten in the query plan with a new operator (WCPS Operator). To help to the identification of those tree parts and also to enable the mapping from the OGC raster coverage model to RDF, relevant mapping languages have been defined in this work. Further details are given in Section\u00a04.\nThe SPARQL Evaluation Engine coordinates the execution of the algorithm of each node of the query plan to generate the expected output sequence of RDF triples. A main contribution of the present work with respect to already existing SPARQL query engines is the implementation of the new WCPS Operator. In the current implementation, this operator enables the querying of a WCPS service to perform spatial, temporal, or spatio-temporal filtering over an existing raster coverage, and to generate an output sequence of RDF triples, according to the specified coverage to RDF mapping. Further details related to the query rewriting process that generates WCPS operator nodes and to the implementation of this operator are given in Section\u00a05.\n\n\n4\nMapping geospatial raster coverages to RDF\nThe proposed solution for the definition of mappings between geospatial raster coverages and RDF triples is based on the already existing mappings languages defined by W3C between relational data and RDF, namely the direct mapping\u00a0(Arenas et al., 2012) and R2RML\u00a0(Das et al., 2012). A brief description of these relational to RDF mappings is first given. Let obs(sensor, time, rain) be the scheme of a relation that records time series of rainfall measurements generated by different sensors. Let also \n\n(\ns\ne\nn\ns\no\nr\n,\nt\ni\nm\ne\n)\n\n be the primary key. For each tuple \n\n(\ns\n,\nt\n,\nr\n)\n\n of \n\no\nb\ns\n\n, a direct mapping generates the following RDF triples:\n\n<obs\/sensor=s;time=t> rdf:type <obs>.\n\n\n<obs\/sensor=s;time=t> <obs#sensor> s.\n\n\n<obs\/sensor=s;time=t> <obs#time> t.\n\n\n<obs\/sensor=s;time=t> <obs#rain> r.\n\nThe generation of triples for foreign keys and tuples of relations without primary keys are omitted due to space limitations.\nContrary to the direct mappings, which use the relation and attribute names as part of the output RDF vocabulary, the R2RML language enables the user to control the vocabulary of the output. Each relation is mapped to RDF using rules called TriplesMap. Each rule has three parts: (i) a LogicalTable that may be either a table name, view name, or SQL statement, which defines the input tuples to be mapped; (ii) a SubjectMap that generates the triple that specifies the rdf:type of each tuple and the subject part of all the triples. Usually, IRIs are generated using templates where primary key attributes are used as parameters; (iii) a sequence of PredicateObjectMap that generates the predicate and object part of all the triples. Each PredicateObjectMap is further decomposed into two parts, one that enables the generation of the triple predicate and another that generates the triple object.\n\nGeospatial coverages are conceptually modelled as collections of mappings. Each coverage field (or attribute) is modelled as a mapping, whose domain is the Cartesian Product of the coverage dimensions (a spatial dimension and optionally also a temporal one) and whose range is the data type of the field (in practice most of the raster coverages have fields of real types). Fig.\u00a02(a) illustrates a spatio-temporal coverage called \n\nM\ne\nt\ne\no\n\n that contains two fields of meteorological data, namely \n\nT\ne\nm\np\n\n (Temperature) and \n\nH\nu\nm\n\n (Humidity). Based on the above conceptual modelling assumption, the proposed direct mapping approach for coverages generates, for each coverage raster cell, one triple to represent its type, and one triple for each field to represent their values. Fig.\u00a02(b) shows the graphical representation of the triples generated for two of the raster cells of the \n\nM\ne\nt\ne\no\n\n coverage. It is first noticed that \n\nM\ne\nt\ne\no\n\n is defined as an subclass of geo:Coverage, which in turn is a subclass of the GeoSPARQL geo:Feature class. Each raster cell is defined as an instance of \n\nM\ne\nt\ne\no\n\n, with a geometric property (hasLocation), a temporal property (hasTime), and a property of a conventional data type for each coverage field. OGC Well Known Text (WKT) representation is used for geometric literals; therefore, the generated RDF is fully compatible with the GeoSPARQL ontology.\nThe Coverage to RDF Mapping Language (C2RML) gives more control to the user over the RDF generation process, enabling both slicing and trimming over the input coverage and also letting the user define its own RDF vocabulary. TripleMap rules of C2RML are similar to those of R2RML. The main difference is that R2RML LogicalTable is replaced by a new Coverage section, which specifies the name of the input coverage and optionally defines trimming and\/or slicing operations over its dimensions. Fig.\u00a02(c) illustrates the RDF generated by a specific C2RML mapping applied to coverage \n\nM\ne\nt\ne\no\n\n. In this example, the Coverage section of the mapping defines a trimming over the spatial dimension of the coverage (Location x between 4 and 5 and Location y between \u22124 and \u22123). It also specifies a slicing over the temporal dimension (time \n=\n t0). To generate the subjects of all the RDF triples the SubjectMap uses the following template \n\n\n\nT\ne\nm\np\nC\no\nv\ne\nr\na\ng\ne\n\/\nL\no\nc\n=\n\n{\nL\no\nc\na\nt\ni\no\nn\n}\n\n\n\n\nwhere the parameter \n\nL\no\nc\na\nt\ni\no\nn\n\n is replaced by the coordinates of each raster cell. Finally, two PredicateObjectMap are used to generate the \n\nh\na\ns\nL\no\nc\n\n and \n\nh\na\ns\nT\ne\nm\np\ne\nr\na\nt\nu\nr\ne\n\n properties, from the coverage spatial dimension and the \n\nT\ne\nm\np\n\n (Temperature) field, respectively.\n\n\n5\nQuery processing\nThe description of the query processing solution that enables the efficient querying of raster coverages is described in the following subsections. Broadly speaking, first the query optimizer identifies those parts of the query execution plan that may be delegated to the underlying WCPS. Those parts of the query plan are replaced by calls to a new WCPS operator. In the current version, this new operator supports only filtering, including dimension trimming and slicing and field filtering. The current implementation is based on the combination of ARQ,\n4\n\n\n4\n\nhttps:\/\/jena.apache.org\/documentation\/query\/index.html.\n the SPARQL Processor of Apache Jena, with Petascope\u00a0(Aiord\u0103chioaie and Baumann, 2010), the OGC standards-based coverage geospatial server of the raster server rasdaman\u00a0(Baumann et al., 1998).\n\n5.1\nSPARQL algebra\nThe SPARQL specification\u00a0(Harris and Seaborne, 2013) includes the definition of an algebra for the evaluation of the queries. A full description of all the SPARQL algebra operations is out of the scope of this paper, thus the discussion will restrict to the most important ones required to evaluate the running example GeoSPARQL query of Fig.\u00a03(a). This query combines data from three sources: (i) a geospatial feature collection feat:municipality that records data of municipalities, including their name, geometry and reference to their DBPedia node; (ii) DBPedia, which contains additional data of municipalities, including a textual description in property dbo:abstract; (iii) the raster coverage cov:Meteo illustrated in Fig.\u00a02(a). For each municipality, the data of coverage cov:Meteo is queried, restricting the time dimension to values between \n\nt\n1\n\n and \n\nt\n2\n\n, and restricting the spatial dimension to cells contained in the municipality geometry. The coverage temperature values are aggregated using functions average, minimum and maximum values, and combined with the name and the abstract of the municipality to generate the final result, i.e.,\u00a0aggregated meteorological data for each municipality.\n\n\nFig.\u00a03(b) shows part of a possible query plan for the above GeoSPARQL query. The output of a SPARQL query, and also of a SPARQL operation of its algebra, is a sequence of solution mappings, where each solution mapping is a function that associates a variable of the query with an RDF term, i.e, either an RDF literal, a resource identifier (IRI) or a blank node. Operators that may appear as leaf nodes in query plans generate solution mappings from the input RDF graphs. Thus, for example, the BGP operator at the bottom left part of the Fig.\u00a03(b), generates a sequence of solution mappings for variables \n\n(\n?\nm\nu\nn\n,\n?\nm\nn\na\nm\ne\n,\n?\nm\ng\ne\no\n,\n?\nm\nd\nb\np\ne\nd\ni\na\n)\n\n, applying a list of triple patterns to the input RDF graph. On the other hand, all the other operators transform input solution mapping sequences into output sequences. Thus, for example, the first JOIN operator at the bottom left part of Fig.\u00a03(b) combines the above sequence of solution mappings with the sequence generated for variables \n\n(\n?\nm\nd\nb\np\ne\nd\ni\na\n,\n?\na\nb\ns\n)\n\n by operators BGP and SERVICE. The FILTER operator selects the input solution mappings on which a given condition holds. Thus, for example, the filter operator at the bottom right part of the query plan of the figure restrict the solution mappings of variables \n\n(\n?\ns\n,\n?\ns\nl\no\nc\n,\n?\nt\ne\nm\np\n,\n?\nt\ni\nm\ne\n)\n\n to those where variable \n\n?\nt\ni\nm\ne\n\n has values between \n\nt\n1\n\n and \n\nt\n2\n\n. Finally, the GROUP operator at the top of the Fig.\u00a03(b), generates mappings for variables \n\n(\n?\nn\na\nm\ne\n,\n?\na\nb\ns\n,\n?\nt\ni\nm\ne\n,\n?\n.\n0\n,\n?\n.\n1\n,\n?\n.\n2\n)\n\n, where values of variables (\n\n?\n.\n0\n,\n?\n.\n1\n,\n?\n.\n2\n\n) are computed by aggregating (through functions \n\na\nv\ng\n\n, \n\nm\ni\nn\n\n and \n\nm\na\nx\n\n) values of the variable \n\n?\nt\ne\nm\np\n\n obtained from mappings with identical values for variables (\n\n?\nn\na\nm\ne\n,\n?\na\nb\ns\n,\n?\nt\ni\nm\ne\n\n).\n\n\n5.2\nWCPS operator\nTo enable the delegation of raster coverage querying on WCPS services\u00a0(Baumann, 2009, 2010), a new leaf SPARQL operator was defined and implemented, namely, the WCPS operator. Broadly, this operator allows the execution of raster coverage processing statements on a WCPS service to generate solution mappings for variables representing coverage dimensions and coverage fields. Clearly, delegating part of the query in the data source moves the processing closer to the data, which is one of the basic principles to follow to achieve efficient query processing.\nThe WCPS coverage processing language\u00a0(Baumann, 2010) is powerful enough to support the evaluation of different types of filters and aggregations on input coverages. The current implementation of the WCPS operator restricts to filtering, and therefore the description below restricts to WCPS coverage processing language expressions for filtering over dimensions and fields.\nDimension filtering is performed in WCPS with slicing and trimming operations. Thus, for example, the following expression retrieves a coverage of temperature values from coverage Meteo in Fig.\u00a02(a), by trimming the spatial dimensions and by slicing the temporal dimension. The result is encoded in CSV format.\n\n\nFor $m in (Meteo)\n\n\n\nReturn encode($m.Temp[x(\u22123,2),y(1,4),time(t2)],\n\"CSV\").\nSpatial, temporal and spatio-temporal filter expressions are used to generate appropriate WCPS slicing and trimming expressions. Spatial and temporal coordinates used at SPARQL level must be adapted to the spatial and temporal resolution of the data source. Thus, a condition of the form \u201c\n\ng\ne\no\nf\n\n: \n\nE\nq\nu\na\nl\ns\n\n(\n?\ns\nl\no\nc\n,\np\n)\n\n\n\u201d, where \n\n?\ns\nl\no\nc\n\n is a variable referencing the spatial dimension of the coverage and \np\n a point literal, must retrieve data despite the fact that \np\n coordinates might not match exactly the coordinates of the centroid of any cell of the coverage. Besides, subsequent post-processing of the WCPS results might be needed in some cases. For example, the evaluation of a predicate \u201c\n\ng\ne\no\nf\n\n: \n\nC\no\nn\nt\na\ni\nn\ns\n\n(\np\n,\n?\ns\nl\no\nc\n)\n\n\n\u201d, where \np\n is a polygon literal, requires a spatial trimming over the minimum bounding rectangle (mbr) of \np\n, done by the WCPS expression, and a post-processing that discards cells that lie inside the mbr but are not contained in \np\n.\nFiltering with conditions different from those of trimming and slicing over dimensions is not possible in WCPS, as it is also not possible to filter over coverage fields. However, it is possible to generate a special value (Not a Number\u2014NaN) in cells where the condition does not hold, by using a \u201cswitch\u201d statement. Those NaN cells are discarded by the WCPS operator after querying the WCPS service. Thus, for example, the following WCPS expression obtains a coverage with data only at areas with temperatures between 10 and 15.\n\n\nFor $m in (Meteo)\n\n\n\nReturn encode(switch\n\n\n\n\n\ncase $>= 10 and $m.Temp <=15\n\n\n\n\nreturn $m.Temp\n\n\n\n\ndefault return nan, \"CSV\")\n\n\n\n5.3\nQuery optimization\nThe SPARQL query optimizer of ARQ transforms the initial query plan produced by the SPARQL query parser into another query plan that is expected to achieve a better performance. An example of a general heuristic applied during the optimization is to move the Filter operations as much as possible down in the query plan tree. As an example, the condition \u201c\n\n?\nt\ni\nm\ne\n>\n=\nt\n1\n&\n&\n?\nt\ni\nm\ne\n<\n=\nt\n2\n\n\u201d of the GeoSPARQL of Fig.\u00a03(a) is placed after optimization in a FILTER operator immediately on top of the BGP operator of the bottom right part of Fig.\u00a03(b). This way, the data corresponding to time instants out of the range [t1,t2] does not have to be processed by the JOIN operator, which leads to a more efficient query plan.\nOnce the above optimizations, already supported by ARQ, are applied, a new optimization step was added to enable the use of the WCPS operator at the appropriate place. To achieve this, maximum subtrees, starting from leaf nodes, must be identified, whose functionality may be replaced by a WCPS processing statement and, therefore, they may be replaced by a WCPS operator. In the current implementation, those maximum subtrees include only a BGP operator, with an optional FILTER on top of it. \n\n\n\n\n\n\nFirst, each node of the query plan tree is tagged with a string. Leaf BGP nodes that may be translated to a WCPS request are tagged with the name of the relevant raster coverage. If a BGP operator combines triple patterns corresponding to a raster coverage, with triple patterns corresponding to either other coverages or other data sources, then it has to be split to be tagged appropriately. The result of the generated BGPs has to be combined with a SEQUENCE operator, which is a special type of n-ary JOIN. The algorithm for this node tagging process is illustrated by the pseudocode of Algorithm 1, whose notation is described in Table\u00a01. Function WCPSTagger recursively tags the nodes of the query plan tree as follows: (i) if the node is a leaf BGP then function BGPTagger is used to process it (line \n2\n); (ii) if the node is a Filter (lines 3\u20137), then its child is recursively tagged using the same function and the Filter is tagged with the same tag of its child; (iii) if the node is of any other operator (lines 9\u201314), then all the children are tagged recursively, and the node is tagged as \u201cSPARQL\u201d. Function BGPTagger processes BGP nodes as follows. First (line \n\n17\n\n), the list of BGP triple patterns is processed to create a list of coverage BGPs (\n\nc\nb\ng\np\ns\n\n), i.e.,\u00a0BGPs corresponding to a single coverage, and a SPARQL BGP (\n\ns\nb\ng\np\n\n), containing all the triple patterns that do not correspond to any coverage. The former are tagged with the name of the corresponding coverage and the latter are tagged as \u201cSPARQL\u201d. The triple patterns with predicates rdf:type and the existing coverage to RDF mappings (see Section\u00a04) are used to determine when a new coverage BGP has to be created. In a second step (line \n\n18\n\n), all the remainder triple patterns are classified to be added to one of the created BGPs. Finally, all the created BGPs (in case more than one is created) are combined using a SEQUENCE operator (line \n\n19\n\n), which is tagged as \u201cSPARQL\u201d. At the end of this tagging process, the operator nodes that have to be used to generate a WCPS operator are tagged with the name of the corresponding coverage. As an example, the FILTER and BGP operators at the bottom right part of Fig.\u00a03(b) are tagged with the name of coverage \u201cMeteo\u201d.\nOnce the query plan tree has been tagged, it has to be processed to insert WCPS operators at the required places. In particular, each BGP operator (or sequence of BGP and FILTER operators) tagged with the name of a coverage must be replaced by a WCPS operator which contains a WCPS processing statement that obtains the required raster coverage data. As an example, the FILTER and BGP operators that are the bottom right part of Fig.\u00a03(b) will be replaced by a WCPS operator with the following processing statement:\n\n\nFor $c in (Meteo)\n\n\n\nReturn encode($c.Temp[time(t1,t2)],\"CSV\").\n\nAdditional optimizations considered as part of future work would require the implementation of specific spatial join and aggregation operators that leverage the underlying processing capabilities of vector and raster subsystems. As an example, in the query of Fig.\u00a03, specific spatial join and aggregation operations could be used to combine the vector data of the municipalities (left part of the tree) and the raster coverage (right part of the tree), leveraging the spatial filtering and aggregation capabilities of the WCPS engine.\n\n\n\n6\nPerformance evaluation\nTo demonstrate the clear benefits in terms of performance that brings to GeoLD the delegation of raster data access in specialized technologies, an evaluation was undertaken that compared the response times of four different SPARQL implementations, namely, STRABON\u00a0(Kyzirakos et al., 2012), SciSPARQL\u00a0(Andrejev and Risch, 2012; Andrejev et al., 2013), GeoSPARQL\n+\n\u00a0(Homburg et al., 2020) and GeoLD.\nAll the software was installed in a 32\u00a0bit virtual machine with Ubuntu 16.04, since SciSPARQL was only available for 32\u00a0bit architectures. The hardware specification used by the virtual machine was the following: 4 processors Intel core i7 3.40\u00a0GHz and 8\u00a0GB of RAM. In general, five different types of queries were performed over all the datasets:\n\n\nSpace Point Query:\n\nIt retrieves the time series of all the cell values (temperature values in our experiment) whose cells intersect the query spatial point, i.e.,\u00a0it performs a slicing over the spatial dimension of the coverage.\n\nTime Point Query:\n\nIt retrieves the spatial raster coverage valid at a query time instant, i.e.,\u00a0it performs a slicing over the temporal dimension of the coverage.\n\nSpace Range Query:\n\nIt obtains the spatio-temporal coverage whose spatial extension is restricted to the query rectangle, i.e.,\u00a0it performs a trimming over the spatial dimension.\n\nTime Range Query:\n\nIt obtains the spatio-temporal coverage whose temporal extension is restricted to the query time period, i.e.,\u00a0it performs a trimming over the temporal dimension.\n\nField Range Query:\n\nIt obtains the triples (space point, time instant, value) obtained from the input coverage such that the value (temperature value in our experiment) lies inside a given query range.\n\n\n\nAll the queries were evaluated with a cold cache system status, to avoid the undesirable effects in the response time of the buffer cache of the DBMS PostgreSQL used by STRABON, which is not present in none of the other solutions.\nTwo different evaluation experiments were done aiming at evaluating the scalability of the approach when either temporal or spatial dimensions of the datasets grow. The spatio-temporal coverages used to test scalability over the time dimension were generated from historical numeric weather predictions of MeteoGalicia,\n5\n\n\n5\n\nhttps:\/\/www.meteogalicia.gal\/modelos.\n the meteorological agency of the Spanish region of Galicia. Five coverages with time dimensions ranging from 10 to 50 time instants and space dimension of 117 \n\u00d7\n 142 pixels were used, whose overall size ranged from 664.k to 3.3M data elements (spatio-temporal cells). Those coverages were loaded in rasdaman to be used by GeoLD and also transformed to RDF to be loaded in the different SPARQL engines.\nAn overview of the achieved results is shown in Fig.\u00a04. For each of the above query types, the charts provide the SPARQL syntax, the WCPS query generated by GeoLD for the WCPS operator, and the response times of the evaluated solutions for each database size (in number of time instants). All the queries could be implemented in GeoLD, STRABON and SciSPARQL. On the other hand, only time point queries, as defined above, could be implemented in the current proof of concept implementation of GeoSPARQL\n+\n. This was mainly due to the lack of appropriate implementation of raster data type functions to extract cell values from tiles. Spatial and temporal point and range queries, with fixed query selectivity (fixed temporal query period and spatial rectangle size) are solved in constant time (with respect to the database size) by GeoLD, as it is shown in the figure. This is due to the direct access to the data elements through array dimensions provided by the rasdaman raster storage structures, which is invoked by the GeoLD WCPS operator. Furthermore, as it was expected, field range queries increase with the database size. Response times of SciSPARQL are two orders of magnitude worse than those of GeoLD. As an example, for a database size of 20 time instants, response times range from 110 to 23\u00a0s, depending on the query type. For this reason, those results are not shown in the Figure. Regarding GeoSPARQL\n+\n, for time point queries, two different encodings were tested for the output raster coverage tiles. The use of a text encoding of the result tiles gives a performance clearly worse than GeoLD, and even worse than STRABON for small coverage sizes. On the other hand, using a more compact raster representation (well-known-binary hexadecimal in our experiment) enables a much more efficient encoding of the result. This shows the importance of the incorporation of specific efficient compact encodings to be used when the result of the query is a coverage. The design and implementation of such compact raster representations is part of future work in GeoLD. All the other query types could not be implemented, however, the evaluation of a query that retrieves the time instants of raster tiles that intersect a given query point showed response times of around 4\u00a0s, which is worse than GeoLD and also worse than STRABON, despite not having to access the raster tiles to obtain the cell values. Finally, regarding STRABON, the spatial relational DBMS-based data storage layer enables the use of specific indexing structures, such as R-Trees for 2D spatial dimensions and B\n+\n-trees for 1D dimensions. In spite of this, as it is also shown in the figure, these structures do not enable achieving a performance with raster data similar to that of a specialized raster engine.\n\nThe scalability over the space dimension was tested using coverages generated from a Digital Elevation Model (DEM) of Galicia with 100 metres of spatial resolution. In particular, four datasets were used with resolutions of 800, 400, 400 and 100\u00a0m, and with sizes (in number of data elements) ranging from around 72\u00a0k (800 m resolution) to 4.6\u00a0M (100 m resolution). Various space range queries using query squares of increasing sizes (ranging from 15 km to 60 km of side length) were evaluated on GeoLD and STRABON. The results are shown in Fig.\u00a05. Clearly, GeoLD outperforms STRABON in all cases, due to the better performance provided by rasdaman for raster data access.\n\n\n7\nConclusion\nThe design and first GeoSPARQL query engine for scientific raster array data, called GeoLD, was briefly described. The system analyses optimized SPARQL algebra trees, generated by state of the art technologies, and identifies maximum parts that may be evaluated by a standard WCPS service. To achieve this, Coverage to RDF mapping solutions (both direct mapping and C2RML) were defined, which are based on the well-known W3C standard used for relational data. A first prototype of the system was implemented as an extension of the ARQ SPARQL engine of Apache Jena. The Rasdaman array database was used to record the raster data and its WCPS implementation Petascope was used to access it through the web. GeoLD enables accessing scientific raster data with conventional linked data technologies, opening the possibility to incorporate scientific datasets to conventional applications with little effort. As it was initially expected, both from these authors intuition and other authors asserts, the solution outperforms an already existing GeoSPARQL implementation, which is based on relational technology. Besides, it does not require specific array extensions, as SciSPARQL and GeoSPARQL\n+\n do. Currently, GeoLD outperforms also SciSPARQL and GeoSPARQL\n+\n in the data access stage, due to their lack of specific raster data storage and access technologies. GeoSPARQL\n+\n however, shows the importance of providing specific raster data encodings to be used when raster coverages are retrieved. Future work is related to (i) completing the generation of WCPS queries for other SPARQL operations (beyond BGP and FILTER), for example aggregates, (ii) the efficient implementation of spatial JOIN operations between vector and raster datasets and (iii) the incorporation of mechanisms to use standard efficient encodings for raster coverages (such as GeoTIFF and NetCDF) in query results.\n\n\nCRediT authorship contribution statement\n\nShahed Bassam Almobydeen: Study of the state of the art, design and implementation of the solution, Preparation of the first draft of the paper. Jos\u00e9 R.R. Viqueira: Supervision of the state of the art study, design of the solution and of its evaluation, Revision of the paper. Manuel Lama: Design of the solution and of its evaluation, Revision of the paper.\n\n","108":"","109":"","110":"\n\n1\nIntroduction\nIt is important to predict how watersheds and other ecosystems will respond to changing environmental conditions for optimal management of natural resources. The study and modeling of natural environments requires integration of diverse observations that sample different properties of these complex systems (Fig. 1\n). Often, data are required at multiple spatial and temporal scales to enable both quantification of fine-scale processes and their aggregation to larger scales. For example, heterogeneous data on water cycle processes, such as precipitation, river discharge, soil moisture, infiltration, evapotranspiration, need to be integrated with data on topography, soil properties, and water use to predict water availability and quality at seasonal to decadal scales (e.g. Krysnova and Arnold, 2008; Maxwell et al., 2015; USGS Water Resources, 2020).\nThe development of long-term monitoring networks and research data management requirements have led to an unprecedented volume and diversity of available environmental data (Rode et al., 2016). Advances in data analytics, scale-aware mechanistic modeling, Machine Learning (ML) and other computational methods create opportunities to integrate diverse environmental data into a predictive framework (Hubbard et al., 2020). Yet, these data are underutilized in studies that seek to improve the understanding and predictions of environmental systems, in part due to major challenges associated with the discovery and integration of relevant datasets for scientific analysis and modeling. Currently data integration across different providers is an arduous, time-intensive task, needing considerable harmonization efforts (refer to Table 1\n for a list of common data integration terms). Most data sources are organized around the provider's requirements and not the user's needs to discover, integrate and utilize the data. For example, in the United States (US), several monitoring networks have been established across federal, state and local agencies that have not adopted common data or metadata standards. Different suites of physical, chemical and biological parameters are measured across regions and time periods using a variety of methods. Data are served using an assortment of formats, variable names, units and schemas (e.g. Sprague et al., 2017). As an example, stream discharge and water level data collected by the US Geological Survey (USGS), US Bureau of Reclamation (USBR), several state agencies, local efforts (e.g. water management districts, watershed associations) and numerous research networks such as the Long Term Ecological Research (LTER), Critical Zone Observatories (CZO), National Ecological Observatory Network (NEON) are published with heterogeneous data formats and access mechanisms. Thus researchers typically create manual, one-off integrated products for their scientific needs, which become outdated when the data change.\nSeveral approaches have been used to integrate disparate datasets over the past few decades. One method, known as data warehousing, integrates data from various sources into a centralized database, which can be queried to retrieve synthesized data streams. Some systems using this approach, like the National Water Quality portal (https:\/\/www.waterqualitydata.us\/) and Ameriflux (https:\/\/ameriflux.lbl.gov\/), combine structurally similar data by requiring sources to provide data in particular formats (Blodgett et al., 2015; Pastorello et al., 2017). However, centralized databases can become outdated and are difficult to maintain as the number of desired sources, data types and volumes grow.\nData federation, also known as the hub and spoke model (Haas et al., 2002), is an alternate approach that has gained traction. Here, data are left at the original sources and an intermediate brokering software maintains a catalog and retrieves data on demand (Genesereth, 2010; Nativi et al., 2013). This allows users to access the latest version of the data from different sources as though it were available in a central location. The brokering approach has been adopted by systems such as the Group on Earth Observations (GEOSS) Discovery and Access Broker (Nativi et al., 2014) and the related BCube brokering framework (Khalsa, 2017). The US National Groundwater Monitoring Network (https:\/\/cida.usgs.gov\/ngwmn\/index.jsp) uses an advanced brokering approach to synthesize datasets from various sources to support a portal with interactive visualizations; however, this system only handles specific types of groundwater monitoring data (water level, quality, lithology), and requires cooperative agreements with providers to standardize their data to facilitate data exchange (https:\/\/acwi.gov\/sogw\/ngwmn_framework_report_july2013.pdf). One of the more successful implementations of a brokering approach is the Consortium of Universities for the Advancement of Hydrologic Science (CUAHSI) Hydrologic Information System (HIS; https:\/\/hiscentral.cuahsi.org), which transforms diverse time-series data using the WaterOneFlow web services into a standardized WaterML format with common variable and unit names from the CUAHSI controlled vocabularies (Horsburgh et al., 2009, 2016). The HIS enables unified access to data synthesized from over 95 providers via the Hydroclient interactive portal. However, the WaterOneFlow web services need to be hosted and maintained by the provider or CUAHSI, which limits its application to data sources that belong to the HIS ecosystem. The HIS system also does not support large data downloads as the Hydroclient limits search and access to 25,000 results. This tends to be problematic for intensive data-driven applications such as ML, where programmatic access to large amounts of data from sources outside of HIS may be needed.\nMore recently, federated computational tools have emerged to provide streamlined access to big datasets across different data sources on the cloud. Examples include the Pangeo platform (http:\/\/pangeo.io), which creates interactive and reproducible open source workflows to discover, analyze and visualize large geoscience datasets on cloud resources, and is integrated with data discovery tools such as intake (https:\/\/github.com\/intake) and the SpatioTemporal Asset Catalog (https:\/\/stacspec.org\/). However, these federated approaches and tools only produce integrated data catalogs or co-located datasets. They do not parse or translate data from different sources into an integrated view.\nThus, despite the advances made by these systems, the most essential and difficult data integration tasks\u2014the conceptual reconciliation of the various ways data is served by the providers and harmonization of data formats, units, and semantics\u2014are still left to the end users. There remains a critical need for generalizable frameworks that can be easily used by environmental scientists or practitioners to integrate vastly diverse data types, structures and formats. Such frameworks would automate integration of disparate, multiscale data on-demand from heterogeneous databases, as they are being dynamically updated on the original sources, and enable users to easily search, subset and retrieve synthesized data (BERAC, 2013).\nTo address this gap, we present BASIN-3D (Broker for Assimilation, Synthesis and Integration of Diverse, Distributed Datasets), a generic, extensible data-brokering software designed to integrate heterogeneous, multiscale data into a coherent framework and create synthesized datasets that scientists can easily utilize. We aimed to develop a broker that would allow its users to create a single access point for diverse environmental databases and data types by retrieving data from the sources on-demand and harmonizing the data streams to provide an integrated view. This would help bridge the gap between the user-centric objective, to easily find, subset and synthesize relevant data, and the provider-centric objective, to organize and publish their entire data collection. BASIN-3D enables users to access the most recent version of the data from each of the sources, as if the data were available from a single provider, without losing the data provenance.\nWe demonstrate the utility of BASIN-3D with two prototype implementations that require integration of diverse time-series measurements at watershed to regional scales, from multiple sources that use different data formats, organization and terminologies. BASIN-3D is the first brokering software to our knowledge that enables custom integration of heterogeneous time-series data from users\u2019 preferred data sources, without requiring coordination with providers. Thus, it is intended to make the typical scientific workflow of acquiring and harmonizing data from diverse sources more repeatable and reproducible. Users of BASIN-3D can remain agnostic to the location, format, structure and authorization requirements of the sources. Multiple clients, such as web portals, visualization tools, and analytical and modeling codes, can connect to the BASIN-3D Application Programming Interfaces (APIs) to access synthesized datasets. The flexibility provided by this approach is essential for modern environmental data science applications, where there is a need to synthesize disparate data on demand, to address evolving science questions or stakeholder needs.\nThis paper is organized as follows. In Section 2, we describe the features and architecture of BASIN-3D. Section 3 provides a demonstration of BASIN-3D to integrate diverse observations at watershed to regional scales for two U.S. Department of Energy (DOE) projects. Section 4 discusses the BASIN-3D approach along with its advantages and limitations, as well as opportunities for building generalizable frameworks that integrate large, complex environmental data.\n\n\n2\nMethods: Data synthesis constructs and features of BASIN-3D\nWe followed a scientist-centered design approach to develop BASIN-3D, by first understanding the end product desired through use cases. Then, we worked backward to identify the corresponding data sources, synthesis and transformations, and interfaces needed (Ramakrishnan et al., 2014). The BASIN-3D data integration approach uses a central abstract data-model schema with mappings to different sources (Genesereth, 2010). BASIN-3D utilizes the Open Geospatial Consortium (OGC) and International Standards Organization (ISO) standard \u201cObservations and Measurements; OGC 10-004r3\/ISO 19156: 2013\u201d and \u201cOGC Timeseries Profile of Observations and Measurement; OGC 15-043r3\u201d schemas (Cox, 2011; Tomkins and Lowe, 2016).\nThe following sections describe the key concepts, architecture and usage models of BASIN-3D. Although the current implementations of BASIN-3D use concepts tailored to time-series data used in watershed science, the framework is applicable for a variety of environmental data types and applications.\n\n2.1\nMultiscale spatial and temporal representations\nEnvironmental observations made at different spatial scales are often grouped using hierarchical relationships to facilitate data management, synthesis and aggregation. The definition of spatial extents and groupings can be domain or study specific, and can include multiple hierarchical relationships (Christianson et al., 2017). For example, an observation made at a point location can be represented as a member of a USGS Hydrologic Unit Code (HUC) hierarchy of hydrological features that includes the river basin and watershed, as well as part of other spatial hierarchies like the study plot and site (Fig. 2\n).\nBASIN-3D uses constructs from the OGC standard to represent multiscale spatial elements with their location features, associated groupings and hierarchies. In particular, BASIN-3D uses \u2018Monitoring Feature\u2019 entities which inherit components of the OGC entities \u2018Feature\u2019, \u2018Sampling Feature\u2019, \u2018Spatial Sampling Feature\u2019 (Appendix 1) [Cox, 2011; Tomkins and Lowe, 2016]. Monitoring Features are classified by a controlled list of Feature Types that represent spatial features at different scales relevant to watershed sciences: \u2018Region\u2019, \u2018Subregion\u2019, \u2018Basin\u2019, \u2018Subbasin\u2019, \u2018Watershed\u2019, \u2018Site\u2019, \u2018Plot\u2019, \u2018Horizontal Path\u2019, \u2018Vertical Path\u2019 and \u2018Point\u2019 (e.g. Fig. 2); this list is specific to each BASIN-3D implementation and can be expanded to include additional Feature Types relevant to other disciplines. Monitoring Features, as an extension of Spatial Sampling Features, are geographic entities that have a shape property to describe their spatial geometry as one of four types specified by OGC: \u2018point\u2019 (e.g., point, specimen), \u2018curve\u2019 (e.g., river, well, tower), \u2018surface\u2019 (e.g., river basin, watershed, site, plot), and \u2018solid\u2019 (e.g. lidar cloud). The physical location coordinates of a Monitoring Feature are represented using the Federal Geographic Data Committee (FGDC) data standard (FGDC 1998), which provides support for multiple spatial reference systems including geographic (latitude\/longitude), grid (Universal Transverse Mercator), and planar (distance\/bearing representation) coordinates. Monitoring Features can be infinitely nested using parent-child spatial relationships. For example, a plot containing multi-level wells will have three types of Monitoring Features defined as a surface (plot)\u00a0>\u00a0curve (well)\u00a0>\u00a0point (sensors at different depths in the well; e.g. Fig. 2).\nBASIN-3D also supports observations collected at different temporal scales and resolutions. Synthesis over different temporal resolutions is enabled via three parameters defined in the OGC observation-based framework: (1) phenomenon time (the time of the observation); (2) aggregation duration (a qualitative description of the duration over which the observation was acquired, e.g., annual, day, hourly); and (3) time reference position (the position of the time value within the observation time, e.g., start, middle, end, instant). These parameters can accommodate different temporal resolutions provided by the data source. For time-series observations, an aggregation duration parameter is typically specified by the data source, and the retrieved data is included in the Observation Result (Section 2.2) as a Time Value Pair (TVP).\n\n\n2.2\nAbstracted representations for diverse observation types\nBASIN-3D also supports diverse observation types with the OGC concepts. It uses a generically-defined \u2018Observation\u2019 entity with three components: the \u2018Observed Property\u2019 describes the measurement (e.g. river discharge, stream chemical concentrations), the \u2018Feature of interest\u2019 defines the subject of observation (e.g. river), and the \u2018Observation Results\u2019 defines the results of the observation using abstracted data structures (e.g., a time-series of coupled timestamps and values). The components of the Observation are linked as follows: Observation Results of an Observed Property are reported for a Feature of interest, typically specified as a representative Monitoring Feature (Section 2.1).\nBASIN-3D maintains a controlled list of Observed Properties and a preferred set of units to which the data provider's variables and units are mapped (Appendix 2). This enables BASIN-3D to harmonize variable names across data used for the same measurement (e.g., Al, Aluminium) into a single Observed Property for a diverse set of observations (e.g., physical, chemical and biological measurements) with standardized units.\nBASIN-3D also uses abstracted Observation Result types, (e.g. time-series, image, grid), instead of discipline-specific representations (e.g., discharge, water quality parameter). For our applications so far, we have implemented an Observation Result for time-series data. Thus time-series data from different science disciplines or research applications (e.g. meteorology, geochemistry) are transformed into Observation Results in the uniform format \u2018Measurement Timeseries Time Value Pair (TVP)\u2019, an array of paired timestamp and data values. All BASIN-3D timestamps follow the ISO 8601 standard (https:\/\/www.iso.org\/iso-8601-date-and-time-format.html). The Observation Result can be extended to represent additional data types beyond time-series measurements, such as categorical data, imagery, and remote sensing (Section 4.1). Other metadata such as the measurement units are provided within the Observation Result object.\n\n\n2.3\nFramework architecture and implementations\nBASIN-3D presents two approaches for data integration \u2013 first as a Python library, and secondly as a Django web-based application (https:\/\/www.djangoproject.com\/) leveraging its vast extensions to provide mechanisms for site administration, authentication and authorization, and documentation. It has a modular architecture consisting of (1) a \u2018Data Acquisition Layer\u2019 that connects to data sources and retrieves data dynamically, (2) a \u2018Data Synthesis Layer\u2019 where data are transformed to the BASIN-3D schema (hereafter referred to as the Synthesis models), and (3) either a \u2018Data Output Layer\u2019 that transforms synthesized data into specified data structures in the Python version (Fig. 3\na) or a \u2018Web Service Layer\u2019 that receives requests from and returns results to different clients in the Django version (Fig. 3b). It also contains an extensible internal catalog to maintain a list of data sources with their authentication information, and a controlled vocabulary of Observed Properties.\nThe Data Acquisition Layer provides functionality to customize data source connections as required for the application using a plugin architecture containing extensible Python classes. The base Plugin classes enable connection to any network-accessible source such as a database, web service or a remote or local filesystem. They also include an extensible HTTP connection module with support for some common authentication methods such as the Hypertext Transfer Protocol (HTTP) authentication API that supports OAuth2 (https:\/\/oauth.net\/2\/) and token-based authentication. Custom data access plugins consist of 2 components: 1) a python module, and 2) a csv file with a mapping of data source variables to BASIN-3D variables. In the plugin python module, the developer extends the base plugin classes and implements the authentication required by the source (if any), constructs queries to retrieve data and metadata required by BASIN-3D (Table 2\n), and maps the structure, format and semantics of the returned data to the Synthesis models. In particular, information about measurement locations (via a mapping to Monitoring Feature objects) and time-series data (via a mapping to Measurement TVP Timeseries objects) is configured in the python plugin module. Mapping to the BASIN-3D synthesis models is open-ended and accommodates a range of scenarios depending on the availability of data or metadata from the data source. Plugin developers can choose to return mapped data from the data source, return data from other supplementary local or remote sources, or return nothing if no relevant information is provided by the data source. The WFSFA implementation (Section 3.2) describes additional examples of plugin configuration. Plugins can be shared between the Python library and Django implementations. Currently both versions are bundled with a plugin to the public USGS National Water Information System (NWIS; https:\/\/nwis.waterdata.usgs.gov) that can be used out-of-the-box to access the NWIS data and also used as a template to create new plugins to connect to new sources. After the plugins are created, it is trivial to query the BASIN-3D APIs for integrated search and access of data across all configured sources. Any custom data access plugin that extends the BASIN-3D Plugin classes can be registered for use with the Data Synthesis Layer using a simple function call.\nThe Data Synthesis Layer transforms data received from the sources into two BASIN-3D synthesis objects that represent time-series data, namely the Monitoring Feature (e.g., Watershed, Site, Point) and Measurement Timeseries TVP (Sections 2.1 and 2.2). The variables used by the data sources are harmonized to a controlled vocabulary of Observed Properties. The synthesis layer supports common query parameters to filter synthesized data by time, location, observed properties and data quality (Table 2).\nThe Data Output Layer translates the BASIN-3D synthesis objects into commonly used analysis formats. BASIN-3D currently supports Python Pandas dataframe objects and csv files for Measurement Timeseries TVP data as well as Python dictionary objects and csv files for the Monitoring Feature and Observed Property metadata. It also supports HDF5 file formats that contain both the data and metadata. The output formats are specified by the data user and can be extended to additional types.\nThe Web Services Layer provides a scalable, fault tolerant and extensible Representational State Transfer (REST) API powered by the Django Application Framework (https:\/\/django-basin3d.readthedocs.io\/). Clients can use the Synthesis Web API to view the catalog of locations and observed properties, make data requests and retrieve results in a Javascript Object Notation (JSON) format.\n\n\n\n3\nResults: Applications of BASIN-3D to integrate water data\n\n3.1\nPython application to synthesize regional-scale water data\nA first application of BASIN-3D is for a Python-based, data-driven framework iNAIADS (iNtegration, Artificial Intelligence, Analytical Data Services) developed for a DOE project that aims to predict how hydrologic disturbances will change water quality in large river corridors of the United States using ML models that account for differences in watershed characteristics, including geomorphology, geology, climate and land use. Thus, the study needs to utilize hydrological, geochemical, climate time-series and other spatial datasets at point to regional scales from a vast array of sources such as NWIS, Daymet (https:\/\/daymet.ornl.gov\/) and the National Hydrography dataset (https:\/\/www.epa.gov\/waterdata\/nhdplus-national-hydrography-dataset-plus). The project requires flexible programmatic access to data sources for exploratory analysis, subsetting and retrieval of data on-demand, and repeatable integration of large datasets into an easily useable format. Here, BASIN-3D is used to integrate data as standardized inputs for the analytical and ML modules, and insulate them from any changes in how data is served by the sources.\nThe BASIN-3D Python library (basin3d) is a standalone software that can be deployed with the simple \u201cpip install\u201d command. It is easy to integrate with typical data wrangling, analysis and plotting capabilities within Jupyter notebooks (http:\/\/jupyter.org) or other Python scripts (Fig. 4\n). To request data, users specify the set of monitoring features, observed properties and time period of interest as parameters within a Python function call (see basin3d documentation: https:\/\/basin3d.readthedocs.io\/). The output is returned as a Python Pandas dataframe with harmonized units and aligned timestamps where each time-series is a column named by the combination of Monitoring Feature ID and the BASIN-3D Observed Property variable name. The Pandas missing value is used if a particular time series does not have a measurement for a given timestamp. The integrated data can be saved in HDF5 file formats for offline retrieval. The outputs also separately return associated metadata as a Python dict object with elements for each time series (i.e., column), which includes metadata such as the Observed Property, and results metadata such as units, statistics, result quality and temporal aggregation. These metadata attributes can be modified as needed, and future extensions will include the original variable name and monitoring feature geolocation details, if provided by the data source, to enable transparency in the plugin mappings.\nAs an example application, basin3d was used to integrate 70 years (1950\u20132020) of daily stream discharge and temperature data from NWIS and meteorological data from Daymet (version 4) for stations in all hydrologic regions within the Continental United States (CONUS) using its Synthesis Python API calls with just a few lines of code (https:\/\/basin3d.readthedocs.io\/en\/latest\/quick_guide.html). The integrated datasets were used to create input and validation datasets for ML models to predict stream temperature. By using basin3d, it would be trivial to extend the integrated datasets to add new years of data. Any changes to the underlying data sources such as a new version of the Daymet dataset being made available or changes in variable semantics or data access mechanisms can be handled within the BASIN-3D plugin, without requiring changes in the analytical or ML modules.\n\n\n3.2\nWeb-based application for an integrated data portal at the watershed-scale\nThe second application of BASIN-3D is for the DOE's Watershed Function Science Focus Area (SFA; http:\/\/watershed.lbl.gov), which seeks to gain a predictive understanding of how perturbations like early snowmelt influence hydrobiogeochemical processes in mountainous watersheds and impact downstream water availability and quality at subseasonal to decadal timescales (Hubbard et al., 2018). The project has two field sites in the Upper Colorado River Basin of the US: the headwaters East River catchment near Crested Butte, CO (ongoing since 2014), and a floodplain site on the Colorado River near Rifle, CO (active from 2009 to 2017). The project, in partnership with several collaborating organizations, generates a vast amount of diverse data, including hydrogeological, geochemical, climate, metagenomic and remote sensing observations (Kakalia et al., 2021). The SFA maintains two private databases for each field site, the East River (ERDB) and Rifle (RiDB) databases that hold the project's sensor- and sample-based observations and provide APIs to access the data (Varadharajan et al., 2019). Data from the remote sensing campaigns are maintained separately on SFA or other data systems such as the National Snow and Ice Data Center (NSIDC). Metagenomic data are held in a specialized database, ggKBase (http:\/\/ggkbase.berkeley.edu). The SFA also utilizes data from pre-existing infrastructure, such as USGS gaging stations, the Environmental Protection Agency's (EPA) Clean Air Status and Trends Network (CASTNET) and National Resources Conservation Service (NRCS) Snow Telemetry (SNOTEL) sites.\nThe project required integration of these diverse data held in different sources to minimize redundant and inconsistent efforts by scientists to retrieve and synthesize data. A critical need was for a software to integrate data from the two SFA private databases that required authentication with data from public sources such as the USGS NWIS and EPA (Fig. 5\n). Hence, the web version of BASIN-3D was used to support integration across the SFA's East River and Rifle field sites and USGS sites across the East-Taylor Watershed, and to support serving the data through a user-friendly interactive web portal.\nIn addition to the core BASIN-3D framework, the implementation needed two additional software components to enable a web-based view of the integrated data: (1) the Watershed SFA Broker Service (subsequently referred to as WFSFA); and (2) the SFA data portal to access the integrated datasets. The WFSFA is a custom Django-based broker implementation of BASIN-3D with plugins to connect to three remote data sources - the ERDB, RiDB and NWIS (Fig. 3b). In each plugin, a mapping between BASIN-3D synthesis objects and the data source objects are defined (see Section 2.3). For example, Monitoring Feature with type point in BASIN-3D is mapped to Location objects in the ERDB and RiDB, and to the site object in NWIS (Table 2). The Monitoring Feature with type watershed is mapped to the HUC watershed object in NWIS, and is explicitly defined for the ERDB and RiDB since they do not provide the watershed information as part of the location metadata. The plugins also transform source variable names to BASIN-3D controlled vocabularies (e.g. Al in the East River database to Aluminium (Al)). Data synthesized by the WFSFA are accessible through Synthesis Web API calls (Table 2).\nAuthenticated SFA users can access and download integrated datasets using a web portal with search and interactive visualization capabilities (Fig. 6\n). To request data, users select the locations, observed properties (parameters or measurement variables), time period and data sources (sites) of interest on the portal's selection widgets. The portal features filtering capabilities to help users identify observed properties measured at a set of locations and vice versa, which dramatically reduces the number of futile searches for which no data is available. Data search requests trigger Synthesis API requests via the WFSFA to dynamically retrieve data from the relevant sources, and return a Measurement Timeseries Time Value Pair (TVP) Observation Result (Fig. 7\n). The portal displays results as a downloadable table of values, as well as interactive Javascript visualizations (https:\/\/dygraphs.com\/). The use of BASIN-3D enables the portal code to be maintained separately from the plugins accessing the data. Updates to ERDB and RiDB are handled by modifying the plugins in the WFSFA, which minimizes maintenance required for the portal code.\n\n\n\n4\nDiscussion\n\n4.1\nThe BASIN-3D brokering approach\nBASIN-3D is a unique tool that serves different purposes than any prior broker-based environmental data integration system. In the era of data-intensive science and ML, there is a need for tools that enable users to integrate data across providers of their choosing, in a manner that makes most sense for their applications. In some cases users may want to integrate data differently, even when connecting to the same providers. It is impossible for any centralized system to anticipate the myriad data sources and ways in which users want to pull and synthesize data.\nHence, we developed BASIN-3D to provide a flexible means for users to customize their data integration and to give them greater control on how data across providers are mapped into a common format. In particular, it was designed to address the following needs from our use cases, for which there was no pre-existing solution.\n\n1.\n\nData integration for modeling and analysis applications - There was a need to pull together large datasets for exploratory analysis and ML (Section 3.1), which requires the ability to query and retrieve data using a programmatic API without any prior coordination with the data sources. It also requires support for JSON, as a concise and easily parsable format used in modern software applications. With BASIN-3D, we integrated data from NWIS (discharge, water temperature), and Daymet (meteorological parameters) for stations in all regions of the CONUS into a single data frame with harmonized timestamps and units using the default APIs provided by the data sources. This data frame was also stored in the HDF5 file format for offline access by the analytical algorithms. BASIN-3D allows users to create their own plugins to flexibly connect to additional data sources of interest.\n\n\n2.\n\nConnection to private databases - A critical need for one of our use cases was to integrate data from a project's multiple private databases with data from public sources such as the USGS or EPA (Section 3.2). We note that even within a single project, there may be a need to synthesize data from multiple sources, some of which may not be available from a public provider and do not provide data conforming to existing standards. BASIN-3D also supports both token and OAuth authentication making it convenient to connect to private databases that use these two common authentication mechanisms.\n\n\n3.\n\nSupporting\nd\niverse\nd\nata\nt\nypes - Although the current version of BASIN-3D only handles time-series data, it is extensible to represent more general observations including remotely-sensed and simulation data because of its use of the OGC concepts. For example, to incorporate remote sensing data, an Earth Observation Result type, or more specifically an Optical Earth Observation Result, can be defined to handle a standard gridded data format, such as that described as eop: Earth Observation in the OGC Earth Observation Metadata Profile for Observation and Measurements (OGC 10-157r4; see Appendix 1b). A rich set of additional parameters such as image resolution and processing information can be selectively included to fully describe the Observation. Similarly, for simulated data a model-specific Result type can be defined if an existing Result type (e.g., gridded or time-series) is not suitable. An Observation parameter can be defined to indicate that the data are simulated. Currently, we have implemented a lean data model that is based on our use cases and can be extended as needed to support additional data types.\nBASIN-3D also provides a mechanism for maintaining and amending a controlled list of Observed Properties for a variety of measurements. It comes with a default vocabulary (Appendix 2), created to support the diversity of measurements in our applications. However this list can be replaced with any preferred terminology or ontology by updating the Observed Property names in the BASIN-3D catalog, updating the mappings of the new terms to the data source terms in a plugin mapping csv file, and registering the updated plugin with the BASIN-3D Synthesis models. Following this, the new controlled vocabulary will work without requiring any changes to the plugin code. The controlled vocabulary focuses on the Observed Property alone and does not distinguish between units, aggregation type or sampling method, which are considered characteristics of the Observation. Separation of the Observed Property from the abstracted data representation Observation Results provides the flexibility to handle diverse observation data types in a similar manner. For example, it is easy to synthesize continuous sensor-based hydrological time-series data with manual geochemical measurements using the Observation Results of type Measurement Timeseries TVP. Thus, BASIN-3D clients (e.g., web portals, visualization, analysis software) only need to use the Measurement Timeseries TVP to work with heterogeneous, time-series data across sources.\n\n\n4.\n\nSupporting\nd\nifferent\ns\npatial\nr\nepresentations - BASIN-3D can be expanded to support a variety of features and groupings by abstracting Spatial Sampling Features to the OGC's four geometries (point, curve, surface, solid). By specifying Monitoring Features with parent-child relationships, observations can be specified and synthesized at any defined spatial scale, as well as aggregated across scales. This provides flexibility with querying and integrating data across different scientific representations of sampling designs and aggregation (e.g. by a particular field site or across a river basin).\n\n\n\nWe also note that the challenges with data integration are not unique to geosciences and are relevant to other fields such as intelligent transportation systems, autonomous vehicles, smart cities, and industrial applications utilizing the Internet of Things (IoT) where similar or alternate solutions to BASIN-3D may exist. One example is a European open source IoT framework FIWARE (Cirillo et al., 2019), where adhoc integration modules and FIWARE IoT agents translate data provided by cities to the Next Generation Service Interface standard data format (OMA, 2012). These \u201cadapters\u201d are similar to our concept of a plugin, which generate data in a uniform format and also generate metadata following the FIWARE and Open and Agile Smart Cities (OASC) data models for downstream applications. More broadly there have been decades of research and applications based on semantic data integration to link data from heterogeneous sources onto a reference ontological model (e.g. Charpenay et al., 2018 and references therein). The semantic tools provide more sophisticated approaches to data harmonization in comparison to BASIN-3D, but do not handle aspects of spatial data integration and authentication necessary for our use cases. Further work is needed to explore and adopt concepts from the considerable body of literature on data integration beyond the geosciences.\n\n\n4.2\nAdvantages and limitations of using BASIN-3D to address data integration challenges\nWe encountered several data integration challenges across our two use case applications. The first is to reconcile differences in concepts, formats, data models and semantics that can vary widely across data sources (Varadharajan et al., 2019). For example location terms such as \u201cbasin\u201d, \u201csite\u201d and \u201cpoint\u201d are used differently by both data providers and consumers. Sampling hierarchies can be organized using any combination of geometries (e.g. a transect containing wells, both curves as per the OGC), which makes it difficult to define a rigorous hierarchical description of sampling feature types. Similarly, data sources use different terms for variables and only a few use well-described ontologies. For example, NWIS has over 20,000 parameter codes distinguishing between sampling methods (e.g. filter size), units and aggregation (e.g. daily versus instantaneous). Additional transformations such as unit conversions, quality checks or other processing may be needed prior to synthesis. For example, NWIS has different parameter codes for discharge in ft3\/sec (00060) and m3\/sec (30208); and queries using the two codes return different results, requiring unit transformations to synthesize discharge data across one source. Some data may need to pass QA\/QC checks before they are ready to be used, which is challenging since a myriad of data quality flags and methods are used across sources.\nData sources also differ in how they organize and provide access to data. In some cases, sources provide a collection of web services instead of a single end point. For example, NWIS has multiple web services for retrieving daily and instantaneous values for surface water, groundwater and water quality data. Sometimes sources do not provide key metadata and queries for discovery and synthesis. For example, we found critical information was not provided or easily accessed, such as the depths of sensors and reference datums in queries retrieving borehole information, the position of the timestamp in queries retrieving time-series data (i.e., whether the time stamp reflected the start, middle or end of the observation), descriptions of sampling protocols and data quality measures, and a list of all the observed properties measured at a given location. Finally, authentication requirements for data systems pose an additional barrier to users.\nWe addressed many of these challenges using a plugin model that provides flexibility to the critical tasks of mapping and transforming data source objects and terminologies to the generalized BASIN-3D synthesis model (Section 2.3). Additional data transformations for unit conversions and quality checks can also be specified in the plugin. By default, both the units and quality flags provided by the source are passed on to the user in the BASIN-3D Observation Results (except for NWIS river discharge, which is harmonized to m3\/sec). BASIN-3D also provides for three simple QA\/QC flags - \u201cchecked\u201d, \u201cunchecked\u201d, and \u201cnull\u201d. The plugin design is flexible and any number of endpoints can be specified within a single plugin to accommodate for differences in how sources organize and provide data. BASIN-3D plugins incorporate two widely-used authentication protocols to lower the bar to access systems that require authentication.\nThe BASIN-3D brokering approach to connecting with data sources, although versatile, also has its limitations. Currently, an understanding of both the data source and the BASIN-3D synthesis models, along with Python familiarity, are needed to build a custom data access plugin. Because of the diversity of data sources, a separate plugin for each data source is typically required. However as efforts to standardize data sources advance, we anticipate that one plugin may be able to support multiple data sources. Ideally the plugins should be designed so they are easy to create or update as sources make structural changes such as adding new data categories or modifying terminologies. However, creation and maintenance of plugins is currently time-consuming especially if data source structures and semantics are difficult to map onto BASIN-3D's schema. Typically it involves the judgement of domain experts, who determine how to map the various terms and formats to the Synthesis models. Plugin development also requires expertise in software engineering, which may not be available to all scientific groups. The Python library is bundled with a plugin to the public USGS National Water Information System (NWIS; https:\/\/nwis.waterdata.usgs.gov) that serves as an example for developers to build custom plugins to new data sources. However, a more general template and testing tools that facilitate plugin creation and maintenance would lower barriers to this approach.\nThe current version of BASIN-3D specifies default preferred units (Appendix 2), but does not include default unit conversion libraries that would accommodate a variety of units provided by sources. Creating a generalized unit conversion tool is difficult, given the wide variety of physical, chemical and biological variables. Although existing unit libraries such as the Udunits-2 (https:\/\/www.unidata.ucar.edu\/software\/udunits\/) or Pint (https:\/\/pint.readthedocs.io\/en\/stable\/) can be utilized, a generalized broker will need custom unit conversions for non-physical units (e.g. for geochemical data).\nMajor feature enhancements to BASIN-3D are needed to scale to additional sources and handle more diverse and big data. First, the synthesis models must be extended to handle additional data types such as geospatial or remote sensing products and model data (Section 3.2). Then, BASIN-3D needs the ability to retrieve large datasets using protocols such as OpenDAP (https:\/\/www.opendap.org\/) and OGC Web Coverage Service (https:\/\/www.unidata.ucar.edu\/software\/tds\/current\/reference\/WCS.html). For large time series data, paging returned results in the JSON is also an option. Retrieving big data can be a challenge to implement if sources do not support large data transfer protocols or provide basic metadata such as the size of the results or number of data points. BASIN-3D will also need the ability to cache data retrieved from prior queries as it retrieves data on-demand for each query and currently does not have a mechanism to store returned results. Caching data can enable efficient data retrieval, improve performance and the user experience particularly when retrieving large datasets, and will enable access to sources during outages (Blodgett et al., 2015). Caching would also enable users to track versions of data over time and potential impacts of change in the data to analyses or simulations (Ghoshal et al., 2018).\nFinally, a generalized data integration framework should provide detailed provenance information along with data usage rules and citations. This can be difficult to implement in a broker, particularly when there are various versions of the data and if sources do not track or provide the information. BASIN-3D currently enables identification of the source name and URL along with returned results but does not have support for additional citation information within its synthesis models. Creating dynamic data citations for on-demand queries is an important area for improvement to credit the sources and provide users of the synthesized data an ability to precisely cite versions of the data retrieved.\n\n\n4.3\nUsing standards to enable a generalizable, extensible brokering framework\nBASIN-3D has been designed to harmonize, integrate and query diverse datasets that result from a range of field investigations, monitoring networks and model simulations. In particular, use of the OGC and FGDC standards provides a means to support flexible synthesis of diverse measurement configurations and data types using abstracted data structures (Section 4.1). These standards are a suitable choice as they have been developed over several years to enable interoperability across data systems and have achieved consensus across and adoption by various organizations.\nWe encountered some challenges implementing these data standards as the underlying construct for integration. First, it was not easy to use the standards partly because they are specified at a high level and do not provide implementation guidance beyond some simple, limited examples. For example, the OGC standards do not specify implementation of Monitoring Features or its parent features for different shapes or resolve how collections of spatial hierarchies should be organized. Standards also use specialized terminologies that domain scientists may not be familiar with. For BASIN-3D, we had to balance constraints of following the OGC standard using the specified terminology (e.g., using observed_property and measurement_tvp_timeseries), while making the concepts and Synthesis API calls logical to domain scientists. Thus in a few cases we deviated from OGC definitions or terminologies to improve the usability of BASIN-3D for scientific researchers or for other practical reasons. For example, while the OGC standards differentiate between the feature being observed and the representative sampling feature upon which the actual observation is made, BASIN-3D does not make this distinction because most data sources only include information on the Sampling Feature and a \u2018Feature\u2019 in one case may be a \u2018Sampling Feature\u2019 in another. Thus all spatial entities are Monitoring Features in BASIN-3D; however, the data model is implemented as hierarchical classes which enables expansion to support any OGC Feature entity. Similarly, all Feature entities use Feature Types instead of specific, entity-based types (e.g., Spatial Sampling Feature Type) for practical implementation.\nIt is also difficult to identify and track multiple, evolving standards potentially applicable to a situation. There are many standards that are highly suitable to a particular data type and sampling design, but few that are generalizable across the broad suite of diverse measurement types and sampling hierarchies that may be relevant to an interdisciplinary watershed study. As an example, we considered the widely-used GeoJSON specification (http:\/\/geojson.org) as the coordinate representation for Monitoring Features. GeoJSON has become a de facto \u201cstandard\u201d for geospatial data proposed by the Internet Engineering Task Force but is yet to be approved (https:\/\/tools.ietf.org\/html\/rfc7946). However, GeoJSON has several limitations including the inability to extend geometries beyond simple types, such as representation of circles or meshes (OGC, 2017). GeoJSON only represents geographic coordinates, using the World Geodetic System 1984 (WGS 84) datum, with longitude-latitude pairs of a relevant shape (e.g. Polygon), which does not accommodate for location information using other reference datums (e.g. a plot's center point with lengths of the four sides). Thus in many implementations, GeoJSON has to be extended in an arbitrary manner by adding attributes into a non-standardized \u2018properties\u2019 field. As another example, the WaterML 2.0 standard is specifically tailored for water time-series observations (https:\/\/www.opengeospatial.org\/standards\/waterml). However, WaterML cannot be used to represent LiDAR, hyperspectral or snow pit observations from the East River. Additionally, JSON representations are more common in modern applications, as they are less verbose than XML-based standards, more easily parsed and better suited for big data, and also easier for domain scientists to use. Hence, we chose the higher-level OGC standard as the default model for data integration since its use enables BASIN-3D to be extensible to diverse data types without using multiple standards, and also allows for JSON representations.\n\n\n\n5\nSummary and conclusions\nThere is a critical need to integrate heterogeneous, multiscale data to address complex environmental challenges. Often, data is distributed across many sources that do not share common structures and formats. Generalized data management frameworks that integrate diverse, distributed data can facilitate their use in analysis and modeling. We developed BASIN-3D, a data-brokering integration framework that retrieves subsets of data from different sources for on-demand queries and integrates the data into a unified view. BASIN-3D applies concepts from OGC and FGDC standards to create an extensible framework that allows creation of custom plugins to map data sources to a common synthesis model. It can be used as a Django application to support web services and portals, as well as a simple Python library for analysis and modeling. We present implementations of BASIN-3D to integrate diverse time-series data for two DOE projects that study the impacts of hydrological perturbations at watershed to regional scales.\nWe encountered several challenges in building this framework, despite having just two applications. It was difficult to harmonize spatial elements, variable names, data quality terms and units across sources, and it required a combination of domain expertise and software engineering to create software mappings to a common data-model schema. Data sources typically do not provide data with a user's view, and sometimes information needed to synthesize data are unavailable or not easily accessible. Although existing standards provide much value in representing diverse data using a generalizable abstracted approach, they are difficult to use and provide minimal guidance on the details of their implementation. Some of these challenges can be addressed using BASIN-3D's generalized brokering framework. The data integration needs of our applications are broadly applicable to a large number of environmental studies of complex systems. For example, the Critical Zone Observatories have similar interdisciplinary data that require integration across platforms (Zaslavsky et al., 2011). BASIN-3D also applies to policy initiatives that seek to integrate data across sources, such as the federal Open water data initiative (https:\/\/acwi.gov\/spatial\/owdi\/) and California's Open and Transparent Water Data Act (https:\/\/water.ca.gov\/ab1755). The BASIN-3D synthesis constructs are generalizable and can be extended to integrate data types beyond time-series for a wide range of environmental field and modeling investigations.\n\n\nAuthorship statement\nCharuleka Varadharajan was responsible for the conceptual development of BASIN-3D and implementation of its first version. Valerie Hendrix and Danielle Christianson have made equally significant contributions as the lead author and are the primary software developers on BASIN-3D. Madison Burrus has contributed to testing the software deployment for the WFSFA broker and portal. Catherine Wong is responsible for documentation and testing of the open source Github repository. Susan Hubbard is a senior author who is the lead principal investigator on the Watershed Function SFA project and was responsible for developing the vision and ideas for the project's data integration. Deborah Agarwal is a senior author who was responsible for supervising the development of BASIN-3D and implementation for the Watershed Function SFA project.\n\n\nComputer code availability\nBASIN-3D (basin3d at version 0.2.0 and django-basin3d at version 0.0.4\u00a0at time of publication) is available open source on Github https:\/\/github.com\/BASIN-3D with Berkeley Lab's modified BSD License (https:\/\/github.com\/BASIN-3D\/basin3d\/blob\/main\/LICENSE). This version contains most of the software concepts and implementation described here. Some newer components, such as the support for HDF5 file formats and the Daymet integration (Section 3.1) are not in the current version, but will be available in future releases. Documentation for basin3d is available at https:\/\/basin3d.readthedocs.io\/. Documentation for the web version is available at https:\/\/django-basin3d.readthedocs.io\/.\n\n","111":"","112":"","113":"","114":"","115":"","116":"","117":"","118":"","119":"\n\n1\nIntroduction\nEfficient water resource management and flood mitigation require accurate streamflow hindcasting and forecasting. With the increasing computational power and available data, simulation models have become more sophisticated in how they represent mechanisms in the modelled system. However, even after using advanced optimization or model calibration techniques, model output errors can still be considerable and inevitable due to uncertainty in the model structure, inputs, and parameters (Liu and Gupta, 2007). To reduce these errors, one of the techniques is a model-output updating approach, also known as error-updating approach (Shamseldin and O'Connor, 2001).\nThis error updating approach involves a simulation model followed by an updating model to correct the errors of the simulation output (predictions). The simulation model is usually a physically-based model, and the updating model consists of one or multiple machine learning methods (Abrahart et al., 2012; Bogner et al., 2016; Humphrey et al., 2016; Papacharalampous et al., 2019; Tyralis et al., 2019a). With the updating procedure, we can improve the predictions by combining both the process knowledge in the physically-based model and the empirical knowledge in the updating model. This updating approach has been widely applied in real-time flood forecasting for operational purposes to improve model accuracy at larger lead times (Humphrey et al., 2016; Kurian et al., 2020; Young et al., 2017). In this paper, we refer to this model-output updating approach as the \u201cerror-correction\u201d model.\nErrors of streamflow predictions are in general temporally autocorrelated (Evin et al., 2014; Sorooshian and Dracup, 1980; Toth et al., 1999). Including autoregressive (AR) structures in the error correction can be valuable for forecasting applications. One application is to use AR models to correct prediction errors from physically-based models (Evin et al., 2014; Li et al., 2016; Toth et al., 1999). Although the AR models perform well in correcting errors, the AR models rely heavily on observed streamflow from the previous time steps and thus only provide accurate estimates for forecasting with relatively short lead times up to a few hours or days. For error correction over longer lead times (weeks-years), or for error correction of historical simulations over time spans that lack observed streamflow data, these methods cannot be used. In these cases to improve prediction accuracy, we can use non-linear methods that explain the nonlinear interaction between the covariates (inputs or state variables of the model) and the response (streamflow error) (Mosavi et al., 2018).\nTherefore, to improve prediction accuracy for long lead times, we used a machine learning model, random forest (RF) (Breiman, 2001), as the updating model to capture potential nonlinear relationships, and we also present an AR structure by using historical daily meteorological values (inputs of the model) as the input to the error-correction model. After the streamflow observations from a catchment were used to train the error-correction model, no observations were needed for that catchment in time spans for which no observations is available. To achieve this goal, the RF model also included an extensive set of simulated state variables, which had not been done before (Humphrey et al., 2016; Noori and Kalin, 2016).\nFor the simulation model, we used a global process-based model: PCR-GLOBWB model. It considers not only natural processes but also human water use (Sutanudjaja et al., 2018; van Beek et al., 2011; van Beek and Bierkens, 2008). The PCR-GLOBWB is capable of providing satisfactory streamflow predictions across the globe (Sutanudjaja et al., 2018). But, in some cases, the PCR-GLOBWB model fails to reproduce streamflow observations due to errors in model structure, drivers, and parameters.\nWe built a model framework that improved PCR-GLOBWB daily streamflow predictions using RF as an error-correction model. In this model, errors in PCR-GLOBWB daily streamflow predictions were estimated by meteorological drivers and state variables of PCR-GLOBWB. Then, errors estimated by RF were used to correct streamflow predictions. We quantified the improvement of streamflow predictions from the PCR-GLOBWB using our error-correction model framework using Random Forests (RF). We also assessed how different catchments would have different variables contributing most to error correction.\nWe implemented six model configurations with two settings in PCR-GLOBWB calibration and three combinations of different predictors used in the RF, including a benchmark model that only considers the memory effects of the catchments using historical meteorological variables.\n\n\n2\nMethods\n\n2.1\nStudy area\nTo investigate how catchment characteristics influence the error correction, the Rhine basin was chosen as the study area. Three gauging stations with different upstream hydrological characteristics were selected, which are Basel and Lobith along the Rhine River and Cochem along a Rhine River's tributary \u2013 the Mosselle River (Fig. 1\n, Table 1\n).\nThe streamflow regime at Basel, located in the Alpine region of the upper Rhine, is dominated by snow and glacier melt in spring and summer, whereas streamflow at Cochem is characterized by rainfall, reaching its maximum in winter and its minimum in late summer (August and September). The flow regime at Lobith, located in the lower Rhine, not only is mainly influenced by rainfall events but also reflects various processes in upstream areas. The streamflow reaches its maximum in late winter and minimum in late summer. The major land use types of the Basel's and Lobith's watersheds are farmland and forest, but the land cover of Basel has more grassland, surface water and glaciers (Schick et al., 2018).\n\n\n2.2\nModel framework\nA model framework was developed that modified the streamflow predictions from the PCR-GLOBWB model by correcting prediction errors using the random forests (RF) model. The following section describes the model structure of the two models (2.2.1 & 2.2.2), the model configurations tested for our model framework, and how the model performance was evaluated (2.2.3).\n\n2.2.1\nPCR-GLOBWB\nPCR-GLOBWB is a grid-based global hydrological model, which considers natural hydrological processes as well as anthropogenic influences such as human water use and reservoir operation (Sutanudjaja et al., 2017). For this study, the model was run with a daily time step and with a spatial resolution of 30 arcmin (\u223c50\u00a0km at the equator). For details regarding the model structure of PCR-GLOBWB, readers are referred to Sutanudjaja et al. (2018), van Beek et al. (2011), and van Beek and Bierkens (2008). In Supplementary Materials, Section 1, we briefly describe the meteorological forcing input variables and the simulated hydrological variables that are mainly used as the predictors for the RF model (see also Fig. 2\n & Table 2\n).\nTo evaluate the effect of PCR-GLOBWB parameterization on the performance of the error-correction model, two parameterizations were used in PCR-GLOBWB, one with default parameter values and the other using the calibrated parameters found by a brute force method (Beven, 2012). This calibration approach was implemented for five parameters (minimum soil depth fraction, saturated hydraulic conductivity, groundwater recession coefficient, degree day factor, and Manning's n) with a search function to maximize the Kling-Gupta efficiency (KGE) value (Ruijsch et al., 2021). The model with default parameter values is referred to as PCRun, and the one calibrated by the brute force approach is called PCRcalibr. Each gauging location was calibrated separately. Detailed information about the calibration setting and the sensitivity analysis of the parameters can be found in the Supplementary Materials, Section 2.\n\n\n2.2.2\nRandom forests (RF)\nRF (Breiman, 2001) is an ensemble tree-based algorithm that samples predictor variables in each split node of an independent tree and aggregates all the trees for prediction. Each regression tree is developed by a subset of observations and variables by minimizing a loss function, in our study, mean squared errors (MSE). RF has been applied in the geoscience field, ranging from hydrological prediction (Mosavi and Ozturk, 2018; Tyralis et al., 2019b), lithology classification (Bressan et al., 2020), landslide susceptibility (Goetz et al., 2015), and remote sensing downscaling (Ebrahimy and Azadbakht, 2019).\nThe ensemble algorithm in the RF can deal with the high correlations occurring between the predictors by randomly selecting a subset of candidate variables for each split. This bootstrapping technique avoids highly influential variables dominating all trees. The RF also samples data with replacement for training each tree. Therefore, not all training data is used in every tree built. The unused data can be then used to estimate the error. The data is often referred to as out-of-bag (OOB) observations. Each observation has an OOB prediction, and we can obtain an overall OOB root-mean-squared error (RMSE) of the n observations, instead of using cross-validation, which requires large computation efforts when the number of predictors is large.\nThis overall OOB RMSE was used for hyperparameter tuning in the RF. The major influential hyperparameter in the RF is the number of candidate predictors considered at each split. The number of trees does not greatly influence the model performance in terms of OOB RMSE once it exceeds a certain threshold value. The hyperparameters were optimized using a grid searching process with the number of trees ranging from 100 to 900 with an increment of 200 and the number of candidate predictors from 3 to the total number of predictors with an increment of 2. In total there were 95 combinations of hyperparameters for tuning. The hyperparameter values that led to the smallest OOB RMSE were used in the RF.\nThe RF was used to estimate streamflow prediction errors of PCR-GLOBWB. These errors were related to variables used in PCR-GLOBWB, which were the meteorological driving variables (x\n\nd\n) and key simulated state variables (x\n\ns\n). The response variable (dependent variable) in the RF was the model error, calculated as:\n\n(1)\n\n\n\ne\nt\n\n=\n\nq\no\nt\n\n\u2212\n\nq\nm\nt\n\n=\nf\n\n(\n\n\nx\nd\nt\n\n,\n\n\nx\ns\nt\n\n\n)\n\n\nf\no\nr\n\ne\na\nc\nh\n\nt\n\n\n\nwhere e indicates prediction error (m\/day), q is streamflow (m\/day) normalized to the catchment area of the gauging station with subscript m representing PCR-GLOBWB predictions and subscript o representing observations. Superscript t represents the time index with a time step of 1 day. The streamflow observations were obtained from the Global Runoff Data Center (GRDC, http:\/\/www.bafg.de\/GRDC). In this paper, we used \u2018residuals\u2019 and \u2018errors\u2019 interchangeably to describe the difference between model predictions and measurements. The prediction errors estimated by the RF were then added to the PCR-GLOBWB predictions to obtain final corrected streamflow predictions.\nIn the training period, the response variable thus included the \u2018observations,\u2019 with the amount equal to the number of timesteps. The predictors used in the RF included the meteorological variables as well as state variables simulated by PCR-GLOBWB, averaged over the watershed of the gauging station, at each time step t (Table 2). We used driving forces and states at time t (predictors) to predict error in discharge at time t (response variable). Historical driving forces at previous time steps (e.g., at t-1, t-2, \u2026) were also included as predictor variables to investigate the memory effects from the meteorological factors and to implicitly deal with AR structures in our RF models. We also included Julian days as a predictor to represent the seasonality of the errors besides the potential evapotranspiration and temperature. We created an RF model separately for each gauging station and for the calibrated PCR-GLOBWB streamflow (PCRcalibr) and the uncalibrated streamflow (PCRun). In the RF, the importance of each predictor was measured by averaging its total decrease in the remaining MSE left after the predictor was used as node splits. This so-called variable importance indicates the contribution of a variable to reducing the MSE and tells us in our study how informative a variable is for error-correction of the hydrograph. The variable importance was calculated from the ranger function by setting variable importance mode as \u201cimpurity\u201d in the ranger package version 0.12.1 (Marvin N. Wright and Ziegler, 2017) in R, which gives us the average of total decrease in MSE described above.\n\n\n2.2.3\nModel configurations and model performance evaluation\nTo investigate how and to what extent the RF can improve streamflow predictions by estimating the prediction errors, we tested six unique combinations of model configurations, including three combinations of predictors used in the RF (Table 3\n) and two configurations for PCR-GLOBWB calibration. One RF was built by using only meteorological driving variables from the current and previous 10 timesteps as predictors, called RFd-lagged. This serves as a baseline model that considers memory effects only. The other two RF configurations used both meteorological variables and simulated concurrent state variables as predictors, referred to as RFd_s and RFd-lagged_s. The RFd_s includes only concurrent meteorological variables, whereas the RFd-lagged_s includes both concurrent and historical meteorological variables from the previous 10 days. The state variables are described in Table 2.\nIn the PCR-GLOBWB configurations, the PCRcalibr was calibrated by the brute force method, and the PCRun used the default parameterization in the PCR-GLOBWB. More information about the two calibration settings is given in Section 2.2.1 and Section 2 of Supplementary Materials. The process of implementing the error-correction model framework is illustrated in Fig. 3\n. Results with error correction were compared with PCR-GLOBWB simulations without error-correction models, expressed as PCRun and PCRcalibr. The RF models served as post-processing models for the PCR-GLOBWB. Therefore, in total by combining the two PCR-GLOBWB calibration settings and three RF predictor settings, we built six model configurations.\nAdditionally, we compared our model framework with a Generalized Least Square (GLS) model. The GLS models are linear regression models fitted by generalized least square, which is a generalized form of ordinary least square (OLS) and minimizes distances relative to the covariance of residuals (squared Mahalanobis distance) (Taboga, 2017) instead of minimizing linear distances of residuals like OLS. This generalized method transforms the residuals so that the error variances are uncorrelated. In the GLS, the autoregressive moving average (ARMA(p, q)) model should be specified to correct the temporal dependency in the residuals. We specified the errors as a third order autoregressive model (AR(3), which can be written as ARMA(3, 0)). The AR(3) model was used because the ACF showed a gradual decreasing trend (q\u00a0=\u00a00) and the PACF dropped sharply after three lags (p\u00a0=\u00a03) (Supplementary Fig. 10). The independent variables in the GLS models were the meteorological variables, the same as the ones used in the RF (Table 2). We used the gls function in the nlme package version 3.1\u2013152 in R to implement GLS (Pinheiro et al., 2021). The conventional autoregressive model was not used, because it is not suitable for estimating streamflow without observations available at a large lead time, which was 10 years in this study. Therefore, we used the GLS model to include the autocorrelated structure of the prediction residuals as a linear form.\nWe also quantified the uncertainty in the corrected streamflow predictions using the Quantile Regression Forests (QRF) (Meinshausen, 2006) in our model framework. The QRF has been implemented for a similar purpose in previous post-processing hydrological studies (Bogner et al., 2016; Papacharalampous et al., 2019; Tyralis et al., 2019a). The QRF predicts conditional quantiles of the response variable, which is the PCR-GLOBWB prediction errors in this study. Then, the QRF conditional quantiles were added to the PCR-GLOBWB predictions to obtain the quantiles of the corrected streamflow predictions. Negative quantiles of the corrected streamflow predictions were replaced with zeros The QRF used the same optimal hyperparameters as in the RF described in the manuscript (Section 2.2.2). The quantile regression was implemented using the ranger package version 0.12.1 (Marvin N. Wright and Ziegler, 2017) in R. We only implemented QRF using current meteorological and state variables as predictors described in Table 2.\nFor the evaluation of model performance, the Nash-Sutcliffe efficiency (NSE) and the Kling-Gupta efficiency (KGE) were calculated. The NSE represents the relative performance of a model compared to a na\u00efve model that uses the observed average as prediction, and its value is dominated by errors in extreme (low\/high) streamflow. However, using observed average as an evaluation baseline fails to capture the true model performance when streamflow shows great seasonality, and this problem was overcome by the KGE. The KGE can cover a wide range of hydrological characteristics when evaluating the predictions (Gupta et al., 2009), including correlation, bias, and variability between observed and predicted streamflow.\nEquations (2) and (3) show how the coefficients are computed, where q represents streamflow with a unit of m\/day in daily time step t, T is the total number of days, \u03c3 is the standard deviation and \u03bc is the average with subscript o representing observations and subscript m representing predictions made by the model configuration used (Table 3):\n\n(2)\n\n\n\n\n\n\nN\nS\nE\n=\n1\n\u2212\n\n\n\n\u2211\n\nt\n=\n1\n\nT\n\n\n\n(\n\n\nq\nm\nt\n\n\u2212\n\nq\no\nt\n\n\n)\n\n2\n\n\n\n\n\u2211\n\nt\n=\n1\n\nT\n\n\n\n(\n\n\nq\no\nt\n\n\u2212\n\n\nq\n\u203e\n\no\n\n\n)\n\n2\n\n\n\n=\n1\n\u2212\n\n\nM\nS\nE\n\n\n\n\u03c3\no\n\n2\n\n\n\n\n\n\n\n\nM\nS\nE\n=\n\n1\nn\n\n\n\u2211\n\nt\n=\n1\n\nT\n\n\n\n(\n\n\nq\nm\nt\n\n\u2212\n\nq\no\nt\n\n\n)\n\n2\n\n\n\n\n\n\n\n\u00a0\n\n\n\u03c3\no\n\n2\n\n=\n\n1\nn\n\n\n\u2211\n\nt\n=\n1\n\nT\n\n\n\n(\n\n\nq\no\nt\n\n\u2212\n\n\nq\n\u203e\n\no\n\n\n)\n\n2\n\n\n\n\n\n\n\n\n\n\n\n(3)\n\n\n\n\n\n\nK\nG\nE\n=\n\n\n\n\n(\n\nr\n\u2212\n1\n\n)\n\n2\n\n+\n\n\n(\n\n\u03b1\n\u2212\n1\n\n)\n\n2\n\n+\n\n\n(\n\n\u03b2\n\u2212\n1\n\n)\n\n2\n\n\n\n,\n\n\n\n\n\n\nr\n=\ncov\n\n(\n\nq\nm\n\n,\n\nq\no\n\n)\n\n\/\n\n(\n\n\n\u03c3\nm\n\n\u22c5\n\n\u03c3\no\n\n\n)\n\n\n\n\n\n\n\n\u03b1\n=\n\n\u03c3\nm\n\n\/\n\n\u03c3\no\n\n\n\n\n\n\n\n\n\u03b2\n=\n\n\u03bc\nm\n\n\/\n\n\u03bc\no\n\n\n\n\n\n\n\n\n\n\nTo assess model performance using independent data, we split the dataset into training and validation by time. The 1981\u20131990 period was used for calibrating PCR-GLOBWB and training the error correction model. The 1991\u20132000 period was used for validation involving a forward simulation of PCR-GLOBWB and applying the trained error correction model.\n\n\n\n\n3\nResults\nHere, we describe the model performance of each model configuration for the validation period, and how variable importance in RF varies with catchments. The result of the GLS and QRF can be found in Supplementary Fig. 11 and Supplementary Fig. 12.\nThe model performance for the validation period is shown in Fig. 4\n. The uncalibrated PCR-GLOBWB run combined with the error-correction model gave considerable improvement regarding KGE values at all locations. Including meteorological and state variables (RFd_s) resulted in slightly better performance compared to using only lagged meteorological variables (RFd-lagged). The addition of state variables as predictors (RFd-lagged_s) contributed to an improvement of simulations compared to the scenario relying only on the concurrent and lagged meteorological variables (RFd-lagged). However, this was not the case for Basel.\nThe performance of error-corrected calibrated and uncalibrated runs was equally good. This is somewhat surprising, as the performance of the uncalibrated PCR-GLOBWB is considerably worse than the calibrated model (Fig. 4). The RF turns out to be capable of correcting the streamflow even when errors of the simulations are considerable.\nFor the calibrated PCR-GLOBWB run, the GLS error-correction model gave the improvement less than the RF using lagged meteorological variables, where GLS improved KGE from 0.68 to 0.72 for Basel, from 0.57 to 0.59 for Cochem, from 0.71 to 0.72 for Lobith during the validation period (Supplementary Fig. 11) while the RF improved KGE to 0.75 for Basel, 0.83 for Cochem, 0.85 for Lobith (Fig. 4). For the uncalibrated PCR-GLOBWB run, we also observed the superior prediction improvement provided by RF over the one provided by the GLS.\nThe time series of discharge and residuals (Fig. 5\n, Fig. 6\n & Fig. 7\n) show that the error-correction model was able to reduce the prediction residuals. The residuals of PCR-GLOBWB streamflow prediction demonstrated strong temporal variations, especially in Basel having a nival regime (Fig. 5). The prediction residuals of PCRcalibr at Basel were large during the melt season, which is around April to August. By error correction, the prediction errors for Basel were reduced, but it was still difficult to capture the large errors during the high-discharge period (Figs. 5 and 8\n). On the other hand, at Cochem and Lobith, the residuals of PCRcalibr streamflow predictions were highly associated to the streamflow observed (Figs. 6 and 7). The error-correction model was able to reduce prediction errors in particular for the recession part of the hydrograph, which is dominated by the base flow, for Cochem and Lobith (Figs. 6 and 7). From the cumulative distribution functions of streamflow from 1991 to 2000, the CDF from including both concurrent hydrological variables and meteorological variables fit better with the CDF from including only lagged meteorological variables for Lobith and Cochem. For Basel, the lagged scenario fits better during high streamflow, and the one including concurrent state and meteorological variables led to a better fit during low streamflow period.\nThe QRF gave the quantiles of the corrected streamflow predictions. In the QRF using the concurrent meteorological and state variables (Supplementary Fig. 13), the medians of the corrected streamflow predictions at all basins were lower than the observations, and the intervals of the 5% and 95% quantile of the corrected predictions included the observations for most period at all basins, except during the extreme high-flow period when the cumulative density functions of the corrected predictions were higher than 0.99.\nIn the random forest model, the most informative variables varied across stations with different meteorological and hydrological characteristics in upstream areas (Fig. 1; Fig. 9\n). Each RF model configuration (RFd-lagged, RFd_s, RFd-lagged_s) gave different results of the variable importance in each catchment. First, in the RFd_s, for Basel in the upper Rhine region, the surface water storage explained the prediction errors most (Fig. 9A). The amount of snow cover in water equivalent and upper soil water storage played the second and third important role. On the other hand, for Cochem and Lobith, the amount of groundwater recharge was the major factor (Fig. 9B&C). Surface water storage also contributed to estimating prediction errors for Lobith. Second, in the RFd-lagged_s, for all catchments, the top 3 influential variables are the same as the RFd_s (Supplementary Fig. 9b). Third, in the RFd-lagged for Basel, the top important variables are the temperature and the precipitation 1\u20134 days prior to the modelling timestep (Supplementary Figure 9Aa). For Cochem, the top important variables are precipitation from the prior 2\u20136 days (Supplementary Figure 9Ba). For Lobith, the top important variables are precipitation from the prior 4\u20138 days (Supplementary Figure 9Ca).\nFor all locations, the most important variable was also highly correlated with prediction errors (Fig. 9). However, a high correlation between a variable and the prediction errors did not necessarily suggest the variable was informative on reducing prediction errors, since some variables correlated with the prediction errors did not contribute to the error reduction greatly, especially for the PCRun-RFds (Fig. 9B). It should be noted that some variables with low correlation values showed a distinct nonlinear relationship with the errors of the PCR-GLBWOB predictions, as shown in Supplementary Fig. 4, Supplementary Fig. 5, and Supplementary Fig. 6.\nThe result of grid searching for tuning RF hyperparameters showed small variations in OOB RMSE between each combination of hyperparameters. The OOB RMSE ranged from 156 to 218 for Basel, from 130 to 160 for Cochem, and from 352 to 454 for Lobith in the RFd_s scenario, in a unit of m3\/s. Other scenarios gave similar results.\n\n\n4\nDiscussion\nPrevious work has documented the effectiveness of machine learning methods, serving as model-output updating models, in improving streamflow predictions from forward simulation models. However, these studies have not extensively evaluated the use of simulated state variables to estimate prediction errors. In this study, therefore, we developed a model framework in which streamflow predictions from PCR-GLOBWB were corrected by the RF estimating the prediction errors using both meteorological and hydrological variables as predictors of the RF.\nWe found that in all cases the RF-based error-correction model can considerably improve the PCR-GLOBWB predictions, in particular when state variables of the simulation model are included as predictors in the RF. The model performance of the error-correction model did not depend on the parameterization in the PCR-GLOBWB model, since the corrected predictions using the uncalibrated PCR-GLOBWB run were approximately as accurate as those using the calibrated one. This result not only indicates the capability of RF in estimating prediction errors but also suggests that one can obtain considerable improvement in predictions using the error-correction procedure without the cumbersome work of calibrating the PCR-GLOBWB model. A similar result was found in a previous study (Noori and Kalin, 2016), which used machine learning methods to avoid calibrating a hydrological simulation model. But they only used two simulated hydrological variables, which were surface runoff and interflow, in the machine learning methods to predict streamflow instead of prediction errors, and their goal was to apply their model framework to ungauged locations.\nBased on the improvement of streamflow predictions for gauged catchment, our model framework has potential for estimating streamflow at ungauged locations by including more catchment-specific variables as predictors in the error-correction models. In this potential extended application, the RF error-correction model should first be trained by data from many catchments with various catchment characteristics so that the RF model can be generalized to capture various types of catchment-specific patterns of the prediction errors. Then, this generalized RF model can be applied to ungauged catchments. Our study indicates proof of concept for the potential future study topic, and further research is required to validate how well the generalized RF error-correction model can estimate streamflow at ungauged location by including more predictors and data from more catchments.\nAdditionally, the satisfactory model performances of the error-correction model, evaluated by the validation data in a different time period from the training data, indicate the possibility of reconstructing missing discharge observations. These potential extended applications need to include large datasets with an adequate number of catchments to be generalized and be validated by spatial and temporal hold-out test.\nAlthough the results indicate the strength of RF, we observed that the RF does not model residuals satisfactorily during high streamflow periods, evidenced by the heteroscedasticity in the corrected predictions. This could be solved by transforming the values before implementing the error-correction process.\nIn the RF-based error-correction model, the result of variable importance indicates how much each variable contributes to estimating prediction errors. In the RFd_s, it was suggested that the surface water and snow cover at Basel with nival regime were influential in reducing prediction errors, while the groundwater components at Lobith and Cochem with pluvial regime played a major role in that. This may suggest that the potential structural errors existing in PCR-GLOBWB can be found by using RF. This finding of snow-related variables being influential at Basel might result from the highly time-varying degree day factor of the snowmelt module (Ruijsch et al., 2021). The finding of groundwater-related variables being influential at Cochem and Lobithm might result from the land use changes, which lead to changes in infiltration rate and capacity. These systematic changes in the catchment were not captured well in PCR-GLOBWB, and it could be captured by our RF-based error-correction model. Moreover, including both meteorological variables from previous 10 days and state variables do not lead to a different list of influential variables for the three catchments. Thus, this could indicate that no hydrological process knowledge is left in the meteorological data to explain the prediction errors in the RF. But it should be noted that the result here does not indicate any causal relation between prediction errors and state variables and that a more extensive set of catchments with diverse characteristics should be used, before any solid conclusion is drawn.\nTo our knowledge, this is the first study that includes an extensive set of simulated hydrological variables in an error-correction model to improve streamflow and to investigate the error structure of predictions using RF. Compared to the benchmark scenario (RFd-lagged), the improvement from RFd_s and RFd-lagged_s is similar, but the difference in the improvement between different scenarios is larger when using uncalibrated PCR-GLOBWB predictions. This shows that with only historical meteorological variables, error-correction RF models can improve predictions from a simulation model, indicating its potential ability to specify implicit AR structures by providing historical meteorological predictor values.\nThe GLS model with an AR(3) structure improved streamflow predictions lower than the RF using meteorological variables from historical time-steps. This RF model can improve streamflow predictions by incorporating temporal auto-correlation by considering the cross-autocorrelation between meteorological variables and prediction errors. This test further justifies using random forests instead of simple autoregressive models when the goal is to improve streamflow predictions over a long lead time. But we acknowledge that simple AR models are more promising for improving streamflow when forecasting over a short lead time is done.\n\n\n5\nConclusions\nIn this study, daily streamflow predictions from PCR-GLOBWB were corrected by the RF model using its meteorological forcing input and simulated hydrological variables. The RF model, serving as an error-correction model, improved discharge predictions considerably, and the improved performances using the uncalibrated PCR-GLOBWB run were equally good as using the calibrated PCR-GLOBWB simulation. The RF model framework developed here not only gives a better way to improve streamflow predictions than calibrating PCR-GLOBWB parameters, but the results also indicate potential for both reconstructing missing streamflow observations and predicting streamflow at any location, which will be explored in future study.\n\n\nComputer code availability\nThe implementation of the RF was done in R version 4.0.2 (R Core Team, 2020) using the ranger package version 0.12.1 (Marvin N. Wright and Ziegler, 2017). The data and source codes used in this study are available on https:\/\/github.com\/co822ee\/PCR-GLOBWB_error-correction (Shen et al., 2020). The model code of PCR-GLOBWB can be obtained through https:\/\/github.com\/UU-Hydro\/PCR-GLOBWB_model (Sutanudjaja et al., 2017a).\n\n\nAuthorship statement\nConception and design of study, research implementation, data analysis, manuscript writing, manuscript editing. Data acquisition, PCR-GLOBWB modelling, manuscript review & editing. Conception and design of study, manuscript review & editing. PCR-GLOBWB model development and maintenance, PCR-GLOBWB modelling, manuscript review & editing. Conception and design of study, manuscript drafting, manuscript review & editing, supervision.\n\n"},"Creator":{"0":["Yin, Binqian","Hu, Qinhong","Zhu, Yingying","Zhao, Chen","Zhou, Keren"],"1":["Ahmed, Nisar","Weibull, Wiktor Waldemar","Grana, Dario"],"2":["Liang, Jiabin","Sun, Yongyang","Lebedev, Maxim","Gurevich, Boris","Nzikou, Michel","Vialle, Stephanie","Glubokovskikh, Stanislav"],"3":["Cedou, Matthieu","Gloaguen, Erwan","Blouin, Martin","Cat\u00e9, Antoine","Paiement, Jean-Philippe","Tirdad, Shiva"],"4":["Wen, Tao","Chen, Chacha","Zheng, Guanjie","Bandstra, Joel","Brantley, Susan L."],"5":["Kadeethum, T.","O\u2019Malley, D.","Choi, Y.","Viswanathan, H.S.","Bouklas, N.","Yoon, H."],"6":["Zuo, Chen","Pan, Zhibin","Yin, Zhen","Guo, Chen"],"7":["Guan, Lingxiao","Wu, Chuanjun","Xia, Qing","Chen, Gang","Li, Ang"],"8":["Todaro, Valeria","D\u2019Oria, Marco","Tanda, Maria Giovanna","G\u00f3mez-Hern\u00e1ndez, J. Jaime"],"9":["Lu, Yongming","Wang, Xiaoyi","Lei, Tao"],"10":["Volk, Michael W.R.","Fu, Roger R.","Trubko, Raisa","Kehayias, Pauli","Glenn, David R.","Lima, Eduardo A."],"11":["Roberts, J.F.","Mwangi, R.","Mukabi, F.","Njui, J.","Nzioka, K.","Ndambiri, J.K.","Bispo, P.C.","Espirito-Santo, F.D.B.","Gou, Y.","Johnson, S.C.M.","Louis, V.","Pacheco-Pascagaza, A.M.","Rodriguez-Veiga, P.","Tansey, K.","Upton, C.","Robb, C.","Balzter, H."],"12":["Liu, Zhong-xian","Sun, Jun","Meng, Si-bo","Feng, Ting","Huang, Lei","Li, Hu"],"13":["Bello, Inuwa Mamuda","Zhang, Ke","Su, Yu","Wang, Jingyu","Aslam, Muhammad Azeem"],"14":["Ketzner, Ryan","Ravindra, Vinay","Bramble, Michael"],"15":["Koochak, Roozbeh","Sayyafzadeh, Mohammad","Nadian, Ali","Bunch, Mark","Haghighi, Manouchehr"],"16":["Seo, Dong-Jun","Shen, Haojing","Lee, Haksu"],"17":["Roncoroni, G.","Forte, E.","Bortolussi, L.","Pipan, M."],"18":["Dong, Bin","Popescu, Alex","Tribaldos, Ver\u00f3nica Rodr\u00edguez","Byna, Suren","Ajo-Franklin, Jonathan","Wu, Kesheng"],"19":["Yang, Xiaodong","Wu, Xiaoping","Yue, Mingxin"],"20":["Soranzo, Enrico","Guardiani, Carlotta","Saif, Ahsan","Wu, Wei"],"21":["Rodr\u00edguez-Montes, Mar\u00eda","Ayarzag\u00fcena, Blanca","Guijarro, Mar\u00eda"],"22":["Liu, Yanhui","Carling, Paul A.","Wang, Yuanjian","Jiang, Enhui","Atkinson, Peter M."],"23":["Yang, Chen","Maxwell, Reed M.","Valent, Richard"],"24":["Wang, Wei","Zhao, Wenbo","Chai, Bo","Du, Juan","Tang, Luosheng","Yi, Xiawei"],"25":["Yang, Tingwei","Xu, Ya","Cao, Danping","Nan, Fangzhou","Du, Nanqiao","Hou, Zhiyu"],"26":["Mamagiannou, Elisavet","Pitenis, Eleftherios","Natsiopoulos, Dimitrios A.","Vergos, Georgios S.","Tziavos, Ilias N."],"27":["Li, Xiao","Zhang, Shengkai","Geng, Tong","Li, JiaXing","Zhu, BenXin","Liu, Laixing","Xiao, Feng"],"28":["Fendrich, Arthur Nicolaus","Neto, Elias Salom\u00e3o Helou","Moreira, Lucas Esperancini Moreira e","Neto, Durval Dourado"],"29":["Alodia, Gabriella","Green, Chris M.","McCaig, Andrew M."],"30":["Trinchero, Paolo","Nardi, Albert","Silva, Orlando","Bruch, Paula"],"31":["Samala, Rahul","Chaudhuri, Abhijit"],"32":["Ge, Xinmin","Zhang, Renxia","Liu, Jianyu","Fan, Yiren","Zhao, Jier","Li, Chaoliu","Hu, Falong"],"33":["Wang, Yaojun","Qiu, Qian","Lan, Zhiqiang","Chen, Keyu","Zhou, Jun","Gao, Peng","Zhang, Wei"],"34":["Khojastehmehr, Mohammad","Bazargan, Mohammad"],"35":["Lesueur, Martin","Rattez, Hadrien","Colom\u00e9s, Oriol"],"36":["Cunjin, Xue","Zhang, Tianyu","Xu, Yangfeng","Su, Fenzhen"],"37":["Zhang, Haoyu","Qiu, Xuelin","Huang, Haibo","Zhao, Minghui","Wang, Qiang"],"38":["Gallo, Leandro C.","Sapienza, Facundo","Domeier, Mathew"],"39":["Huang, Xu","Luo, Chuyao","Ye, Yunming","Li, Xutao","Zhang, Bowen"],"40":["Fendrock, Michaela","Chen, Christine Y.","Olson, Kristian J.","Lowenstein, Tim K.","McGee, David"],"41":["Salinas, A.","Rodr\u00edguez-Camacho, J.","Port\u00ed, J.","Carri\u00f3n, M.C.","Fornieles-Callej\u00f3n, J.","Toledo-Redondo, S."],"42":["Yu, Shuyan","Deng, Hao","Liu, Zhankun","Chen, Jin","Gu, Xiaotang","Li, Jiaxin","Xiao, Keyan","Mao, Xiancheng"],"43":["Vianna, F.D.","Farenzena, B.A.","Pinho, M.S.","Silvestrini, J.H."],"44":["Jur\u00e1nek, Roman","V\u00fdravsk\u00fd, Jakub","Kol\u00e1\u0159, Martin","Motl, David","Zem\u010d\u00edk, Pavel"],"45":["Newman, D.R.","Cockburn, J.M.H.","Dr\u01cegu\u0163, L.","Lindsay, J.B."],"46":["Zhang, Xiang","Liu, Shuming","Lei, Zhen","Qu, Juntong"],"47":["Abbaszadeh, Maliheh","Soltani-Mohammadi, Saeed","Ahmed, Ali Najah"],"48":["Zhang, Ting","Liu, Qingyang","Wang, Xianwu","Ji, Xin","Du, Yi"],"49":["Gholami Vijouyeh, Ali","Kadkhodaie, Ali","Sedghi, Mohammad Hassanpour","Gholami Vijouyeh, Hamed"],"50":["Singh, Jaspreet","Pradhan, Sarada Prasad","Singh, Mahendra","Yuan, Bingxiang"],"51":["Nespoli, Massimo","Belardinelli, Maria Elina","Cal\u00f2, Marco","Tramelli, Anna","Bonafede, Maurizio"],"52":["Ning, Yanrui","Kazemi, Hossein","Tahmasebi, Pejman"],"53":["Yuan, Yijun","Li, Yingcai","Li, Guorui","Liu, Sheng"],"54":["Davydzenka, Tsimur","Tahmasebi, Pejman","Carroll, Mark"],"55":["Dawuda, Ismael","Srinivasan, Sanjay"],"56":["Tschannen, Valentin","Ghanim, Ammar","Ettrich, Norman"],"57":["Guo, Hengliang","Xu, Bowen","Yang, Hong","Li, Bingyang","Yue, Yuanyuan","Zhao, Shan"],"58":["Herrero, Mar\u00eda J.","P\u00e9rez-Fortes, A. Patricia","Escavy, Jos\u00e9 I.","Insua-Ar\u00e9valo, Juan M.","De la Horra, Ra\u00fal","L\u00f3pez-Acevedo, Francisco","Trigos, Laura"],"59":["Wu, Li-Guang","Li, Yang","Jollands, Michael C.","Vermeesch, Pieter","Li, Xian-Hua"],"60":["Pizarro, Alonso","Ettmer, Bernd","Link, Oscar"],"61":["Kol\u00e1\u0159, Petr","Petru\u017e\u00e1lek, Mat\u011bj"],"62":["Clare, Mariana C.A.","Kramer, Stephan C.","Cotter, Colin J.","Piggott, Matthew D."],"63":["Bajracharya, Prashanta","Jain, Shaleen"],"64":["Lu, Renchao","Nagel, Thomas","Poonoosamy, Jenna","Naumov, Dmitri","Fischer, Thomas","Montoya, Vanessa","Kolditz, Olaf","Shao, Haibing"],"65":["Peacock, Jared","Kappler, Karl","Heagy, Lindsey","Ronan, Timothy","Kelbert, Anna","Frassetto, Andrew"],"66":["de Jong, Kor","Panja, Debabrata","Karssenberg, Derek","van Kreveld, Marc"],"67":["Duman, Huseyin","Sanli, D. Ugur"],"68":["Di Franco, Sabina","Salzano, Roberto","Boldrini, Enrico","Salvatori, Rosamaria"],"69":["Abdellatif, Alhasan","Elsheikh, Ahmed H.","Graham, Gavin","Busby, Daniel","Berthet, Philippe"],"70":["Baraboshkin, Evgeny E.","Demidov, Andrey E.","Orlov, Denis M.","Koroteev, Dmitry A."],"71":["Zhang, Chunjie","Zuo, Renguang","Xiong, Yihui","Zhao, Xinfu","Zhao, Kuidong"],"72":["Mariethoz, Gregoire"],"73":["Rivillas-Ospina, German","Casas, Diego","Maza-Chamorro, Mauro Antonio","Bol\u00edvar, Marianella","Ruiz, Gabriel","Guerrero, Roberto","Horrillo-Caraballo, Jos\u00e9 M.","Guerrero, Milton","D\u00edaz, Karina","Rio, Roberto del","Campos, Erick"],"74":["Li, Rongyan","Xu, Zeyu","Su, Cheng","Yang, Rong"],"75":["Misaghian, Niloo","Agnaou, Mehrez","Sadeghi, Mohammad Amin","Fathiannasab, Hamed","Hadji, Isma","Roberts, Edward","Gostick, Jeff"],"76":["Scharf, T.","Kirkland, C.L.","Daggitt, M.L.","Barham, M.","Puzyrev, V."],"77":["Hasenh\u00fcndl, Martin","Blanckaert, Koen"],"78":["Ma, Xiaogang"],"79":["Du, Wenying","Gong, Yue","Chen, NengCheng"],"80":["Deng, Hao","Zheng, Yang","Chen, Jin","Yu, Shuyan","Xiao, Keyan","Mao, Xiancheng"],"81":["Ko, Jihoon","Lee, Kyuhan","Hwang, Hyunjin","Oh, Seok-Geun","Son, Seok-Woo","Shin, Kijung"],"82":["Yang, Na","Zhang, Zhenkai","Yang, Jianhua","Hong, Zenglin"],"83":["Nikitin, Nikolay O.","Revin, Ilia","Hvatov, Alexander","Vychuzhanin, Pavel","Kalyuzhnaya, Anna V."],"84":["Anshori, Raden Muhammad","Samodra, Guruh","Mardiatno, Djati","Sartohadi, Junun"],"85":["Lim, Changbin","Hsu, John R.C.","Lee, Jung Lyul"],"86":["Keller, Chaim","Hall, John K."],"87":["Gon\u00e7alves, \u00cdtalo Gomes","Guadagnin, Felipe","Cordova, Diogo Peixoto"],"88":["Chang, A.","Gross, L.","H\u00f6rning, S."],"89":["Daigavane, Ameya","Wagstaff, Kiri L.","Doran, Gary","Cochrane, Corey J.","Jackman, Caitriona M.","Rymer, Abigail"],"90":["Zhou, Guiyun","Song, Lihui","Liu, Yi"],"91":["Lauzon, Dany","Marcotte, Denis"],"92":["Wang, Deyi","Zhang, Chengkun","Han, Min"],"93":["Oriani, Fabio","Treble, Pauline C.","Baker, Andy","Mariethoz, Gregoire"],"94":["Farhadi, Hadi","Esmaeily, Ali","Najafzadeh, Mohammad"],"95":["Ogohara, Kazunori","Gichu, Ryusei"],"96":["Otzen, Mikkel","Finlay, Christopher C.","Hansen, Thomas Mejer"],"97":["Wu, Siqi","Wang, Qing","Zeng, Qihong","Zhang, Youyan","Shao, Yanlin","Deng, Fan","Liu, Yuangang","Wei, Wei"],"98":["Peredo, Oscar F.","Herrero, Jos\u00e9 R."],"99":["Castillo-Reyes, Octavio","Modesto, David","Queralt, Pilar","Marcuello, Alex","Ledo, Juanjo","Amor-Martin, Adrian","de la Puente, Josep","Garc\u00eda-Castillo, Luis Emilio"],"100":["Battaglia, Maurizio","Calahorrano-Di Patre, Antonina","Flinders, Ashton F."],"101":["Singh, Upkar","Dhipu, T.M.","Vinayachandran, P.N.","Natarajan, Vijay"],"102":["Herring, Teddi","Heagy, Lindsey J.","Pidlisecky, Adam","Cey, Edwin"],"103":["Escobar G\u00f3mez, J.D.","Torres-Verd\u00edn, C."],"104":["Yu, Nian","Li, Ruiheng","Kong, Wenxin","Gao, Lei","Wu, Xialan","Wang, Enci"],"105":["Sun, Ziheng","Sandoval, Laura","Crystal-Ornelas, Robert","Mousavi, S. Mostafa","Wang, Jinbo","Lin, Cindy","Cristea, Nicoleta","Tong, Daniel","Carande, Wendy Hawley","Ma, Xiaogang","Rao, Yuhan","Bednar, James A.","Tan, Amanda","Wang, Jianwu","Purushotham, Sanjay","Gill, Thomas E.","Chastang, Julien","Howard, Daniel","Holt, Benjamin","Gangodagamage, Chandana","Zhao, Peisheng","Rivas, Pablo","Chester, Zachary","Orduz, Javier","John, Aji"],"106":["Lim, Theodore","Wang, Kaidi"],"107":["Almobydeen, Shahed Bassam","Viqueira, Jos\u00e9 R.R.","Lama, Manuel"],"108":["Galaz, J.","Cienfuegos, R.","Echeverr\u00eda, A.","Pereira, S.","Bert\u00edn, C.","Prato, G.","Karich, J.C."],"109":["Ciardelli, Caio","Bozda\u011f, Ebru","Peter, Daniel","van der Lee, Suzan"],"110":["Varadharajan, Charuleka","Hendrix, Valerie C.","Christianson, Danielle S.","Burrus, Madison","Wong, Catherine","Hubbard, Susan S.","Agarwal, Deborah A."],"111":["Legentil, Capucine","Pellerin, Jeanne","Cupillard, Paul","Froehly, Algiane","Caumon, Guillaume"],"112":["Xia, Ding","Ge, Yunfeng","Tang, Huiming","Zhang, Bocheng","Shen, Peiwu"],"113":["Graff, Mario","Moctezuma, Daniela","Miranda-Jim\u00e9nez, Sabino","Tellez, Eric S."],"114":["Kappler, K.N.","Schneider, D.D.","MacLean, L.S.","Bleier, T.E.","Lemon, J.J."],"115":["Zhou, Guiyun","Song, Lihui","Liu, Yi"],"116":["Guedes, Victor Jos\u00e9 Cavalcanti Bezerra","Maciel, Susanne Taina Ramalho","Rocha, Marcelo Peres"],"117":["Goldfarb, Eric J.","Ikeda, Ken","Ketcham, Richard A.","Prodanovi\u0107, Ma\u0161a","Tisato, Nicola"],"118":["Cicconeto, Fernando","Vieira, Lucas Valadares","Abel, Mara","Alvarenga, Renata dos Santos","Carbonera, Joel Luis","Garcia, Luan Fonseca"],"119":["Shen, Youchen","Ruijsch, Jessica","Lu, Meng","Sutanudjaja, Edwin H.","Karssenberg, Derek"]},"Date":{"0":"2022-11-30","1":"2022-11-30","2":"2022-11-30","3":"2022-10-31","4":"2022-10-31","5":"2022-10-31","6":"2022-10-31","7":"2022-10-31","8":"2022-10-31","9":"2022-10-31","10":"2022-10-31","11":"2022-10-31","12":"2022-10-31","13":"2022-10-31","14":"2022-10-31","15":"2022-09-30","16":"2022-09-30","17":"2022-09-30","18":"2022-09-30","19":"2022-09-30","20":"2022-09-30","21":"2022-09-30","22":"2022-09-30","23":"2022-09-30","24":"2022-09-30","25":"2022-09-30","26":"2022-09-30","27":"2022-09-30","28":"2022-09-30","29":"2022-09-30","30":"2022-09-30","31":"2022-09-30","32":"2022-09-30","33":"2022-09-30","34":"2022-09-30","35":"2022-09-30","36":"2022-09-30","37":"2022-09-30","38":"2022-09-30","39":"2022-09-30","40":"2022-09-30","41":"2022-08-31","42":"2022-08-31","43":"2022-08-31","44":"2022-08-31","45":"2022-08-31","46":"2022-08-31","47":"2022-08-31","48":"2022-08-31","49":"2022-08-31","50":"2022-07-31","51":"2022-07-31","52":"2022-07-31","53":"2022-07-31","54":"2022-07-31","55":"2022-07-31","56":"2022-07-31","57":"2022-07-31","58":"2022-06-30","59":"2022-06-30","60":"2022-06-30","61":"2022-06-30","62":"2022-06-30","63":"2022-06-30","64":"2022-06-30","65":"2022-05-31","66":"2022-05-31","67":"2022-05-31","68":"2022-05-31","69":"2022-05-31","70":"2022-05-31","71":"2022-05-31","72":"2022-05-31","73":"2022-05-31","74":"2022-05-31","75":"2022-05-31","76":"2022-05-31","77":"2022-05-31","78":"2022-04-30","79":"2022-04-30","80":"2022-04-30","81":"2022-04-30","82":"2022-04-30","83":"2022-04-30","84":"2022-04-30","85":"2022-04-30","86":"2022-04-30","87":"2022-04-30","88":"2022-04-30","89":"2022-04-30","90":"2022-04-30","91":"2022-03-31","92":"2022-03-31","93":"2022-03-31","94":"2022-03-31","95":"2022-03-31","96":"2022-03-31","97":"2022-03-31","98":"2022-03-31","99":"2022-03-31","100":"2022-03-31","101":"2022-02-28","102":"2022-02-28","103":"2022-02-28","104":"2022-02-28","105":"2022-02-28","106":"2022-02-28","107":"2022-02-28","108":"2022-02-28","109":"2022-02-28","110":"2022-02-28","111":"2022-02-28","112":"2022-02-28","113":"2022-02-28","114":"2022-02-28","115":"2022-02-28","116":"2022-02-28","117":"2022-02-28","118":"2022-02-28","119":"2022-02-28"},"Keywords":{"0":["Image segmentation","Neural network","Shale","Loss function","Ensemble learning","Imbalanced dataset"],"1":["AVO inversion","Gradient descent","Adjoint-state method","Objective function","L-BFGS optimization algorithm"],"2":["Micro-tomographic images","Multi-mineral segmentation","Convolutional neural network"],"3":["Geology","Automatic cartography","Segmentation","Data augmentation","Deep learning","Airborne magnetic","3D modeling","Supervised","Semi-supervised"],"4":["Soil reaction front","Neural network","Hybrid neural network","Physics-based model"],"5":["Non-intrusive","Data-driven","Reduced order modeling","Generative adversarial networks","Finite element","Poroelasticity"],"6":["Multiple-point statistics","K nearest neighbor","Geostatistical modeling","3D digital rock"],"7":["Viewshed analysis","Digital elevation model","R3 algorithm","XDraw algorithm","Reference plane algorithm"],"8":["Stochastic inverse modeling","Uncertainty characterization","Covariance localization and inflation","Python"],"9":["Elastic reverse time migration","Anisotropy","Dip-angle gathers","Optical flow"],"10":["Magnetic microscopy","Quantum diamond microscope","MATLAB toolbox","Rock magnetism"],"11":["Earth observation","Machine learning","Change detection","Forest monitoring"],"12":["Coupled FEM-IBIEM","Equivalent linearization method","Seismic response","3-D sedimentary basin","Site effect"],"13":["Segmentation","Dense convolution","Multiscale","Neural network"],"14":["Computational geometry","Point-in-polygon","Spherical polygons","Binary search tree"],"15":["Generative Adversarial Networks","Mode collapse","Geospatial variability","Stochastic modeling","Dimensionality reduction","Non-Gaussian geostatistics"],"16":["Data assimilation","Extremes","Conditional bias","Information content"],"17":["Horizon extraction","Deep Learning","Neural Network","Reflection seismic","GPR"],"18":["Distributed Acoustic Sensing","DAS","Data compression","Lossless","Real-time","Post-hoc","Parallel","HDF5","ZSTD","TurboPFOR","ZigZag"],"19":["Large-loop TEM method","Small-loop TEM method","Finite element method","Topography","3-D modeling"],"20":["Deep Q-Network","Factor of safety","Method of slices","Non-circular failure","Reinforcement Learning","Slip surface","Slope stability"],"21":["Stratosphere","Polar night jet","Region growing","Machine learning","Climate change","Decision trees"],"22":["Multichannel network","Remote sensing","Complex network analysis","River network topology","Graph theory"],"23":["Water age","Particle tracking","Multi-GPU with MPI","Domain decomposition","Load balancing"],"24":["High-steep slope","Nap-of-the-object photogrammetry","Discontinuity interpretation","Stability evaluation","Rock cavity rate"],"25":["Dispersion curve","Deep learning","Residual block","Surface wave"],"26":["GOCE mission","Gravity gradients","Reference frame transformation","FIR","IIR","Wavelet filtering","MATLAB GUI"],"27":["Satellite altimetry","Elevation changes","CryoSat-2","Crossover","Ice shelf"],"28":["Disaggregation","Inverse problem","Spatially-explicit","Geoprocessing","Flexible regression"],"29":["Geomorphometry","Multibeam bathymetry","Abyssal hills","Oceanic core complex","Slow-spreading ridge"],"30":["Electrochemical migration","Anion exclusion","Multicomponent transport","PFLOTRAN"],"31":["Gas hydrate dissociation","THMC modeling","Deformation","Finite volume method","Finite element method","Coupled processes","Effective medium model"],"32":["Low field NMR relaxation","Clay-rich shale","Numerical simulation","Off-resonance frequency","Pulse sequence parameters","Magnetic field inhomogeneity"],"33":["Microseismic events classification","Dual-channel convolutional neural network","Wavelet packet decomposition"],"34":["Reactive transport modeling","Sandstone acidizing","Geochemistry","Computational fluid dynamics"],"35":["Flow simulations","Finite element method","Permeability upscaling","\u03bcCT scan","Unfitted boundary method"],"36":["Argo trajectory","Trajectory semantics","Hierarchical representation","Graph-based model","Ocean currents"],"37":["Wide-angle seismic data","Converted shear wave","Graphical user interface","Automatic modeling","Conversion efficiency","Ocean bottom seismometer"],"38":["Paleomagnetism","Euler pole","Apparent polar wander path","Optimization"],"39":["Weather forecast","Radar images prediction","Deep learning","Artificial neural networks"],"40":["Machine learning","Computer vision","Carbonates","Paleoclimate"],"41":["Schumann resonances","Data processing","ELF station"],"42":["Geochemical anomalies","Tensor dictionary learning","Sparse representation","Au deposits"],"43":["Gravity currents","Lobes and clefts structures","Computer vision methods","Feature point tracking"],"44":["Segmentation","Deep learning","EDS spectra","Automated mineralogy"],"45":["Terrain analysis","Multiscale analysis","Scale selection","Scale space"],"46":["Basalt","Sandstone","CO2 transfer","Pore-scale modeling","Mathematical morphology"],"47":["Support vector machine","Classification","Particle swarm optimization algorithm","Grid search","Model selection","Particle swarm optimization"],"48":["Pore structure","Deep learning","Training image","Multiple-point connectivity","Representative elementary volume"],"49":["Fuzzy logic","Neuro-fuzzy","Neural network","Genetic algorithm","Simulated annealing","Committee machine"],"50":["Jointed rock mass","Block shape","Discrete fracture network","PFC","3DEC","Python","GUI"],"51":["Numerical model","Single forces","TPE inclusions","Loads","Mindlin"],"52":["Machine learning","Time series forecasting","Oil production prediction","ARIMA","LSTM"],"53":["Attenuation","Coherent noise","Nonlinear filtering","Seismic data processing"],"54":["Data augmentation","Image classification","Machine learning","Remote sensing. introduction"],"55":["Point bar","Shale drape","Lateral accretion","Inclined heterolithic stratification","Curvilinear geometry","CO2 sequestration"],"56":["Well-tie","Deep learning","Bayesian optimization","Automation"],"57":["Parallel computing","Time weighted dynamic time warping algorithm","Time series analysis","Remote sensing image","GPU","CUDA"],"58":["UAV","3D models","Photogrammetry","Semi-automated rock-mass characterization"],"59":["Diffusion modeling","Timescale","Ti in quartz","Ca in olivine","Li in zircon"],"60":["Bridge scour","Erosion","Deposition","Bridge piers","Floods","Software"],"61":["Acoustic emission","Event detection and localization","Recurrent neural network","Automatic seismic event processing"],"62":["Adjoint methods","Uncertainty","Sediment transport","Morphology","Tsunami inversion","Model calibration"],"63":["Width function","Hypsometric curve","Hydrologic response","Hydrologic similarity","Hierarchical clustering","Divergence measures"],"64":["Reactive transport modeling","Operator splitting approach","Integration-point collocation scheme"],"65":["Magnetotellurics","Time series","HDF5","Open-source software"],"66":["Modelling framework","Flow accumulation","High-performance computing","Asynchronous many-tasks","HPX","LUE"],"67":["Campaign GPS","Episodic GPS","Noise analysis","Variance components estimation","Trend uncertainty"],"68":["Snow","Field spectroscopy","Metadata","Reflectance","Data model","Interoperability"],"69":["Generative Adversarial Networks (GANs)","Stochastic fields","Multipoint geostatistics"],"70":["Core box image","Segmentation","Convolutional neural networks","Geology","Template-like augmentation","Core column extraction","Machine vision"],"71":["Geologically-constrained deep learning","Adversarial autoencoder","Fractal","Geochemical exploration"],"72":[],"73":["WAVEWATCH III","Wave and wind climate","Data access","Visualization"],"74":["Fission track identification","Deep learning","Convolutional neural network","Semantic segmentation"],"75":["Pore network modeling","Diffusive conductance","Deep learning model","Convolutional neural network"],"76":["Particle segmentation","Grain separation","Boundary detection","Shape measurement","Zircon provenance analysis"],"77":["Bathymetry","Rivers","Subaquatic canyons","Channels","Matlab","Morphology"],"78":["Knowledge graph","Open data","Machine learning","Artificial intelligence","Data science"],"79":["Machine learning","Explanatory factors","Urban waterlogging susceptibility","Weakly labeled support vector machine","Particle swarm optimization"],"80":["Mineral prospectivity mapping","Convolutional neural networks","3D geological models","Gold deposits","Structure-controlled deposits"],"81":["Precipitation nowcasting","Precipitation estimation","Deep learning","Pre-training","Class imbalance"],"82":["Mineral prospectivity prediction","Data augmentation","Continuous buffer distance","Convolutional neural networks"],"83":["Machine learning","CRM","Hybrid model","Oil production forecasting","Seismic analysis","CNN","Composite AI"],"84":["Landslides","Inventory","Mobile","Participatory mapping","Volunteered geographic information"],"85":["Parabolic model","Equilibrium beach concept","Wave orthogonality hypothesis","Predominant wave direction","MATLAB-Based GUI software"],"86":["Atmospheric refraction","Ray tracing","Digital terrain model","Inversion layers","Sunrise"],"87":["Machine learning","Gaussian process","Variational inference","Kriging"],"88":["Full waveform inversion","Stochastic inverse modeling","Random Mixing","Geostatistics"],"89":["Unsupervised anomaly detection","Magnetic field boundary","Time-series analysis","Cassini","Plasma spectrometer","Europa Clipper"],"90":[],"91":["Variogram-based inversion","Calibration method","Stochastic simulation","Conditioning to indirect data","Comparative study"],"92":["Deep neural network","Remote sensing scene classification","CNN","Attention"],"93":["Laminae count","Stalagmite","Tree rings","Geochronology","Dynamic time warping"],"94":["Elimination and choice expressing reality","Flood","Support vector machine","Band selection","Sentinel-2"],"95":["Mars","Dust storm","Deep learning","Segmentation","Remote sensing"],"96":["Spherical sequential simulation","Linear inverse problems","Spherical geometry","Geomagnetism","Geophysical methods","Earth observation"],"97":["Cavity automatic recognition","Convolutional neural network","Digital outcrop","Deep learning"],"98":["Geostatistics","Anisotropy","Parallel computing","Algorithms"],"99":["Magnetotelluric method","Geophysical electromagnetics","Numerical solutions","High-order edge finite element","High-performance computing"],"100":["Open-source","Time-lapse gravity","Temporal gravity","Gravity monitoring","MATLAB","Relative gravity","Cotopaxi"],"101":["Bay of Bengal","High salinity core","Tracking","Skeleton","Surface front","Visualization"],"102":["Electrical resistivity tomography","Parametric inversion","Hybrid inversion"],"103":["Perturbation theory","Sensitivity function","Inverse modeling","Diffusion","Anisotropy","Heterogeneity","Intrinsic permeability","Parameter estimation"],"104":["Magnetotelluric forward modeling","Unstructured grids","Hybrid grids","Electrical anisotropy","Mesh refinement"],"105":["Geosphere","Hydrology","Atmosphere","Artificial intelligence\/machine learning","Big data","Cyberinfrastructure"],"106":["Emulation modeling","Surrogate modeling","ParFlow.CLM","Machine learning"],"107":["Geospatial linked data","Scientific linked data","Array linked data","Raster linked data","GeoSPARQL","Spatial query processing"],"108":["Tsunami","Simulation library","Javascript","GPU","Web","Visualization"],"109":["Computational seismology","Full-waveform inversion","Numerical wave propagation","Seismic tomography","Spectral-element method","Spherical harmonics","Visualization"],"110":["Data integration","Multiscale diverse data","Synthesis","Environmental data"],"111":["Triangle meshing","Gas\u2013water contact","Geomodeling","Uncertainties","Inverse problem"],"112":["Rock joints","Shear damage zones","SDZM toolbox"],"113":["Twitter exploratory analysis","Mobility patterns","Open-source Python library"],"114":[],"115":["Parallel computing","Digital elevation model","Flow direction","Flat terrain"],"116":["Python","Open-source","Seismic refraction","Time-terms inversion","Tomographic inversion"],"117":["Computed tomography","Digital rock physics","Numerical modeling","Density","Porosity","Ultrasonic velocity"],"118":["Ontology","Deep-marine depositional system","Turbidite","Artificial intelligence"],"119":["Inverse modelling","Streamflow forecasting","PCR-GLOBWB","Machine learning","Hydrograph"]},"SCOPUS_ID":{"0":"85136623433","1":"85136589915","2":"85136497058","3":"85136656108","4":"85136539415","5":"85136489547","6":"85136083855","7":"85135916444","8":"85135905994","9":"85135518561","10":"85135418964","11":"85135399467","12":"85135386468","13":"85135370144","14":"85134890160","15":"85134698478","16":"85134691394","17":"85134675074","18":"85134332799","19":"85134193984","20":"85133870395","21":"85133852903","22":"85133764317","23":"85133758736","24":"85133641824","25":"85133621031","26":"85133478204","27":"85132927900","28":"85132871890","29":"85132785052","30":"85132781906","31":"85132754088","32":"85132538434","33":"85132528847","34":"85132509958","35":"85132423441","36":"85132354145","37":"85132331735","38":"85132226220","39":"85131967085","40":"85131803319","41":"85131462280","42":"85131413387","43":"85131131229","44":"85131055092","45":"85130558623","46":"85130545326","47":"85130520033","48":"85130511468","49":"85130251562","50":"85130343316","51":"85130102357","52":"85129995340","53":"85129994861","54":"85129936052","55":"85129764590","56":"85129708936","57":"85129485464","58":"85129450142","59":"85129278470","60":"85129243370","61":"85129059539","62":"85128221483","63":"85127311598","64":"85127027477","65":"85127366407","66":"85126874478","67":"85126749488","68":"85126556710","69":"85126553935","70":"85126544984","71":"85126533086","72":"85126293638","73":"85126290347","74":"85126106988","75":"85125867077","76":"85125809418","77":"85125671046","78":"85126826141","79":"85125504009","80":"85125013134","81":"85125012901","82":"85125008046","83":"85124981588","84":"85124968424","85":"85124967949","86":"85124627770","87":"85124320202","88":"85124263636","89":"85124195967","90":"85124159038","91":"85123854818","92":"85123852914","93":"85123720152","94":"85123699911","95":"85123598026","96":"85123319602","97":"85123193801","98":"85123029257","99":"85122959705","100":"85122805585","101":"85124097226","102":"85124081640","103":"85122644387","104":"85122622554","105":"85122572031","106":"85122506606","107":"85122291537","108":"85122030400","109":"85121928198","110":"85121834632","111":"85121362591","112":"85121255870","113":"85121205039","114":"85121097533","115":"85120920451","116":"85120857832","117":"85120831991","118":"85120744901","119":"85120716963"}}